{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "9SStKf4G0V5H"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "import torchaudio\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from skimage.util import img_as_ubyte\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import io\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_XtxbKLZq5KX",
    "outputId": "37023990-2b40-4e1f-e63e-1dba18c3acdb"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CYxHegIM0Z4i",
    "outputId": "f68f6fab-6d0f-4c1f-9d0a-527e166123f6"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive/\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "h9DATQwS0ivD"
   },
   "outputs": [],
   "source": [
    "class MimiiDataset(Dataset):\n",
    "    def __init__(self,audio_dir, n_fft = 1024, win_length = 1024,\n",
    "                 hop_length = 512,power = 2,n_mels = 128,pad_mode = 'reflect',\n",
    "                 sr = 16000,center = True,norm = None):\n",
    "      \n",
    "        super(MimiiDataset, self).__init__()\n",
    "        self.audio_dir = audio_dir\n",
    "        self.n_mels = n_mels\n",
    "        self.n_fft = n_fft\n",
    "        self.win_length = win_length\n",
    "        self.hop_length = hop_length\n",
    "        self.power = power\n",
    "        self.pad_mode = pad_mode\n",
    "        self.sr = sr\n",
    "        self.center = center\n",
    "        self.norm = norm\n",
    "\n",
    "    def get_files(self):\n",
    "       return self.train_files, self.test_files\n",
    "    \n",
    "    def get_data(self,device, id):\n",
    "        \n",
    "        self.train_files, self.train_labels = self._train_file_list(device, id)\n",
    "        self.test_files, self.test_labels = self._test_file_list(device, id)\n",
    "        \n",
    "        self.train_data = self.get_audios(self.train_files)\n",
    "        self.test_data = self.get_audios(self.test_files)\n",
    "        \n",
    "        return self.train_data, self.test_data, self.train_labels, self.test_labels\n",
    "    \n",
    "    def _train_file_list(self, device, id):\n",
    "        query = os.path.abspath(\n",
    "            f\"{self.audio_dir}/{device}/train/normal_id_0{id}*.wav\"\n",
    "        )\n",
    "        train_normal_files = sorted(glob.glob(query))\n",
    "        train_normal_labels = np.zeros(len(train_normal_files))\n",
    "        \n",
    "        query = os.path.abspath(\n",
    "                f\"{self.audio_dir}/{device}/train/anomaly_id_0{id}*.wav\"\n",
    "            )\n",
    "        train_anomaly_files = sorted(glob.glob(query))\n",
    "        train_anomaly_labels = np.ones(len(train_anomaly_files))\n",
    "        \n",
    "        train_file_list = np.concatenate((train_normal_files, train_anomaly_files), axis=0)\n",
    "        train_labels = np.concatenate((train_normal_labels, train_anomaly_labels), axis=0)\n",
    "        \n",
    "        return train_file_list, train_labels\n",
    "    \n",
    "    def _test_file_list(self, device, id):     \n",
    "        query = os.path.abspath(\n",
    "            f\"{self.audio_dir}/{device}/test/normal_id_0{id}*.wav\"\n",
    "            )\n",
    "        test_normal_files = sorted(glob.glob(query))\n",
    "        test_normal_labels = np.zeros(len(test_normal_files))\n",
    "        \n",
    "        query = os.path.abspath(\n",
    "            f\"{self.audio_dir}/{device}/test/anomaly_id_0{id}*.wav\"\n",
    "            )\n",
    "        test_anomaly_files = sorted(glob.glob(query))\n",
    "        test_anomaly_labels = np.ones(len(test_anomaly_files))\n",
    "        \n",
    "        test_file_list = np.concatenate((test_normal_files, \n",
    "                                          test_anomaly_files), axis=0)\n",
    "        test_labels = np.concatenate((test_normal_labels,\n",
    "                                      test_anomaly_labels), axis=0)\n",
    "          \n",
    "        return test_file_list, test_labels\n",
    "\n",
    "    def normalize(self,tensor):\n",
    "        tensor_minusmean = tensor - tensor.mean()\n",
    "        return tensor_minusmean/np.absolute(tensor_minusmean).max()\n",
    "\n",
    "    def make0min(self,tensornd):\n",
    "        tensor = tensornd.numpy()\n",
    "        res = np.where(tensor == 0, 1E-19 , tensor)\n",
    "        return torch.from_numpy(res)\n",
    "\n",
    "    def spectrogrameToImage(self,specgram):\n",
    "        # specgram = torchaudio.transforms.MelSpectrogram(n_fft=1024, win_length=1024, \n",
    "        #                                                 hop_length=512, power=2, \n",
    "        #                                                 normalized=True, n_mels=128)(waveform )\n",
    "        specgram= self.make0min(specgram)\n",
    "        specgram = specgram.log2()[0,:,:].numpy()\n",
    "        \n",
    "        tr2image = transforms.Compose([transforms.ToPILImage()])\n",
    "\n",
    "        specgram= self.normalize(specgram)\n",
    "        # specgram = img_as_ubyte(specgram)\n",
    "        specgramImage = tr2image(specgram)\n",
    "        return specgramImage\n",
    "\n",
    "    def get_logmelspectrogram(self, waveform):\n",
    "        melspec = librosa.feature.melspectrogram(\n",
    "          n_fft=self.n_fft, win_length=self.win_length, hop_length=self.hop_length,\n",
    "          power=self.power,n_mels=self.n_mels,pad_mode=self.pad_mode,sr=self.sr,\n",
    "          center=self.center,norm=self.norm,htk=True,\n",
    "          y = waveform.numpy()\n",
    "        )\n",
    "\n",
    "        logmelspec = librosa.power_to_db(melspec)\n",
    "\n",
    "        return logmelspec\n",
    "\n",
    "    def get_melspectrogram(self,waveform):\n",
    "        melspec = librosa.feature.melspectrogram(\n",
    "            n_fft=self.n_fft, win_length=self.win_length, hop_length=self.hop_length,\n",
    "            power=self.power,n_mels=self.n_mels,pad_mode=self.pad_mode,sr=self.sr,\n",
    "            center=self.center,norm=self.norm,htk=True,\n",
    "            y = waveform.numpy()\n",
    "        )\n",
    "\n",
    "        return melspec\n",
    "    \n",
    "    def get_mfcc(self,waveform):\n",
    "        mfcc = librosa.feature.mfcc(    \n",
    "            n_fft=self.n_fft, win_length=self.win_length, \n",
    "            hop_length=self.hop_length,pad_mode=self.pad_mode,sr=self.sr,\n",
    "            center=self.center,norm=self.norm,n_mfcc=40,\n",
    "            y = waveform.numpy()\n",
    "        )\n",
    "\n",
    "        return mfcc\n",
    "\n",
    "    def get_chroma_stft(self,waveform):\n",
    "        stft = librosa.feature.chroma_stft(\n",
    "            n_fft=self.n_fft, win_length=self.win_length, \n",
    "            hop_length=self.hop_length,pad_mode=self.pad_mode,sr=self.sr,\n",
    "            center=self.center,norm=self.norm,n_chroma=12,\n",
    "            y=waveform.numpy()\n",
    "        )\n",
    "\n",
    "        return stft\n",
    "\n",
    "    def get_spectral_contrast(self,waveform):\n",
    "        spec_contrast = librosa.feature.spectral_contrast(    \n",
    "            n_fft=self.n_fft, win_length=self.win_length,center=self.center,\n",
    "            hop_length=self.hop_length,pad_mode=self.pad_mode,sr=self.sr,\n",
    "            y = waveform.numpy()\n",
    "        )\n",
    "\n",
    "        return spec_contrast\n",
    "    \n",
    "    def get_tonnetz(self,waveform):\n",
    "        harmonic = librosa.effects.harmonic(waveform.numpy())\n",
    "        tonnetz = librosa.feature.tonnetz(y=harmonic,sr=self.sr)\n",
    "\n",
    "        return tonnetz\n",
    "\n",
    "    def get_audios(self, file_list):\n",
    "        data = []\n",
    "        for i in range(len(file_list)):\n",
    "          y, sr = torchaudio.load(file_list[i])  \n",
    "          data.append(y)\n",
    "\n",
    "        return data\n",
    "    def _derive_data(self, file_list):\n",
    "        train_data = []\n",
    "        test_data = []\n",
    "        train_mode = True\n",
    "        for file_list in [self.train_files, self.test_files]:\n",
    "          tr2tensor = transforms.Compose([transforms.PILToTensor()])\n",
    "          data = []\n",
    "          for j in range(len(file_list)):\n",
    "            y, sr = torchaudio.load(file_list[j])  \n",
    "            spec = self.get_melspectrogram(y)\n",
    "            spec = self.spectrogrameToImage(spec)\n",
    "            spec = spec.convert('RGB')\n",
    "            vectors = tr2tensor(spec)\n",
    "            if train_mode:     \n",
    "              train_data.append(vectors)\n",
    "            else:\n",
    "              test_data.append(vectors)\n",
    "            \n",
    "          train_mode = False\n",
    "                \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "S96soeIc0o13"
   },
   "outputs": [],
   "source": [
    "dataset = MimiiDataset('/content/drive/MyDrive/mimii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Gn2zdn92doi1"
   },
   "outputs": [],
   "source": [
    "_, _, y_train, y_test = dataset.get_data('fan', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "SgjpeWy_RV1C"
   },
   "outputs": [],
   "source": [
    "train_mixed_f = torch.load('/content/drive/MyDrive/mixed_features/train_mf_fan0.pt')\n",
    "test_mixed_f = torch.load('/content/drive/MyDrive/mixed_features/test_mf_fan0.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "jWMPVGu1qiEq"
   },
   "outputs": [],
   "source": [
    "train_data = DataLoader(train_mixed_f, batch_size=32, shuffle = True)\n",
    "test_data = DataLoader(test_mixed_f, batch_size = 32, shuffle= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "vNTBTRe6qnBq"
   },
   "outputs": [],
   "source": [
    "class UNet_FC(nn.Module):\n",
    "\n",
    "  def __init__(self, in_features):\n",
    "    super().__init__()\n",
    "    \n",
    "    self.bn = nn.BatchNorm1d(128)\n",
    "    self.relu = nn.ReLU()\n",
    "\n",
    "    self.fc0 = nn.Linear(in_features=in_features,out_features=in_features)\n",
    "\n",
    "    # encoder\n",
    "    self.fc1 = nn.Linear(in_features=in_features, out_features=128)\n",
    "    self.fc2 = nn.Linear(in_features=128,out_features=128)\n",
    "    self.fc3 = nn.Linear(in_features=128, out_features=128)\n",
    "    self.fc4 = nn.Linear(in_features=128, out_features=128)\n",
    "    self.fc5 = nn.Linear(in_features=128, out_features=8)\n",
    "\n",
    "    # decoder\n",
    "    self.fc6 = nn.Linear(in_features=8, out_features=128)\n",
    "    self.fc7 = nn.Linear(in_features=128*2, out_features=128)\n",
    "    self.fc8 = nn.Linear(in_features=128*2, out_features=128)\n",
    "    self.fc9 = nn.Linear(in_features=128*2, out_features=128)\n",
    "\n",
    "    self.out = nn.Linear(in_features=128*2, out_features=in_features)\n",
    "\n",
    "  def encoder(self, x):\n",
    "    input = self.fc0(x)\n",
    "\n",
    "    x1 = self.relu(self.bn(self.fc1(input)))\n",
    "    x2 = self.relu(self.bn(self.fc2(x1)))\n",
    "    x3 = self.relu(self.bn(self.fc3(x2)))\n",
    "    x4 = self.relu(self.bn(self.fc4(x3)))\n",
    "    x5 = self.relu(self.fc5(x4))\n",
    "\n",
    "    return [x5, x4, x3, x2, x1]\n",
    "\n",
    "  def decoder(self, x):\n",
    "    x6 = self.relu(self.fc6(x[0]))\n",
    "    con1 = torch.cat((x6,x[1]), 1) \n",
    "    x7 = self.relu(self.bn(self.fc7(con1)))\n",
    "    con2 = torch.cat((x7,x[2]), 1)\n",
    "    x8 = self.relu(self.bn(self.fc8(con2)))\n",
    "    con3 = torch.cat((x8,x[3]), 1)\n",
    "    x9 = self.relu(self.bn(self.fc9(con3)))\n",
    "    con4 = torch.cat((x9,x[4]), 1)\n",
    "\n",
    "    x10 = self.out(con4)\n",
    "\n",
    "    return x10\n",
    "  \n",
    "  def forward(self, x):\n",
    "    # encoded = self.encoder(x)\n",
    "\n",
    "    # decoded = self.decoder(encoded)\n",
    "    input = self.fc0(x)\n",
    "\n",
    "    x1 = self.relu(self.bn(self.fc1(input)))\n",
    "    x2 = self.relu(self.bn(self.fc2(x1)))\n",
    "    x3 = self.relu(self.bn(self.fc3(x2)))\n",
    "    x4 = self.relu(self.bn(self.fc4(x3)))\n",
    "    x5 = self.relu(self.fc5(x4))\n",
    "\n",
    "    xy = [x5, x4, x3, x2, x1]\n",
    "\n",
    "    x6 = self.relu(self.fc6(xy[0]))\n",
    "    con1 = torch.cat((x6,xy[1]), 1) \n",
    "    x7 = self.relu(self.bn(self.fc7(con1)))\n",
    "    con2 = torch.cat((x7,xy[2]), 1)\n",
    "    x8 = self.relu(self.bn(self.fc8(con2)))\n",
    "    con3 = torch.cat((x8,xy[3]), 1)\n",
    "    x9 = self.relu(self.bn(self.fc9(con3)))\n",
    "    con4 = torch.cat((x9,xy[4]), 1)\n",
    "\n",
    "    x10 = self.out(con4)\n",
    "\n",
    "    # return decoded\n",
    "    return x10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "ZfgcBtQ3qn5l"
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, data_tr, data_val, scheduler = None,\n",
    "          epochs = 3000, device = 'cpu'):\n",
    "    # X_val, Y_val = next(iter(data_val))\n",
    "    losses = []\n",
    "    prev_avg_loss = 100000\n",
    "    for epoch in range(epochs):\n",
    "        train_avg_loss = 0\n",
    "        test_avg_loss = 0\n",
    "        # model.train()  # train mode\n",
    "        for batch in data_tr:\n",
    "          # data to device\n",
    "          batch = batch.to(device)\n",
    "          # set parameter gradients to zero\n",
    "          optimizer.zero_grad()\n",
    "          # forward\n",
    "          # print(Y_batch.shape)\n",
    "          predictions = model(batch)\n",
    "          loss = criterion(predictions, batch)\n",
    "          loss.backward() # backward-pass\n",
    "          optimizer.step()  # update weights\n",
    "          # calculate loss to show the user\n",
    "          if scheduler:\n",
    "            scheduler.step(loss)\n",
    "          train_avg_loss += loss / len(data_tr)\n",
    "\n",
    "        # model.eval()\n",
    "        for batch in data_val:\n",
    "          with torch.no_grad():\n",
    "            preds = model(batch.to(device)).cpu()\n",
    "            loss = criterion(preds,batch)\n",
    "            test_avg_loss += loss / len(data_val)\n",
    "                    \n",
    "        losses.append(train_avg_loss.item())\n",
    "        # if (epoch+1)%50 == 0:\n",
    "        print(\"{}/{} train_loss: {} test_loss:{}\".format(epoch+1, epochs, train_avg_loss, test_avg_loss))\n",
    "        # if test_avg_loss < 70:\n",
    "        #   break\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "ptkVTF55quOL"
   },
   "outputs": [],
   "source": [
    "unet = UNet_FC(in_features=193).to(device)\n",
    "optimizer = Adam(params = unet.parameters(), lr = 10e-3)\n",
    "# optimizer = Adam(params = unet.parameters())\n",
    "criterion = nn.MSELoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, \n",
    "                                                       min_lr=10e-4, mode = 'min',\n",
    "                                                       patience = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QkfmYl9oXhcB",
    "outputId": "052b3e3b-5613-46a4-92a7-3a01d18ae8b9"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1/3000 train_loss: 336703.5625 test_loss:337784.9375\n",
      "2/3000 train_loss: 326802.65625 test_loss:323809.375\n",
      "3/3000 train_loss: 308806.25 test_loss:300902.25\n",
      "4/3000 train_loss: 282431.4375 test_loss:270277.8125\n",
      "5/3000 train_loss: 249983.546875 test_loss:238207.96875\n",
      "6/3000 train_loss: 214499.90625 test_loss:197481.15625\n",
      "7/3000 train_loss: 175055.28125 test_loss:157924.390625\n",
      "8/3000 train_loss: 136537.609375 test_loss:119742.1171875\n",
      "9/3000 train_loss: 100522.6953125 test_loss:86333.609375\n",
      "10/3000 train_loss: 69309.515625 test_loss:58320.1484375\n",
      "11/3000 train_loss: 45683.64453125 test_loss:37589.5\n",
      "12/3000 train_loss: 27855.443359375 test_loss:22158.869140625\n",
      "13/3000 train_loss: 16227.04296875 test_loss:12611.32421875\n",
      "14/3000 train_loss: 8735.1728515625 test_loss:7439.0966796875\n",
      "15/3000 train_loss: 5008.49365234375 test_loss:4063.345703125\n",
      "16/3000 train_loss: 2826.610595703125 test_loss:2583.845703125\n",
      "17/3000 train_loss: 1998.7498779296875 test_loss:2110.165283203125\n",
      "18/3000 train_loss: 1534.1708984375 test_loss:1527.17919921875\n",
      "19/3000 train_loss: 1247.467041015625 test_loss:1286.1988525390625\n",
      "20/3000 train_loss: 1171.5048828125 test_loss:1234.970458984375\n",
      "21/3000 train_loss: 922.5572509765625 test_loss:1193.4794921875\n",
      "22/3000 train_loss: 865.8171997070312 test_loss:899.4818115234375\n",
      "23/3000 train_loss: 783.729248046875 test_loss:817.6021118164062\n",
      "24/3000 train_loss: 724.657958984375 test_loss:786.4686279296875\n",
      "25/3000 train_loss: 723.0145263671875 test_loss:776.5476684570312\n",
      "26/3000 train_loss: 743.5527954101562 test_loss:806.414794921875\n",
      "27/3000 train_loss: 715.2820434570312 test_loss:752.9959716796875\n",
      "28/3000 train_loss: 660.83837890625 test_loss:734.6246948242188\n",
      "29/3000 train_loss: 625.6235961914062 test_loss:753.2727661132812\n",
      "30/3000 train_loss: 604.8427124023438 test_loss:674.697509765625\n",
      "31/3000 train_loss: 619.7593383789062 test_loss:755.1116943359375\n",
      "32/3000 train_loss: 594.2799072265625 test_loss:616.0210571289062\n",
      "33/3000 train_loss: 552.5181274414062 test_loss:582.6295776367188\n",
      "34/3000 train_loss: 561.0841674804688 test_loss:596.214111328125\n",
      "35/3000 train_loss: 496.32965087890625 test_loss:524.2230224609375\n",
      "36/3000 train_loss: 466.2120361328125 test_loss:535.9889526367188\n",
      "37/3000 train_loss: 455.10064697265625 test_loss:575.6375122070312\n",
      "38/3000 train_loss: 426.333740234375 test_loss:503.3905029296875\n",
      "39/3000 train_loss: 452.5159912109375 test_loss:503.344970703125\n",
      "40/3000 train_loss: 456.0337829589844 test_loss:484.1544189453125\n",
      "41/3000 train_loss: 434.5270690917969 test_loss:446.2615966796875\n",
      "42/3000 train_loss: 411.189697265625 test_loss:462.6857604980469\n",
      "43/3000 train_loss: 391.1849060058594 test_loss:488.7906494140625\n",
      "44/3000 train_loss: 383.3044738769531 test_loss:463.282470703125\n",
      "45/3000 train_loss: 376.7896728515625 test_loss:521.205078125\n",
      "46/3000 train_loss: 384.5702209472656 test_loss:454.134765625\n",
      "47/3000 train_loss: 408.3973388671875 test_loss:508.2908935546875\n",
      "48/3000 train_loss: 527.6759643554688 test_loss:544.3051147460938\n",
      "49/3000 train_loss: 422.7535400390625 test_loss:491.7795715332031\n",
      "50/3000 train_loss: 405.8748474121094 test_loss:458.9945373535156\n",
      "51/3000 train_loss: 380.92303466796875 test_loss:516.5106201171875\n",
      "52/3000 train_loss: 352.1592102050781 test_loss:406.27032470703125\n",
      "53/3000 train_loss: 339.6598205566406 test_loss:424.9139709472656\n",
      "54/3000 train_loss: 342.94830322265625 test_loss:419.3446350097656\n",
      "55/3000 train_loss: 361.8276672363281 test_loss:407.7413330078125\n",
      "56/3000 train_loss: 325.7696228027344 test_loss:434.8066711425781\n",
      "57/3000 train_loss: 340.8364562988281 test_loss:415.6227111816406\n",
      "58/3000 train_loss: 297.19256591796875 test_loss:396.2881164550781\n",
      "59/3000 train_loss: 315.6772155761719 test_loss:385.1727294921875\n",
      "60/3000 train_loss: 307.0144958496094 test_loss:390.0574035644531\n",
      "61/3000 train_loss: 280.1385803222656 test_loss:447.7838439941406\n",
      "62/3000 train_loss: 305.281494140625 test_loss:404.50262451171875\n",
      "63/3000 train_loss: 301.9189453125 test_loss:369.3858947753906\n",
      "64/3000 train_loss: 308.4671630859375 test_loss:391.825927734375\n",
      "65/3000 train_loss: 293.8013000488281 test_loss:395.06109619140625\n",
      "66/3000 train_loss: 269.366455078125 test_loss:397.0242004394531\n",
      "67/3000 train_loss: 292.90142822265625 test_loss:360.735595703125\n",
      "68/3000 train_loss: 283.71575927734375 test_loss:361.5195617675781\n",
      "69/3000 train_loss: 292.2701416015625 test_loss:428.11639404296875\n",
      "70/3000 train_loss: 278.9296875 test_loss:355.3377380371094\n",
      "71/3000 train_loss: 264.7022399902344 test_loss:349.4350891113281\n",
      "72/3000 train_loss: 250.87796020507812 test_loss:365.2583923339844\n",
      "73/3000 train_loss: 277.2047424316406 test_loss:348.892333984375\n",
      "74/3000 train_loss: 298.3182678222656 test_loss:347.3539123535156\n",
      "75/3000 train_loss: 258.6208801269531 test_loss:366.35113525390625\n",
      "76/3000 train_loss: 227.48199462890625 test_loss:360.8511657714844\n",
      "77/3000 train_loss: 254.53736877441406 test_loss:374.1009216308594\n",
      "78/3000 train_loss: 252.21060180664062 test_loss:341.1320495605469\n",
      "79/3000 train_loss: 244.42401123046875 test_loss:378.35723876953125\n",
      "80/3000 train_loss: 231.12240600585938 test_loss:333.1077575683594\n",
      "81/3000 train_loss: 268.88720703125 test_loss:332.7197570800781\n",
      "82/3000 train_loss: 248.85447692871094 test_loss:319.11456298828125\n",
      "83/3000 train_loss: 240.6455078125 test_loss:351.7708435058594\n",
      "84/3000 train_loss: 288.26776123046875 test_loss:361.4485778808594\n",
      "85/3000 train_loss: 253.2911376953125 test_loss:312.3051452636719\n",
      "86/3000 train_loss: 208.8268585205078 test_loss:314.14337158203125\n",
      "87/3000 train_loss: 241.3708038330078 test_loss:314.369384765625\n",
      "88/3000 train_loss: 212.54507446289062 test_loss:321.2083740234375\n",
      "89/3000 train_loss: 239.94744873046875 test_loss:319.61114501953125\n",
      "90/3000 train_loss: 226.15487670898438 test_loss:306.8282775878906\n",
      "91/3000 train_loss: 200.8168487548828 test_loss:313.0082092285156\n",
      "92/3000 train_loss: 238.71542358398438 test_loss:331.571533203125\n",
      "93/3000 train_loss: 254.7288055419922 test_loss:318.3181457519531\n",
      "94/3000 train_loss: 235.76327514648438 test_loss:315.00408935546875\n",
      "95/3000 train_loss: 224.86708068847656 test_loss:317.7829895019531\n",
      "96/3000 train_loss: 207.43170166015625 test_loss:326.56072998046875\n",
      "97/3000 train_loss: 198.20919799804688 test_loss:317.78460693359375\n",
      "98/3000 train_loss: 214.35885620117188 test_loss:292.26043701171875\n",
      "99/3000 train_loss: 239.0379638671875 test_loss:329.5063781738281\n",
      "100/3000 train_loss: 213.6739501953125 test_loss:314.4664001464844\n",
      "101/3000 train_loss: 277.5003356933594 test_loss:306.5195617675781\n",
      "102/3000 train_loss: 232.81861877441406 test_loss:286.53387451171875\n",
      "103/3000 train_loss: 214.67295837402344 test_loss:283.64654541015625\n",
      "104/3000 train_loss: 218.91500854492188 test_loss:410.2555236816406\n",
      "105/3000 train_loss: 561.1478271484375 test_loss:531.7471923828125\n",
      "106/3000 train_loss: 320.9345397949219 test_loss:395.4718017578125\n",
      "107/3000 train_loss: 301.4364318847656 test_loss:330.28192138671875\n",
      "108/3000 train_loss: 227.08248901367188 test_loss:288.67315673828125\n",
      "109/3000 train_loss: 229.14479064941406 test_loss:352.8554992675781\n",
      "110/3000 train_loss: 255.9830322265625 test_loss:288.8763427734375\n",
      "111/3000 train_loss: 216.19317626953125 test_loss:307.1079406738281\n",
      "112/3000 train_loss: 211.55226135253906 test_loss:308.4627685546875\n",
      "113/3000 train_loss: 189.2601318359375 test_loss:286.76287841796875\n",
      "114/3000 train_loss: 194.7664794921875 test_loss:294.0428466796875\n",
      "115/3000 train_loss: 212.13626098632812 test_loss:301.8636474609375\n",
      "116/3000 train_loss: 205.55618286132812 test_loss:327.352294921875\n",
      "117/3000 train_loss: 204.87521362304688 test_loss:336.92559814453125\n",
      "118/3000 train_loss: 226.4522705078125 test_loss:295.7225646972656\n",
      "119/3000 train_loss: 196.42738342285156 test_loss:282.3387145996094\n",
      "120/3000 train_loss: 191.84710693359375 test_loss:291.1360168457031\n",
      "121/3000 train_loss: 192.15155029296875 test_loss:294.211669921875\n",
      "122/3000 train_loss: 165.04933166503906 test_loss:258.1798400878906\n",
      "123/3000 train_loss: 173.60873413085938 test_loss:255.27684020996094\n",
      "124/3000 train_loss: 170.69857788085938 test_loss:263.32061767578125\n",
      "125/3000 train_loss: 181.13131713867188 test_loss:296.1242980957031\n",
      "126/3000 train_loss: 181.52886962890625 test_loss:235.4738311767578\n",
      "127/3000 train_loss: 184.78085327148438 test_loss:260.0143127441406\n",
      "128/3000 train_loss: 182.3361053466797 test_loss:268.61517333984375\n",
      "129/3000 train_loss: 186.7160186767578 test_loss:266.3759765625\n",
      "130/3000 train_loss: 187.262939453125 test_loss:253.10073852539062\n",
      "131/3000 train_loss: 193.62640380859375 test_loss:276.48602294921875\n",
      "132/3000 train_loss: 183.03538513183594 test_loss:313.62890625\n",
      "133/3000 train_loss: 205.73159790039062 test_loss:266.5799255371094\n",
      "134/3000 train_loss: 171.3199462890625 test_loss:269.6808166503906\n",
      "135/3000 train_loss: 181.62600708007812 test_loss:259.9070739746094\n",
      "136/3000 train_loss: 171.1033935546875 test_loss:271.0896911621094\n",
      "137/3000 train_loss: 163.46005249023438 test_loss:264.4308776855469\n",
      "138/3000 train_loss: 158.77902221679688 test_loss:250.1485595703125\n",
      "139/3000 train_loss: 184.3631591796875 test_loss:267.1309814453125\n",
      "140/3000 train_loss: 162.61549377441406 test_loss:241.47767639160156\n",
      "141/3000 train_loss: 161.54205322265625 test_loss:255.94073486328125\n",
      "142/3000 train_loss: 200.75453186035156 test_loss:277.3023986816406\n",
      "143/3000 train_loss: 181.9410858154297 test_loss:274.1162109375\n",
      "144/3000 train_loss: 168.27804565429688 test_loss:242.63047790527344\n",
      "145/3000 train_loss: 177.46849060058594 test_loss:264.3065185546875\n",
      "146/3000 train_loss: 294.4140625 test_loss:471.5312805175781\n",
      "147/3000 train_loss: 290.0994567871094 test_loss:477.6995849609375\n",
      "148/3000 train_loss: 275.0031433105469 test_loss:340.6021728515625\n",
      "149/3000 train_loss: 205.47332763671875 test_loss:349.1575927734375\n",
      "150/3000 train_loss: 200.23284912109375 test_loss:271.5385437011719\n",
      "151/3000 train_loss: 189.4469757080078 test_loss:308.0466613769531\n",
      "152/3000 train_loss: 179.7043914794922 test_loss:284.9761962890625\n",
      "153/3000 train_loss: 172.3909149169922 test_loss:242.4527130126953\n",
      "154/3000 train_loss: 158.08909606933594 test_loss:226.34364318847656\n",
      "155/3000 train_loss: 187.07212829589844 test_loss:233.2703857421875\n",
      "156/3000 train_loss: 162.42922973632812 test_loss:265.0191955566406\n",
      "157/3000 train_loss: 166.38218688964844 test_loss:242.3677978515625\n",
      "158/3000 train_loss: 160.8179168701172 test_loss:251.51019287109375\n",
      "159/3000 train_loss: 142.16075134277344 test_loss:239.75460815429688\n",
      "160/3000 train_loss: 155.3581085205078 test_loss:259.2791748046875\n",
      "161/3000 train_loss: 200.7666778564453 test_loss:279.7922668457031\n",
      "162/3000 train_loss: 166.0951690673828 test_loss:259.2535400390625\n",
      "163/3000 train_loss: 124.16676330566406 test_loss:215.41567993164062\n",
      "164/3000 train_loss: 147.38626098632812 test_loss:274.3481140136719\n",
      "165/3000 train_loss: 141.61561584472656 test_loss:232.40505981445312\n",
      "166/3000 train_loss: 156.35125732421875 test_loss:260.7157287597656\n",
      "167/3000 train_loss: 141.53125 test_loss:229.58782958984375\n",
      "168/3000 train_loss: 145.5731658935547 test_loss:231.36724853515625\n",
      "169/3000 train_loss: 147.39695739746094 test_loss:233.50115966796875\n",
      "170/3000 train_loss: 158.56048583984375 test_loss:236.82662963867188\n",
      "171/3000 train_loss: 152.6410369873047 test_loss:224.45040893554688\n",
      "172/3000 train_loss: 154.31077575683594 test_loss:226.25526428222656\n",
      "173/3000 train_loss: 146.7898406982422 test_loss:218.21673583984375\n",
      "174/3000 train_loss: 139.71954345703125 test_loss:252.0400390625\n",
      "175/3000 train_loss: 152.51373291015625 test_loss:301.1835021972656\n",
      "176/3000 train_loss: 182.2179412841797 test_loss:220.5775604248047\n",
      "177/3000 train_loss: 142.52716064453125 test_loss:234.60353088378906\n",
      "178/3000 train_loss: 164.102294921875 test_loss:205.41468811035156\n",
      "179/3000 train_loss: 137.06166076660156 test_loss:236.88363647460938\n",
      "180/3000 train_loss: 144.7079315185547 test_loss:219.30352783203125\n",
      "181/3000 train_loss: 181.9881134033203 test_loss:203.52386474609375\n",
      "182/3000 train_loss: 135.2123260498047 test_loss:209.2799835205078\n",
      "183/3000 train_loss: 140.3392791748047 test_loss:217.6186065673828\n",
      "184/3000 train_loss: 139.24761962890625 test_loss:236.33518981933594\n",
      "185/3000 train_loss: 139.90260314941406 test_loss:212.8032989501953\n",
      "186/3000 train_loss: 157.001708984375 test_loss:220.81024169921875\n",
      "187/3000 train_loss: 132.41859436035156 test_loss:200.5354461669922\n",
      "188/3000 train_loss: 128.457275390625 test_loss:228.3362274169922\n",
      "189/3000 train_loss: 124.37824249267578 test_loss:230.32508850097656\n",
      "190/3000 train_loss: 137.41746520996094 test_loss:211.48300170898438\n",
      "191/3000 train_loss: 148.2062225341797 test_loss:190.32916259765625\n",
      "192/3000 train_loss: 158.03125 test_loss:197.3588409423828\n",
      "193/3000 train_loss: 142.49253845214844 test_loss:228.6990203857422\n",
      "194/3000 train_loss: 142.16502380371094 test_loss:197.46995544433594\n",
      "195/3000 train_loss: 130.78936767578125 test_loss:188.0292205810547\n",
      "196/3000 train_loss: 124.53154754638672 test_loss:219.90618896484375\n",
      "197/3000 train_loss: 130.04078674316406 test_loss:217.2200164794922\n",
      "198/3000 train_loss: 139.035888671875 test_loss:182.11715698242188\n",
      "199/3000 train_loss: 146.23587036132812 test_loss:226.2467498779297\n",
      "200/3000 train_loss: 124.3497314453125 test_loss:206.11062622070312\n",
      "201/3000 train_loss: 129.945556640625 test_loss:186.59486389160156\n",
      "202/3000 train_loss: 116.63969421386719 test_loss:195.50930786132812\n",
      "203/3000 train_loss: 142.2609100341797 test_loss:189.70651245117188\n",
      "204/3000 train_loss: 132.58380126953125 test_loss:228.2578887939453\n",
      "205/3000 train_loss: 119.90272521972656 test_loss:183.75637817382812\n",
      "206/3000 train_loss: 117.40633392333984 test_loss:223.31654357910156\n",
      "207/3000 train_loss: 141.96542358398438 test_loss:188.15492248535156\n",
      "208/3000 train_loss: 118.81820678710938 test_loss:213.734375\n",
      "209/3000 train_loss: 119.06414031982422 test_loss:184.77044677734375\n",
      "210/3000 train_loss: 143.22056579589844 test_loss:214.13693237304688\n",
      "211/3000 train_loss: 125.76007843017578 test_loss:194.9988555908203\n",
      "212/3000 train_loss: 130.89767456054688 test_loss:214.146728515625\n",
      "213/3000 train_loss: 124.65185546875 test_loss:198.53260803222656\n",
      "214/3000 train_loss: 130.96963500976562 test_loss:202.58802795410156\n",
      "215/3000 train_loss: 130.2165985107422 test_loss:196.666748046875\n",
      "216/3000 train_loss: 139.0471649169922 test_loss:217.0327606201172\n",
      "217/3000 train_loss: 115.27333068847656 test_loss:187.52577209472656\n",
      "218/3000 train_loss: 128.0819854736328 test_loss:175.419677734375\n",
      "219/3000 train_loss: 132.034912109375 test_loss:198.4054412841797\n",
      "220/3000 train_loss: 116.53007507324219 test_loss:194.2669219970703\n",
      "221/3000 train_loss: 120.355224609375 test_loss:184.11892700195312\n",
      "222/3000 train_loss: 128.181396484375 test_loss:199.45559692382812\n",
      "223/3000 train_loss: 122.83905029296875 test_loss:212.082275390625\n",
      "224/3000 train_loss: 135.28897094726562 test_loss:192.6020965576172\n",
      "225/3000 train_loss: 121.34630584716797 test_loss:210.8098602294922\n",
      "226/3000 train_loss: 112.91236877441406 test_loss:170.3026885986328\n",
      "227/3000 train_loss: 120.981689453125 test_loss:214.66339111328125\n",
      "228/3000 train_loss: 130.71881103515625 test_loss:207.09127807617188\n",
      "229/3000 train_loss: 125.21215057373047 test_loss:215.91845703125\n",
      "230/3000 train_loss: 116.13118743896484 test_loss:196.72738647460938\n",
      "231/3000 train_loss: 88.93975830078125 test_loss:207.96412658691406\n",
      "232/3000 train_loss: 114.57990264892578 test_loss:218.3116455078125\n",
      "233/3000 train_loss: 116.22185516357422 test_loss:203.2711639404297\n",
      "234/3000 train_loss: 125.5860595703125 test_loss:194.34593200683594\n",
      "235/3000 train_loss: 131.79942321777344 test_loss:181.8548583984375\n",
      "236/3000 train_loss: 119.76742553710938 test_loss:183.15032958984375\n",
      "237/3000 train_loss: 121.56389617919922 test_loss:178.4918975830078\n",
      "238/3000 train_loss: 94.56553649902344 test_loss:211.98402404785156\n",
      "239/3000 train_loss: 127.14915466308594 test_loss:189.88446044921875\n",
      "240/3000 train_loss: 103.47696685791016 test_loss:175.8128662109375\n",
      "241/3000 train_loss: 100.74877166748047 test_loss:203.14691162109375\n",
      "242/3000 train_loss: 89.66960144042969 test_loss:173.52734375\n",
      "243/3000 train_loss: 101.15071105957031 test_loss:206.74400329589844\n",
      "244/3000 train_loss: 97.87443542480469 test_loss:159.99351501464844\n",
      "245/3000 train_loss: 103.1621322631836 test_loss:183.50064086914062\n",
      "246/3000 train_loss: 111.5442886352539 test_loss:181.02149963378906\n",
      "247/3000 train_loss: 109.41838836669922 test_loss:147.14244079589844\n",
      "248/3000 train_loss: 131.75704956054688 test_loss:160.36322021484375\n",
      "249/3000 train_loss: 122.482421875 test_loss:197.5613555908203\n",
      "250/3000 train_loss: 119.88939666748047 test_loss:198.28817749023438\n",
      "251/3000 train_loss: 110.7464370727539 test_loss:179.97293090820312\n",
      "252/3000 train_loss: 102.92117309570312 test_loss:163.69183349609375\n",
      "253/3000 train_loss: 92.07565307617188 test_loss:164.59698486328125\n",
      "254/3000 train_loss: 109.86034393310547 test_loss:190.5311279296875\n",
      "255/3000 train_loss: 118.43624114990234 test_loss:179.3604736328125\n",
      "256/3000 train_loss: 108.24678039550781 test_loss:155.22116088867188\n",
      "257/3000 train_loss: 96.45458221435547 test_loss:189.376220703125\n",
      "258/3000 train_loss: 99.87584686279297 test_loss:168.286376953125\n",
      "259/3000 train_loss: 98.96969604492188 test_loss:164.46437072753906\n",
      "260/3000 train_loss: 96.57501983642578 test_loss:157.15525817871094\n",
      "261/3000 train_loss: 104.01937866210938 test_loss:167.7881317138672\n",
      "262/3000 train_loss: 104.22420501708984 test_loss:158.7477569580078\n",
      "263/3000 train_loss: 93.51715850830078 test_loss:184.20217895507812\n",
      "264/3000 train_loss: 100.57673645019531 test_loss:156.23660278320312\n",
      "265/3000 train_loss: 99.16946411132812 test_loss:156.07687377929688\n",
      "266/3000 train_loss: 111.43399047851562 test_loss:191.8932342529297\n",
      "267/3000 train_loss: 105.02120971679688 test_loss:154.4015350341797\n",
      "268/3000 train_loss: 107.74462890625 test_loss:157.8009490966797\n",
      "269/3000 train_loss: 109.5057601928711 test_loss:178.41200256347656\n",
      "270/3000 train_loss: 115.15611267089844 test_loss:182.9525604248047\n",
      "271/3000 train_loss: 106.15771484375 test_loss:168.76800537109375\n",
      "272/3000 train_loss: 100.26834869384766 test_loss:167.0581817626953\n",
      "273/3000 train_loss: 98.24009704589844 test_loss:181.40191650390625\n",
      "274/3000 train_loss: 112.78390502929688 test_loss:194.90586853027344\n",
      "275/3000 train_loss: 133.78390502929688 test_loss:148.92361450195312\n",
      "276/3000 train_loss: 103.96246337890625 test_loss:165.0062255859375\n",
      "277/3000 train_loss: 87.39002990722656 test_loss:164.9787139892578\n",
      "278/3000 train_loss: 91.92705535888672 test_loss:167.3295135498047\n",
      "279/3000 train_loss: 103.2795639038086 test_loss:160.47039794921875\n",
      "280/3000 train_loss: 99.63951873779297 test_loss:165.33358764648438\n",
      "281/3000 train_loss: 105.98870849609375 test_loss:190.99082946777344\n",
      "282/3000 train_loss: 92.17326354980469 test_loss:169.93898010253906\n",
      "283/3000 train_loss: 105.2762451171875 test_loss:179.10191345214844\n",
      "284/3000 train_loss: 95.49737548828125 test_loss:156.95895385742188\n",
      "285/3000 train_loss: 85.06649017333984 test_loss:148.64991760253906\n",
      "286/3000 train_loss: 101.58198547363281 test_loss:190.2561798095703\n",
      "287/3000 train_loss: 87.26407623291016 test_loss:145.52574157714844\n",
      "288/3000 train_loss: 101.28751373291016 test_loss:174.313720703125\n",
      "289/3000 train_loss: 93.58795166015625 test_loss:158.96592712402344\n",
      "290/3000 train_loss: 95.52972412109375 test_loss:178.40403747558594\n",
      "291/3000 train_loss: 98.0551528930664 test_loss:149.32058715820312\n",
      "292/3000 train_loss: 98.52323150634766 test_loss:172.6967010498047\n",
      "293/3000 train_loss: 121.22311401367188 test_loss:162.02418518066406\n",
      "294/3000 train_loss: 94.57239532470703 test_loss:158.5456085205078\n",
      "295/3000 train_loss: 97.88381958007812 test_loss:155.91592407226562\n",
      "296/3000 train_loss: 93.88465881347656 test_loss:176.48736572265625\n",
      "297/3000 train_loss: 101.42767333984375 test_loss:176.4559783935547\n",
      "298/3000 train_loss: 100.70813751220703 test_loss:162.78640747070312\n",
      "299/3000 train_loss: 106.86128234863281 test_loss:184.9611053466797\n",
      "300/3000 train_loss: 102.5437240600586 test_loss:135.37489318847656\n",
      "301/3000 train_loss: 100.63282012939453 test_loss:163.239501953125\n",
      "302/3000 train_loss: 96.59121704101562 test_loss:175.2854766845703\n",
      "303/3000 train_loss: 88.94314575195312 test_loss:149.81143188476562\n",
      "304/3000 train_loss: 95.20919036865234 test_loss:172.74371337890625\n",
      "305/3000 train_loss: 93.7366714477539 test_loss:166.4232635498047\n",
      "306/3000 train_loss: 85.55713653564453 test_loss:134.51942443847656\n",
      "307/3000 train_loss: 77.24170684814453 test_loss:163.79014587402344\n",
      "308/3000 train_loss: 97.42605590820312 test_loss:152.94329833984375\n",
      "309/3000 train_loss: 93.92144775390625 test_loss:169.70684814453125\n",
      "310/3000 train_loss: 87.63739013671875 test_loss:155.86392211914062\n",
      "311/3000 train_loss: 82.8661117553711 test_loss:155.8516082763672\n",
      "312/3000 train_loss: 105.33291625976562 test_loss:137.37411499023438\n",
      "313/3000 train_loss: 84.9886245727539 test_loss:172.813720703125\n",
      "314/3000 train_loss: 85.84446716308594 test_loss:140.09051513671875\n",
      "315/3000 train_loss: 106.53148651123047 test_loss:140.79824829101562\n",
      "316/3000 train_loss: 94.69296264648438 test_loss:164.31973266601562\n",
      "317/3000 train_loss: 73.66758728027344 test_loss:184.15814208984375\n",
      "318/3000 train_loss: 76.40076446533203 test_loss:128.55751037597656\n",
      "319/3000 train_loss: 105.31784057617188 test_loss:143.7495880126953\n",
      "320/3000 train_loss: 84.81310272216797 test_loss:160.50816345214844\n",
      "321/3000 train_loss: 78.712890625 test_loss:159.7388458251953\n",
      "322/3000 train_loss: 92.24794006347656 test_loss:135.74514770507812\n",
      "323/3000 train_loss: 90.61441040039062 test_loss:164.8016815185547\n",
      "324/3000 train_loss: 95.18806457519531 test_loss:177.12022399902344\n",
      "325/3000 train_loss: 81.49336242675781 test_loss:141.3111572265625\n",
      "326/3000 train_loss: 84.29302215576172 test_loss:155.1348114013672\n",
      "327/3000 train_loss: 95.04589080810547 test_loss:152.47300720214844\n",
      "328/3000 train_loss: 85.88702392578125 test_loss:164.5984649658203\n",
      "329/3000 train_loss: 105.65241241455078 test_loss:131.31187438964844\n",
      "330/3000 train_loss: 78.41883087158203 test_loss:144.4708709716797\n",
      "331/3000 train_loss: 85.94111633300781 test_loss:165.14181518554688\n",
      "332/3000 train_loss: 105.46287536621094 test_loss:153.73379516601562\n",
      "333/3000 train_loss: 84.37369537353516 test_loss:172.75323486328125\n",
      "334/3000 train_loss: 93.60489654541016 test_loss:138.3992156982422\n",
      "335/3000 train_loss: 72.41956329345703 test_loss:139.29476928710938\n",
      "336/3000 train_loss: 83.043212890625 test_loss:143.83724975585938\n",
      "337/3000 train_loss: 89.19245147705078 test_loss:132.8463897705078\n",
      "338/3000 train_loss: 79.63994598388672 test_loss:157.64846801757812\n",
      "339/3000 train_loss: 94.14845275878906 test_loss:140.5212860107422\n",
      "340/3000 train_loss: 74.09393310546875 test_loss:156.001708984375\n",
      "341/3000 train_loss: 75.2490005493164 test_loss:152.42691040039062\n",
      "342/3000 train_loss: 83.37319946289062 test_loss:134.70516967773438\n",
      "343/3000 train_loss: 94.03691864013672 test_loss:153.72735595703125\n",
      "344/3000 train_loss: 86.84097290039062 test_loss:130.86195373535156\n",
      "345/3000 train_loss: 90.590087890625 test_loss:145.01715087890625\n",
      "346/3000 train_loss: 82.91767883300781 test_loss:156.37220764160156\n",
      "347/3000 train_loss: 80.17218780517578 test_loss:131.563720703125\n",
      "348/3000 train_loss: 87.8099365234375 test_loss:173.6694793701172\n",
      "349/3000 train_loss: 88.2389907836914 test_loss:133.90757751464844\n",
      "350/3000 train_loss: 76.57809448242188 test_loss:125.15670776367188\n",
      "351/3000 train_loss: 78.12798309326172 test_loss:143.16270446777344\n",
      "352/3000 train_loss: 77.1880874633789 test_loss:179.76113891601562\n",
      "353/3000 train_loss: 82.33451843261719 test_loss:151.61465454101562\n",
      "354/3000 train_loss: 77.83219909667969 test_loss:137.13088989257812\n",
      "355/3000 train_loss: 93.01051330566406 test_loss:154.92276000976562\n",
      "356/3000 train_loss: 89.91020202636719 test_loss:151.01388549804688\n",
      "357/3000 train_loss: 79.46497344970703 test_loss:137.4488525390625\n",
      "358/3000 train_loss: 90.3283462524414 test_loss:143.40528869628906\n",
      "359/3000 train_loss: 84.98905181884766 test_loss:166.04061889648438\n",
      "360/3000 train_loss: 77.3781509399414 test_loss:147.00592041015625\n",
      "361/3000 train_loss: 81.39459228515625 test_loss:150.74081420898438\n",
      "362/3000 train_loss: 74.67880249023438 test_loss:140.60440063476562\n",
      "363/3000 train_loss: 82.43250274658203 test_loss:151.419677734375\n",
      "364/3000 train_loss: 82.69686889648438 test_loss:141.90762329101562\n",
      "365/3000 train_loss: 70.67803955078125 test_loss:155.92446899414062\n",
      "366/3000 train_loss: 82.31649017333984 test_loss:125.20549774169922\n",
      "367/3000 train_loss: 80.97977447509766 test_loss:154.53573608398438\n",
      "368/3000 train_loss: 69.63841247558594 test_loss:141.4468536376953\n",
      "369/3000 train_loss: 78.1924819946289 test_loss:146.6223907470703\n",
      "370/3000 train_loss: 77.7764892578125 test_loss:129.0361328125\n",
      "371/3000 train_loss: 74.4248046875 test_loss:134.03916931152344\n",
      "372/3000 train_loss: 73.52247619628906 test_loss:137.51177978515625\n",
      "373/3000 train_loss: 79.33036041259766 test_loss:161.55783081054688\n",
      "374/3000 train_loss: 80.17957305908203 test_loss:138.67013549804688\n",
      "375/3000 train_loss: 82.05477905273438 test_loss:127.36634063720703\n",
      "376/3000 train_loss: 79.67936706542969 test_loss:169.107177734375\n",
      "377/3000 train_loss: 69.17758178710938 test_loss:123.79884338378906\n",
      "378/3000 train_loss: 78.1645736694336 test_loss:135.24952697753906\n",
      "379/3000 train_loss: 77.95020294189453 test_loss:146.70297241210938\n",
      "380/3000 train_loss: 83.03948974609375 test_loss:135.48675537109375\n",
      "381/3000 train_loss: 79.12602996826172 test_loss:145.65768432617188\n",
      "382/3000 train_loss: 67.31716918945312 test_loss:146.974365234375\n",
      "383/3000 train_loss: 79.27091217041016 test_loss:132.85154724121094\n",
      "384/3000 train_loss: 75.35520935058594 test_loss:146.70506286621094\n",
      "385/3000 train_loss: 76.87035369873047 test_loss:128.9183349609375\n",
      "386/3000 train_loss: 78.10335540771484 test_loss:126.88626098632812\n",
      "387/3000 train_loss: 69.73762512207031 test_loss:144.20913696289062\n",
      "388/3000 train_loss: 76.69092559814453 test_loss:130.29136657714844\n",
      "389/3000 train_loss: 87.44329071044922 test_loss:162.09527587890625\n",
      "390/3000 train_loss: 87.72019958496094 test_loss:139.22511291503906\n",
      "391/3000 train_loss: 76.2193832397461 test_loss:140.7700653076172\n",
      "392/3000 train_loss: 72.67704010009766 test_loss:139.8108673095703\n",
      "393/3000 train_loss: 71.9437026977539 test_loss:136.82847595214844\n",
      "394/3000 train_loss: 72.36225128173828 test_loss:142.18191528320312\n",
      "395/3000 train_loss: 83.32998657226562 test_loss:137.67984008789062\n",
      "396/3000 train_loss: 86.121826171875 test_loss:123.33329010009766\n",
      "397/3000 train_loss: 74.89796447753906 test_loss:162.72357177734375\n",
      "398/3000 train_loss: 75.21211242675781 test_loss:137.84910583496094\n",
      "399/3000 train_loss: 68.56348419189453 test_loss:129.458984375\n",
      "400/3000 train_loss: 84.15225982666016 test_loss:151.50619506835938\n",
      "401/3000 train_loss: 85.1571273803711 test_loss:137.72860717773438\n",
      "402/3000 train_loss: 93.2869644165039 test_loss:129.00782775878906\n",
      "403/3000 train_loss: 71.34137725830078 test_loss:154.27391052246094\n",
      "404/3000 train_loss: 71.79287719726562 test_loss:118.57748413085938\n",
      "405/3000 train_loss: 85.19391632080078 test_loss:127.20832824707031\n",
      "406/3000 train_loss: 81.06059265136719 test_loss:139.49346923828125\n",
      "407/3000 train_loss: 77.43753051757812 test_loss:126.13803100585938\n",
      "408/3000 train_loss: 70.75836181640625 test_loss:141.404541015625\n",
      "409/3000 train_loss: 78.26405334472656 test_loss:138.9637908935547\n",
      "410/3000 train_loss: 69.6834487915039 test_loss:121.46443176269531\n",
      "411/3000 train_loss: 70.39678955078125 test_loss:128.18174743652344\n",
      "412/3000 train_loss: 79.37083435058594 test_loss:144.10150146484375\n",
      "413/3000 train_loss: 101.20889282226562 test_loss:143.5810089111328\n",
      "414/3000 train_loss: 72.2298583984375 test_loss:125.25907135009766\n",
      "415/3000 train_loss: 82.63902282714844 test_loss:127.93269348144531\n",
      "416/3000 train_loss: 81.45622253417969 test_loss:136.0885467529297\n",
      "417/3000 train_loss: 84.76055908203125 test_loss:129.70396423339844\n",
      "418/3000 train_loss: 82.58049774169922 test_loss:133.33328247070312\n",
      "419/3000 train_loss: 84.76629638671875 test_loss:155.41612243652344\n",
      "420/3000 train_loss: 72.2200927734375 test_loss:142.61700439453125\n",
      "421/3000 train_loss: 81.04203033447266 test_loss:155.88861083984375\n",
      "422/3000 train_loss: 64.42227172851562 test_loss:139.1920928955078\n",
      "423/3000 train_loss: 70.02378845214844 test_loss:120.3585433959961\n",
      "424/3000 train_loss: 69.4927978515625 test_loss:125.94134521484375\n",
      "425/3000 train_loss: 68.49122619628906 test_loss:131.43907165527344\n",
      "426/3000 train_loss: 75.24876403808594 test_loss:138.2479248046875\n",
      "427/3000 train_loss: 74.68370056152344 test_loss:153.97000122070312\n",
      "428/3000 train_loss: 71.30667114257812 test_loss:118.0011978149414\n",
      "429/3000 train_loss: 104.14907836914062 test_loss:133.53550720214844\n",
      "430/3000 train_loss: 73.88656616210938 test_loss:162.52212524414062\n",
      "431/3000 train_loss: 80.34687805175781 test_loss:138.17398071289062\n",
      "432/3000 train_loss: 74.63213348388672 test_loss:123.30204772949219\n",
      "433/3000 train_loss: 84.22587585449219 test_loss:113.61991119384766\n",
      "434/3000 train_loss: 75.90023040771484 test_loss:131.26397705078125\n",
      "435/3000 train_loss: 73.42094421386719 test_loss:143.5457305908203\n",
      "436/3000 train_loss: 70.20745849609375 test_loss:138.71409606933594\n",
      "437/3000 train_loss: 79.15166473388672 test_loss:131.4662628173828\n",
      "438/3000 train_loss: 72.30494689941406 test_loss:153.8259735107422\n",
      "439/3000 train_loss: 76.46920013427734 test_loss:123.08790588378906\n",
      "440/3000 train_loss: 81.36461639404297 test_loss:128.9952850341797\n",
      "441/3000 train_loss: 82.95237731933594 test_loss:137.85073852539062\n",
      "442/3000 train_loss: 73.66280364990234 test_loss:125.1426010131836\n",
      "443/3000 train_loss: 80.21885681152344 test_loss:130.98779296875\n",
      "444/3000 train_loss: 80.25054931640625 test_loss:165.365234375\n",
      "445/3000 train_loss: 80.37639617919922 test_loss:131.815673828125\n",
      "446/3000 train_loss: 83.93486022949219 test_loss:153.66737365722656\n",
      "447/3000 train_loss: 73.82646942138672 test_loss:135.78810119628906\n",
      "448/3000 train_loss: 78.90898132324219 test_loss:133.95651245117188\n",
      "449/3000 train_loss: 71.61344909667969 test_loss:125.23799896240234\n",
      "450/3000 train_loss: 78.10581970214844 test_loss:126.1714859008789\n",
      "451/3000 train_loss: 69.59068298339844 test_loss:122.9135513305664\n",
      "452/3000 train_loss: 63.73423767089844 test_loss:131.18540954589844\n",
      "453/3000 train_loss: 74.92992401123047 test_loss:115.3598861694336\n",
      "454/3000 train_loss: 71.72652435302734 test_loss:128.9463653564453\n",
      "455/3000 train_loss: 59.27616500854492 test_loss:120.06501007080078\n",
      "456/3000 train_loss: 56.800933837890625 test_loss:129.80543518066406\n",
      "457/3000 train_loss: 68.81417083740234 test_loss:124.3852310180664\n",
      "458/3000 train_loss: 68.40646362304688 test_loss:134.44869995117188\n",
      "459/3000 train_loss: 64.41032409667969 test_loss:148.19451904296875\n",
      "460/3000 train_loss: 81.77708435058594 test_loss:136.4527130126953\n",
      "461/3000 train_loss: 80.19538879394531 test_loss:139.5037384033203\n",
      "462/3000 train_loss: 70.28550720214844 test_loss:118.69198608398438\n",
      "463/3000 train_loss: 64.66197204589844 test_loss:115.32904815673828\n",
      "464/3000 train_loss: 68.92122650146484 test_loss:133.34243774414062\n",
      "465/3000 train_loss: 61.87521743774414 test_loss:146.65740966796875\n",
      "466/3000 train_loss: 73.10139465332031 test_loss:131.06507873535156\n",
      "467/3000 train_loss: 72.92980194091797 test_loss:121.18433380126953\n",
      "468/3000 train_loss: 60.890743255615234 test_loss:131.8553924560547\n",
      "469/3000 train_loss: 87.07811737060547 test_loss:161.9674072265625\n",
      "470/3000 train_loss: 65.5006103515625 test_loss:138.8595733642578\n",
      "471/3000 train_loss: 75.19308471679688 test_loss:122.03975677490234\n",
      "472/3000 train_loss: 64.86495971679688 test_loss:147.48330688476562\n",
      "473/3000 train_loss: 59.88650894165039 test_loss:131.42526245117188\n",
      "474/3000 train_loss: 63.20518493652344 test_loss:137.15682983398438\n",
      "475/3000 train_loss: 74.80970001220703 test_loss:124.2925033569336\n",
      "476/3000 train_loss: 60.642982482910156 test_loss:131.01358032226562\n",
      "477/3000 train_loss: 61.01568603515625 test_loss:124.0242919921875\n",
      "478/3000 train_loss: 83.18965148925781 test_loss:140.01634216308594\n",
      "479/3000 train_loss: 79.83853149414062 test_loss:133.7658233642578\n",
      "480/3000 train_loss: 62.210941314697266 test_loss:119.81514739990234\n",
      "481/3000 train_loss: 68.04146575927734 test_loss:134.63511657714844\n",
      "482/3000 train_loss: 67.29042053222656 test_loss:117.35749053955078\n",
      "483/3000 train_loss: 54.778221130371094 test_loss:110.27777862548828\n",
      "484/3000 train_loss: 66.57622528076172 test_loss:129.06796264648438\n",
      "485/3000 train_loss: 70.77467346191406 test_loss:138.6940155029297\n",
      "486/3000 train_loss: 64.07821655273438 test_loss:118.08167266845703\n",
      "487/3000 train_loss: 61.56235122680664 test_loss:119.32022094726562\n",
      "488/3000 train_loss: 79.515380859375 test_loss:129.87789916992188\n",
      "489/3000 train_loss: 53.80109786987305 test_loss:121.88142395019531\n",
      "490/3000 train_loss: 61.496612548828125 test_loss:128.14761352539062\n",
      "491/3000 train_loss: 65.20285034179688 test_loss:121.18729400634766\n",
      "492/3000 train_loss: 82.7101058959961 test_loss:115.3275375366211\n",
      "493/3000 train_loss: 70.19091796875 test_loss:109.81645965576172\n",
      "494/3000 train_loss: 74.31891632080078 test_loss:108.69740295410156\n",
      "495/3000 train_loss: 68.05432891845703 test_loss:118.91129302978516\n",
      "496/3000 train_loss: 74.74258422851562 test_loss:137.1991424560547\n",
      "497/3000 train_loss: 64.08526611328125 test_loss:119.77178192138672\n",
      "498/3000 train_loss: 75.66299438476562 test_loss:144.3395233154297\n",
      "499/3000 train_loss: 69.6240005493164 test_loss:128.71827697753906\n",
      "500/3000 train_loss: 68.39291381835938 test_loss:134.72528076171875\n",
      "501/3000 train_loss: 71.93336486816406 test_loss:116.69652557373047\n",
      "502/3000 train_loss: 77.64437866210938 test_loss:108.24090576171875\n",
      "503/3000 train_loss: 85.06525421142578 test_loss:113.72443389892578\n",
      "504/3000 train_loss: 80.11031341552734 test_loss:108.71946716308594\n",
      "505/3000 train_loss: 66.11576080322266 test_loss:113.38731384277344\n",
      "506/3000 train_loss: 66.84683227539062 test_loss:120.02527618408203\n",
      "507/3000 train_loss: 68.1133041381836 test_loss:143.80055236816406\n",
      "508/3000 train_loss: 71.61742401123047 test_loss:123.16476440429688\n",
      "509/3000 train_loss: 64.45789337158203 test_loss:124.13050842285156\n",
      "510/3000 train_loss: 67.03829193115234 test_loss:125.1641616821289\n",
      "511/3000 train_loss: 64.49564361572266 test_loss:120.23014068603516\n",
      "512/3000 train_loss: 79.95755767822266 test_loss:128.50669860839844\n",
      "513/3000 train_loss: 73.8516845703125 test_loss:126.51374053955078\n",
      "514/3000 train_loss: 65.74750518798828 test_loss:119.59139251708984\n",
      "515/3000 train_loss: 75.04551696777344 test_loss:112.50232696533203\n",
      "516/3000 train_loss: 73.62508392333984 test_loss:115.45016479492188\n",
      "517/3000 train_loss: 57.78795623779297 test_loss:147.6874542236328\n",
      "518/3000 train_loss: 66.02234649658203 test_loss:113.00552368164062\n",
      "519/3000 train_loss: 61.83601760864258 test_loss:113.22254943847656\n",
      "520/3000 train_loss: 59.70878601074219 test_loss:113.90296173095703\n",
      "521/3000 train_loss: 77.63229370117188 test_loss:124.87693786621094\n",
      "522/3000 train_loss: 63.71930694580078 test_loss:112.6268310546875\n",
      "523/3000 train_loss: 77.00508880615234 test_loss:125.0511474609375\n",
      "524/3000 train_loss: 64.33512878417969 test_loss:118.71742248535156\n",
      "525/3000 train_loss: 66.89637756347656 test_loss:108.40089416503906\n",
      "526/3000 train_loss: 71.57756042480469 test_loss:120.9917984008789\n",
      "527/3000 train_loss: 66.2287368774414 test_loss:115.60228729248047\n",
      "528/3000 train_loss: 80.2958984375 test_loss:126.10356140136719\n",
      "529/3000 train_loss: 57.80224609375 test_loss:121.38973999023438\n",
      "530/3000 train_loss: 65.32714080810547 test_loss:116.73603057861328\n",
      "531/3000 train_loss: 54.5820426940918 test_loss:116.55345916748047\n",
      "532/3000 train_loss: 72.76397705078125 test_loss:123.90071105957031\n",
      "533/3000 train_loss: 62.960113525390625 test_loss:115.19418334960938\n",
      "534/3000 train_loss: 63.202232360839844 test_loss:119.22013854980469\n",
      "535/3000 train_loss: 69.99762725830078 test_loss:131.9243621826172\n",
      "536/3000 train_loss: 63.347930908203125 test_loss:107.23604583740234\n",
      "537/3000 train_loss: 65.36732482910156 test_loss:117.8497085571289\n",
      "538/3000 train_loss: 62.21018981933594 test_loss:135.97154235839844\n",
      "539/3000 train_loss: 68.32633209228516 test_loss:143.4468536376953\n",
      "540/3000 train_loss: 72.83885955810547 test_loss:112.16989135742188\n",
      "541/3000 train_loss: 59.13143539428711 test_loss:102.43555450439453\n",
      "542/3000 train_loss: 57.6071891784668 test_loss:120.85047149658203\n",
      "543/3000 train_loss: 59.4200325012207 test_loss:112.2493896484375\n",
      "544/3000 train_loss: 74.64265441894531 test_loss:114.36283874511719\n",
      "545/3000 train_loss: 65.31452178955078 test_loss:109.07890319824219\n",
      "546/3000 train_loss: 59.00484085083008 test_loss:114.2366943359375\n",
      "547/3000 train_loss: 71.15959167480469 test_loss:138.89002990722656\n",
      "548/3000 train_loss: 62.305267333984375 test_loss:108.56509399414062\n",
      "549/3000 train_loss: 64.97278594970703 test_loss:112.5431900024414\n",
      "550/3000 train_loss: 62.021034240722656 test_loss:116.3499984741211\n",
      "551/3000 train_loss: 75.59497833251953 test_loss:120.43824768066406\n",
      "552/3000 train_loss: 70.69686889648438 test_loss:133.65179443359375\n",
      "553/3000 train_loss: 57.212730407714844 test_loss:117.11892700195312\n",
      "554/3000 train_loss: 69.74890899658203 test_loss:115.4106216430664\n",
      "555/3000 train_loss: 67.94515228271484 test_loss:114.25577545166016\n",
      "556/3000 train_loss: 61.79161834716797 test_loss:108.14836120605469\n",
      "557/3000 train_loss: 63.768821716308594 test_loss:128.5177459716797\n",
      "558/3000 train_loss: 55.646671295166016 test_loss:131.83482360839844\n",
      "559/3000 train_loss: 78.73636627197266 test_loss:116.03974151611328\n",
      "560/3000 train_loss: 76.61029815673828 test_loss:127.41525268554688\n",
      "561/3000 train_loss: 68.59248352050781 test_loss:128.38482666015625\n",
      "562/3000 train_loss: 57.474273681640625 test_loss:125.57463073730469\n",
      "563/3000 train_loss: 61.95058822631836 test_loss:116.68902587890625\n",
      "564/3000 train_loss: 72.45164489746094 test_loss:110.84555053710938\n",
      "565/3000 train_loss: 60.32548141479492 test_loss:110.1323471069336\n",
      "566/3000 train_loss: 66.94973754882812 test_loss:124.4641342163086\n",
      "567/3000 train_loss: 73.3859634399414 test_loss:128.81863403320312\n",
      "568/3000 train_loss: 72.08053588867188 test_loss:131.0848388671875\n",
      "569/3000 train_loss: 57.90049743652344 test_loss:124.0810546875\n",
      "570/3000 train_loss: 67.08757781982422 test_loss:109.38375854492188\n",
      "571/3000 train_loss: 60.26680374145508 test_loss:137.35511779785156\n",
      "572/3000 train_loss: 61.242637634277344 test_loss:126.29612731933594\n",
      "573/3000 train_loss: 54.318809509277344 test_loss:106.32574462890625\n",
      "574/3000 train_loss: 63.12372589111328 test_loss:141.7085723876953\n",
      "575/3000 train_loss: 57.30961227416992 test_loss:103.17958068847656\n",
      "576/3000 train_loss: 57.73445129394531 test_loss:113.28324890136719\n",
      "577/3000 train_loss: 59.86901092529297 test_loss:129.0816650390625\n",
      "578/3000 train_loss: 51.74563217163086 test_loss:107.31208038330078\n",
      "579/3000 train_loss: 60.24540710449219 test_loss:141.61253356933594\n",
      "580/3000 train_loss: 63.5869140625 test_loss:111.33663940429688\n",
      "581/3000 train_loss: 57.30684280395508 test_loss:130.53060913085938\n",
      "582/3000 train_loss: 55.64007568359375 test_loss:127.10298156738281\n",
      "583/3000 train_loss: 66.62191772460938 test_loss:116.92305755615234\n",
      "584/3000 train_loss: 55.59495162963867 test_loss:115.37373352050781\n",
      "585/3000 train_loss: 55.283138275146484 test_loss:108.83447265625\n",
      "586/3000 train_loss: 54.322898864746094 test_loss:101.02413940429688\n",
      "587/3000 train_loss: 64.57555389404297 test_loss:104.98858642578125\n",
      "588/3000 train_loss: 59.07795333862305 test_loss:123.07048034667969\n",
      "589/3000 train_loss: 57.26934814453125 test_loss:111.23242950439453\n",
      "590/3000 train_loss: 65.8592529296875 test_loss:120.29478454589844\n",
      "591/3000 train_loss: 65.28205108642578 test_loss:109.20880126953125\n",
      "592/3000 train_loss: 61.97998809814453 test_loss:128.86471557617188\n",
      "593/3000 train_loss: 60.136924743652344 test_loss:113.74872589111328\n",
      "594/3000 train_loss: 56.591712951660156 test_loss:110.43163299560547\n",
      "595/3000 train_loss: 52.79376220703125 test_loss:118.97015380859375\n",
      "596/3000 train_loss: 52.962974548339844 test_loss:103.11074829101562\n",
      "597/3000 train_loss: 61.82279968261719 test_loss:105.75381469726562\n",
      "598/3000 train_loss: 64.1954345703125 test_loss:141.56809997558594\n",
      "599/3000 train_loss: 65.21147918701172 test_loss:110.80299377441406\n",
      "600/3000 train_loss: 71.71978759765625 test_loss:124.30449676513672\n",
      "601/3000 train_loss: 72.95999145507812 test_loss:122.92754364013672\n",
      "602/3000 train_loss: 56.101837158203125 test_loss:112.63933563232422\n",
      "603/3000 train_loss: 56.920005798339844 test_loss:132.69732666015625\n",
      "604/3000 train_loss: 65.4342041015625 test_loss:114.08769226074219\n",
      "605/3000 train_loss: 57.3739013671875 test_loss:127.5066146850586\n",
      "606/3000 train_loss: 61.042938232421875 test_loss:123.54279327392578\n",
      "607/3000 train_loss: 56.68596267700195 test_loss:121.82086181640625\n",
      "608/3000 train_loss: 63.85721206665039 test_loss:128.6975555419922\n",
      "609/3000 train_loss: 69.83612823486328 test_loss:146.32080078125\n",
      "610/3000 train_loss: 71.77188873291016 test_loss:104.94143676757812\n",
      "611/3000 train_loss: 62.02467727661133 test_loss:113.87948608398438\n",
      "612/3000 train_loss: 62.70429229736328 test_loss:140.83975219726562\n",
      "613/3000 train_loss: 69.20602416992188 test_loss:106.35076904296875\n",
      "614/3000 train_loss: 62.644649505615234 test_loss:117.29428100585938\n",
      "615/3000 train_loss: 56.48027420043945 test_loss:108.02500915527344\n",
      "616/3000 train_loss: 54.054866790771484 test_loss:116.12334442138672\n",
      "617/3000 train_loss: 57.841243743896484 test_loss:111.33868408203125\n",
      "618/3000 train_loss: 59.762611389160156 test_loss:106.7057876586914\n",
      "619/3000 train_loss: 62.03288650512695 test_loss:130.2001190185547\n",
      "620/3000 train_loss: 61.641719818115234 test_loss:132.42816162109375\n",
      "621/3000 train_loss: 66.5370864868164 test_loss:121.45220947265625\n",
      "622/3000 train_loss: 58.84250259399414 test_loss:106.91197967529297\n",
      "623/3000 train_loss: 70.865966796875 test_loss:126.53026580810547\n",
      "624/3000 train_loss: 73.73385620117188 test_loss:97.32951354980469\n",
      "625/3000 train_loss: 62.10589599609375 test_loss:105.27033233642578\n",
      "626/3000 train_loss: 56.826778411865234 test_loss:114.07088470458984\n",
      "627/3000 train_loss: 70.1417465209961 test_loss:126.51480102539062\n",
      "628/3000 train_loss: 59.976036071777344 test_loss:112.98385620117188\n",
      "629/3000 train_loss: 55.85832595825195 test_loss:117.7824935913086\n",
      "630/3000 train_loss: 73.68035125732422 test_loss:125.50354766845703\n",
      "631/3000 train_loss: 61.46392059326172 test_loss:121.19342803955078\n",
      "632/3000 train_loss: 60.157474517822266 test_loss:120.22660827636719\n",
      "633/3000 train_loss: 56.40567398071289 test_loss:112.98258972167969\n",
      "634/3000 train_loss: 59.748661041259766 test_loss:127.81456756591797\n",
      "635/3000 train_loss: 68.90817260742188 test_loss:104.18441009521484\n",
      "636/3000 train_loss: 70.9142074584961 test_loss:109.19345092773438\n",
      "637/3000 train_loss: 78.52902221679688 test_loss:117.08390808105469\n",
      "638/3000 train_loss: 59.689449310302734 test_loss:122.1465072631836\n",
      "639/3000 train_loss: 64.99701690673828 test_loss:98.90400695800781\n",
      "640/3000 train_loss: 62.87910461425781 test_loss:128.961669921875\n",
      "641/3000 train_loss: 69.5304946899414 test_loss:127.01968383789062\n",
      "642/3000 train_loss: 50.03866958618164 test_loss:111.92327880859375\n",
      "643/3000 train_loss: 55.79265594482422 test_loss:112.91032409667969\n",
      "644/3000 train_loss: 59.88344192504883 test_loss:118.30518341064453\n",
      "645/3000 train_loss: 60.31178283691406 test_loss:136.29171752929688\n",
      "646/3000 train_loss: 56.31864929199219 test_loss:118.10761260986328\n",
      "647/3000 train_loss: 56.02952575683594 test_loss:119.66041564941406\n",
      "648/3000 train_loss: 58.35129928588867 test_loss:115.84876251220703\n",
      "649/3000 train_loss: 63.499542236328125 test_loss:120.92511749267578\n",
      "650/3000 train_loss: 66.39851379394531 test_loss:135.7353973388672\n",
      "651/3000 train_loss: 77.01038360595703 test_loss:126.38456726074219\n",
      "652/3000 train_loss: 61.70907974243164 test_loss:118.85115814208984\n",
      "653/3000 train_loss: 53.03177261352539 test_loss:123.2462158203125\n",
      "654/3000 train_loss: 57.520206451416016 test_loss:112.13052368164062\n",
      "655/3000 train_loss: 53.55424880981445 test_loss:110.28435516357422\n",
      "656/3000 train_loss: 51.105045318603516 test_loss:105.07439422607422\n",
      "657/3000 train_loss: 43.73997497558594 test_loss:99.19747924804688\n",
      "658/3000 train_loss: 61.17034912109375 test_loss:104.89501190185547\n",
      "659/3000 train_loss: 55.23875045776367 test_loss:117.10653686523438\n",
      "660/3000 train_loss: 57.95663070678711 test_loss:106.10569763183594\n",
      "661/3000 train_loss: 60.22262954711914 test_loss:108.22259521484375\n",
      "662/3000 train_loss: 65.2196273803711 test_loss:124.29255676269531\n",
      "663/3000 train_loss: 57.38138198852539 test_loss:116.2800521850586\n",
      "664/3000 train_loss: 59.742862701416016 test_loss:116.7203598022461\n",
      "665/3000 train_loss: 71.91744232177734 test_loss:113.76092529296875\n",
      "666/3000 train_loss: 64.0595703125 test_loss:124.28533172607422\n",
      "667/3000 train_loss: 59.3814582824707 test_loss:105.89839935302734\n",
      "668/3000 train_loss: 51.15531921386719 test_loss:120.90151977539062\n",
      "669/3000 train_loss: 58.95322036743164 test_loss:121.65779876708984\n",
      "670/3000 train_loss: 48.09345245361328 test_loss:113.28672790527344\n",
      "671/3000 train_loss: 50.36503982543945 test_loss:115.52477264404297\n",
      "672/3000 train_loss: 61.61193084716797 test_loss:112.27132415771484\n",
      "673/3000 train_loss: 58.31184387207031 test_loss:115.49775695800781\n",
      "674/3000 train_loss: 63.650936126708984 test_loss:110.84019470214844\n",
      "675/3000 train_loss: 61.37972640991211 test_loss:107.16700744628906\n",
      "676/3000 train_loss: 63.85807800292969 test_loss:111.55426788330078\n",
      "677/3000 train_loss: 56.10430145263672 test_loss:127.40345001220703\n",
      "678/3000 train_loss: 56.68742370605469 test_loss:119.58918762207031\n",
      "679/3000 train_loss: 50.60679244995117 test_loss:106.6693344116211\n",
      "680/3000 train_loss: 54.81682205200195 test_loss:116.27619171142578\n",
      "681/3000 train_loss: 60.3404541015625 test_loss:130.1950225830078\n",
      "682/3000 train_loss: 56.70166015625 test_loss:116.52650451660156\n",
      "683/3000 train_loss: 55.59529113769531 test_loss:114.08683776855469\n",
      "684/3000 train_loss: 63.051082611083984 test_loss:117.10655212402344\n",
      "685/3000 train_loss: 52.64311981201172 test_loss:106.66793060302734\n",
      "686/3000 train_loss: 53.13685989379883 test_loss:137.84109497070312\n",
      "687/3000 train_loss: 53.14275360107422 test_loss:120.00930786132812\n",
      "688/3000 train_loss: 54.39614486694336 test_loss:114.63296508789062\n",
      "689/3000 train_loss: 53.4914436340332 test_loss:109.31195831298828\n",
      "690/3000 train_loss: 47.296871185302734 test_loss:106.20405578613281\n",
      "691/3000 train_loss: 62.04631042480469 test_loss:133.0496368408203\n",
      "692/3000 train_loss: 61.62641143798828 test_loss:108.55958557128906\n",
      "693/3000 train_loss: 54.35224914550781 test_loss:122.29143524169922\n",
      "694/3000 train_loss: 60.404850006103516 test_loss:95.45510864257812\n",
      "695/3000 train_loss: 50.17784118652344 test_loss:114.52501678466797\n",
      "696/3000 train_loss: 58.23945617675781 test_loss:108.99716186523438\n",
      "697/3000 train_loss: 52.56401062011719 test_loss:113.10816955566406\n",
      "698/3000 train_loss: 48.23958969116211 test_loss:107.90248107910156\n",
      "699/3000 train_loss: 43.815956115722656 test_loss:97.1711654663086\n",
      "700/3000 train_loss: 47.619232177734375 test_loss:114.08027648925781\n",
      "701/3000 train_loss: 54.69087600708008 test_loss:122.36229705810547\n",
      "702/3000 train_loss: 56.33055114746094 test_loss:110.59783935546875\n",
      "703/3000 train_loss: 58.68671417236328 test_loss:124.68576049804688\n",
      "704/3000 train_loss: 63.73044204711914 test_loss:111.75942993164062\n",
      "705/3000 train_loss: 49.57852554321289 test_loss:116.27042388916016\n",
      "706/3000 train_loss: 62.987640380859375 test_loss:99.07756042480469\n",
      "707/3000 train_loss: 57.66712951660156 test_loss:115.6341781616211\n",
      "708/3000 train_loss: 62.50218200683594 test_loss:105.16842651367188\n",
      "709/3000 train_loss: 75.7998046875 test_loss:124.3674545288086\n",
      "710/3000 train_loss: 56.310665130615234 test_loss:107.00035095214844\n",
      "711/3000 train_loss: 53.810142517089844 test_loss:106.7790756225586\n",
      "712/3000 train_loss: 53.159339904785156 test_loss:117.24406433105469\n",
      "713/3000 train_loss: 68.12224578857422 test_loss:123.51151275634766\n",
      "714/3000 train_loss: 56.74303436279297 test_loss:116.21424865722656\n",
      "715/3000 train_loss: 49.25663375854492 test_loss:118.85028839111328\n",
      "716/3000 train_loss: 54.53927230834961 test_loss:116.66983032226562\n",
      "717/3000 train_loss: 51.94287109375 test_loss:110.5472640991211\n",
      "718/3000 train_loss: 55.35971450805664 test_loss:119.5018310546875\n",
      "719/3000 train_loss: 53.33706283569336 test_loss:113.03182983398438\n",
      "720/3000 train_loss: 51.8792610168457 test_loss:100.71839904785156\n",
      "721/3000 train_loss: 58.625370025634766 test_loss:103.85205078125\n",
      "722/3000 train_loss: 45.70970153808594 test_loss:114.55321502685547\n",
      "723/3000 train_loss: 46.381465911865234 test_loss:107.07354736328125\n",
      "724/3000 train_loss: 54.456260681152344 test_loss:111.88417053222656\n",
      "725/3000 train_loss: 50.5080680847168 test_loss:112.24600219726562\n",
      "726/3000 train_loss: 51.45166015625 test_loss:115.0654525756836\n",
      "727/3000 train_loss: 56.36552810668945 test_loss:117.15452575683594\n",
      "728/3000 train_loss: 53.350128173828125 test_loss:134.4752960205078\n",
      "729/3000 train_loss: 60.89137268066406 test_loss:115.68665313720703\n",
      "730/3000 train_loss: 60.924896240234375 test_loss:98.89815521240234\n",
      "731/3000 train_loss: 56.310447692871094 test_loss:107.4197006225586\n",
      "732/3000 train_loss: 56.64528274536133 test_loss:109.41783142089844\n",
      "733/3000 train_loss: 49.628028869628906 test_loss:108.0755386352539\n",
      "734/3000 train_loss: 54.22989273071289 test_loss:115.2553939819336\n",
      "735/3000 train_loss: 52.56747055053711 test_loss:114.06322479248047\n",
      "736/3000 train_loss: 53.25761795043945 test_loss:117.76882934570312\n",
      "737/3000 train_loss: 58.04811096191406 test_loss:106.255126953125\n",
      "738/3000 train_loss: 66.9716796875 test_loss:121.68951416015625\n",
      "739/3000 train_loss: 51.69279098510742 test_loss:107.33588409423828\n",
      "740/3000 train_loss: 64.85589599609375 test_loss:121.20845031738281\n",
      "741/3000 train_loss: 62.376190185546875 test_loss:132.4147491455078\n",
      "742/3000 train_loss: 58.50374221801758 test_loss:104.50774383544922\n",
      "743/3000 train_loss: 63.35844421386719 test_loss:117.39796447753906\n",
      "744/3000 train_loss: 52.57071304321289 test_loss:104.66307830810547\n",
      "745/3000 train_loss: 48.763572692871094 test_loss:101.21580505371094\n",
      "746/3000 train_loss: 53.349159240722656 test_loss:105.8907470703125\n",
      "747/3000 train_loss: 52.56351852416992 test_loss:121.46891784667969\n",
      "748/3000 train_loss: 51.24736404418945 test_loss:109.34430694580078\n",
      "749/3000 train_loss: 59.98911666870117 test_loss:111.10275268554688\n",
      "750/3000 train_loss: 50.703590393066406 test_loss:119.01783752441406\n",
      "751/3000 train_loss: 52.214595794677734 test_loss:105.86412811279297\n",
      "752/3000 train_loss: 63.157806396484375 test_loss:118.42234802246094\n",
      "753/3000 train_loss: 58.627010345458984 test_loss:103.24842834472656\n",
      "754/3000 train_loss: 49.597694396972656 test_loss:108.66145324707031\n",
      "755/3000 train_loss: 51.37156677246094 test_loss:113.07000732421875\n",
      "756/3000 train_loss: 50.28262710571289 test_loss:103.93585205078125\n",
      "757/3000 train_loss: 48.107635498046875 test_loss:112.98599243164062\n",
      "758/3000 train_loss: 56.98392868041992 test_loss:97.13468170166016\n",
      "759/3000 train_loss: 55.5141716003418 test_loss:116.1608657836914\n",
      "760/3000 train_loss: 54.69717788696289 test_loss:97.37677764892578\n",
      "761/3000 train_loss: 56.48707580566406 test_loss:125.87461853027344\n",
      "762/3000 train_loss: 53.9669075012207 test_loss:112.26619720458984\n",
      "763/3000 train_loss: 51.08333969116211 test_loss:104.50189208984375\n",
      "764/3000 train_loss: 44.63740921020508 test_loss:100.82784271240234\n",
      "765/3000 train_loss: 51.46702575683594 test_loss:116.68778228759766\n",
      "766/3000 train_loss: 58.875877380371094 test_loss:129.9390869140625\n",
      "767/3000 train_loss: 52.911598205566406 test_loss:104.06965637207031\n",
      "768/3000 train_loss: 67.02572631835938 test_loss:121.13642883300781\n",
      "769/3000 train_loss: 51.173919677734375 test_loss:108.60197448730469\n",
      "770/3000 train_loss: 64.78306579589844 test_loss:124.1858901977539\n",
      "771/3000 train_loss: 57.80506896972656 test_loss:112.70380401611328\n",
      "772/3000 train_loss: 53.29752731323242 test_loss:114.56290435791016\n",
      "773/3000 train_loss: 55.64339065551758 test_loss:124.56291198730469\n",
      "774/3000 train_loss: 53.56913375854492 test_loss:99.98461151123047\n",
      "775/3000 train_loss: 50.14298629760742 test_loss:101.37637329101562\n",
      "776/3000 train_loss: 51.39694595336914 test_loss:113.14605712890625\n",
      "777/3000 train_loss: 51.851924896240234 test_loss:88.64364624023438\n",
      "778/3000 train_loss: 47.51558303833008 test_loss:109.62190246582031\n",
      "779/3000 train_loss: 49.48948669433594 test_loss:100.66895294189453\n",
      "780/3000 train_loss: 48.1549186706543 test_loss:108.80178833007812\n",
      "781/3000 train_loss: 65.08972930908203 test_loss:124.38069152832031\n",
      "782/3000 train_loss: 62.966434478759766 test_loss:104.90088653564453\n",
      "783/3000 train_loss: 49.918338775634766 test_loss:118.50747680664062\n",
      "784/3000 train_loss: 53.9100341796875 test_loss:101.73899841308594\n",
      "785/3000 train_loss: 52.39057159423828 test_loss:116.5487060546875\n",
      "786/3000 train_loss: 60.149131774902344 test_loss:113.8727035522461\n",
      "787/3000 train_loss: 65.44427490234375 test_loss:100.28388214111328\n",
      "788/3000 train_loss: 69.31591796875 test_loss:101.79051208496094\n",
      "789/3000 train_loss: 56.06485366821289 test_loss:104.7339096069336\n",
      "790/3000 train_loss: 56.06317138671875 test_loss:108.52981567382812\n",
      "791/3000 train_loss: 55.82744598388672 test_loss:99.90507507324219\n",
      "792/3000 train_loss: 49.7685432434082 test_loss:110.4390869140625\n",
      "793/3000 train_loss: 60.41378402709961 test_loss:109.34886932373047\n",
      "794/3000 train_loss: 52.57564163208008 test_loss:123.96415710449219\n",
      "795/3000 train_loss: 65.47600555419922 test_loss:114.89867401123047\n",
      "796/3000 train_loss: 54.57916259765625 test_loss:103.7081527709961\n",
      "797/3000 train_loss: 46.670223236083984 test_loss:102.36246490478516\n",
      "798/3000 train_loss: 48.36492919921875 test_loss:97.75387573242188\n",
      "799/3000 train_loss: 56.61581039428711 test_loss:106.00777435302734\n",
      "800/3000 train_loss: 52.21638488769531 test_loss:98.60799407958984\n",
      "801/3000 train_loss: 51.92411804199219 test_loss:110.32209777832031\n",
      "802/3000 train_loss: 50.78648376464844 test_loss:113.10330963134766\n",
      "803/3000 train_loss: 44.64088439941406 test_loss:103.89035034179688\n",
      "804/3000 train_loss: 50.04080581665039 test_loss:104.08358764648438\n",
      "805/3000 train_loss: 51.24304962158203 test_loss:105.50328826904297\n",
      "806/3000 train_loss: 56.115989685058594 test_loss:113.81607818603516\n",
      "807/3000 train_loss: 48.16097640991211 test_loss:100.26264953613281\n",
      "808/3000 train_loss: 49.71099090576172 test_loss:104.20846557617188\n",
      "809/3000 train_loss: 50.50079345703125 test_loss:107.46985626220703\n",
      "810/3000 train_loss: 52.65989303588867 test_loss:100.17095184326172\n",
      "811/3000 train_loss: 51.645729064941406 test_loss:114.07313537597656\n",
      "812/3000 train_loss: 53.4617805480957 test_loss:112.66277313232422\n",
      "813/3000 train_loss: 51.11273956298828 test_loss:102.59349822998047\n",
      "814/3000 train_loss: 53.82664489746094 test_loss:102.90384674072266\n",
      "815/3000 train_loss: 49.55369186401367 test_loss:96.6323471069336\n",
      "816/3000 train_loss: 57.47547912597656 test_loss:105.4183349609375\n",
      "817/3000 train_loss: 45.339149475097656 test_loss:91.72539520263672\n",
      "818/3000 train_loss: 44.50620651245117 test_loss:109.55765533447266\n",
      "819/3000 train_loss: 49.673465728759766 test_loss:99.88292694091797\n",
      "820/3000 train_loss: 54.33877944946289 test_loss:113.21949768066406\n",
      "821/3000 train_loss: 47.063926696777344 test_loss:93.5780029296875\n",
      "822/3000 train_loss: 49.665870666503906 test_loss:99.34183502197266\n",
      "823/3000 train_loss: 68.79857635498047 test_loss:105.9095687866211\n",
      "824/3000 train_loss: 54.07561111450195 test_loss:107.46696472167969\n",
      "825/3000 train_loss: 55.708229064941406 test_loss:103.7635269165039\n",
      "826/3000 train_loss: 56.274898529052734 test_loss:105.62081909179688\n",
      "827/3000 train_loss: 71.8832778930664 test_loss:104.96832275390625\n",
      "828/3000 train_loss: 57.25597381591797 test_loss:102.45060729980469\n",
      "829/3000 train_loss: 46.79020690917969 test_loss:118.60043334960938\n",
      "830/3000 train_loss: 44.34749984741211 test_loss:98.74331665039062\n",
      "831/3000 train_loss: 55.52973175048828 test_loss:122.67254638671875\n",
      "832/3000 train_loss: 57.51445388793945 test_loss:130.2010955810547\n",
      "833/3000 train_loss: 54.732120513916016 test_loss:113.6317367553711\n",
      "834/3000 train_loss: 58.84466552734375 test_loss:106.20072174072266\n",
      "835/3000 train_loss: 54.67378234863281 test_loss:95.79954528808594\n",
      "836/3000 train_loss: 52.57002639770508 test_loss:111.5571517944336\n",
      "837/3000 train_loss: 53.729591369628906 test_loss:95.8897476196289\n",
      "838/3000 train_loss: 49.1494255065918 test_loss:100.58753967285156\n",
      "839/3000 train_loss: 44.640464782714844 test_loss:108.22291564941406\n",
      "840/3000 train_loss: 49.16269302368164 test_loss:96.32856750488281\n",
      "841/3000 train_loss: 49.94451141357422 test_loss:102.47901916503906\n",
      "842/3000 train_loss: 55.66925811767578 test_loss:108.20587921142578\n",
      "843/3000 train_loss: 46.20075988769531 test_loss:107.73509216308594\n",
      "844/3000 train_loss: 55.63623046875 test_loss:106.34428405761719\n",
      "845/3000 train_loss: 49.80973434448242 test_loss:121.92255401611328\n",
      "846/3000 train_loss: 48.816654205322266 test_loss:103.09941864013672\n",
      "847/3000 train_loss: 50.076175689697266 test_loss:101.42338562011719\n",
      "848/3000 train_loss: 42.63393783569336 test_loss:100.55025482177734\n",
      "849/3000 train_loss: 49.408687591552734 test_loss:130.369140625\n",
      "850/3000 train_loss: 50.86220169067383 test_loss:103.17365264892578\n",
      "851/3000 train_loss: 57.77663040161133 test_loss:99.47183990478516\n",
      "852/3000 train_loss: 53.070858001708984 test_loss:108.3979263305664\n",
      "853/3000 train_loss: 56.661216735839844 test_loss:108.72953033447266\n",
      "854/3000 train_loss: 48.79560089111328 test_loss:96.59417724609375\n",
      "855/3000 train_loss: 44.43339157104492 test_loss:108.89099884033203\n",
      "856/3000 train_loss: 44.97616195678711 test_loss:90.1424789428711\n",
      "857/3000 train_loss: 52.87765121459961 test_loss:95.00678253173828\n",
      "858/3000 train_loss: 45.61529541015625 test_loss:102.90597534179688\n",
      "859/3000 train_loss: 54.88840866088867 test_loss:101.7263412475586\n",
      "860/3000 train_loss: 51.98408508300781 test_loss:92.61068725585938\n",
      "861/3000 train_loss: 57.92597961425781 test_loss:96.9939193725586\n",
      "862/3000 train_loss: 58.35365676879883 test_loss:100.41065979003906\n",
      "863/3000 train_loss: 40.2918815612793 test_loss:102.12968444824219\n",
      "864/3000 train_loss: 53.32582092285156 test_loss:91.35771942138672\n",
      "865/3000 train_loss: 64.68111419677734 test_loss:103.53824615478516\n",
      "866/3000 train_loss: 56.200923919677734 test_loss:116.52446746826172\n",
      "867/3000 train_loss: 50.283470153808594 test_loss:105.48765563964844\n",
      "868/3000 train_loss: 51.23772430419922 test_loss:108.18354034423828\n",
      "869/3000 train_loss: 56.57575988769531 test_loss:117.83224487304688\n",
      "870/3000 train_loss: 52.10218811035156 test_loss:123.47048950195312\n",
      "871/3000 train_loss: 51.83467102050781 test_loss:101.31539154052734\n",
      "872/3000 train_loss: 50.87669372558594 test_loss:107.16288757324219\n",
      "873/3000 train_loss: 53.26319885253906 test_loss:108.68146514892578\n",
      "874/3000 train_loss: 52.12847137451172 test_loss:104.03193664550781\n",
      "875/3000 train_loss: 48.95195770263672 test_loss:94.56552124023438\n",
      "876/3000 train_loss: 48.158935546875 test_loss:107.1424789428711\n",
      "877/3000 train_loss: 49.813873291015625 test_loss:108.65701293945312\n",
      "878/3000 train_loss: 48.51095199584961 test_loss:89.54728698730469\n",
      "879/3000 train_loss: 53.48030090332031 test_loss:92.60784912109375\n",
      "880/3000 train_loss: 44.827632904052734 test_loss:99.39698791503906\n",
      "881/3000 train_loss: 53.26640319824219 test_loss:98.44906616210938\n",
      "882/3000 train_loss: 49.91390609741211 test_loss:105.93629455566406\n",
      "883/3000 train_loss: 45.33808517456055 test_loss:105.70219421386719\n",
      "884/3000 train_loss: 46.38956069946289 test_loss:100.11154174804688\n",
      "885/3000 train_loss: 50.45418930053711 test_loss:101.6130599975586\n",
      "886/3000 train_loss: 43.83356857299805 test_loss:90.4301986694336\n",
      "887/3000 train_loss: 46.362857818603516 test_loss:89.80702209472656\n",
      "888/3000 train_loss: 53.06007385253906 test_loss:95.1024169921875\n",
      "889/3000 train_loss: 54.010868072509766 test_loss:88.34822082519531\n",
      "890/3000 train_loss: 54.961639404296875 test_loss:99.78336334228516\n",
      "891/3000 train_loss: 50.53736877441406 test_loss:121.63864135742188\n",
      "892/3000 train_loss: 58.248497009277344 test_loss:103.36832427978516\n",
      "893/3000 train_loss: 51.7122688293457 test_loss:99.16142272949219\n",
      "894/3000 train_loss: 47.25511932373047 test_loss:108.09861755371094\n",
      "895/3000 train_loss: 56.7812385559082 test_loss:111.24592590332031\n",
      "896/3000 train_loss: 52.774784088134766 test_loss:96.7606201171875\n",
      "897/3000 train_loss: 55.3111457824707 test_loss:103.0709228515625\n",
      "898/3000 train_loss: 51.7124137878418 test_loss:116.03121185302734\n",
      "899/3000 train_loss: 54.62822723388672 test_loss:104.16800689697266\n",
      "900/3000 train_loss: 57.12232208251953 test_loss:117.15281677246094\n",
      "901/3000 train_loss: 55.24303436279297 test_loss:142.82391357421875\n",
      "902/3000 train_loss: 53.47124481201172 test_loss:101.66307067871094\n",
      "903/3000 train_loss: 49.53922653198242 test_loss:101.68463897705078\n",
      "904/3000 train_loss: 49.88393020629883 test_loss:109.72351837158203\n",
      "905/3000 train_loss: 49.38275146484375 test_loss:94.86311340332031\n",
      "906/3000 train_loss: 50.109161376953125 test_loss:103.03189849853516\n",
      "907/3000 train_loss: 49.535484313964844 test_loss:110.68804931640625\n",
      "908/3000 train_loss: 48.8111686706543 test_loss:110.97486114501953\n",
      "909/3000 train_loss: 46.201744079589844 test_loss:101.27577209472656\n",
      "910/3000 train_loss: 41.13381576538086 test_loss:98.089111328125\n",
      "911/3000 train_loss: 44.20298385620117 test_loss:113.61151885986328\n",
      "912/3000 train_loss: 48.80759811401367 test_loss:94.32173919677734\n",
      "913/3000 train_loss: 54.19840621948242 test_loss:109.55391693115234\n",
      "914/3000 train_loss: 44.66892623901367 test_loss:97.49783325195312\n",
      "915/3000 train_loss: 49.48656463623047 test_loss:111.62686157226562\n",
      "916/3000 train_loss: 47.26326370239258 test_loss:98.49435424804688\n",
      "917/3000 train_loss: 45.13533401489258 test_loss:95.61515808105469\n",
      "918/3000 train_loss: 57.29713439941406 test_loss:106.66513061523438\n",
      "919/3000 train_loss: 51.022647857666016 test_loss:88.07887268066406\n",
      "920/3000 train_loss: 59.32896041870117 test_loss:110.30973052978516\n",
      "921/3000 train_loss: 43.40541076660156 test_loss:96.6379623413086\n",
      "922/3000 train_loss: 62.925148010253906 test_loss:107.50117492675781\n",
      "923/3000 train_loss: 60.33570098876953 test_loss:109.6246337890625\n",
      "924/3000 train_loss: 49.495208740234375 test_loss:108.99247741699219\n",
      "925/3000 train_loss: 49.42645263671875 test_loss:89.98030853271484\n",
      "926/3000 train_loss: 40.47901916503906 test_loss:105.47372436523438\n",
      "927/3000 train_loss: 39.589866638183594 test_loss:90.0550765991211\n",
      "928/3000 train_loss: 48.6287841796875 test_loss:99.95355224609375\n",
      "929/3000 train_loss: 48.461612701416016 test_loss:96.97374725341797\n",
      "930/3000 train_loss: 46.07076644897461 test_loss:84.00741577148438\n",
      "931/3000 train_loss: 52.069644927978516 test_loss:119.38758087158203\n",
      "932/3000 train_loss: 47.796424865722656 test_loss:96.36856842041016\n",
      "933/3000 train_loss: 38.8907356262207 test_loss:93.91959381103516\n",
      "934/3000 train_loss: 49.000091552734375 test_loss:94.88189697265625\n",
      "935/3000 train_loss: 44.55799865722656 test_loss:103.81544494628906\n",
      "936/3000 train_loss: 43.48994827270508 test_loss:106.22665405273438\n",
      "937/3000 train_loss: 48.90476989746094 test_loss:99.8156509399414\n",
      "938/3000 train_loss: 50.503231048583984 test_loss:90.10913848876953\n",
      "939/3000 train_loss: 51.543670654296875 test_loss:91.37060546875\n",
      "940/3000 train_loss: 47.63907241821289 test_loss:96.98258209228516\n",
      "941/3000 train_loss: 54.87455368041992 test_loss:92.03496551513672\n",
      "942/3000 train_loss: 44.659542083740234 test_loss:108.11029815673828\n",
      "943/3000 train_loss: 48.697322845458984 test_loss:99.1634750366211\n",
      "944/3000 train_loss: 45.617942810058594 test_loss:92.75897216796875\n",
      "945/3000 train_loss: 51.04647445678711 test_loss:91.89562225341797\n",
      "946/3000 train_loss: 38.11067199707031 test_loss:101.42631530761719\n",
      "947/3000 train_loss: 44.64830017089844 test_loss:97.23419952392578\n",
      "948/3000 train_loss: 46.1577033996582 test_loss:95.50079345703125\n",
      "949/3000 train_loss: 47.183876037597656 test_loss:104.08517456054688\n",
      "950/3000 train_loss: 44.233726501464844 test_loss:131.05209350585938\n",
      "951/3000 train_loss: 46.576377868652344 test_loss:100.9473876953125\n",
      "952/3000 train_loss: 48.84138870239258 test_loss:95.39881896972656\n",
      "953/3000 train_loss: 48.577537536621094 test_loss:80.1041259765625\n",
      "954/3000 train_loss: 43.73509216308594 test_loss:100.8237075805664\n",
      "955/3000 train_loss: 45.65229034423828 test_loss:110.12714385986328\n",
      "956/3000 train_loss: 46.09552764892578 test_loss:88.61930847167969\n",
      "957/3000 train_loss: 43.64833068847656 test_loss:102.03282165527344\n",
      "958/3000 train_loss: 41.55288314819336 test_loss:95.2745361328125\n",
      "959/3000 train_loss: 49.38787078857422 test_loss:92.68431091308594\n",
      "960/3000 train_loss: 52.23263931274414 test_loss:100.78318786621094\n",
      "961/3000 train_loss: 53.71361541748047 test_loss:103.19064331054688\n",
      "962/3000 train_loss: 42.758541107177734 test_loss:92.2768783569336\n",
      "963/3000 train_loss: 48.834957122802734 test_loss:103.32634735107422\n",
      "964/3000 train_loss: 44.43547058105469 test_loss:103.0188980102539\n",
      "965/3000 train_loss: 53.37623596191406 test_loss:102.65968322753906\n",
      "966/3000 train_loss: 46.785179138183594 test_loss:104.83132934570312\n",
      "967/3000 train_loss: 46.55571746826172 test_loss:94.69320678710938\n",
      "968/3000 train_loss: 45.66694641113281 test_loss:99.50704956054688\n",
      "969/3000 train_loss: 48.08588790893555 test_loss:84.76139068603516\n",
      "970/3000 train_loss: 47.711158752441406 test_loss:102.80841064453125\n",
      "971/3000 train_loss: 51.22138595581055 test_loss:93.45468139648438\n",
      "972/3000 train_loss: 48.47126007080078 test_loss:102.7474365234375\n",
      "973/3000 train_loss: 49.59931182861328 test_loss:96.31401062011719\n",
      "974/3000 train_loss: 55.760414123535156 test_loss:96.47086334228516\n",
      "975/3000 train_loss: 54.22653579711914 test_loss:91.21215057373047\n",
      "976/3000 train_loss: 49.478912353515625 test_loss:115.27438354492188\n",
      "977/3000 train_loss: 49.30521774291992 test_loss:111.53433227539062\n",
      "978/3000 train_loss: 47.001827239990234 test_loss:101.68264770507812\n",
      "979/3000 train_loss: 40.079933166503906 test_loss:92.62716674804688\n",
      "980/3000 train_loss: 53.064022064208984 test_loss:99.19811248779297\n",
      "981/3000 train_loss: 53.66799545288086 test_loss:97.21755981445312\n",
      "982/3000 train_loss: 47.03753662109375 test_loss:97.5142822265625\n",
      "983/3000 train_loss: 44.24808883666992 test_loss:107.3692398071289\n",
      "984/3000 train_loss: 51.768436431884766 test_loss:95.95388793945312\n",
      "985/3000 train_loss: 40.5386848449707 test_loss:103.36241912841797\n",
      "986/3000 train_loss: 46.22983932495117 test_loss:104.0956039428711\n",
      "987/3000 train_loss: 44.518863677978516 test_loss:111.40025329589844\n",
      "988/3000 train_loss: 42.15795135498047 test_loss:99.77654266357422\n",
      "989/3000 train_loss: 50.169639587402344 test_loss:118.9539794921875\n",
      "990/3000 train_loss: 45.24281692504883 test_loss:105.03224182128906\n",
      "991/3000 train_loss: 44.405128479003906 test_loss:100.89939880371094\n",
      "992/3000 train_loss: 41.45500564575195 test_loss:139.55419921875\n",
      "993/3000 train_loss: 62.86079788208008 test_loss:102.6376953125\n",
      "994/3000 train_loss: 46.4290657043457 test_loss:108.00371551513672\n",
      "995/3000 train_loss: 46.84906005859375 test_loss:108.30879211425781\n",
      "996/3000 train_loss: 52.37226104736328 test_loss:112.39500427246094\n",
      "997/3000 train_loss: 38.522335052490234 test_loss:97.78990936279297\n",
      "998/3000 train_loss: 42.35007095336914 test_loss:88.9867172241211\n",
      "999/3000 train_loss: 44.255733489990234 test_loss:89.11320495605469\n",
      "1000/3000 train_loss: 38.86575698852539 test_loss:87.80307006835938\n",
      "1001/3000 train_loss: 38.810760498046875 test_loss:92.5784683227539\n",
      "1002/3000 train_loss: 45.46143341064453 test_loss:93.42826843261719\n",
      "1003/3000 train_loss: 56.31864547729492 test_loss:104.8155288696289\n",
      "1004/3000 train_loss: 54.764244079589844 test_loss:82.63426208496094\n",
      "1005/3000 train_loss: 47.03491973876953 test_loss:96.82969665527344\n",
      "1006/3000 train_loss: 49.0562858581543 test_loss:99.96098327636719\n",
      "1007/3000 train_loss: 51.36350631713867 test_loss:114.61807250976562\n",
      "1008/3000 train_loss: 49.56093978881836 test_loss:94.67402648925781\n",
      "1009/3000 train_loss: 43.84653854370117 test_loss:102.17332458496094\n",
      "1010/3000 train_loss: 48.73942565917969 test_loss:104.68543243408203\n",
      "1011/3000 train_loss: 50.80913543701172 test_loss:97.36264038085938\n",
      "1012/3000 train_loss: 49.35066223144531 test_loss:102.87850189208984\n",
      "1013/3000 train_loss: 43.89485549926758 test_loss:83.62596130371094\n",
      "1014/3000 train_loss: 44.40113830566406 test_loss:92.31204223632812\n",
      "1015/3000 train_loss: 49.334625244140625 test_loss:96.43335723876953\n",
      "1016/3000 train_loss: 47.1601676940918 test_loss:96.34463500976562\n",
      "1017/3000 train_loss: 52.133453369140625 test_loss:94.62171936035156\n",
      "1018/3000 train_loss: 51.34225845336914 test_loss:98.98371887207031\n",
      "1019/3000 train_loss: 39.119285583496094 test_loss:89.53433227539062\n",
      "1020/3000 train_loss: 48.76641845703125 test_loss:99.2629165649414\n",
      "1021/3000 train_loss: 42.920127868652344 test_loss:103.78398895263672\n",
      "1022/3000 train_loss: 56.424251556396484 test_loss:113.28697967529297\n",
      "1023/3000 train_loss: 50.03871536254883 test_loss:95.50270080566406\n",
      "1024/3000 train_loss: 52.6013069152832 test_loss:103.62265014648438\n",
      "1025/3000 train_loss: 53.43254089355469 test_loss:91.00116729736328\n",
      "1026/3000 train_loss: 51.973121643066406 test_loss:92.3202133178711\n",
      "1027/3000 train_loss: 53.87926483154297 test_loss:93.83541107177734\n",
      "1028/3000 train_loss: 45.327796936035156 test_loss:83.46784973144531\n",
      "1029/3000 train_loss: 40.54735565185547 test_loss:92.13328552246094\n",
      "1030/3000 train_loss: 44.743080139160156 test_loss:96.32255554199219\n",
      "1031/3000 train_loss: 42.003150939941406 test_loss:87.9307861328125\n",
      "1032/3000 train_loss: 50.00106430053711 test_loss:112.80984497070312\n",
      "1033/3000 train_loss: 41.39986038208008 test_loss:91.08554077148438\n",
      "1034/3000 train_loss: 42.448150634765625 test_loss:82.34073638916016\n",
      "1035/3000 train_loss: 45.61418914794922 test_loss:87.91812133789062\n",
      "1036/3000 train_loss: 41.09552001953125 test_loss:93.49674987792969\n",
      "1037/3000 train_loss: 42.273765563964844 test_loss:100.79354858398438\n",
      "1038/3000 train_loss: 40.31082534790039 test_loss:95.77042388916016\n",
      "1039/3000 train_loss: 42.70559310913086 test_loss:85.52268981933594\n",
      "1040/3000 train_loss: 43.271446228027344 test_loss:95.06498718261719\n",
      "1041/3000 train_loss: 50.777549743652344 test_loss:88.24219512939453\n",
      "1042/3000 train_loss: 52.49418258666992 test_loss:91.58837890625\n",
      "1043/3000 train_loss: 47.48833084106445 test_loss:107.16592407226562\n",
      "1044/3000 train_loss: 43.763885498046875 test_loss:95.24630737304688\n",
      "1045/3000 train_loss: 46.42654800415039 test_loss:95.78073120117188\n",
      "1046/3000 train_loss: 41.07099914550781 test_loss:84.7210464477539\n",
      "1047/3000 train_loss: 37.36566925048828 test_loss:127.8359603881836\n",
      "1048/3000 train_loss: 63.144920349121094 test_loss:110.32440185546875\n",
      "1049/3000 train_loss: 48.05723190307617 test_loss:102.01856994628906\n",
      "1050/3000 train_loss: 45.668338775634766 test_loss:102.45999908447266\n",
      "1051/3000 train_loss: 37.844573974609375 test_loss:83.55760192871094\n",
      "1052/3000 train_loss: 49.647705078125 test_loss:110.47440338134766\n",
      "1053/3000 train_loss: 46.906314849853516 test_loss:101.18165588378906\n",
      "1054/3000 train_loss: 44.420623779296875 test_loss:103.9673843383789\n",
      "1055/3000 train_loss: 46.28785705566406 test_loss:88.09317016601562\n",
      "1056/3000 train_loss: 43.55696105957031 test_loss:117.26256561279297\n",
      "1057/3000 train_loss: 45.11537551879883 test_loss:84.91619110107422\n",
      "1058/3000 train_loss: 42.56160354614258 test_loss:105.82292938232422\n",
      "1059/3000 train_loss: 45.88227081298828 test_loss:96.43008422851562\n",
      "1060/3000 train_loss: 45.749717712402344 test_loss:95.08744812011719\n",
      "1061/3000 train_loss: 50.095394134521484 test_loss:102.38907623291016\n",
      "1062/3000 train_loss: 40.5159912109375 test_loss:99.8530044555664\n",
      "1063/3000 train_loss: 48.50699234008789 test_loss:94.91986083984375\n",
      "1064/3000 train_loss: 44.054534912109375 test_loss:116.9944839477539\n",
      "1065/3000 train_loss: 42.493431091308594 test_loss:94.17929077148438\n",
      "1066/3000 train_loss: 45.233131408691406 test_loss:105.80059814453125\n",
      "1067/3000 train_loss: 48.39040756225586 test_loss:92.91261291503906\n",
      "1068/3000 train_loss: 47.41596221923828 test_loss:105.32986450195312\n",
      "1069/3000 train_loss: 42.181617736816406 test_loss:87.72354888916016\n",
      "1070/3000 train_loss: 51.04683303833008 test_loss:90.96806335449219\n",
      "1071/3000 train_loss: 40.074859619140625 test_loss:94.02940368652344\n",
      "1072/3000 train_loss: 42.64033889770508 test_loss:94.7645263671875\n",
      "1073/3000 train_loss: 48.376834869384766 test_loss:98.4855728149414\n",
      "1074/3000 train_loss: 49.98353958129883 test_loss:93.63822937011719\n",
      "1075/3000 train_loss: 43.17599868774414 test_loss:109.57193756103516\n",
      "1076/3000 train_loss: 46.98719024658203 test_loss:85.20938110351562\n",
      "1077/3000 train_loss: 42.91072082519531 test_loss:88.56888580322266\n",
      "1078/3000 train_loss: 46.62595748901367 test_loss:96.45557403564453\n",
      "1079/3000 train_loss: 44.08880615234375 test_loss:91.41901397705078\n",
      "1080/3000 train_loss: 40.3845100402832 test_loss:84.70938110351562\n",
      "1081/3000 train_loss: 40.74153518676758 test_loss:96.30310821533203\n",
      "1082/3000 train_loss: 53.08728790283203 test_loss:105.32809448242188\n",
      "1083/3000 train_loss: 43.52735137939453 test_loss:94.73995208740234\n",
      "1084/3000 train_loss: 47.43634796142578 test_loss:91.66343688964844\n",
      "1085/3000 train_loss: 40.5662956237793 test_loss:98.24542999267578\n",
      "1086/3000 train_loss: 52.90040588378906 test_loss:107.40559387207031\n",
      "1087/3000 train_loss: 47.38774871826172 test_loss:106.66759490966797\n",
      "1088/3000 train_loss: 46.988731384277344 test_loss:87.28692626953125\n",
      "1089/3000 train_loss: 43.57539367675781 test_loss:93.97132873535156\n",
      "1090/3000 train_loss: 42.29086685180664 test_loss:83.20203399658203\n",
      "1091/3000 train_loss: 41.7604866027832 test_loss:89.92626953125\n",
      "1092/3000 train_loss: 37.94227981567383 test_loss:89.603515625\n",
      "1093/3000 train_loss: 40.158164978027344 test_loss:90.43360137939453\n",
      "1094/3000 train_loss: 47.60149383544922 test_loss:87.54261016845703\n",
      "1095/3000 train_loss: 42.86159133911133 test_loss:91.27093505859375\n",
      "1096/3000 train_loss: 47.37498474121094 test_loss:89.51234436035156\n",
      "1097/3000 train_loss: 50.17509460449219 test_loss:109.43365478515625\n",
      "1098/3000 train_loss: 40.3535041809082 test_loss:96.06845092773438\n",
      "1099/3000 train_loss: 52.20790100097656 test_loss:101.2569580078125\n",
      "1100/3000 train_loss: 48.7561149597168 test_loss:97.68063354492188\n",
      "1101/3000 train_loss: 39.87822341918945 test_loss:86.2294692993164\n",
      "1102/3000 train_loss: 42.32557678222656 test_loss:92.03388977050781\n",
      "1103/3000 train_loss: 47.03081512451172 test_loss:103.31460571289062\n",
      "1104/3000 train_loss: 50.43375778198242 test_loss:98.2745590209961\n",
      "1105/3000 train_loss: 52.23788833618164 test_loss:100.82091522216797\n",
      "1106/3000 train_loss: 49.69657516479492 test_loss:119.98590850830078\n",
      "1107/3000 train_loss: 62.34077453613281 test_loss:89.8516845703125\n",
      "1108/3000 train_loss: 47.72411346435547 test_loss:102.65254974365234\n",
      "1109/3000 train_loss: 60.151187896728516 test_loss:108.38053894042969\n",
      "1110/3000 train_loss: 45.83257293701172 test_loss:92.40013122558594\n",
      "1111/3000 train_loss: 46.27009201049805 test_loss:101.50829315185547\n",
      "1112/3000 train_loss: 49.79740524291992 test_loss:88.39926147460938\n",
      "1113/3000 train_loss: 40.20123291015625 test_loss:86.06553649902344\n",
      "1114/3000 train_loss: 39.010868072509766 test_loss:98.80989074707031\n",
      "1115/3000 train_loss: 48.10039138793945 test_loss:108.06444549560547\n",
      "1116/3000 train_loss: 47.09579086303711 test_loss:87.11141204833984\n",
      "1117/3000 train_loss: 41.99492263793945 test_loss:93.42791748046875\n",
      "1118/3000 train_loss: 38.11498260498047 test_loss:100.5794448852539\n",
      "1119/3000 train_loss: 51.74249267578125 test_loss:80.19586944580078\n",
      "1120/3000 train_loss: 50.148834228515625 test_loss:89.08936309814453\n",
      "1121/3000 train_loss: 42.3927116394043 test_loss:93.0009994506836\n",
      "1122/3000 train_loss: 41.34876251220703 test_loss:90.73336029052734\n",
      "1123/3000 train_loss: 40.751792907714844 test_loss:85.4296875\n",
      "1124/3000 train_loss: 37.75279235839844 test_loss:90.82194519042969\n",
      "1125/3000 train_loss: 41.97813415527344 test_loss:79.27954864501953\n",
      "1126/3000 train_loss: 41.65656280517578 test_loss:97.2077865600586\n",
      "1127/3000 train_loss: 38.59173583984375 test_loss:92.5662612915039\n",
      "1128/3000 train_loss: 43.01603317260742 test_loss:83.06510162353516\n",
      "1129/3000 train_loss: 43.25662612915039 test_loss:95.92807006835938\n",
      "1130/3000 train_loss: 48.224952697753906 test_loss:100.1379623413086\n",
      "1131/3000 train_loss: 52.555606842041016 test_loss:96.72349548339844\n",
      "1132/3000 train_loss: 42.95619201660156 test_loss:87.6353988647461\n",
      "1133/3000 train_loss: 36.55337142944336 test_loss:93.7780532836914\n",
      "1134/3000 train_loss: 42.87041091918945 test_loss:104.02452087402344\n",
      "1135/3000 train_loss: 40.961021423339844 test_loss:85.7874755859375\n",
      "1136/3000 train_loss: 41.68653869628906 test_loss:93.23355865478516\n",
      "1137/3000 train_loss: 50.19422912597656 test_loss:96.8628158569336\n",
      "1138/3000 train_loss: 44.75887680053711 test_loss:92.89983367919922\n",
      "1139/3000 train_loss: 42.813438415527344 test_loss:87.09200286865234\n",
      "1140/3000 train_loss: 43.29220962524414 test_loss:84.05517578125\n",
      "1141/3000 train_loss: 42.42811965942383 test_loss:95.46804809570312\n",
      "1142/3000 train_loss: 42.79545974731445 test_loss:88.3405532836914\n",
      "1143/3000 train_loss: 61.569793701171875 test_loss:97.95870971679688\n",
      "1144/3000 train_loss: 58.62480545043945 test_loss:111.2564926147461\n",
      "1145/3000 train_loss: 48.070369720458984 test_loss:96.5668716430664\n",
      "1146/3000 train_loss: 44.18855285644531 test_loss:97.91161346435547\n",
      "1147/3000 train_loss: 37.596622467041016 test_loss:91.77046203613281\n",
      "1148/3000 train_loss: 37.94906997680664 test_loss:99.0203857421875\n",
      "1149/3000 train_loss: 40.265743255615234 test_loss:91.12353515625\n",
      "1150/3000 train_loss: 42.73539733886719 test_loss:94.99838256835938\n",
      "1151/3000 train_loss: 42.72713088989258 test_loss:98.822998046875\n",
      "1152/3000 train_loss: 47.6234130859375 test_loss:100.9526596069336\n",
      "1153/3000 train_loss: 43.80879211425781 test_loss:90.02616882324219\n",
      "1154/3000 train_loss: 48.19746398925781 test_loss:82.73980712890625\n",
      "1155/3000 train_loss: 43.1861457824707 test_loss:104.04649353027344\n",
      "1156/3000 train_loss: 42.77012252807617 test_loss:82.06883239746094\n",
      "1157/3000 train_loss: 43.79585647583008 test_loss:106.66812133789062\n",
      "1158/3000 train_loss: 51.82863998413086 test_loss:86.17214965820312\n",
      "1159/3000 train_loss: 39.85414505004883 test_loss:98.1290512084961\n",
      "1160/3000 train_loss: 41.045257568359375 test_loss:90.40849304199219\n",
      "1161/3000 train_loss: 41.29933547973633 test_loss:95.30992126464844\n",
      "1162/3000 train_loss: 44.06890869140625 test_loss:98.9262466430664\n",
      "1163/3000 train_loss: 53.1594352722168 test_loss:85.06769561767578\n",
      "1164/3000 train_loss: 51.58198547363281 test_loss:90.88935852050781\n",
      "1165/3000 train_loss: 50.35137176513672 test_loss:88.2793960571289\n",
      "1166/3000 train_loss: 47.25279998779297 test_loss:97.36892700195312\n",
      "1167/3000 train_loss: 47.450862884521484 test_loss:87.50109100341797\n",
      "1168/3000 train_loss: 38.723636627197266 test_loss:90.29573822021484\n",
      "1169/3000 train_loss: 47.54853820800781 test_loss:88.40571594238281\n",
      "1170/3000 train_loss: 35.6508903503418 test_loss:93.14514923095703\n",
      "1171/3000 train_loss: 41.42123031616211 test_loss:99.9049072265625\n",
      "1172/3000 train_loss: 45.970027923583984 test_loss:100.09733581542969\n",
      "1173/3000 train_loss: 41.993228912353516 test_loss:96.05615997314453\n",
      "1174/3000 train_loss: 37.198822021484375 test_loss:91.0948486328125\n",
      "1175/3000 train_loss: 43.78748321533203 test_loss:83.12921905517578\n",
      "1176/3000 train_loss: 40.46109390258789 test_loss:93.28470611572266\n",
      "1177/3000 train_loss: 38.26546096801758 test_loss:86.57901000976562\n",
      "1178/3000 train_loss: 41.47644805908203 test_loss:98.813232421875\n",
      "1179/3000 train_loss: 56.465904235839844 test_loss:117.79772186279297\n",
      "1180/3000 train_loss: 45.38130187988281 test_loss:89.941650390625\n",
      "1181/3000 train_loss: 48.65127182006836 test_loss:82.67073822021484\n",
      "1182/3000 train_loss: 47.37971878051758 test_loss:98.13394165039062\n",
      "1183/3000 train_loss: 47.89488983154297 test_loss:107.119873046875\n",
      "1184/3000 train_loss: 44.202880859375 test_loss:87.88035583496094\n",
      "1185/3000 train_loss: 39.40314483642578 test_loss:84.91727447509766\n",
      "1186/3000 train_loss: 43.22486114501953 test_loss:101.81843566894531\n",
      "1187/3000 train_loss: 40.87543869018555 test_loss:91.87499237060547\n",
      "1188/3000 train_loss: 39.949806213378906 test_loss:98.69683837890625\n",
      "1189/3000 train_loss: 44.82761764526367 test_loss:95.86641693115234\n",
      "1190/3000 train_loss: 44.59883117675781 test_loss:94.69610595703125\n",
      "1191/3000 train_loss: 44.84327697753906 test_loss:102.67471313476562\n",
      "1192/3000 train_loss: 43.28788375854492 test_loss:110.35325622558594\n",
      "1193/3000 train_loss: 52.29083251953125 test_loss:93.19723510742188\n",
      "1194/3000 train_loss: 53.82848358154297 test_loss:89.66084289550781\n",
      "1195/3000 train_loss: 40.474754333496094 test_loss:90.5651626586914\n",
      "1196/3000 train_loss: 51.141883850097656 test_loss:78.82250213623047\n",
      "1197/3000 train_loss: 38.557254791259766 test_loss:83.85701751708984\n",
      "1198/3000 train_loss: 38.858543395996094 test_loss:96.27091979980469\n",
      "1199/3000 train_loss: 39.665016174316406 test_loss:100.7168960571289\n",
      "1200/3000 train_loss: 43.38975143432617 test_loss:104.9359359741211\n",
      "1201/3000 train_loss: 45.53315734863281 test_loss:103.98066711425781\n",
      "1202/3000 train_loss: 50.35364532470703 test_loss:101.86600494384766\n",
      "1203/3000 train_loss: 43.439517974853516 test_loss:85.82464599609375\n",
      "1204/3000 train_loss: 40.16984176635742 test_loss:87.92613983154297\n",
      "1205/3000 train_loss: 37.05870819091797 test_loss:100.79391479492188\n",
      "1206/3000 train_loss: 46.010379791259766 test_loss:98.58204650878906\n",
      "1207/3000 train_loss: 40.37746810913086 test_loss:101.54995727539062\n",
      "1208/3000 train_loss: 40.919761657714844 test_loss:107.59571075439453\n",
      "1209/3000 train_loss: 46.61643981933594 test_loss:97.27301788330078\n",
      "1210/3000 train_loss: 45.221595764160156 test_loss:91.36124420166016\n",
      "1211/3000 train_loss: 34.95302963256836 test_loss:84.04585266113281\n",
      "1212/3000 train_loss: 42.26301193237305 test_loss:91.77383422851562\n",
      "1213/3000 train_loss: 41.39018249511719 test_loss:86.23594665527344\n",
      "1214/3000 train_loss: 43.6830940246582 test_loss:93.68914031982422\n",
      "1215/3000 train_loss: 38.455204010009766 test_loss:113.70278930664062\n",
      "1216/3000 train_loss: 42.875083923339844 test_loss:92.03424072265625\n",
      "1217/3000 train_loss: 40.814449310302734 test_loss:85.98311614990234\n",
      "1218/3000 train_loss: 40.7804069519043 test_loss:89.83580017089844\n",
      "1219/3000 train_loss: 53.601295471191406 test_loss:86.23103332519531\n",
      "1220/3000 train_loss: 41.16730499267578 test_loss:87.28277587890625\n",
      "1221/3000 train_loss: 40.69801330566406 test_loss:91.90750885009766\n",
      "1222/3000 train_loss: 41.68783187866211 test_loss:93.97322082519531\n",
      "1223/3000 train_loss: 51.6030387878418 test_loss:108.412109375\n",
      "1224/3000 train_loss: 42.16796112060547 test_loss:105.18417358398438\n",
      "1225/3000 train_loss: 39.89118194580078 test_loss:90.02909088134766\n",
      "1226/3000 train_loss: 41.84492111206055 test_loss:80.89443969726562\n",
      "1227/3000 train_loss: 39.75682830810547 test_loss:84.65956115722656\n",
      "1228/3000 train_loss: 47.09821701049805 test_loss:85.34217834472656\n",
      "1229/3000 train_loss: 42.19303512573242 test_loss:109.9487533569336\n",
      "1230/3000 train_loss: 37.36418914794922 test_loss:98.33425903320312\n",
      "1231/3000 train_loss: 40.541873931884766 test_loss:97.77706909179688\n",
      "1232/3000 train_loss: 52.300209045410156 test_loss:99.76361083984375\n",
      "1233/3000 train_loss: 37.96790313720703 test_loss:84.34230041503906\n",
      "1234/3000 train_loss: 47.740570068359375 test_loss:104.6497573852539\n",
      "1235/3000 train_loss: 41.86151123046875 test_loss:86.27618408203125\n",
      "1236/3000 train_loss: 39.83916091918945 test_loss:89.95616912841797\n",
      "1237/3000 train_loss: 39.49794387817383 test_loss:92.92756652832031\n",
      "1238/3000 train_loss: 47.14078903198242 test_loss:93.35812377929688\n",
      "1239/3000 train_loss: 46.32179260253906 test_loss:103.637939453125\n",
      "1240/3000 train_loss: 37.67581558227539 test_loss:89.0870361328125\n",
      "1241/3000 train_loss: 46.898223876953125 test_loss:126.51799774169922\n",
      "1242/3000 train_loss: 50.52031707763672 test_loss:96.97715759277344\n",
      "1243/3000 train_loss: 38.75975799560547 test_loss:93.02177429199219\n",
      "1244/3000 train_loss: 37.96092224121094 test_loss:108.02758026123047\n",
      "1245/3000 train_loss: 54.66824722290039 test_loss:85.96253967285156\n",
      "1246/3000 train_loss: 43.614776611328125 test_loss:83.69046020507812\n",
      "1247/3000 train_loss: 43.361690521240234 test_loss:86.30592346191406\n",
      "1248/3000 train_loss: 37.778778076171875 test_loss:88.10096740722656\n",
      "1249/3000 train_loss: 37.07944869995117 test_loss:91.57575988769531\n",
      "1250/3000 train_loss: 38.54793167114258 test_loss:93.62918090820312\n",
      "1251/3000 train_loss: 45.676849365234375 test_loss:87.68714904785156\n",
      "1252/3000 train_loss: 40.90946960449219 test_loss:89.91453552246094\n",
      "1253/3000 train_loss: 42.53239822387695 test_loss:100.11784362792969\n",
      "1254/3000 train_loss: 46.94563674926758 test_loss:83.19522094726562\n",
      "1255/3000 train_loss: 40.45161056518555 test_loss:94.7887954711914\n",
      "1256/3000 train_loss: 43.757240295410156 test_loss:80.73845672607422\n",
      "1257/3000 train_loss: 43.31975555419922 test_loss:106.13442993164062\n",
      "1258/3000 train_loss: 44.76695251464844 test_loss:96.60449981689453\n",
      "1259/3000 train_loss: 49.63139724731445 test_loss:97.18913269042969\n",
      "1260/3000 train_loss: 44.007686614990234 test_loss:91.92668151855469\n",
      "1261/3000 train_loss: 38.32245635986328 test_loss:84.19888305664062\n",
      "1262/3000 train_loss: 41.07368850708008 test_loss:84.87263488769531\n",
      "1263/3000 train_loss: 39.56755065917969 test_loss:89.42500305175781\n",
      "1264/3000 train_loss: 39.69620132446289 test_loss:93.6243667602539\n",
      "1265/3000 train_loss: 45.22776794433594 test_loss:86.68647766113281\n",
      "1266/3000 train_loss: 41.634586334228516 test_loss:92.96200561523438\n",
      "1267/3000 train_loss: 37.41806411743164 test_loss:85.55196380615234\n",
      "1268/3000 train_loss: 41.63753128051758 test_loss:82.35600280761719\n",
      "1269/3000 train_loss: 36.323726654052734 test_loss:94.27855682373047\n",
      "1270/3000 train_loss: 41.877479553222656 test_loss:92.81749725341797\n",
      "1271/3000 train_loss: 42.35009002685547 test_loss:87.44696807861328\n",
      "1272/3000 train_loss: 40.32402038574219 test_loss:93.18177032470703\n",
      "1273/3000 train_loss: 42.39186477661133 test_loss:120.97344970703125\n",
      "1274/3000 train_loss: 45.54875946044922 test_loss:83.9129638671875\n",
      "1275/3000 train_loss: 42.75755310058594 test_loss:97.9035873413086\n",
      "1276/3000 train_loss: 42.267555236816406 test_loss:94.73218536376953\n",
      "1277/3000 train_loss: 42.579307556152344 test_loss:95.40186309814453\n",
      "1278/3000 train_loss: 41.4229736328125 test_loss:97.29338073730469\n",
      "1279/3000 train_loss: 41.39789581298828 test_loss:93.9603042602539\n",
      "1280/3000 train_loss: 45.465667724609375 test_loss:83.72057342529297\n",
      "1281/3000 train_loss: 43.88839340209961 test_loss:112.0452880859375\n",
      "1282/3000 train_loss: 43.1536865234375 test_loss:98.53169250488281\n",
      "1283/3000 train_loss: 40.96495819091797 test_loss:87.84908294677734\n",
      "1284/3000 train_loss: 38.6279411315918 test_loss:105.83209228515625\n",
      "1285/3000 train_loss: 45.70253372192383 test_loss:92.80033111572266\n",
      "1286/3000 train_loss: 47.25564193725586 test_loss:89.71304321289062\n",
      "1287/3000 train_loss: 56.06106185913086 test_loss:92.79888916015625\n",
      "1288/3000 train_loss: 41.52970886230469 test_loss:79.80486297607422\n",
      "1289/3000 train_loss: 44.485355377197266 test_loss:75.30964660644531\n",
      "1290/3000 train_loss: 38.167232513427734 test_loss:83.96165466308594\n",
      "1291/3000 train_loss: 44.161521911621094 test_loss:112.12124633789062\n",
      "1292/3000 train_loss: 47.40155792236328 test_loss:84.28833770751953\n",
      "1293/3000 train_loss: 48.354095458984375 test_loss:142.9404296875\n",
      "1294/3000 train_loss: 54.879791259765625 test_loss:90.94274139404297\n",
      "1295/3000 train_loss: 49.57273864746094 test_loss:104.4531021118164\n",
      "1296/3000 train_loss: 44.467010498046875 test_loss:75.12421417236328\n",
      "1297/3000 train_loss: 38.72792434692383 test_loss:83.44865417480469\n",
      "1298/3000 train_loss: 39.84872817993164 test_loss:92.53651428222656\n",
      "1299/3000 train_loss: 48.923004150390625 test_loss:103.35418701171875\n",
      "1300/3000 train_loss: 39.65843200683594 test_loss:95.57438659667969\n",
      "1301/3000 train_loss: 38.1689453125 test_loss:79.19195556640625\n",
      "1302/3000 train_loss: 39.08745193481445 test_loss:86.54380798339844\n",
      "1303/3000 train_loss: 37.21728515625 test_loss:89.47359466552734\n",
      "1304/3000 train_loss: 38.72679901123047 test_loss:104.2131576538086\n",
      "1305/3000 train_loss: 45.45386505126953 test_loss:106.82750701904297\n",
      "1306/3000 train_loss: 51.5052490234375 test_loss:86.34716033935547\n",
      "1307/3000 train_loss: 46.28778839111328 test_loss:89.79569244384766\n",
      "1308/3000 train_loss: 48.97597885131836 test_loss:116.82767486572266\n",
      "1309/3000 train_loss: 48.4134635925293 test_loss:96.62837219238281\n",
      "1310/3000 train_loss: 48.37106704711914 test_loss:93.33556365966797\n",
      "1311/3000 train_loss: 39.885128021240234 test_loss:96.18690490722656\n",
      "1312/3000 train_loss: 36.08203887939453 test_loss:103.64300537109375\n",
      "1313/3000 train_loss: 38.685997009277344 test_loss:93.92874908447266\n",
      "1314/3000 train_loss: 44.06709671020508 test_loss:94.4768295288086\n",
      "1315/3000 train_loss: 44.68360137939453 test_loss:94.1550064086914\n",
      "1316/3000 train_loss: 46.4237060546875 test_loss:100.59326171875\n",
      "1317/3000 train_loss: 39.001991271972656 test_loss:88.5548324584961\n",
      "1318/3000 train_loss: 40.24568557739258 test_loss:84.77031707763672\n",
      "1319/3000 train_loss: 41.804630279541016 test_loss:106.13650512695312\n",
      "1320/3000 train_loss: 35.16508102416992 test_loss:100.56182861328125\n",
      "1321/3000 train_loss: 44.70946502685547 test_loss:91.59463500976562\n",
      "1322/3000 train_loss: 70.7325210571289 test_loss:92.90766906738281\n",
      "1323/3000 train_loss: 56.5025520324707 test_loss:96.33938598632812\n",
      "1324/3000 train_loss: 41.704978942871094 test_loss:92.69026184082031\n",
      "1325/3000 train_loss: 41.88878631591797 test_loss:101.87995147705078\n",
      "1326/3000 train_loss: 40.415321350097656 test_loss:83.82259368896484\n",
      "1327/3000 train_loss: 40.221805572509766 test_loss:86.26082611083984\n",
      "1328/3000 train_loss: 38.75225830078125 test_loss:80.27994537353516\n",
      "1329/3000 train_loss: 53.54408645629883 test_loss:86.79010772705078\n",
      "1330/3000 train_loss: 45.70475769042969 test_loss:88.64820861816406\n",
      "1331/3000 train_loss: 34.64249801635742 test_loss:77.78632354736328\n",
      "1332/3000 train_loss: 42.718753814697266 test_loss:100.1522216796875\n",
      "1333/3000 train_loss: 42.00632858276367 test_loss:89.60545349121094\n",
      "1334/3000 train_loss: 49.79328155517578 test_loss:104.3675308227539\n",
      "1335/3000 train_loss: 44.26849365234375 test_loss:96.08822631835938\n",
      "1336/3000 train_loss: 46.758888244628906 test_loss:84.4103775024414\n",
      "1337/3000 train_loss: 43.44966125488281 test_loss:86.26229858398438\n",
      "1338/3000 train_loss: 42.753719329833984 test_loss:81.9683837890625\n",
      "1339/3000 train_loss: 38.452301025390625 test_loss:83.34435272216797\n",
      "1340/3000 train_loss: 43.31483840942383 test_loss:95.60040283203125\n",
      "1341/3000 train_loss: 40.39109802246094 test_loss:95.54840850830078\n",
      "1342/3000 train_loss: 41.639251708984375 test_loss:82.96199798583984\n",
      "1343/3000 train_loss: 39.38807678222656 test_loss:118.76412200927734\n",
      "1344/3000 train_loss: 45.28839874267578 test_loss:94.86311340332031\n",
      "1345/3000 train_loss: 34.249507904052734 test_loss:89.3377456665039\n",
      "1346/3000 train_loss: 36.260345458984375 test_loss:83.20086669921875\n",
      "1347/3000 train_loss: 38.51993179321289 test_loss:89.93470764160156\n",
      "1348/3000 train_loss: 40.629661560058594 test_loss:84.24578094482422\n",
      "1349/3000 train_loss: 47.88371276855469 test_loss:97.00054931640625\n",
      "1350/3000 train_loss: 42.21979522705078 test_loss:102.70282745361328\n",
      "1351/3000 train_loss: 50.40674591064453 test_loss:92.13126373291016\n",
      "1352/3000 train_loss: 40.510894775390625 test_loss:92.00112915039062\n",
      "1353/3000 train_loss: 41.56016159057617 test_loss:92.92692565917969\n",
      "1354/3000 train_loss: 49.44554138183594 test_loss:83.68295288085938\n",
      "1355/3000 train_loss: 53.18882369995117 test_loss:107.84233093261719\n",
      "1356/3000 train_loss: 46.96800231933594 test_loss:88.48811340332031\n",
      "1357/3000 train_loss: 41.05870819091797 test_loss:85.5023193359375\n",
      "1358/3000 train_loss: 33.86225891113281 test_loss:75.05303192138672\n",
      "1359/3000 train_loss: 38.799774169921875 test_loss:114.40227508544922\n",
      "1360/3000 train_loss: 37.530643463134766 test_loss:90.63188934326172\n",
      "1361/3000 train_loss: 37.286468505859375 test_loss:81.16786193847656\n",
      "1362/3000 train_loss: 37.110538482666016 test_loss:92.74694061279297\n",
      "1363/3000 train_loss: 39.97657012939453 test_loss:90.19246673583984\n",
      "1364/3000 train_loss: 43.02629470825195 test_loss:99.67108917236328\n",
      "1365/3000 train_loss: 41.09687423706055 test_loss:100.08744049072266\n",
      "1366/3000 train_loss: 40.80445098876953 test_loss:80.96271514892578\n",
      "1367/3000 train_loss: 42.16881561279297 test_loss:92.71438598632812\n",
      "1368/3000 train_loss: 43.63214111328125 test_loss:89.60862731933594\n",
      "1369/3000 train_loss: 44.00639724731445 test_loss:108.3118896484375\n",
      "1370/3000 train_loss: 44.77674102783203 test_loss:76.2177963256836\n",
      "1371/3000 train_loss: 45.85225296020508 test_loss:94.07657623291016\n",
      "1372/3000 train_loss: 38.95270538330078 test_loss:83.70478057861328\n",
      "1373/3000 train_loss: 45.69959259033203 test_loss:84.24749755859375\n",
      "1374/3000 train_loss: 40.911651611328125 test_loss:89.55315399169922\n",
      "1375/3000 train_loss: 41.201961517333984 test_loss:90.81024169921875\n",
      "1376/3000 train_loss: 36.531578063964844 test_loss:80.58631134033203\n",
      "1377/3000 train_loss: 38.976619720458984 test_loss:78.9510498046875\n",
      "1378/3000 train_loss: 41.45383834838867 test_loss:75.13660430908203\n",
      "1379/3000 train_loss: 35.31181335449219 test_loss:81.20732116699219\n",
      "1380/3000 train_loss: 39.09776306152344 test_loss:92.48249053955078\n",
      "1381/3000 train_loss: 34.07366180419922 test_loss:89.68196868896484\n",
      "1382/3000 train_loss: 46.76711654663086 test_loss:81.2298812866211\n",
      "1383/3000 train_loss: 52.15203094482422 test_loss:98.37812042236328\n",
      "1384/3000 train_loss: 37.97494888305664 test_loss:80.4122314453125\n",
      "1385/3000 train_loss: 39.253082275390625 test_loss:83.05716705322266\n",
      "1386/3000 train_loss: 39.342533111572266 test_loss:76.03665924072266\n",
      "1387/3000 train_loss: 36.08554458618164 test_loss:87.99604797363281\n",
      "1388/3000 train_loss: 37.951377868652344 test_loss:92.0908203125\n",
      "1389/3000 train_loss: 37.12770080566406 test_loss:83.47883605957031\n",
      "1390/3000 train_loss: 37.23304748535156 test_loss:85.20462036132812\n",
      "1391/3000 train_loss: 53.36501693725586 test_loss:83.56132507324219\n",
      "1392/3000 train_loss: 50.82279968261719 test_loss:88.7681655883789\n",
      "1393/3000 train_loss: 40.79092025756836 test_loss:88.03596496582031\n",
      "1394/3000 train_loss: 34.827728271484375 test_loss:101.84126281738281\n",
      "1395/3000 train_loss: 39.559478759765625 test_loss:77.69519805908203\n",
      "1396/3000 train_loss: 40.10380935668945 test_loss:72.97351837158203\n",
      "1397/3000 train_loss: 41.43836975097656 test_loss:98.4197006225586\n",
      "1398/3000 train_loss: 45.46401596069336 test_loss:86.23950958251953\n",
      "1399/3000 train_loss: 44.203033447265625 test_loss:84.30180358886719\n",
      "1400/3000 train_loss: 44.31013107299805 test_loss:81.87081909179688\n",
      "1401/3000 train_loss: 43.72671890258789 test_loss:84.69983673095703\n",
      "1402/3000 train_loss: 38.78553771972656 test_loss:74.10063171386719\n",
      "1403/3000 train_loss: 37.64808654785156 test_loss:84.4190902709961\n",
      "1404/3000 train_loss: 41.08348083496094 test_loss:86.3369369506836\n",
      "1405/3000 train_loss: 42.535736083984375 test_loss:75.780029296875\n",
      "1406/3000 train_loss: 34.00925064086914 test_loss:85.07952117919922\n",
      "1407/3000 train_loss: 35.71851348876953 test_loss:90.71472930908203\n",
      "1408/3000 train_loss: 37.72064208984375 test_loss:83.8913345336914\n",
      "1409/3000 train_loss: 47.23006057739258 test_loss:85.25762939453125\n",
      "1410/3000 train_loss: 49.12565231323242 test_loss:81.31848907470703\n",
      "1411/3000 train_loss: 42.65562438964844 test_loss:88.29383087158203\n",
      "1412/3000 train_loss: 47.67438507080078 test_loss:86.85816192626953\n",
      "1413/3000 train_loss: 48.45472717285156 test_loss:102.61389923095703\n",
      "1414/3000 train_loss: 53.04198455810547 test_loss:94.90901184082031\n",
      "1415/3000 train_loss: 47.19938278198242 test_loss:87.21519470214844\n",
      "1416/3000 train_loss: 41.56002426147461 test_loss:79.89826965332031\n",
      "1417/3000 train_loss: 39.47331237792969 test_loss:79.20625305175781\n",
      "1418/3000 train_loss: 44.044246673583984 test_loss:79.38815307617188\n",
      "1419/3000 train_loss: 37.06854248046875 test_loss:90.49152374267578\n",
      "1420/3000 train_loss: 35.614418029785156 test_loss:87.16123962402344\n",
      "1421/3000 train_loss: 37.91797637939453 test_loss:74.6792984008789\n",
      "1422/3000 train_loss: 42.43463134765625 test_loss:92.6091537475586\n",
      "1423/3000 train_loss: 40.97836685180664 test_loss:103.82072448730469\n",
      "1424/3000 train_loss: 44.778350830078125 test_loss:87.0815200805664\n",
      "1425/3000 train_loss: 45.41252517700195 test_loss:94.83863830566406\n",
      "1426/3000 train_loss: 41.94068145751953 test_loss:90.93083190917969\n",
      "1427/3000 train_loss: 41.61874008178711 test_loss:101.55838012695312\n",
      "1428/3000 train_loss: 37.225975036621094 test_loss:89.3307876586914\n",
      "1429/3000 train_loss: 40.6296501159668 test_loss:96.80226135253906\n",
      "1430/3000 train_loss: 38.307403564453125 test_loss:80.915771484375\n",
      "1431/3000 train_loss: 45.20912170410156 test_loss:87.11873626708984\n",
      "1432/3000 train_loss: 36.5769157409668 test_loss:80.83821868896484\n",
      "1433/3000 train_loss: 36.9701042175293 test_loss:82.905029296875\n",
      "1434/3000 train_loss: 36.63946533203125 test_loss:96.94342041015625\n",
      "1435/3000 train_loss: 39.70673751831055 test_loss:87.11554718017578\n",
      "1436/3000 train_loss: 40.579383850097656 test_loss:100.66635131835938\n",
      "1437/3000 train_loss: 41.26161575317383 test_loss:81.68704986572266\n",
      "1438/3000 train_loss: 43.402503967285156 test_loss:83.12191772460938\n",
      "1439/3000 train_loss: 38.686729431152344 test_loss:103.85462188720703\n",
      "1440/3000 train_loss: 35.59503173828125 test_loss:84.40240478515625\n",
      "1441/3000 train_loss: 47.43110656738281 test_loss:94.96924591064453\n",
      "1442/3000 train_loss: 42.19328689575195 test_loss:81.139892578125\n",
      "1443/3000 train_loss: 37.745399475097656 test_loss:86.83245849609375\n",
      "1444/3000 train_loss: 37.362064361572266 test_loss:78.54130554199219\n",
      "1445/3000 train_loss: 37.3098030090332 test_loss:77.10038757324219\n",
      "1446/3000 train_loss: 32.2666015625 test_loss:85.7810287475586\n",
      "1447/3000 train_loss: 39.20598602294922 test_loss:103.75883483886719\n",
      "1448/3000 train_loss: 40.01548385620117 test_loss:86.46920776367188\n",
      "1449/3000 train_loss: 50.545589447021484 test_loss:103.35391998291016\n",
      "1450/3000 train_loss: 45.203857421875 test_loss:81.33467102050781\n",
      "1451/3000 train_loss: 47.02283477783203 test_loss:85.62553405761719\n",
      "1452/3000 train_loss: 40.64973449707031 test_loss:79.84452819824219\n",
      "1453/3000 train_loss: 41.711143493652344 test_loss:85.23629760742188\n",
      "1454/3000 train_loss: 33.38627243041992 test_loss:91.55142974853516\n",
      "1455/3000 train_loss: 42.46710205078125 test_loss:98.13497924804688\n",
      "1456/3000 train_loss: 36.435829162597656 test_loss:89.74774169921875\n",
      "1457/3000 train_loss: 34.7410888671875 test_loss:87.84968566894531\n",
      "1458/3000 train_loss: 34.491783142089844 test_loss:76.24761962890625\n",
      "1459/3000 train_loss: 36.080684661865234 test_loss:78.14439392089844\n",
      "1460/3000 train_loss: 34.683860778808594 test_loss:72.81491088867188\n",
      "1461/3000 train_loss: 41.71211242675781 test_loss:86.08386993408203\n",
      "1462/3000 train_loss: 37.646690368652344 test_loss:82.38068389892578\n",
      "1463/3000 train_loss: 44.159481048583984 test_loss:106.32125854492188\n",
      "1464/3000 train_loss: 50.27715301513672 test_loss:88.12957763671875\n",
      "1465/3000 train_loss: 44.26812744140625 test_loss:86.85498046875\n",
      "1466/3000 train_loss: 40.030269622802734 test_loss:81.08940124511719\n",
      "1467/3000 train_loss: 33.530029296875 test_loss:79.84143829345703\n",
      "1468/3000 train_loss: 38.59959411621094 test_loss:80.09164428710938\n",
      "1469/3000 train_loss: 38.79304504394531 test_loss:86.60049438476562\n",
      "1470/3000 train_loss: 34.09204864501953 test_loss:77.62944030761719\n",
      "1471/3000 train_loss: 37.202640533447266 test_loss:83.8910903930664\n",
      "1472/3000 train_loss: 41.165931701660156 test_loss:88.10359954833984\n",
      "1473/3000 train_loss: 38.276763916015625 test_loss:85.35137939453125\n",
      "1474/3000 train_loss: 38.895694732666016 test_loss:78.85750579833984\n",
      "1475/3000 train_loss: 42.03533935546875 test_loss:84.69145965576172\n",
      "1476/3000 train_loss: 40.4747428894043 test_loss:74.33946990966797\n",
      "1477/3000 train_loss: 32.95124053955078 test_loss:84.21653747558594\n",
      "1478/3000 train_loss: 35.2688102722168 test_loss:76.66665649414062\n",
      "1479/3000 train_loss: 40.7745361328125 test_loss:79.22735595703125\n",
      "1480/3000 train_loss: 38.76221466064453 test_loss:98.94864654541016\n",
      "1481/3000 train_loss: 37.827430725097656 test_loss:76.93134307861328\n",
      "1482/3000 train_loss: 42.49628448486328 test_loss:92.4515609741211\n",
      "1483/3000 train_loss: 40.540748596191406 test_loss:77.63752746582031\n",
      "1484/3000 train_loss: 41.01754379272461 test_loss:80.60871887207031\n",
      "1485/3000 train_loss: 40.05460739135742 test_loss:89.507568359375\n",
      "1486/3000 train_loss: 40.216712951660156 test_loss:88.49747467041016\n",
      "1487/3000 train_loss: 36.25399398803711 test_loss:79.71954345703125\n",
      "1488/3000 train_loss: 37.5091552734375 test_loss:84.53359985351562\n",
      "1489/3000 train_loss: 39.634552001953125 test_loss:79.59281158447266\n",
      "1490/3000 train_loss: 34.66933822631836 test_loss:89.06421661376953\n",
      "1491/3000 train_loss: 39.27803039550781 test_loss:92.91089630126953\n",
      "1492/3000 train_loss: 31.71617317199707 test_loss:88.15453338623047\n",
      "1493/3000 train_loss: 35.51533126831055 test_loss:93.63410949707031\n",
      "1494/3000 train_loss: 49.96961975097656 test_loss:94.08959197998047\n",
      "1495/3000 train_loss: 53.98455047607422 test_loss:83.94730377197266\n",
      "1496/3000 train_loss: 39.3494758605957 test_loss:80.73565673828125\n",
      "1497/3000 train_loss: 44.28765869140625 test_loss:86.7766342163086\n",
      "1498/3000 train_loss: 35.23150634765625 test_loss:84.37482452392578\n",
      "1499/3000 train_loss: 40.39522171020508 test_loss:90.60633087158203\n",
      "1500/3000 train_loss: 36.403743743896484 test_loss:85.744873046875\n",
      "1501/3000 train_loss: 37.56196594238281 test_loss:91.96646881103516\n",
      "1502/3000 train_loss: 38.349082946777344 test_loss:80.9982681274414\n",
      "1503/3000 train_loss: 38.417938232421875 test_loss:83.0462875366211\n",
      "1504/3000 train_loss: 39.248477935791016 test_loss:86.9550552368164\n",
      "1505/3000 train_loss: 38.24543380737305 test_loss:84.85194396972656\n",
      "1506/3000 train_loss: 40.60007095336914 test_loss:86.39695739746094\n",
      "1507/3000 train_loss: 41.27284622192383 test_loss:83.39883422851562\n",
      "1508/3000 train_loss: 42.336483001708984 test_loss:81.30570983886719\n",
      "1509/3000 train_loss: 36.810298919677734 test_loss:82.42324829101562\n",
      "1510/3000 train_loss: 46.539947509765625 test_loss:77.26237487792969\n",
      "1511/3000 train_loss: 32.851959228515625 test_loss:82.05549621582031\n",
      "1512/3000 train_loss: 37.43525695800781 test_loss:87.48516845703125\n",
      "1513/3000 train_loss: 38.78592300415039 test_loss:82.9850082397461\n",
      "1514/3000 train_loss: 42.516780853271484 test_loss:88.61369323730469\n",
      "1515/3000 train_loss: 39.1025390625 test_loss:84.3104019165039\n",
      "1516/3000 train_loss: 37.50151062011719 test_loss:90.40081787109375\n",
      "1517/3000 train_loss: 41.05352783203125 test_loss:117.24671936035156\n",
      "1518/3000 train_loss: 37.712074279785156 test_loss:72.75373077392578\n",
      "1519/3000 train_loss: 35.92385482788086 test_loss:92.36922454833984\n",
      "1520/3000 train_loss: 35.07563781738281 test_loss:83.19499206542969\n",
      "1521/3000 train_loss: 35.72296905517578 test_loss:88.30654907226562\n",
      "1522/3000 train_loss: 38.99623107910156 test_loss:76.675537109375\n",
      "1523/3000 train_loss: 42.85096740722656 test_loss:78.61298370361328\n",
      "1524/3000 train_loss: 32.39567184448242 test_loss:89.94996643066406\n",
      "1525/3000 train_loss: 38.484073638916016 test_loss:81.38983917236328\n",
      "1526/3000 train_loss: 51.933162689208984 test_loss:101.71954345703125\n",
      "1527/3000 train_loss: 34.664878845214844 test_loss:104.6939926147461\n",
      "1528/3000 train_loss: 45.34988784790039 test_loss:93.17664337158203\n",
      "1529/3000 train_loss: 46.264427185058594 test_loss:86.44561767578125\n",
      "1530/3000 train_loss: 44.85327911376953 test_loss:101.74077606201172\n",
      "1531/3000 train_loss: 42.36457061767578 test_loss:85.77191162109375\n",
      "1532/3000 train_loss: 34.22383117675781 test_loss:82.68978881835938\n",
      "1533/3000 train_loss: 36.74634552001953 test_loss:89.75418853759766\n",
      "1534/3000 train_loss: 36.35266876220703 test_loss:80.79757690429688\n",
      "1535/3000 train_loss: 36.78230285644531 test_loss:76.84043884277344\n",
      "1536/3000 train_loss: 37.51179122924805 test_loss:80.7075424194336\n",
      "1537/3000 train_loss: 37.86480712890625 test_loss:81.85845947265625\n",
      "1538/3000 train_loss: 40.443450927734375 test_loss:91.61371612548828\n",
      "1539/3000 train_loss: 33.72562026977539 test_loss:82.1551742553711\n",
      "1540/3000 train_loss: 35.53782653808594 test_loss:78.7433853149414\n",
      "1541/3000 train_loss: 34.310401916503906 test_loss:83.14306640625\n",
      "1542/3000 train_loss: 33.528472900390625 test_loss:76.63082122802734\n",
      "1543/3000 train_loss: 39.00136184692383 test_loss:84.20252227783203\n",
      "1544/3000 train_loss: 38.39954376220703 test_loss:78.59931945800781\n",
      "1545/3000 train_loss: 46.24402618408203 test_loss:87.36048126220703\n",
      "1546/3000 train_loss: 37.685428619384766 test_loss:85.3094711303711\n",
      "1547/3000 train_loss: 33.39301300048828 test_loss:75.22723388671875\n",
      "1548/3000 train_loss: 37.25968551635742 test_loss:87.73442840576172\n",
      "1549/3000 train_loss: 36.401512145996094 test_loss:77.00532531738281\n",
      "1550/3000 train_loss: 42.91007995605469 test_loss:84.03119659423828\n",
      "1551/3000 train_loss: 43.56528091430664 test_loss:77.84130096435547\n",
      "1552/3000 train_loss: 41.46118927001953 test_loss:91.66693115234375\n",
      "1553/3000 train_loss: 35.95684814453125 test_loss:89.2733383178711\n",
      "1554/3000 train_loss: 43.141319274902344 test_loss:87.55025482177734\n",
      "1555/3000 train_loss: 37.349365234375 test_loss:71.11369323730469\n",
      "1556/3000 train_loss: 33.54637908935547 test_loss:87.59225463867188\n",
      "1557/3000 train_loss: 42.829498291015625 test_loss:93.78045654296875\n",
      "1558/3000 train_loss: 38.99778747558594 test_loss:77.28657531738281\n",
      "1559/3000 train_loss: 30.4035587310791 test_loss:72.48336029052734\n",
      "1560/3000 train_loss: 36.79016876220703 test_loss:75.52558898925781\n",
      "1561/3000 train_loss: 32.50197219848633 test_loss:98.41357421875\n",
      "1562/3000 train_loss: 36.66752624511719 test_loss:83.6983871459961\n",
      "1563/3000 train_loss: 35.455711364746094 test_loss:85.95382690429688\n",
      "1564/3000 train_loss: 34.801753997802734 test_loss:75.12789154052734\n",
      "1565/3000 train_loss: 43.38698959350586 test_loss:97.7769546508789\n",
      "1566/3000 train_loss: 44.11135482788086 test_loss:85.68626403808594\n",
      "1567/3000 train_loss: 37.37941360473633 test_loss:83.64872741699219\n",
      "1568/3000 train_loss: 34.6928825378418 test_loss:82.22898864746094\n",
      "1569/3000 train_loss: 44.56090545654297 test_loss:73.30015563964844\n",
      "1570/3000 train_loss: 41.63107681274414 test_loss:82.40249633789062\n",
      "1571/3000 train_loss: 36.69032287597656 test_loss:97.289794921875\n",
      "1572/3000 train_loss: 47.08623504638672 test_loss:95.49226379394531\n",
      "1573/3000 train_loss: 40.17923355102539 test_loss:94.15217590332031\n",
      "1574/3000 train_loss: 39.25222396850586 test_loss:101.13672637939453\n",
      "1575/3000 train_loss: 34.929443359375 test_loss:81.48316955566406\n",
      "1576/3000 train_loss: 38.549190521240234 test_loss:74.95531463623047\n",
      "1577/3000 train_loss: 39.270626068115234 test_loss:99.08717346191406\n",
      "1578/3000 train_loss: 43.29656982421875 test_loss:86.60184478759766\n",
      "1579/3000 train_loss: 38.46613693237305 test_loss:89.65514373779297\n",
      "1580/3000 train_loss: 41.4127311706543 test_loss:85.74834442138672\n",
      "1581/3000 train_loss: 37.06721496582031 test_loss:89.51921844482422\n",
      "1582/3000 train_loss: 39.00242233276367 test_loss:82.52266693115234\n",
      "1583/3000 train_loss: 36.485008239746094 test_loss:88.05115509033203\n",
      "1584/3000 train_loss: 40.542625427246094 test_loss:75.47859191894531\n",
      "1585/3000 train_loss: 38.410301208496094 test_loss:78.10218811035156\n",
      "1586/3000 train_loss: 34.7789306640625 test_loss:74.85538482666016\n",
      "1587/3000 train_loss: 32.48957824707031 test_loss:79.56070709228516\n",
      "1588/3000 train_loss: 41.838134765625 test_loss:95.84114074707031\n",
      "1589/3000 train_loss: 41.385948181152344 test_loss:88.70423889160156\n",
      "1590/3000 train_loss: 39.83563232421875 test_loss:86.21122741699219\n",
      "1591/3000 train_loss: 39.496307373046875 test_loss:82.87313079833984\n",
      "1592/3000 train_loss: 33.324825286865234 test_loss:91.65550231933594\n",
      "1593/3000 train_loss: 39.46000289916992 test_loss:82.70087432861328\n",
      "1594/3000 train_loss: 35.45343017578125 test_loss:84.3831787109375\n",
      "1595/3000 train_loss: 49.14332580566406 test_loss:80.41739654541016\n",
      "1596/3000 train_loss: 37.89451217651367 test_loss:77.9970932006836\n",
      "1597/3000 train_loss: 36.058082580566406 test_loss:89.69447326660156\n",
      "1598/3000 train_loss: 42.29526138305664 test_loss:92.08458709716797\n",
      "1599/3000 train_loss: 38.82533645629883 test_loss:80.75807189941406\n",
      "1600/3000 train_loss: 36.74574661254883 test_loss:97.81965637207031\n",
      "1601/3000 train_loss: 48.46734619140625 test_loss:97.29798126220703\n",
      "1602/3000 train_loss: 38.03141784667969 test_loss:76.23802947998047\n",
      "1603/3000 train_loss: 33.591705322265625 test_loss:83.81366729736328\n",
      "1604/3000 train_loss: 36.37595748901367 test_loss:78.07604217529297\n",
      "1605/3000 train_loss: 39.93040084838867 test_loss:88.3814926147461\n",
      "1606/3000 train_loss: 40.72593688964844 test_loss:102.72350311279297\n",
      "1607/3000 train_loss: 36.01484298706055 test_loss:83.80767822265625\n",
      "1608/3000 train_loss: 40.7236328125 test_loss:81.67537689208984\n",
      "1609/3000 train_loss: 42.71718978881836 test_loss:97.82001495361328\n",
      "1610/3000 train_loss: 39.52952194213867 test_loss:88.74483489990234\n",
      "1611/3000 train_loss: 41.0675048828125 test_loss:100.42781066894531\n",
      "1612/3000 train_loss: 42.08938980102539 test_loss:96.94440460205078\n",
      "1613/3000 train_loss: 33.58972930908203 test_loss:87.35002899169922\n",
      "1614/3000 train_loss: 29.84180450439453 test_loss:88.81832885742188\n",
      "1615/3000 train_loss: 34.290096282958984 test_loss:82.38361358642578\n",
      "1616/3000 train_loss: 37.39527130126953 test_loss:92.62792205810547\n",
      "1617/3000 train_loss: 41.05851364135742 test_loss:97.62332916259766\n",
      "1618/3000 train_loss: 32.72553253173828 test_loss:88.98323822021484\n",
      "1619/3000 train_loss: 59.43288040161133 test_loss:83.35253143310547\n",
      "1620/3000 train_loss: 39.74317169189453 test_loss:72.00391387939453\n",
      "1621/3000 train_loss: 39.22968292236328 test_loss:97.58485412597656\n",
      "1622/3000 train_loss: 40.80211639404297 test_loss:78.68077850341797\n",
      "1623/3000 train_loss: 36.63554000854492 test_loss:82.94355773925781\n",
      "1624/3000 train_loss: 37.70101547241211 test_loss:85.84089660644531\n",
      "1625/3000 train_loss: 39.59263610839844 test_loss:85.28783416748047\n",
      "1626/3000 train_loss: 37.68435287475586 test_loss:79.28375244140625\n",
      "1627/3000 train_loss: 36.43288040161133 test_loss:87.82403564453125\n",
      "1628/3000 train_loss: 38.598602294921875 test_loss:85.59476470947266\n",
      "1629/3000 train_loss: 38.525203704833984 test_loss:83.1676025390625\n",
      "1630/3000 train_loss: 38.916751861572266 test_loss:84.2306900024414\n",
      "1631/3000 train_loss: 38.119178771972656 test_loss:93.82563018798828\n",
      "1632/3000 train_loss: 35.01714324951172 test_loss:100.08819580078125\n",
      "1633/3000 train_loss: 36.84983825683594 test_loss:79.62909698486328\n",
      "1634/3000 train_loss: 35.80095291137695 test_loss:73.45270538330078\n",
      "1635/3000 train_loss: 35.242652893066406 test_loss:93.70603942871094\n",
      "1636/3000 train_loss: 45.454994201660156 test_loss:96.3464126586914\n",
      "1637/3000 train_loss: 46.116485595703125 test_loss:79.76377868652344\n",
      "1638/3000 train_loss: 36.661582946777344 test_loss:84.76073455810547\n",
      "1639/3000 train_loss: 41.181358337402344 test_loss:92.83503723144531\n",
      "1640/3000 train_loss: 34.02977752685547 test_loss:87.46277618408203\n",
      "1641/3000 train_loss: 36.013145446777344 test_loss:73.59042358398438\n",
      "1642/3000 train_loss: 39.597869873046875 test_loss:78.68243408203125\n",
      "1643/3000 train_loss: 37.866127014160156 test_loss:84.51673889160156\n",
      "1644/3000 train_loss: 41.76664352416992 test_loss:90.51470947265625\n",
      "1645/3000 train_loss: 41.615779876708984 test_loss:88.0076675415039\n",
      "1646/3000 train_loss: 42.21067428588867 test_loss:77.40142059326172\n",
      "1647/3000 train_loss: 43.647491455078125 test_loss:85.6454849243164\n",
      "1648/3000 train_loss: 52.547916412353516 test_loss:83.95709228515625\n",
      "1649/3000 train_loss: 43.35430145263672 test_loss:97.72296905517578\n",
      "1650/3000 train_loss: 40.61222839355469 test_loss:78.01139068603516\n",
      "1651/3000 train_loss: 37.56216049194336 test_loss:80.94259643554688\n",
      "1652/3000 train_loss: 38.475032806396484 test_loss:82.37765502929688\n",
      "1653/3000 train_loss: 37.52412033081055 test_loss:90.16937255859375\n",
      "1654/3000 train_loss: 33.54254150390625 test_loss:82.84034729003906\n",
      "1655/3000 train_loss: 37.386680603027344 test_loss:80.91170501708984\n",
      "1656/3000 train_loss: 41.4570198059082 test_loss:81.96612548828125\n",
      "1657/3000 train_loss: 38.560089111328125 test_loss:78.6884994506836\n",
      "1658/3000 train_loss: 35.05072784423828 test_loss:79.77152252197266\n",
      "1659/3000 train_loss: 40.81788635253906 test_loss:89.66888427734375\n",
      "1660/3000 train_loss: 52.357818603515625 test_loss:82.12006378173828\n",
      "1661/3000 train_loss: 34.946693420410156 test_loss:92.04167938232422\n",
      "1662/3000 train_loss: 34.68218231201172 test_loss:79.35714721679688\n",
      "1663/3000 train_loss: 39.08245849609375 test_loss:91.22496032714844\n",
      "1664/3000 train_loss: 36.34144973754883 test_loss:72.43052673339844\n",
      "1665/3000 train_loss: 35.61672592163086 test_loss:81.66079711914062\n",
      "1666/3000 train_loss: 35.112091064453125 test_loss:81.91455078125\n",
      "1667/3000 train_loss: 36.325706481933594 test_loss:91.48692321777344\n",
      "1668/3000 train_loss: 33.239837646484375 test_loss:80.2968978881836\n",
      "1669/3000 train_loss: 40.062496185302734 test_loss:96.10706329345703\n",
      "1670/3000 train_loss: 38.64268493652344 test_loss:82.43315124511719\n",
      "1671/3000 train_loss: 37.21217346191406 test_loss:99.41849517822266\n",
      "1672/3000 train_loss: 36.48380661010742 test_loss:71.96217346191406\n",
      "1673/3000 train_loss: 43.91044616699219 test_loss:79.92668151855469\n",
      "1674/3000 train_loss: 35.0367317199707 test_loss:83.25914001464844\n",
      "1675/3000 train_loss: 43.73208999633789 test_loss:116.46154022216797\n",
      "1676/3000 train_loss: 36.05101776123047 test_loss:88.12113189697266\n",
      "1677/3000 train_loss: 37.790138244628906 test_loss:87.38897705078125\n",
      "1678/3000 train_loss: 34.86833953857422 test_loss:74.50841522216797\n",
      "1679/3000 train_loss: 35.004512786865234 test_loss:81.67919921875\n",
      "1680/3000 train_loss: 37.084476470947266 test_loss:74.3985824584961\n",
      "1681/3000 train_loss: 35.28843688964844 test_loss:82.50794219970703\n",
      "1682/3000 train_loss: 43.91961669921875 test_loss:92.24760437011719\n",
      "1683/3000 train_loss: 36.5966796875 test_loss:78.42681884765625\n",
      "1684/3000 train_loss: 36.3411750793457 test_loss:89.38594055175781\n",
      "1685/3000 train_loss: 36.283382415771484 test_loss:77.75945281982422\n",
      "1686/3000 train_loss: 44.994529724121094 test_loss:86.64004516601562\n",
      "1687/3000 train_loss: 35.399662017822266 test_loss:77.39852905273438\n",
      "1688/3000 train_loss: 33.02226638793945 test_loss:94.22772216796875\n",
      "1689/3000 train_loss: 33.31417465209961 test_loss:84.18000030517578\n",
      "1690/3000 train_loss: 34.07823181152344 test_loss:93.92887878417969\n",
      "1691/3000 train_loss: 34.997657775878906 test_loss:91.59024047851562\n",
      "1692/3000 train_loss: 36.010345458984375 test_loss:76.42111206054688\n",
      "1693/3000 train_loss: 35.2138671875 test_loss:88.15382385253906\n",
      "1694/3000 train_loss: 37.96040725708008 test_loss:79.5231704711914\n",
      "1695/3000 train_loss: 35.55278778076172 test_loss:109.45938110351562\n",
      "1696/3000 train_loss: 38.911376953125 test_loss:85.81922149658203\n",
      "1697/3000 train_loss: 34.81011962890625 test_loss:85.57671356201172\n",
      "1698/3000 train_loss: 36.7298698425293 test_loss:82.10061645507812\n",
      "1699/3000 train_loss: 40.157737731933594 test_loss:87.85975646972656\n",
      "1700/3000 train_loss: 36.884376525878906 test_loss:86.91789245605469\n",
      "1701/3000 train_loss: 33.57457733154297 test_loss:80.26553344726562\n",
      "1702/3000 train_loss: 48.3723030090332 test_loss:77.43161010742188\n",
      "1703/3000 train_loss: 45.938323974609375 test_loss:98.74388122558594\n",
      "1704/3000 train_loss: 39.75522994995117 test_loss:74.14202880859375\n",
      "1705/3000 train_loss: 39.86742401123047 test_loss:99.35026550292969\n",
      "1706/3000 train_loss: 43.61845397949219 test_loss:80.08518981933594\n",
      "1707/3000 train_loss: 37.348960876464844 test_loss:77.42665100097656\n",
      "1708/3000 train_loss: 45.096744537353516 test_loss:114.93315124511719\n",
      "1709/3000 train_loss: 45.01218032836914 test_loss:80.46118927001953\n",
      "1710/3000 train_loss: 39.46699142456055 test_loss:78.76983642578125\n",
      "1711/3000 train_loss: 34.852088928222656 test_loss:73.97158813476562\n",
      "1712/3000 train_loss: 37.9299430847168 test_loss:118.87955474853516\n",
      "1713/3000 train_loss: 39.76948547363281 test_loss:78.24663543701172\n",
      "1714/3000 train_loss: 41.27836990356445 test_loss:81.81228637695312\n",
      "1715/3000 train_loss: 36.98179626464844 test_loss:81.78181457519531\n",
      "1716/3000 train_loss: 34.908721923828125 test_loss:84.09532928466797\n",
      "1717/3000 train_loss: 42.226837158203125 test_loss:81.73717498779297\n",
      "1718/3000 train_loss: 39.67573165893555 test_loss:89.71480560302734\n",
      "1719/3000 train_loss: 50.47987365722656 test_loss:99.97137451171875\n",
      "1720/3000 train_loss: 38.50693130493164 test_loss:87.92129516601562\n",
      "1721/3000 train_loss: 38.63101577758789 test_loss:85.45101928710938\n",
      "1722/3000 train_loss: 37.55230712890625 test_loss:101.53244018554688\n",
      "1723/3000 train_loss: 42.38739776611328 test_loss:72.93436431884766\n",
      "1724/3000 train_loss: 34.162845611572266 test_loss:80.56881713867188\n",
      "1725/3000 train_loss: 36.9621696472168 test_loss:85.26123046875\n",
      "1726/3000 train_loss: 32.59751510620117 test_loss:89.04795837402344\n",
      "1727/3000 train_loss: 42.30500793457031 test_loss:91.88522338867188\n",
      "1728/3000 train_loss: 44.62860870361328 test_loss:92.44143676757812\n",
      "1729/3000 train_loss: 39.87263107299805 test_loss:77.60476684570312\n",
      "1730/3000 train_loss: 34.482933044433594 test_loss:98.76458740234375\n",
      "1731/3000 train_loss: 37.73665237426758 test_loss:85.37496185302734\n",
      "1732/3000 train_loss: 34.17586898803711 test_loss:84.54002380371094\n",
      "1733/3000 train_loss: 37.4248161315918 test_loss:96.30877685546875\n",
      "1734/3000 train_loss: 41.45524597167969 test_loss:90.95339965820312\n",
      "1735/3000 train_loss: 47.69374465942383 test_loss:83.01820373535156\n",
      "1736/3000 train_loss: 40.968753814697266 test_loss:89.07158660888672\n",
      "1737/3000 train_loss: 36.072601318359375 test_loss:84.98767852783203\n",
      "1738/3000 train_loss: 32.6799201965332 test_loss:78.5182876586914\n",
      "1739/3000 train_loss: 40.792049407958984 test_loss:95.04524230957031\n",
      "1740/3000 train_loss: 39.55621337890625 test_loss:81.94439697265625\n",
      "1741/3000 train_loss: 43.483238220214844 test_loss:89.23677062988281\n",
      "1742/3000 train_loss: 35.47542190551758 test_loss:79.48255920410156\n",
      "1743/3000 train_loss: 34.798851013183594 test_loss:80.51947021484375\n",
      "1744/3000 train_loss: 35.213478088378906 test_loss:78.36971282958984\n",
      "1745/3000 train_loss: 36.321964263916016 test_loss:88.37323760986328\n",
      "1746/3000 train_loss: 38.31361389160156 test_loss:95.90826416015625\n",
      "1747/3000 train_loss: 41.79167938232422 test_loss:87.25691986083984\n",
      "1748/3000 train_loss: 40.67009353637695 test_loss:87.55270385742188\n",
      "1749/3000 train_loss: 42.66692352294922 test_loss:82.70518493652344\n",
      "1750/3000 train_loss: 37.769466400146484 test_loss:74.79896545410156\n",
      "1751/3000 train_loss: 38.75044250488281 test_loss:74.578857421875\n",
      "1752/3000 train_loss: 37.93199157714844 test_loss:84.30729675292969\n",
      "1753/3000 train_loss: 39.49644088745117 test_loss:108.65637969970703\n",
      "1754/3000 train_loss: 45.25407791137695 test_loss:87.56819152832031\n",
      "1755/3000 train_loss: 40.12568283081055 test_loss:81.13036346435547\n",
      "1756/3000 train_loss: 47.972782135009766 test_loss:85.99357604980469\n",
      "1757/3000 train_loss: 36.568538665771484 test_loss:79.31548309326172\n",
      "1758/3000 train_loss: 40.28927993774414 test_loss:86.01133728027344\n",
      "1759/3000 train_loss: 34.20576858520508 test_loss:74.87775421142578\n",
      "1760/3000 train_loss: 38.42093276977539 test_loss:89.20806884765625\n",
      "1761/3000 train_loss: 41.421363830566406 test_loss:84.41963958740234\n",
      "1762/3000 train_loss: 35.5428581237793 test_loss:87.25653076171875\n",
      "1763/3000 train_loss: 35.61772155761719 test_loss:78.58833312988281\n",
      "1764/3000 train_loss: 31.43314552307129 test_loss:87.45957946777344\n",
      "1765/3000 train_loss: 35.68822479248047 test_loss:74.9739761352539\n",
      "1766/3000 train_loss: 31.553024291992188 test_loss:91.89189910888672\n",
      "1767/3000 train_loss: 28.468326568603516 test_loss:90.2593994140625\n",
      "1768/3000 train_loss: 30.627975463867188 test_loss:88.9114990234375\n",
      "1769/3000 train_loss: 36.084983825683594 test_loss:83.89851379394531\n",
      "1770/3000 train_loss: 35.2840576171875 test_loss:89.18280792236328\n",
      "1771/3000 train_loss: 33.286258697509766 test_loss:78.36285400390625\n",
      "1772/3000 train_loss: 36.10959243774414 test_loss:88.03330993652344\n",
      "1773/3000 train_loss: 38.976566314697266 test_loss:90.17203521728516\n",
      "1774/3000 train_loss: 38.06686782836914 test_loss:70.46421813964844\n",
      "1775/3000 train_loss: 41.08740997314453 test_loss:88.00315856933594\n",
      "1776/3000 train_loss: 41.453128814697266 test_loss:88.51671600341797\n",
      "1777/3000 train_loss: 37.8435173034668 test_loss:87.42929077148438\n",
      "1778/3000 train_loss: 39.01695251464844 test_loss:81.42589569091797\n",
      "1779/3000 train_loss: 39.05508804321289 test_loss:77.73941802978516\n",
      "1780/3000 train_loss: 36.56924057006836 test_loss:83.74665832519531\n",
      "1781/3000 train_loss: 39.59370040893555 test_loss:82.53980255126953\n",
      "1782/3000 train_loss: 33.458534240722656 test_loss:81.7958984375\n",
      "1783/3000 train_loss: 37.39109420776367 test_loss:88.72164154052734\n",
      "1784/3000 train_loss: 33.0696907043457 test_loss:83.43355560302734\n",
      "1785/3000 train_loss: 44.40598678588867 test_loss:77.59748077392578\n",
      "1786/3000 train_loss: 41.497413635253906 test_loss:93.67050170898438\n",
      "1787/3000 train_loss: 37.28194808959961 test_loss:78.04391479492188\n",
      "1788/3000 train_loss: 36.21699142456055 test_loss:89.76544189453125\n",
      "1789/3000 train_loss: 48.28152847290039 test_loss:85.93859100341797\n",
      "1790/3000 train_loss: 56.458003997802734 test_loss:88.7822036743164\n",
      "1791/3000 train_loss: 41.09992980957031 test_loss:82.19454956054688\n",
      "1792/3000 train_loss: 34.13850402832031 test_loss:75.97598266601562\n",
      "1793/3000 train_loss: 33.709625244140625 test_loss:88.83547973632812\n",
      "1794/3000 train_loss: 35.903533935546875 test_loss:90.24007415771484\n",
      "1795/3000 train_loss: 38.732486724853516 test_loss:83.27154541015625\n",
      "1796/3000 train_loss: 32.94615173339844 test_loss:90.04708099365234\n",
      "1797/3000 train_loss: 29.624095916748047 test_loss:79.27090454101562\n",
      "1798/3000 train_loss: 34.661834716796875 test_loss:73.00680541992188\n",
      "1799/3000 train_loss: 35.955135345458984 test_loss:86.53378295898438\n",
      "1800/3000 train_loss: 33.68537902832031 test_loss:86.08816528320312\n",
      "1801/3000 train_loss: 38.57975387573242 test_loss:90.43252563476562\n",
      "1802/3000 train_loss: 38.99681854248047 test_loss:85.15135192871094\n",
      "1803/3000 train_loss: 37.91664123535156 test_loss:95.60541534423828\n",
      "1804/3000 train_loss: 33.628265380859375 test_loss:78.72806549072266\n",
      "1805/3000 train_loss: 44.304656982421875 test_loss:85.9106216430664\n",
      "1806/3000 train_loss: 35.50320053100586 test_loss:75.70582580566406\n",
      "1807/3000 train_loss: 39.97520446777344 test_loss:86.82831573486328\n",
      "1808/3000 train_loss: 37.41840744018555 test_loss:96.59503173828125\n",
      "1809/3000 train_loss: 37.4042854309082 test_loss:82.09771728515625\n",
      "1810/3000 train_loss: 43.725181579589844 test_loss:81.75732421875\n",
      "1811/3000 train_loss: 38.65358352661133 test_loss:76.45115661621094\n",
      "1812/3000 train_loss: 31.501527786254883 test_loss:78.38256072998047\n",
      "1813/3000 train_loss: 33.87538146972656 test_loss:86.89659881591797\n",
      "1814/3000 train_loss: 32.018009185791016 test_loss:77.52047729492188\n",
      "1815/3000 train_loss: 30.893352508544922 test_loss:87.72555541992188\n",
      "1816/3000 train_loss: 35.59883499145508 test_loss:79.0431137084961\n",
      "1817/3000 train_loss: 38.91572189331055 test_loss:75.75859832763672\n",
      "1818/3000 train_loss: 36.72264099121094 test_loss:79.7002944946289\n",
      "1819/3000 train_loss: 42.791744232177734 test_loss:99.45848846435547\n",
      "1820/3000 train_loss: 44.29389190673828 test_loss:81.38831329345703\n",
      "1821/3000 train_loss: 40.428829193115234 test_loss:88.44802856445312\n",
      "1822/3000 train_loss: 35.83634567260742 test_loss:76.4213638305664\n",
      "1823/3000 train_loss: 36.430965423583984 test_loss:72.76805114746094\n",
      "1824/3000 train_loss: 35.06415557861328 test_loss:80.58792877197266\n",
      "1825/3000 train_loss: 32.336273193359375 test_loss:69.44055938720703\n",
      "1826/3000 train_loss: 34.20416259765625 test_loss:89.79403686523438\n",
      "1827/3000 train_loss: 33.98685836791992 test_loss:76.11001586914062\n",
      "1828/3000 train_loss: 40.92966842651367 test_loss:80.26408386230469\n",
      "1829/3000 train_loss: 42.45947265625 test_loss:87.86441802978516\n",
      "1830/3000 train_loss: 37.56637191772461 test_loss:72.98302459716797\n",
      "1831/3000 train_loss: 37.273643493652344 test_loss:75.7500228881836\n",
      "1832/3000 train_loss: 38.00025939941406 test_loss:75.46012878417969\n",
      "1833/3000 train_loss: 35.95763397216797 test_loss:78.61592102050781\n",
      "1834/3000 train_loss: 39.80670166015625 test_loss:85.92042541503906\n",
      "1835/3000 train_loss: 35.04358673095703 test_loss:83.21736145019531\n",
      "1836/3000 train_loss: 38.44783401489258 test_loss:79.37584686279297\n",
      "1837/3000 train_loss: 34.503509521484375 test_loss:78.36175537109375\n",
      "1838/3000 train_loss: 38.60636901855469 test_loss:85.56592559814453\n",
      "1839/3000 train_loss: 31.129127502441406 test_loss:81.03736877441406\n",
      "1840/3000 train_loss: 31.768569946289062 test_loss:91.31385803222656\n",
      "1841/3000 train_loss: 39.748985290527344 test_loss:81.53974151611328\n",
      "1842/3000 train_loss: 34.04209518432617 test_loss:81.78917694091797\n",
      "1843/3000 train_loss: 36.531734466552734 test_loss:82.94322204589844\n",
      "1844/3000 train_loss: 32.0702018737793 test_loss:76.79348754882812\n",
      "1845/3000 train_loss: 37.998172760009766 test_loss:93.50492858886719\n",
      "1846/3000 train_loss: 34.60881805419922 test_loss:74.20037078857422\n",
      "1847/3000 train_loss: 34.35848617553711 test_loss:82.63921356201172\n",
      "1848/3000 train_loss: 37.267372131347656 test_loss:86.7256088256836\n",
      "1849/3000 train_loss: 34.40049743652344 test_loss:76.95771789550781\n",
      "1850/3000 train_loss: 36.5992431640625 test_loss:89.4743423461914\n",
      "1851/3000 train_loss: 38.81401824951172 test_loss:91.14737701416016\n",
      "1852/3000 train_loss: 34.04288864135742 test_loss:78.72679901123047\n",
      "1853/3000 train_loss: 40.995361328125 test_loss:81.497314453125\n",
      "1854/3000 train_loss: 32.9771842956543 test_loss:76.8254623413086\n",
      "1855/3000 train_loss: 35.762413024902344 test_loss:80.32530975341797\n",
      "1856/3000 train_loss: 35.0656852722168 test_loss:76.67570495605469\n",
      "1857/3000 train_loss: 36.73813247680664 test_loss:80.5863265991211\n",
      "1858/3000 train_loss: 38.46771240234375 test_loss:87.08289337158203\n",
      "1859/3000 train_loss: 28.760784149169922 test_loss:71.4471206665039\n",
      "1860/3000 train_loss: 36.51151657104492 test_loss:84.50355529785156\n",
      "1861/3000 train_loss: 31.946008682250977 test_loss:73.09854125976562\n",
      "1862/3000 train_loss: 39.246864318847656 test_loss:95.43553924560547\n",
      "1863/3000 train_loss: 34.322669982910156 test_loss:83.93865966796875\n",
      "1864/3000 train_loss: 38.104888916015625 test_loss:82.8496322631836\n",
      "1865/3000 train_loss: 39.25168228149414 test_loss:91.25138854980469\n",
      "1866/3000 train_loss: 37.54935836791992 test_loss:78.76019287109375\n",
      "1867/3000 train_loss: 31.59865379333496 test_loss:75.7656021118164\n",
      "1868/3000 train_loss: 40.825077056884766 test_loss:84.4034652709961\n",
      "1869/3000 train_loss: 45.58121109008789 test_loss:91.3625259399414\n",
      "1870/3000 train_loss: 34.487281799316406 test_loss:82.71951293945312\n",
      "1871/3000 train_loss: 32.12980270385742 test_loss:74.610107421875\n",
      "1872/3000 train_loss: 39.555442810058594 test_loss:78.8415756225586\n",
      "1873/3000 train_loss: 37.83534240722656 test_loss:96.11486053466797\n",
      "1874/3000 train_loss: 35.23080062866211 test_loss:83.40742492675781\n",
      "1875/3000 train_loss: 32.086334228515625 test_loss:78.03783416748047\n",
      "1876/3000 train_loss: 30.311382293701172 test_loss:82.09317016601562\n",
      "1877/3000 train_loss: 37.815460205078125 test_loss:78.46775817871094\n",
      "1878/3000 train_loss: 44.81671142578125 test_loss:80.40571594238281\n",
      "1879/3000 train_loss: 44.4261360168457 test_loss:90.26359558105469\n",
      "1880/3000 train_loss: 32.448509216308594 test_loss:82.65259552001953\n",
      "1881/3000 train_loss: 36.8250617980957 test_loss:90.51677703857422\n",
      "1882/3000 train_loss: 40.60913848876953 test_loss:75.82289123535156\n",
      "1883/3000 train_loss: 34.458656311035156 test_loss:86.88204956054688\n",
      "1884/3000 train_loss: 32.7082405090332 test_loss:71.81803131103516\n",
      "1885/3000 train_loss: 33.63937759399414 test_loss:81.03215026855469\n",
      "1886/3000 train_loss: 36.79634475708008 test_loss:69.81846618652344\n",
      "1887/3000 train_loss: 33.16813659667969 test_loss:82.62960052490234\n",
      "1888/3000 train_loss: 40.42181396484375 test_loss:71.82986450195312\n",
      "1889/3000 train_loss: 39.67974090576172 test_loss:79.10818481445312\n",
      "1890/3000 train_loss: 34.63544464111328 test_loss:91.0085220336914\n",
      "1891/3000 train_loss: 32.519168853759766 test_loss:98.95223999023438\n",
      "1892/3000 train_loss: 34.13570785522461 test_loss:81.80447387695312\n",
      "1893/3000 train_loss: 34.72675323486328 test_loss:84.39115905761719\n",
      "1894/3000 train_loss: 34.99055099487305 test_loss:82.15676879882812\n",
      "1895/3000 train_loss: 38.34291458129883 test_loss:79.6396255493164\n",
      "1896/3000 train_loss: 33.82541275024414 test_loss:77.8818359375\n",
      "1897/3000 train_loss: 33.177574157714844 test_loss:95.18492889404297\n",
      "1898/3000 train_loss: 30.46280288696289 test_loss:77.51683807373047\n",
      "1899/3000 train_loss: 32.71483612060547 test_loss:76.7589111328125\n",
      "1900/3000 train_loss: 33.85844421386719 test_loss:77.42250061035156\n",
      "1901/3000 train_loss: 37.14491271972656 test_loss:90.04725646972656\n",
      "1902/3000 train_loss: 32.02336120605469 test_loss:76.555908203125\n",
      "1903/3000 train_loss: 36.13182067871094 test_loss:79.73345947265625\n",
      "1904/3000 train_loss: 32.57186508178711 test_loss:89.93434143066406\n",
      "1905/3000 train_loss: 35.12977600097656 test_loss:87.25524139404297\n",
      "1906/3000 train_loss: 35.534576416015625 test_loss:68.78612518310547\n",
      "1907/3000 train_loss: 29.23992156982422 test_loss:84.13154602050781\n",
      "1908/3000 train_loss: 37.44207000732422 test_loss:76.66722869873047\n",
      "1909/3000 train_loss: 33.95501708984375 test_loss:82.43783569335938\n",
      "1910/3000 train_loss: 34.014225006103516 test_loss:87.28732299804688\n",
      "1911/3000 train_loss: 40.96265411376953 test_loss:96.41707611083984\n",
      "1912/3000 train_loss: 51.968589782714844 test_loss:100.6607437133789\n",
      "1913/3000 train_loss: 38.48058319091797 test_loss:85.02046203613281\n",
      "1914/3000 train_loss: 37.81226348876953 test_loss:83.20120239257812\n",
      "1915/3000 train_loss: 35.32289123535156 test_loss:89.81145477294922\n",
      "1916/3000 train_loss: 34.917110443115234 test_loss:69.17730712890625\n",
      "1917/3000 train_loss: 32.026405334472656 test_loss:80.43833923339844\n",
      "1918/3000 train_loss: 38.366668701171875 test_loss:80.24369049072266\n",
      "1919/3000 train_loss: 37.181514739990234 test_loss:91.26274871826172\n",
      "1920/3000 train_loss: 41.669376373291016 test_loss:77.2556381225586\n",
      "1921/3000 train_loss: 37.023155212402344 test_loss:80.97906494140625\n",
      "1922/3000 train_loss: 33.914737701416016 test_loss:86.8439712524414\n",
      "1923/3000 train_loss: 38.171207427978516 test_loss:81.71150970458984\n",
      "1924/3000 train_loss: 34.361576080322266 test_loss:80.86600494384766\n",
      "1925/3000 train_loss: 28.751483917236328 test_loss:79.47482299804688\n",
      "1926/3000 train_loss: 31.450105667114258 test_loss:72.97010040283203\n",
      "1927/3000 train_loss: 37.53107833862305 test_loss:86.28273010253906\n",
      "1928/3000 train_loss: 38.69585037231445 test_loss:74.82901000976562\n",
      "1929/3000 train_loss: 38.28024673461914 test_loss:78.95344543457031\n",
      "1930/3000 train_loss: 42.6739616394043 test_loss:91.88711547851562\n",
      "1931/3000 train_loss: 41.61189651489258 test_loss:93.6878890991211\n",
      "1932/3000 train_loss: 36.726898193359375 test_loss:73.15878295898438\n",
      "1933/3000 train_loss: 35.4503288269043 test_loss:78.0347671508789\n",
      "1934/3000 train_loss: 35.77735900878906 test_loss:71.35420989990234\n",
      "1935/3000 train_loss: 35.12855911254883 test_loss:85.64017486572266\n",
      "1936/3000 train_loss: 38.510189056396484 test_loss:81.31783294677734\n",
      "1937/3000 train_loss: 42.52061462402344 test_loss:82.36407470703125\n",
      "1938/3000 train_loss: 45.554168701171875 test_loss:70.4658203125\n",
      "1939/3000 train_loss: 32.89108657836914 test_loss:78.48780822753906\n",
      "1940/3000 train_loss: 35.895389556884766 test_loss:83.81610107421875\n",
      "1941/3000 train_loss: 31.98017692565918 test_loss:71.10659790039062\n",
      "1942/3000 train_loss: 34.737884521484375 test_loss:85.2470932006836\n",
      "1943/3000 train_loss: 34.37222671508789 test_loss:75.48644256591797\n",
      "1944/3000 train_loss: 35.61478042602539 test_loss:98.04515838623047\n",
      "1945/3000 train_loss: 32.78317642211914 test_loss:74.07115173339844\n",
      "1946/3000 train_loss: 29.03879737854004 test_loss:100.74502563476562\n",
      "1947/3000 train_loss: 38.22269821166992 test_loss:85.90110778808594\n",
      "1948/3000 train_loss: 44.809940338134766 test_loss:74.02803802490234\n",
      "1949/3000 train_loss: 37.059444427490234 test_loss:86.96061706542969\n",
      "1950/3000 train_loss: 32.6760368347168 test_loss:73.06370544433594\n",
      "1951/3000 train_loss: 29.49759292602539 test_loss:90.72604370117188\n",
      "1952/3000 train_loss: 31.59963035583496 test_loss:80.04102325439453\n",
      "1953/3000 train_loss: 41.05226135253906 test_loss:80.97134399414062\n",
      "1954/3000 train_loss: 47.86592483520508 test_loss:79.23544311523438\n",
      "1955/3000 train_loss: 36.49875259399414 test_loss:84.0629653930664\n",
      "1956/3000 train_loss: 35.097713470458984 test_loss:83.78514099121094\n",
      "1957/3000 train_loss: 36.25876235961914 test_loss:80.50984191894531\n",
      "1958/3000 train_loss: 36.81233596801758 test_loss:74.1794662475586\n",
      "1959/3000 train_loss: 39.208499908447266 test_loss:87.8352279663086\n",
      "1960/3000 train_loss: 40.23735046386719 test_loss:75.98038482666016\n",
      "1961/3000 train_loss: 33.556480407714844 test_loss:78.46660614013672\n",
      "1962/3000 train_loss: 36.6795654296875 test_loss:72.26255798339844\n",
      "1963/3000 train_loss: 42.76170349121094 test_loss:82.99358367919922\n",
      "1964/3000 train_loss: 44.440711975097656 test_loss:92.48777770996094\n",
      "1965/3000 train_loss: 35.73681640625 test_loss:73.94337463378906\n",
      "1966/3000 train_loss: 31.581043243408203 test_loss:82.58718872070312\n",
      "1967/3000 train_loss: 41.057762145996094 test_loss:85.27439880371094\n",
      "1968/3000 train_loss: 35.62236785888672 test_loss:76.63870239257812\n",
      "1969/3000 train_loss: 44.60712432861328 test_loss:71.90330505371094\n",
      "1970/3000 train_loss: 38.03394317626953 test_loss:78.96078491210938\n",
      "1971/3000 train_loss: 35.864776611328125 test_loss:81.6032485961914\n",
      "1972/3000 train_loss: 31.25412368774414 test_loss:78.1588363647461\n",
      "1973/3000 train_loss: 30.779882431030273 test_loss:81.22303771972656\n",
      "1974/3000 train_loss: 36.48896026611328 test_loss:88.28250122070312\n",
      "1975/3000 train_loss: 38.186805725097656 test_loss:80.1406478881836\n",
      "1976/3000 train_loss: 31.79399299621582 test_loss:75.37409973144531\n",
      "1977/3000 train_loss: 33.65565872192383 test_loss:82.81721496582031\n",
      "1978/3000 train_loss: 37.84553146362305 test_loss:82.44832611083984\n",
      "1979/3000 train_loss: 32.22438049316406 test_loss:86.09159088134766\n",
      "1980/3000 train_loss: 33.26633071899414 test_loss:70.41402435302734\n",
      "1981/3000 train_loss: 32.21302795410156 test_loss:86.99262237548828\n",
      "1982/3000 train_loss: 41.01033401489258 test_loss:84.94447326660156\n",
      "1983/3000 train_loss: 38.46814727783203 test_loss:78.05915069580078\n",
      "1984/3000 train_loss: 35.19517135620117 test_loss:75.2627944946289\n",
      "1985/3000 train_loss: 31.80417823791504 test_loss:71.7959976196289\n",
      "1986/3000 train_loss: 34.643062591552734 test_loss:77.74887084960938\n",
      "1987/3000 train_loss: 32.96017837524414 test_loss:75.17627716064453\n",
      "1988/3000 train_loss: 29.537742614746094 test_loss:73.13239288330078\n",
      "1989/3000 train_loss: 36.05265426635742 test_loss:75.89453887939453\n",
      "1990/3000 train_loss: 34.44697570800781 test_loss:72.33613586425781\n",
      "1991/3000 train_loss: 31.096111297607422 test_loss:74.8062744140625\n",
      "1992/3000 train_loss: 36.36156463623047 test_loss:76.32109832763672\n",
      "1993/3000 train_loss: 34.001922607421875 test_loss:94.7624282836914\n",
      "1994/3000 train_loss: 38.13493347167969 test_loss:84.69368743896484\n",
      "1995/3000 train_loss: 32.575592041015625 test_loss:87.74933624267578\n",
      "1996/3000 train_loss: 34.81019592285156 test_loss:72.67613983154297\n",
      "1997/3000 train_loss: 38.90046691894531 test_loss:74.90042877197266\n",
      "1998/3000 train_loss: 43.7130012512207 test_loss:81.1629638671875\n",
      "1999/3000 train_loss: 36.963035583496094 test_loss:82.8553466796875\n",
      "2000/3000 train_loss: 33.54606628417969 test_loss:82.26628112792969\n",
      "2001/3000 train_loss: 31.456212997436523 test_loss:74.65171813964844\n",
      "2002/3000 train_loss: 28.101001739501953 test_loss:81.43521118164062\n",
      "2003/3000 train_loss: 40.17226791381836 test_loss:77.89295196533203\n",
      "2004/3000 train_loss: 34.97007751464844 test_loss:76.9539566040039\n",
      "2005/3000 train_loss: 31.149728775024414 test_loss:77.05116271972656\n",
      "2006/3000 train_loss: 37.373714447021484 test_loss:79.56255340576172\n",
      "2007/3000 train_loss: 36.64148712158203 test_loss:100.76148986816406\n",
      "2008/3000 train_loss: 33.64739990234375 test_loss:85.27716064453125\n",
      "2009/3000 train_loss: 41.50653839111328 test_loss:78.55854797363281\n",
      "2010/3000 train_loss: 38.41971969604492 test_loss:92.58497619628906\n",
      "2011/3000 train_loss: 40.322296142578125 test_loss:83.63671875\n",
      "2012/3000 train_loss: 39.89474868774414 test_loss:95.40901947021484\n",
      "2013/3000 train_loss: 37.335609436035156 test_loss:88.25330352783203\n",
      "2014/3000 train_loss: 31.947721481323242 test_loss:81.9600601196289\n",
      "2015/3000 train_loss: 31.18574333190918 test_loss:86.8972396850586\n",
      "2016/3000 train_loss: 33.96586608886719 test_loss:71.29998779296875\n",
      "2017/3000 train_loss: 35.36994552612305 test_loss:76.73688507080078\n",
      "2018/3000 train_loss: 33.24692153930664 test_loss:94.16276550292969\n",
      "2019/3000 train_loss: 35.28800582885742 test_loss:86.62608337402344\n",
      "2020/3000 train_loss: 36.017295837402344 test_loss:90.30977630615234\n",
      "2021/3000 train_loss: 40.88695526123047 test_loss:72.85957336425781\n",
      "2022/3000 train_loss: 32.88584518432617 test_loss:75.69662475585938\n",
      "2023/3000 train_loss: 32.01913070678711 test_loss:71.75550079345703\n",
      "2024/3000 train_loss: 36.391231536865234 test_loss:81.3027114868164\n",
      "2025/3000 train_loss: 32.400421142578125 test_loss:74.52145385742188\n",
      "2026/3000 train_loss: 35.4139289855957 test_loss:71.72601318359375\n",
      "2027/3000 train_loss: 38.181880950927734 test_loss:79.27397918701172\n",
      "2028/3000 train_loss: 42.063255310058594 test_loss:82.35157012939453\n",
      "2029/3000 train_loss: 39.264976501464844 test_loss:100.85250854492188\n",
      "2030/3000 train_loss: 36.039669036865234 test_loss:73.47157287597656\n",
      "2031/3000 train_loss: 36.02499008178711 test_loss:79.4700927734375\n",
      "2032/3000 train_loss: 35.44308853149414 test_loss:84.00692749023438\n",
      "2033/3000 train_loss: 32.204002380371094 test_loss:78.07268524169922\n",
      "2034/3000 train_loss: 35.06542205810547 test_loss:72.45463562011719\n",
      "2035/3000 train_loss: 32.5919303894043 test_loss:76.49408721923828\n",
      "2036/3000 train_loss: 30.719552993774414 test_loss:73.67857360839844\n",
      "2037/3000 train_loss: 35.61705017089844 test_loss:70.46979522705078\n",
      "2038/3000 train_loss: 36.032230377197266 test_loss:73.77371215820312\n",
      "2039/3000 train_loss: 32.353599548339844 test_loss:68.93800354003906\n",
      "2040/3000 train_loss: 31.77094268798828 test_loss:79.34721374511719\n",
      "2041/3000 train_loss: 37.46720886230469 test_loss:80.50831604003906\n",
      "2042/3000 train_loss: 34.268951416015625 test_loss:76.78246307373047\n",
      "2043/3000 train_loss: 37.09616470336914 test_loss:80.75039672851562\n",
      "2044/3000 train_loss: 39.50692367553711 test_loss:72.88465118408203\n",
      "2045/3000 train_loss: 32.54075622558594 test_loss:73.12535858154297\n",
      "2046/3000 train_loss: 32.44762420654297 test_loss:67.91366577148438\n",
      "2047/3000 train_loss: 29.656557083129883 test_loss:75.35624694824219\n",
      "2048/3000 train_loss: 34.94190216064453 test_loss:82.40267944335938\n",
      "2049/3000 train_loss: 36.328304290771484 test_loss:74.1650390625\n",
      "2050/3000 train_loss: 35.31996154785156 test_loss:82.45294952392578\n",
      "2051/3000 train_loss: 30.385295867919922 test_loss:71.57373046875\n",
      "2052/3000 train_loss: 32.44905090332031 test_loss:70.27426147460938\n",
      "2053/3000 train_loss: 31.391525268554688 test_loss:80.69149017333984\n",
      "2054/3000 train_loss: 32.51299285888672 test_loss:75.8682632446289\n",
      "2055/3000 train_loss: 36.686241149902344 test_loss:82.19987487792969\n",
      "2056/3000 train_loss: 35.630619049072266 test_loss:83.55609893798828\n",
      "2057/3000 train_loss: 33.68859100341797 test_loss:77.86018371582031\n",
      "2058/3000 train_loss: 37.89656066894531 test_loss:79.87110900878906\n",
      "2059/3000 train_loss: 34.78343200683594 test_loss:74.31547546386719\n",
      "2060/3000 train_loss: 39.83512878417969 test_loss:81.5416030883789\n",
      "2061/3000 train_loss: 35.54281997680664 test_loss:73.0620346069336\n",
      "2062/3000 train_loss: 31.90259552001953 test_loss:81.16120910644531\n",
      "2063/3000 train_loss: 32.85660171508789 test_loss:78.791748046875\n",
      "2064/3000 train_loss: 33.434669494628906 test_loss:80.58428955078125\n",
      "2065/3000 train_loss: 31.569177627563477 test_loss:69.02886199951172\n",
      "2066/3000 train_loss: 31.39230728149414 test_loss:80.7320556640625\n",
      "2067/3000 train_loss: 37.643043518066406 test_loss:90.442626953125\n",
      "2068/3000 train_loss: 39.33014678955078 test_loss:74.01862335205078\n",
      "2069/3000 train_loss: 30.750078201293945 test_loss:71.00141906738281\n",
      "2070/3000 train_loss: 30.395442962646484 test_loss:90.83426666259766\n",
      "2071/3000 train_loss: 33.022254943847656 test_loss:85.24710083007812\n",
      "2072/3000 train_loss: 32.53782272338867 test_loss:75.4548110961914\n",
      "2073/3000 train_loss: 27.309804916381836 test_loss:75.8975830078125\n",
      "2074/3000 train_loss: 32.033355712890625 test_loss:77.8251953125\n",
      "2075/3000 train_loss: 33.05186462402344 test_loss:76.649169921875\n",
      "2076/3000 train_loss: 32.466148376464844 test_loss:73.64188385009766\n",
      "2077/3000 train_loss: 32.40758514404297 test_loss:73.75279235839844\n",
      "2078/3000 train_loss: 33.02398681640625 test_loss:78.67216491699219\n",
      "2079/3000 train_loss: 36.29741287231445 test_loss:77.33918762207031\n",
      "2080/3000 train_loss: 33.430538177490234 test_loss:79.70292663574219\n",
      "2081/3000 train_loss: 36.366512298583984 test_loss:82.81400299072266\n",
      "2082/3000 train_loss: 39.667320251464844 test_loss:87.19332122802734\n",
      "2083/3000 train_loss: 38.526031494140625 test_loss:80.72090911865234\n",
      "2084/3000 train_loss: 43.595375061035156 test_loss:94.3807601928711\n",
      "2085/3000 train_loss: 38.10115051269531 test_loss:82.76109313964844\n",
      "2086/3000 train_loss: 39.15684127807617 test_loss:80.1707763671875\n",
      "2087/3000 train_loss: 30.848941802978516 test_loss:76.56634521484375\n",
      "2088/3000 train_loss: 33.41080856323242 test_loss:89.60242462158203\n",
      "2089/3000 train_loss: 32.77219009399414 test_loss:86.61368560791016\n",
      "2090/3000 train_loss: 38.7238883972168 test_loss:88.77806854248047\n",
      "2091/3000 train_loss: 36.63393783569336 test_loss:85.83451843261719\n",
      "2092/3000 train_loss: 36.566566467285156 test_loss:77.5343246459961\n",
      "2093/3000 train_loss: 30.4633846282959 test_loss:81.98686981201172\n",
      "2094/3000 train_loss: 31.176118850708008 test_loss:79.51696014404297\n",
      "2095/3000 train_loss: 33.55301284790039 test_loss:74.06365966796875\n",
      "2096/3000 train_loss: 30.444467544555664 test_loss:76.41230773925781\n",
      "2097/3000 train_loss: 33.96068572998047 test_loss:78.2934341430664\n",
      "2098/3000 train_loss: 33.10760498046875 test_loss:79.57646942138672\n",
      "2099/3000 train_loss: 33.52108383178711 test_loss:76.82601165771484\n",
      "2100/3000 train_loss: 36.75700378417969 test_loss:81.97510528564453\n",
      "2101/3000 train_loss: 29.959423065185547 test_loss:81.51252746582031\n",
      "2102/3000 train_loss: 40.456722259521484 test_loss:78.41608428955078\n",
      "2103/3000 train_loss: 36.45547103881836 test_loss:80.1187744140625\n",
      "2104/3000 train_loss: 42.80881118774414 test_loss:94.58549499511719\n",
      "2105/3000 train_loss: 33.10684585571289 test_loss:74.70063018798828\n",
      "2106/3000 train_loss: 37.22135925292969 test_loss:78.04285430908203\n",
      "2107/3000 train_loss: 34.94315719604492 test_loss:69.7726821899414\n",
      "2108/3000 train_loss: 33.48377227783203 test_loss:68.03819274902344\n",
      "2109/3000 train_loss: 35.69414138793945 test_loss:80.93716430664062\n",
      "2110/3000 train_loss: 31.6165828704834 test_loss:71.49827575683594\n",
      "2111/3000 train_loss: 31.02046775817871 test_loss:77.6906967163086\n",
      "2112/3000 train_loss: 33.784584045410156 test_loss:65.95543670654297\n",
      "2113/3000 train_loss: 43.61375045776367 test_loss:80.8574447631836\n",
      "2114/3000 train_loss: 34.896240234375 test_loss:89.75651550292969\n",
      "2115/3000 train_loss: 33.18793487548828 test_loss:81.62069702148438\n",
      "2116/3000 train_loss: 36.287208557128906 test_loss:82.65967559814453\n",
      "2117/3000 train_loss: 33.08928298950195 test_loss:68.5994873046875\n",
      "2118/3000 train_loss: 32.11315155029297 test_loss:79.0916976928711\n",
      "2119/3000 train_loss: 31.993440628051758 test_loss:76.99163818359375\n",
      "2120/3000 train_loss: 26.8586483001709 test_loss:79.80652618408203\n",
      "2121/3000 train_loss: 35.250179290771484 test_loss:78.13085174560547\n",
      "2122/3000 train_loss: 31.884817123413086 test_loss:73.45063781738281\n",
      "2123/3000 train_loss: 35.89065170288086 test_loss:78.25569152832031\n",
      "2124/3000 train_loss: 36.484947204589844 test_loss:82.32030487060547\n",
      "2125/3000 train_loss: 34.308040618896484 test_loss:72.95108032226562\n",
      "2126/3000 train_loss: 32.49082946777344 test_loss:80.46662902832031\n",
      "2127/3000 train_loss: 35.49879455566406 test_loss:78.68538665771484\n",
      "2128/3000 train_loss: 34.08521270751953 test_loss:88.76937866210938\n",
      "2129/3000 train_loss: 32.36585998535156 test_loss:76.92359161376953\n",
      "2130/3000 train_loss: 36.493553161621094 test_loss:71.12946319580078\n",
      "2131/3000 train_loss: 36.2191276550293 test_loss:88.77912139892578\n",
      "2132/3000 train_loss: 30.576128005981445 test_loss:73.0697021484375\n",
      "2133/3000 train_loss: 29.698360443115234 test_loss:79.5464859008789\n",
      "2134/3000 train_loss: 37.49126434326172 test_loss:73.87078094482422\n",
      "2135/3000 train_loss: 35.72587585449219 test_loss:86.71196746826172\n",
      "2136/3000 train_loss: 34.477210998535156 test_loss:86.06439208984375\n",
      "2137/3000 train_loss: 36.60669708251953 test_loss:81.25184631347656\n",
      "2138/3000 train_loss: 36.493690490722656 test_loss:81.33881378173828\n",
      "2139/3000 train_loss: 35.87380599975586 test_loss:82.76081848144531\n",
      "2140/3000 train_loss: 38.10514450073242 test_loss:90.12509155273438\n",
      "2141/3000 train_loss: 35.9150276184082 test_loss:79.89582824707031\n",
      "2142/3000 train_loss: 36.7643928527832 test_loss:80.85395050048828\n",
      "2143/3000 train_loss: 33.000858306884766 test_loss:69.22898864746094\n",
      "2144/3000 train_loss: 25.556970596313477 test_loss:77.3349609375\n",
      "2145/3000 train_loss: 33.69784927368164 test_loss:90.18009948730469\n",
      "2146/3000 train_loss: 32.79043197631836 test_loss:78.7759780883789\n",
      "2147/3000 train_loss: 26.96365737915039 test_loss:91.68919372558594\n",
      "2148/3000 train_loss: 34.725399017333984 test_loss:79.75917053222656\n",
      "2149/3000 train_loss: 31.50154685974121 test_loss:76.63902282714844\n",
      "2150/3000 train_loss: 36.88630294799805 test_loss:76.08890533447266\n",
      "2151/3000 train_loss: 35.079620361328125 test_loss:76.65132904052734\n",
      "2152/3000 train_loss: 30.750089645385742 test_loss:70.39557647705078\n",
      "2153/3000 train_loss: 32.725318908691406 test_loss:76.40680694580078\n",
      "2154/3000 train_loss: 36.01879119873047 test_loss:70.99268341064453\n",
      "2155/3000 train_loss: 37.11606979370117 test_loss:79.00083923339844\n",
      "2156/3000 train_loss: 33.327003479003906 test_loss:89.74317169189453\n",
      "2157/3000 train_loss: 33.46527099609375 test_loss:74.06319427490234\n",
      "2158/3000 train_loss: 29.27913475036621 test_loss:67.73107147216797\n",
      "2159/3000 train_loss: 32.84944152832031 test_loss:90.66616821289062\n",
      "2160/3000 train_loss: 32.470699310302734 test_loss:84.62033081054688\n",
      "2161/3000 train_loss: 42.867950439453125 test_loss:104.03892517089844\n",
      "2162/3000 train_loss: 36.50999069213867 test_loss:76.10116577148438\n",
      "2163/3000 train_loss: 35.424442291259766 test_loss:70.95162963867188\n",
      "2164/3000 train_loss: 30.03732681274414 test_loss:67.58866882324219\n",
      "2165/3000 train_loss: 30.687440872192383 test_loss:73.15026092529297\n",
      "2166/3000 train_loss: 33.49028015136719 test_loss:73.73564910888672\n",
      "2167/3000 train_loss: 34.971824645996094 test_loss:71.60670471191406\n",
      "2168/3000 train_loss: 44.52009201049805 test_loss:89.7070541381836\n",
      "2169/3000 train_loss: 41.38127517700195 test_loss:78.26565551757812\n",
      "2170/3000 train_loss: 28.596525192260742 test_loss:71.98045349121094\n",
      "2171/3000 train_loss: 30.300996780395508 test_loss:73.00017547607422\n",
      "2172/3000 train_loss: 32.2488899230957 test_loss:75.62276458740234\n",
      "2173/3000 train_loss: 35.77961730957031 test_loss:74.34087371826172\n",
      "2174/3000 train_loss: 32.029991149902344 test_loss:78.0229263305664\n",
      "2175/3000 train_loss: 27.265987396240234 test_loss:85.95453643798828\n",
      "2176/3000 train_loss: 31.451919555664062 test_loss:75.96863555908203\n",
      "2177/3000 train_loss: 33.395599365234375 test_loss:77.10528564453125\n",
      "2178/3000 train_loss: 31.382539749145508 test_loss:78.44023132324219\n",
      "2179/3000 train_loss: 31.419532775878906 test_loss:81.0075912475586\n",
      "2180/3000 train_loss: 30.477388381958008 test_loss:84.52178955078125\n",
      "2181/3000 train_loss: 31.65373420715332 test_loss:78.2542953491211\n",
      "2182/3000 train_loss: 32.10048294067383 test_loss:72.60748291015625\n",
      "2183/3000 train_loss: 42.407379150390625 test_loss:71.66329956054688\n",
      "2184/3000 train_loss: 35.35269546508789 test_loss:76.8795166015625\n",
      "2185/3000 train_loss: 29.223926544189453 test_loss:76.85501098632812\n",
      "2186/3000 train_loss: 32.9559440612793 test_loss:80.5860366821289\n",
      "2187/3000 train_loss: 33.689674377441406 test_loss:77.06085205078125\n",
      "2188/3000 train_loss: 33.83949279785156 test_loss:73.86104583740234\n",
      "2189/3000 train_loss: 29.27751350402832 test_loss:80.85768127441406\n",
      "2190/3000 train_loss: 33.538673400878906 test_loss:77.47698211669922\n",
      "2191/3000 train_loss: 35.091312408447266 test_loss:74.3573226928711\n",
      "2192/3000 train_loss: 31.429468154907227 test_loss:77.96138763427734\n",
      "2193/3000 train_loss: 34.147281646728516 test_loss:82.060546875\n",
      "2194/3000 train_loss: 33.62773513793945 test_loss:71.55316162109375\n",
      "2195/3000 train_loss: 34.01337814331055 test_loss:78.49894714355469\n",
      "2196/3000 train_loss: 29.557680130004883 test_loss:70.50531768798828\n",
      "2197/3000 train_loss: 30.48508644104004 test_loss:77.29122161865234\n",
      "2198/3000 train_loss: 34.17710494995117 test_loss:80.49836730957031\n",
      "2199/3000 train_loss: 35.29207229614258 test_loss:77.5162582397461\n",
      "2200/3000 train_loss: 33.44921875 test_loss:80.99585723876953\n",
      "2201/3000 train_loss: 28.2910213470459 test_loss:74.61216735839844\n",
      "2202/3000 train_loss: 32.076786041259766 test_loss:88.772705078125\n",
      "2203/3000 train_loss: 32.54688262939453 test_loss:89.93833923339844\n",
      "2204/3000 train_loss: 35.42512130737305 test_loss:70.94371032714844\n",
      "2205/3000 train_loss: 37.034080505371094 test_loss:86.90214538574219\n",
      "2206/3000 train_loss: 38.7614860534668 test_loss:72.56890869140625\n",
      "2207/3000 train_loss: 34.37241744995117 test_loss:99.39421081542969\n",
      "2208/3000 train_loss: 30.338361740112305 test_loss:75.79059600830078\n",
      "2209/3000 train_loss: 35.47545623779297 test_loss:76.2370834350586\n",
      "2210/3000 train_loss: 32.618408203125 test_loss:77.02435302734375\n",
      "2211/3000 train_loss: 35.70980453491211 test_loss:79.81886291503906\n",
      "2212/3000 train_loss: 31.726459503173828 test_loss:76.93559265136719\n",
      "2213/3000 train_loss: 31.066429138183594 test_loss:73.5596923828125\n",
      "2214/3000 train_loss: 33.20856475830078 test_loss:76.50336456298828\n",
      "2215/3000 train_loss: 32.37330627441406 test_loss:79.11756896972656\n",
      "2216/3000 train_loss: 35.40531921386719 test_loss:78.56060791015625\n",
      "2217/3000 train_loss: 40.525455474853516 test_loss:84.03797912597656\n",
      "2218/3000 train_loss: 37.50794219970703 test_loss:77.47303009033203\n",
      "2219/3000 train_loss: 33.233890533447266 test_loss:72.1250991821289\n",
      "2220/3000 train_loss: 33.189109802246094 test_loss:91.91244506835938\n",
      "2221/3000 train_loss: 37.27141571044922 test_loss:90.9223403930664\n",
      "2222/3000 train_loss: 34.17631149291992 test_loss:77.31481170654297\n",
      "2223/3000 train_loss: 33.86437225341797 test_loss:78.3836441040039\n",
      "2224/3000 train_loss: 34.26805877685547 test_loss:75.94094848632812\n",
      "2225/3000 train_loss: 29.494510650634766 test_loss:76.76054382324219\n",
      "2226/3000 train_loss: 36.85813903808594 test_loss:70.41543579101562\n",
      "2227/3000 train_loss: 35.35210037231445 test_loss:81.27735137939453\n",
      "2228/3000 train_loss: 35.62763977050781 test_loss:74.33842468261719\n",
      "2229/3000 train_loss: 38.501670837402344 test_loss:102.72449493408203\n",
      "2230/3000 train_loss: 38.0067138671875 test_loss:69.55187225341797\n",
      "2231/3000 train_loss: 29.89628028869629 test_loss:81.16673278808594\n",
      "2232/3000 train_loss: 40.043731689453125 test_loss:77.28401947021484\n",
      "2233/3000 train_loss: 33.5315055847168 test_loss:72.36470794677734\n",
      "2234/3000 train_loss: 37.688533782958984 test_loss:74.38047790527344\n",
      "2235/3000 train_loss: 30.129892349243164 test_loss:80.16022491455078\n",
      "2236/3000 train_loss: 27.27831268310547 test_loss:86.41773223876953\n",
      "2237/3000 train_loss: 33.57502746582031 test_loss:77.46571350097656\n",
      "2238/3000 train_loss: 29.582773208618164 test_loss:73.16539001464844\n",
      "2239/3000 train_loss: 31.976940155029297 test_loss:83.12042236328125\n",
      "2240/3000 train_loss: 28.609752655029297 test_loss:78.16686248779297\n",
      "2241/3000 train_loss: 30.794836044311523 test_loss:74.69413757324219\n",
      "2242/3000 train_loss: 32.612037658691406 test_loss:73.62127685546875\n",
      "2243/3000 train_loss: 33.05016326904297 test_loss:92.611572265625\n",
      "2244/3000 train_loss: 32.7009162902832 test_loss:83.19088745117188\n",
      "2245/3000 train_loss: 35.100341796875 test_loss:79.21847534179688\n",
      "2246/3000 train_loss: 32.475345611572266 test_loss:81.8966064453125\n",
      "2247/3000 train_loss: 32.310630798339844 test_loss:73.45327758789062\n",
      "2248/3000 train_loss: 30.01448631286621 test_loss:73.95423889160156\n",
      "2249/3000 train_loss: 30.993011474609375 test_loss:79.49762725830078\n",
      "2250/3000 train_loss: 32.77458953857422 test_loss:75.90019989013672\n",
      "2251/3000 train_loss: 30.695728302001953 test_loss:83.36724853515625\n",
      "2252/3000 train_loss: 35.12140655517578 test_loss:73.93656158447266\n",
      "2253/3000 train_loss: 39.20107650756836 test_loss:84.17945861816406\n",
      "2254/3000 train_loss: 30.396203994750977 test_loss:110.43708801269531\n",
      "2255/3000 train_loss: 39.24296569824219 test_loss:92.28752136230469\n",
      "2256/3000 train_loss: 30.97737693786621 test_loss:87.57762145996094\n",
      "2257/3000 train_loss: 30.708955764770508 test_loss:71.5758056640625\n",
      "2258/3000 train_loss: 27.929542541503906 test_loss:77.87432861328125\n",
      "2259/3000 train_loss: 32.67366027832031 test_loss:74.79481506347656\n",
      "2260/3000 train_loss: 38.66567611694336 test_loss:76.74610900878906\n",
      "2261/3000 train_loss: 35.365291595458984 test_loss:104.1348648071289\n",
      "2262/3000 train_loss: 32.81037902832031 test_loss:78.81401824951172\n",
      "2263/3000 train_loss: 30.06719207763672 test_loss:66.78868103027344\n",
      "2264/3000 train_loss: 38.85198211669922 test_loss:90.72805786132812\n",
      "2265/3000 train_loss: 36.139503479003906 test_loss:73.2302474975586\n",
      "2266/3000 train_loss: 29.346342086791992 test_loss:73.69676208496094\n",
      "2267/3000 train_loss: 33.43413543701172 test_loss:82.19530487060547\n",
      "2268/3000 train_loss: 29.342483520507812 test_loss:75.4732666015625\n",
      "2269/3000 train_loss: 33.054683685302734 test_loss:84.18814086914062\n",
      "2270/3000 train_loss: 30.68798065185547 test_loss:73.58349609375\n",
      "2271/3000 train_loss: 27.88593101501465 test_loss:72.59381103515625\n",
      "2272/3000 train_loss: 29.46558952331543 test_loss:89.7613754272461\n",
      "2273/3000 train_loss: 34.2513313293457 test_loss:84.33121490478516\n",
      "2274/3000 train_loss: 35.234642028808594 test_loss:74.52294158935547\n",
      "2275/3000 train_loss: 29.501501083374023 test_loss:86.09174346923828\n",
      "2276/3000 train_loss: 35.39509963989258 test_loss:79.30280303955078\n",
      "2277/3000 train_loss: 34.1014289855957 test_loss:92.92180633544922\n",
      "2278/3000 train_loss: 34.41721725463867 test_loss:79.37073516845703\n",
      "2279/3000 train_loss: 28.924983978271484 test_loss:69.22923278808594\n",
      "2280/3000 train_loss: 33.47088623046875 test_loss:99.73977661132812\n",
      "2281/3000 train_loss: 32.37908935546875 test_loss:70.30825805664062\n",
      "2282/3000 train_loss: 32.16014099121094 test_loss:84.49038696289062\n",
      "2283/3000 train_loss: 35.57560348510742 test_loss:93.75361633300781\n",
      "2284/3000 train_loss: 34.166770935058594 test_loss:80.18535614013672\n",
      "2285/3000 train_loss: 40.406768798828125 test_loss:86.76809692382812\n",
      "2286/3000 train_loss: 37.94445037841797 test_loss:90.49308776855469\n",
      "2287/3000 train_loss: 32.43439865112305 test_loss:85.37539672851562\n",
      "2288/3000 train_loss: 36.8607177734375 test_loss:75.18061828613281\n",
      "2289/3000 train_loss: 33.79104995727539 test_loss:79.75582122802734\n",
      "2290/3000 train_loss: 44.56373596191406 test_loss:91.83668518066406\n",
      "2291/3000 train_loss: 36.27399826049805 test_loss:80.0308609008789\n",
      "2292/3000 train_loss: 35.4008674621582 test_loss:86.28789520263672\n",
      "2293/3000 train_loss: 28.2289981842041 test_loss:80.85845184326172\n",
      "2294/3000 train_loss: 31.358570098876953 test_loss:70.19269561767578\n",
      "2295/3000 train_loss: 32.593544006347656 test_loss:71.96292877197266\n",
      "2296/3000 train_loss: 35.712791442871094 test_loss:75.85520935058594\n",
      "2297/3000 train_loss: 28.282541275024414 test_loss:82.75299835205078\n",
      "2298/3000 train_loss: 30.10989761352539 test_loss:76.4747085571289\n",
      "2299/3000 train_loss: 34.3569450378418 test_loss:73.43035888671875\n",
      "2300/3000 train_loss: 29.2002010345459 test_loss:79.53083801269531\n",
      "2301/3000 train_loss: 28.473587036132812 test_loss:81.72146606445312\n",
      "2302/3000 train_loss: 29.403461456298828 test_loss:77.09325408935547\n",
      "2303/3000 train_loss: 32.78597640991211 test_loss:83.17655181884766\n",
      "2304/3000 train_loss: 30.020660400390625 test_loss:77.6469497680664\n",
      "2305/3000 train_loss: 34.08987045288086 test_loss:85.74577331542969\n",
      "2306/3000 train_loss: 32.492584228515625 test_loss:80.58844757080078\n",
      "2307/3000 train_loss: 29.070167541503906 test_loss:75.84371948242188\n",
      "2308/3000 train_loss: 32.64340591430664 test_loss:88.52490234375\n",
      "2309/3000 train_loss: 31.598894119262695 test_loss:69.73360443115234\n",
      "2310/3000 train_loss: 30.553796768188477 test_loss:81.64339447021484\n",
      "2311/3000 train_loss: 30.122507095336914 test_loss:77.00300598144531\n",
      "2312/3000 train_loss: 43.183815002441406 test_loss:100.70659637451172\n",
      "2313/3000 train_loss: 44.637508392333984 test_loss:76.89585876464844\n",
      "2314/3000 train_loss: 35.65298843383789 test_loss:74.03477478027344\n",
      "2315/3000 train_loss: 30.517091751098633 test_loss:71.29127502441406\n",
      "2316/3000 train_loss: 32.53333282470703 test_loss:84.4030532836914\n",
      "2317/3000 train_loss: 34.472145080566406 test_loss:74.32025909423828\n",
      "2318/3000 train_loss: 36.18121337890625 test_loss:87.69710540771484\n",
      "2319/3000 train_loss: 47.529876708984375 test_loss:88.35521697998047\n",
      "2320/3000 train_loss: 38.45182418823242 test_loss:88.2373046875\n",
      "2321/3000 train_loss: 33.264644622802734 test_loss:77.05193328857422\n",
      "2322/3000 train_loss: 33.79560089111328 test_loss:87.925537109375\n",
      "2323/3000 train_loss: 34.08750534057617 test_loss:73.21450805664062\n",
      "2324/3000 train_loss: 32.82653045654297 test_loss:74.89608001708984\n",
      "2325/3000 train_loss: 33.61454391479492 test_loss:87.71867370605469\n",
      "2326/3000 train_loss: 30.39809226989746 test_loss:83.8555679321289\n",
      "2327/3000 train_loss: 30.790512084960938 test_loss:82.38832092285156\n",
      "2328/3000 train_loss: 29.526363372802734 test_loss:68.7660140991211\n",
      "2329/3000 train_loss: 30.04106903076172 test_loss:74.84718322753906\n",
      "2330/3000 train_loss: 36.273990631103516 test_loss:77.59119415283203\n",
      "2331/3000 train_loss: 31.188079833984375 test_loss:84.42225646972656\n",
      "2332/3000 train_loss: 35.0335578918457 test_loss:82.68364715576172\n",
      "2333/3000 train_loss: 37.27376937866211 test_loss:88.24688720703125\n",
      "2334/3000 train_loss: 34.18744659423828 test_loss:79.8432846069336\n",
      "2335/3000 train_loss: 35.023773193359375 test_loss:78.5108871459961\n",
      "2336/3000 train_loss: 33.80403137207031 test_loss:82.83747863769531\n",
      "2337/3000 train_loss: 34.74613571166992 test_loss:85.72427368164062\n",
      "2338/3000 train_loss: 31.202491760253906 test_loss:82.695068359375\n",
      "2339/3000 train_loss: 33.23344802856445 test_loss:84.12461853027344\n",
      "2340/3000 train_loss: 31.17299461364746 test_loss:82.85594940185547\n",
      "2341/3000 train_loss: 37.531646728515625 test_loss:72.554443359375\n",
      "2342/3000 train_loss: 30.092344284057617 test_loss:80.212890625\n",
      "2343/3000 train_loss: 29.830209732055664 test_loss:77.460693359375\n",
      "2344/3000 train_loss: 33.54587173461914 test_loss:81.40384674072266\n",
      "2345/3000 train_loss: 32.95114517211914 test_loss:76.64993286132812\n",
      "2346/3000 train_loss: 39.46867370605469 test_loss:83.66312408447266\n",
      "2347/3000 train_loss: 46.397979736328125 test_loss:104.8539810180664\n",
      "2348/3000 train_loss: 56.271453857421875 test_loss:84.18408966064453\n",
      "2349/3000 train_loss: 36.45772171020508 test_loss:71.83953094482422\n",
      "2350/3000 train_loss: 32.30690002441406 test_loss:82.36511993408203\n",
      "2351/3000 train_loss: 31.53929901123047 test_loss:73.59334564208984\n",
      "2352/3000 train_loss: 33.02994918823242 test_loss:71.4961166381836\n",
      "2353/3000 train_loss: 31.179309844970703 test_loss:72.34166717529297\n",
      "2354/3000 train_loss: 30.83745574951172 test_loss:75.99174499511719\n",
      "2355/3000 train_loss: 31.38060760498047 test_loss:68.86553192138672\n",
      "2356/3000 train_loss: 34.580570220947266 test_loss:80.49574279785156\n",
      "2357/3000 train_loss: 35.9669075012207 test_loss:72.22489166259766\n",
      "2358/3000 train_loss: 31.333999633789062 test_loss:76.4235610961914\n",
      "2359/3000 train_loss: 35.28254318237305 test_loss:77.22398376464844\n",
      "2360/3000 train_loss: 30.599714279174805 test_loss:82.04332733154297\n",
      "2361/3000 train_loss: 31.83717918395996 test_loss:77.37940979003906\n",
      "2362/3000 train_loss: 27.85927391052246 test_loss:77.76651763916016\n",
      "2363/3000 train_loss: 27.383739471435547 test_loss:75.18775939941406\n",
      "2364/3000 train_loss: 30.255584716796875 test_loss:77.26773834228516\n",
      "2365/3000 train_loss: 31.35272789001465 test_loss:74.42529296875\n",
      "2366/3000 train_loss: 33.01467514038086 test_loss:71.51465606689453\n",
      "2367/3000 train_loss: 30.135211944580078 test_loss:80.956298828125\n",
      "2368/3000 train_loss: 32.77033615112305 test_loss:64.67296600341797\n",
      "2369/3000 train_loss: 33.15398406982422 test_loss:87.41888427734375\n",
      "2370/3000 train_loss: 33.7385368347168 test_loss:78.6187973022461\n",
      "2371/3000 train_loss: 34.57439041137695 test_loss:91.4069595336914\n",
      "2372/3000 train_loss: 36.230464935302734 test_loss:88.66773223876953\n",
      "2373/3000 train_loss: 33.34676742553711 test_loss:86.620361328125\n",
      "2374/3000 train_loss: 32.81940460205078 test_loss:66.77241516113281\n",
      "2375/3000 train_loss: 29.809125900268555 test_loss:78.45365905761719\n",
      "2376/3000 train_loss: 32.45421600341797 test_loss:62.24053955078125\n",
      "2377/3000 train_loss: 32.84685134887695 test_loss:77.47335815429688\n",
      "2378/3000 train_loss: 29.682754516601562 test_loss:81.56269073486328\n",
      "2379/3000 train_loss: 29.835222244262695 test_loss:74.09925842285156\n",
      "2380/3000 train_loss: 29.131479263305664 test_loss:75.86811065673828\n",
      "2381/3000 train_loss: 32.562660217285156 test_loss:64.12169647216797\n",
      "2382/3000 train_loss: 31.35540008544922 test_loss:70.59687805175781\n",
      "2383/3000 train_loss: 33.365867614746094 test_loss:78.44107055664062\n",
      "2384/3000 train_loss: 33.44240188598633 test_loss:72.32372283935547\n",
      "2385/3000 train_loss: 28.844772338867188 test_loss:70.8386459350586\n",
      "2386/3000 train_loss: 31.522537231445312 test_loss:72.26697540283203\n",
      "2387/3000 train_loss: 36.08964157104492 test_loss:75.511962890625\n",
      "2388/3000 train_loss: 34.10761260986328 test_loss:78.463134765625\n",
      "2389/3000 train_loss: 31.765138626098633 test_loss:80.14722442626953\n",
      "2390/3000 train_loss: 32.82347106933594 test_loss:84.48090362548828\n",
      "2391/3000 train_loss: 39.01118087768555 test_loss:71.88674926757812\n",
      "2392/3000 train_loss: 31.816967010498047 test_loss:71.42680358886719\n",
      "2393/3000 train_loss: 36.16977310180664 test_loss:75.05775451660156\n",
      "2394/3000 train_loss: 30.373912811279297 test_loss:80.19461059570312\n",
      "2395/3000 train_loss: 32.41887283325195 test_loss:77.97669219970703\n",
      "2396/3000 train_loss: 29.267093658447266 test_loss:71.47821807861328\n",
      "2397/3000 train_loss: 30.126039505004883 test_loss:83.3722152709961\n",
      "2398/3000 train_loss: 34.76679611206055 test_loss:72.00910186767578\n",
      "2399/3000 train_loss: 36.009517669677734 test_loss:71.5984878540039\n",
      "2400/3000 train_loss: 34.7747917175293 test_loss:89.12750244140625\n",
      "2401/3000 train_loss: 32.96005630493164 test_loss:79.12228393554688\n",
      "2402/3000 train_loss: 35.826759338378906 test_loss:99.18920135498047\n",
      "2403/3000 train_loss: 30.816442489624023 test_loss:70.10897827148438\n",
      "2404/3000 train_loss: 30.19301986694336 test_loss:77.04842376708984\n",
      "2405/3000 train_loss: 29.722681045532227 test_loss:77.70584106445312\n",
      "2406/3000 train_loss: 33.982017517089844 test_loss:85.13700866699219\n",
      "2407/3000 train_loss: 30.25257682800293 test_loss:69.8868408203125\n",
      "2408/3000 train_loss: 29.502422332763672 test_loss:68.15036010742188\n",
      "2409/3000 train_loss: 33.306907653808594 test_loss:90.2532958984375\n",
      "2410/3000 train_loss: 35.327449798583984 test_loss:88.85643768310547\n",
      "2411/3000 train_loss: 32.97223663330078 test_loss:82.52230072021484\n",
      "2412/3000 train_loss: 29.617685317993164 test_loss:78.36051940917969\n",
      "2413/3000 train_loss: 29.70084571838379 test_loss:74.94417572021484\n",
      "2414/3000 train_loss: 31.222352981567383 test_loss:91.26231384277344\n",
      "2415/3000 train_loss: 33.85498046875 test_loss:88.36125183105469\n",
      "2416/3000 train_loss: 34.02605056762695 test_loss:75.30427551269531\n",
      "2417/3000 train_loss: 30.557903289794922 test_loss:76.85142517089844\n",
      "2418/3000 train_loss: 28.88283920288086 test_loss:70.7630615234375\n",
      "2419/3000 train_loss: 32.063087463378906 test_loss:82.3555908203125\n",
      "2420/3000 train_loss: 42.565338134765625 test_loss:72.14485168457031\n",
      "2421/3000 train_loss: 31.313344955444336 test_loss:76.72428894042969\n",
      "2422/3000 train_loss: 31.813066482543945 test_loss:76.38458251953125\n",
      "2423/3000 train_loss: 32.4066047668457 test_loss:82.97615814208984\n",
      "2424/3000 train_loss: 29.979595184326172 test_loss:70.68202209472656\n",
      "2425/3000 train_loss: 31.13854217529297 test_loss:98.87726593017578\n",
      "2426/3000 train_loss: 42.47377395629883 test_loss:81.62551879882812\n",
      "2427/3000 train_loss: 34.429195404052734 test_loss:80.6690444946289\n",
      "2428/3000 train_loss: 28.96744728088379 test_loss:74.94507598876953\n",
      "2429/3000 train_loss: 36.520633697509766 test_loss:108.37574768066406\n",
      "2430/3000 train_loss: 52.384090423583984 test_loss:79.7373275756836\n",
      "2431/3000 train_loss: 30.70197105407715 test_loss:73.36621856689453\n",
      "2432/3000 train_loss: 31.612802505493164 test_loss:65.47156524658203\n",
      "2433/3000 train_loss: 33.192623138427734 test_loss:78.79706573486328\n",
      "2434/3000 train_loss: 34.78217315673828 test_loss:77.09199523925781\n",
      "2435/3000 train_loss: 29.573511123657227 test_loss:69.57759857177734\n",
      "2436/3000 train_loss: 37.28932571411133 test_loss:82.78903198242188\n",
      "2437/3000 train_loss: 39.2486457824707 test_loss:93.07954406738281\n",
      "2438/3000 train_loss: 32.09229278564453 test_loss:73.11841583251953\n",
      "2439/3000 train_loss: 31.391935348510742 test_loss:72.74717712402344\n",
      "2440/3000 train_loss: 29.65374755859375 test_loss:86.3857192993164\n",
      "2441/3000 train_loss: 31.593931198120117 test_loss:87.18536376953125\n",
      "2442/3000 train_loss: 33.36476135253906 test_loss:76.5616455078125\n",
      "2443/3000 train_loss: 33.71305465698242 test_loss:76.80936431884766\n",
      "2444/3000 train_loss: 34.37086868286133 test_loss:80.25603485107422\n",
      "2445/3000 train_loss: 34.5147590637207 test_loss:83.48109436035156\n",
      "2446/3000 train_loss: 30.576868057250977 test_loss:80.05732727050781\n",
      "2447/3000 train_loss: 32.263526916503906 test_loss:67.33734130859375\n",
      "2448/3000 train_loss: 29.490995407104492 test_loss:81.1134262084961\n",
      "2449/3000 train_loss: 25.759498596191406 test_loss:75.05958557128906\n",
      "2450/3000 train_loss: 30.925289154052734 test_loss:73.88763427734375\n",
      "2451/3000 train_loss: 31.959396362304688 test_loss:72.2152099609375\n",
      "2452/3000 train_loss: 39.603759765625 test_loss:83.47366333007812\n",
      "2453/3000 train_loss: 29.27769660949707 test_loss:70.38067626953125\n",
      "2454/3000 train_loss: 30.36981773376465 test_loss:91.12252807617188\n",
      "2455/3000 train_loss: 32.99939727783203 test_loss:77.81925964355469\n",
      "2456/3000 train_loss: 31.319807052612305 test_loss:87.26174926757812\n",
      "2457/3000 train_loss: 31.941165924072266 test_loss:79.03218841552734\n",
      "2458/3000 train_loss: 31.791152954101562 test_loss:73.78778839111328\n",
      "2459/3000 train_loss: 32.63435363769531 test_loss:75.66010284423828\n",
      "2460/3000 train_loss: 32.19488525390625 test_loss:77.01424407958984\n",
      "2461/3000 train_loss: 27.395666122436523 test_loss:79.1703872680664\n",
      "2462/3000 train_loss: 28.735950469970703 test_loss:67.44779205322266\n",
      "2463/3000 train_loss: 26.405874252319336 test_loss:79.75739288330078\n",
      "2464/3000 train_loss: 33.32310104370117 test_loss:76.8311996459961\n",
      "2465/3000 train_loss: 36.53786849975586 test_loss:80.32832336425781\n",
      "2466/3000 train_loss: 31.204280853271484 test_loss:90.43559265136719\n",
      "2467/3000 train_loss: 38.314659118652344 test_loss:80.44945526123047\n",
      "2468/3000 train_loss: 41.71001434326172 test_loss:72.38422393798828\n",
      "2469/3000 train_loss: 44.77505111694336 test_loss:80.36197662353516\n",
      "2470/3000 train_loss: 33.781925201416016 test_loss:96.43396759033203\n",
      "2471/3000 train_loss: 35.223670959472656 test_loss:85.62870788574219\n",
      "2472/3000 train_loss: 27.963520050048828 test_loss:73.61843872070312\n",
      "2473/3000 train_loss: 32.463130950927734 test_loss:75.79444885253906\n",
      "2474/3000 train_loss: 31.515026092529297 test_loss:81.61268615722656\n",
      "2475/3000 train_loss: 34.370059967041016 test_loss:76.02642822265625\n",
      "2476/3000 train_loss: 30.946569442749023 test_loss:88.74925994873047\n",
      "2477/3000 train_loss: 30.735471725463867 test_loss:82.32850646972656\n",
      "2478/3000 train_loss: 28.080429077148438 test_loss:74.98650360107422\n",
      "2479/3000 train_loss: 28.41973304748535 test_loss:87.10990905761719\n",
      "2480/3000 train_loss: 31.059961318969727 test_loss:72.25004577636719\n",
      "2481/3000 train_loss: 30.206634521484375 test_loss:77.53890228271484\n",
      "2482/3000 train_loss: 31.388927459716797 test_loss:78.91057586669922\n",
      "2483/3000 train_loss: 28.25882911682129 test_loss:81.00247192382812\n",
      "2484/3000 train_loss: 38.632667541503906 test_loss:81.00938415527344\n",
      "2485/3000 train_loss: 34.54249954223633 test_loss:83.09945678710938\n",
      "2486/3000 train_loss: 37.383548736572266 test_loss:79.2181625366211\n",
      "2487/3000 train_loss: 36.8294563293457 test_loss:70.72579956054688\n",
      "2488/3000 train_loss: 32.024208068847656 test_loss:79.30245971679688\n",
      "2489/3000 train_loss: 34.49959182739258 test_loss:76.22110748291016\n",
      "2490/3000 train_loss: 34.854454040527344 test_loss:117.0394287109375\n",
      "2491/3000 train_loss: 29.96860694885254 test_loss:75.91036987304688\n",
      "2492/3000 train_loss: 29.666297912597656 test_loss:77.50105285644531\n",
      "2493/3000 train_loss: 33.58652114868164 test_loss:97.40054321289062\n",
      "2494/3000 train_loss: 38.55531311035156 test_loss:69.4869384765625\n",
      "2495/3000 train_loss: 27.67630958557129 test_loss:69.21763610839844\n",
      "2496/3000 train_loss: 30.36666488647461 test_loss:73.23725891113281\n",
      "2497/3000 train_loss: 33.53917694091797 test_loss:78.0009765625\n",
      "2498/3000 train_loss: 29.140735626220703 test_loss:78.87290954589844\n",
      "2499/3000 train_loss: 25.04361343383789 test_loss:66.24345397949219\n",
      "2500/3000 train_loss: 29.12733268737793 test_loss:77.778076171875\n",
      "2501/3000 train_loss: 36.555049896240234 test_loss:86.91638946533203\n",
      "2502/3000 train_loss: 32.240272521972656 test_loss:73.48362731933594\n",
      "2503/3000 train_loss: 32.937416076660156 test_loss:82.33992767333984\n",
      "2504/3000 train_loss: 30.418500900268555 test_loss:77.15876770019531\n",
      "2505/3000 train_loss: 30.4211483001709 test_loss:74.779296875\n",
      "2506/3000 train_loss: 26.72821044921875 test_loss:82.3110122680664\n",
      "2507/3000 train_loss: 32.29030227661133 test_loss:89.33070373535156\n",
      "2508/3000 train_loss: 32.43403625488281 test_loss:92.37855529785156\n",
      "2509/3000 train_loss: 33.83102798461914 test_loss:78.85004425048828\n",
      "2510/3000 train_loss: 33.16340637207031 test_loss:90.97012329101562\n",
      "2511/3000 train_loss: 36.400028228759766 test_loss:86.62332153320312\n",
      "2512/3000 train_loss: 29.942508697509766 test_loss:76.63534545898438\n",
      "2513/3000 train_loss: 32.033172607421875 test_loss:78.46430206298828\n",
      "2514/3000 train_loss: 32.40890884399414 test_loss:91.03559112548828\n",
      "2515/3000 train_loss: 36.59284591674805 test_loss:76.43348693847656\n",
      "2516/3000 train_loss: 34.463783264160156 test_loss:76.50102233886719\n",
      "2517/3000 train_loss: 33.40159606933594 test_loss:73.43585205078125\n",
      "2518/3000 train_loss: 30.658321380615234 test_loss:74.54374694824219\n",
      "2519/3000 train_loss: 37.395320892333984 test_loss:72.77017974853516\n",
      "2520/3000 train_loss: 29.45978546142578 test_loss:88.37572479248047\n",
      "2521/3000 train_loss: 36.65816879272461 test_loss:103.85108947753906\n",
      "2522/3000 train_loss: 32.748111724853516 test_loss:71.497314453125\n",
      "2523/3000 train_loss: 29.479158401489258 test_loss:85.31982421875\n",
      "2524/3000 train_loss: 31.187131881713867 test_loss:83.45703125\n",
      "2525/3000 train_loss: 31.04941177368164 test_loss:71.42513275146484\n",
      "2526/3000 train_loss: 32.03611373901367 test_loss:104.35067749023438\n",
      "2527/3000 train_loss: 38.266361236572266 test_loss:78.26216888427734\n",
      "2528/3000 train_loss: 31.39011001586914 test_loss:82.50814056396484\n",
      "2529/3000 train_loss: 34.55580520629883 test_loss:79.79808807373047\n",
      "2530/3000 train_loss: 35.06178665161133 test_loss:82.50146484375\n",
      "2531/3000 train_loss: 33.65098571777344 test_loss:77.37100982666016\n",
      "2532/3000 train_loss: 27.265735626220703 test_loss:70.79644775390625\n",
      "2533/3000 train_loss: 29.276721954345703 test_loss:67.97010803222656\n",
      "2534/3000 train_loss: 28.942630767822266 test_loss:77.75454711914062\n",
      "2535/3000 train_loss: 30.888408660888672 test_loss:69.2006607055664\n",
      "2536/3000 train_loss: 30.24654769897461 test_loss:66.58345031738281\n",
      "2537/3000 train_loss: 39.37181854248047 test_loss:100.04568481445312\n",
      "2538/3000 train_loss: 30.18702507019043 test_loss:84.58363342285156\n",
      "2539/3000 train_loss: 29.872295379638672 test_loss:69.74868774414062\n",
      "2540/3000 train_loss: 25.277645111083984 test_loss:83.29266357421875\n",
      "2541/3000 train_loss: 31.74555778503418 test_loss:77.8006820678711\n",
      "2542/3000 train_loss: 29.457260131835938 test_loss:75.01022338867188\n",
      "2543/3000 train_loss: 31.59370994567871 test_loss:75.43135833740234\n",
      "2544/3000 train_loss: 27.187480926513672 test_loss:75.61466217041016\n",
      "2545/3000 train_loss: 28.381141662597656 test_loss:82.30433654785156\n",
      "2546/3000 train_loss: 31.418176651000977 test_loss:83.58809661865234\n",
      "2547/3000 train_loss: 27.66715431213379 test_loss:82.04869079589844\n",
      "2548/3000 train_loss: 31.526762008666992 test_loss:78.21202850341797\n",
      "2549/3000 train_loss: 31.16726303100586 test_loss:75.21590423583984\n",
      "2550/3000 train_loss: 40.012107849121094 test_loss:83.59626007080078\n",
      "2551/3000 train_loss: 31.323278427124023 test_loss:81.48307800292969\n",
      "2552/3000 train_loss: 28.077869415283203 test_loss:97.13798522949219\n",
      "2553/3000 train_loss: 30.424623489379883 test_loss:77.62413024902344\n",
      "2554/3000 train_loss: 28.989822387695312 test_loss:86.43125915527344\n",
      "2555/3000 train_loss: 31.91286849975586 test_loss:83.21211242675781\n",
      "2556/3000 train_loss: 37.59257888793945 test_loss:73.08470153808594\n",
      "2557/3000 train_loss: 36.31079864501953 test_loss:74.3475570678711\n",
      "2558/3000 train_loss: 27.713932037353516 test_loss:74.68154907226562\n",
      "2559/3000 train_loss: 26.036853790283203 test_loss:80.9017562866211\n",
      "2560/3000 train_loss: 28.402122497558594 test_loss:73.63714599609375\n",
      "2561/3000 train_loss: 28.98350715637207 test_loss:80.17033386230469\n",
      "2562/3000 train_loss: 33.303077697753906 test_loss:79.20430755615234\n",
      "2563/3000 train_loss: 35.07709884643555 test_loss:66.30209350585938\n",
      "2564/3000 train_loss: 34.01778793334961 test_loss:75.95733642578125\n",
      "2565/3000 train_loss: 30.49038314819336 test_loss:75.04158020019531\n",
      "2566/3000 train_loss: 28.19007682800293 test_loss:75.89071655273438\n",
      "2567/3000 train_loss: 30.01934814453125 test_loss:79.30119323730469\n",
      "2568/3000 train_loss: 30.471389770507812 test_loss:72.47318267822266\n",
      "2569/3000 train_loss: 31.22786521911621 test_loss:85.0864028930664\n",
      "2570/3000 train_loss: 32.59358215332031 test_loss:88.44386291503906\n",
      "2571/3000 train_loss: 36.3572998046875 test_loss:85.88055419921875\n",
      "2572/3000 train_loss: 42.01970672607422 test_loss:69.64347076416016\n",
      "2573/3000 train_loss: 30.59872817993164 test_loss:67.40101623535156\n",
      "2574/3000 train_loss: 28.21548080444336 test_loss:81.94461822509766\n",
      "2575/3000 train_loss: 36.322818756103516 test_loss:70.6512222290039\n",
      "2576/3000 train_loss: 30.260887145996094 test_loss:79.9474868774414\n",
      "2577/3000 train_loss: 35.18722915649414 test_loss:79.52725219726562\n",
      "2578/3000 train_loss: 29.23367691040039 test_loss:72.7029037475586\n",
      "2579/3000 train_loss: 31.430456161499023 test_loss:74.44499206542969\n",
      "2580/3000 train_loss: 28.31395721435547 test_loss:73.4159927368164\n",
      "2581/3000 train_loss: 30.36805534362793 test_loss:81.6568603515625\n",
      "2582/3000 train_loss: 28.05008316040039 test_loss:73.13323974609375\n",
      "2583/3000 train_loss: 37.07039260864258 test_loss:72.6559829711914\n",
      "2584/3000 train_loss: 31.608850479125977 test_loss:96.28854370117188\n",
      "2585/3000 train_loss: 39.34184646606445 test_loss:82.71778106689453\n",
      "2586/3000 train_loss: 31.320146560668945 test_loss:78.78316497802734\n",
      "2587/3000 train_loss: 28.107812881469727 test_loss:75.7679443359375\n",
      "2588/3000 train_loss: 26.33551025390625 test_loss:81.4526596069336\n",
      "2589/3000 train_loss: 33.499141693115234 test_loss:82.00089263916016\n",
      "2590/3000 train_loss: 36.090030670166016 test_loss:70.84190368652344\n",
      "2591/3000 train_loss: 27.63953399658203 test_loss:73.4031982421875\n",
      "2592/3000 train_loss: 29.094680786132812 test_loss:81.20948028564453\n",
      "2593/3000 train_loss: 31.963762283325195 test_loss:79.64836120605469\n",
      "2594/3000 train_loss: 29.809492111206055 test_loss:68.46187591552734\n",
      "2595/3000 train_loss: 32.571434020996094 test_loss:78.93185424804688\n",
      "2596/3000 train_loss: 34.85075759887695 test_loss:82.23049926757812\n",
      "2597/3000 train_loss: 37.50614929199219 test_loss:78.19927978515625\n",
      "2598/3000 train_loss: 32.44655990600586 test_loss:75.45742797851562\n",
      "2599/3000 train_loss: 31.68364715576172 test_loss:74.19210052490234\n",
      "2600/3000 train_loss: 27.08904266357422 test_loss:72.46118927001953\n",
      "2601/3000 train_loss: 33.58613967895508 test_loss:79.24407196044922\n",
      "2602/3000 train_loss: 34.50114822387695 test_loss:99.40963745117188\n",
      "2603/3000 train_loss: 34.11100769042969 test_loss:70.08505249023438\n",
      "2604/3000 train_loss: 32.3175048828125 test_loss:72.1791763305664\n",
      "2605/3000 train_loss: 27.165781021118164 test_loss:74.7182846069336\n",
      "2606/3000 train_loss: 33.63018035888672 test_loss:77.30294799804688\n",
      "2607/3000 train_loss: 32.90686798095703 test_loss:71.29389953613281\n",
      "2608/3000 train_loss: 34.38887023925781 test_loss:91.11797332763672\n",
      "2609/3000 train_loss: 29.64321517944336 test_loss:76.48348999023438\n",
      "2610/3000 train_loss: 27.983592987060547 test_loss:73.45291137695312\n",
      "2611/3000 train_loss: 28.029537200927734 test_loss:87.4315414428711\n",
      "2612/3000 train_loss: 34.230995178222656 test_loss:74.39048767089844\n",
      "2613/3000 train_loss: 30.756397247314453 test_loss:81.32950592041016\n",
      "2614/3000 train_loss: 33.27891540527344 test_loss:89.50810241699219\n",
      "2615/3000 train_loss: 37.70564651489258 test_loss:71.9168472290039\n",
      "2616/3000 train_loss: 31.096254348754883 test_loss:78.14418029785156\n",
      "2617/3000 train_loss: 38.031028747558594 test_loss:87.07585906982422\n",
      "2618/3000 train_loss: 27.827545166015625 test_loss:72.06706237792969\n",
      "2619/3000 train_loss: 31.648767471313477 test_loss:77.70021057128906\n",
      "2620/3000 train_loss: 31.414091110229492 test_loss:79.44225311279297\n",
      "2621/3000 train_loss: 29.886539459228516 test_loss:70.7118911743164\n",
      "2622/3000 train_loss: 37.8654899597168 test_loss:77.08653259277344\n",
      "2623/3000 train_loss: 34.36076736450195 test_loss:71.07482147216797\n",
      "2624/3000 train_loss: 34.198787689208984 test_loss:79.58360290527344\n",
      "2625/3000 train_loss: 30.58756446838379 test_loss:76.53900146484375\n",
      "2626/3000 train_loss: 31.112634658813477 test_loss:69.96817016601562\n",
      "2627/3000 train_loss: 27.324665069580078 test_loss:75.63506317138672\n",
      "2628/3000 train_loss: 30.212448120117188 test_loss:73.91256713867188\n",
      "2629/3000 train_loss: 27.11518669128418 test_loss:65.99150085449219\n",
      "2630/3000 train_loss: 31.820344924926758 test_loss:82.30635070800781\n",
      "2631/3000 train_loss: 31.665424346923828 test_loss:71.32798767089844\n",
      "2632/3000 train_loss: 28.75724983215332 test_loss:85.83956909179688\n",
      "2633/3000 train_loss: 27.004974365234375 test_loss:70.5666732788086\n",
      "2634/3000 train_loss: 28.69171142578125 test_loss:76.92420959472656\n",
      "2635/3000 train_loss: 29.13450813293457 test_loss:72.16141510009766\n",
      "2636/3000 train_loss: 25.90923500061035 test_loss:72.26947021484375\n",
      "2637/3000 train_loss: 32.171836853027344 test_loss:88.05181884765625\n",
      "2638/3000 train_loss: 37.53137969970703 test_loss:83.86681365966797\n",
      "2639/3000 train_loss: 37.999691009521484 test_loss:79.00729370117188\n",
      "2640/3000 train_loss: 37.69926071166992 test_loss:117.44010925292969\n",
      "2641/3000 train_loss: 30.756925582885742 test_loss:67.18875122070312\n",
      "2642/3000 train_loss: 30.235069274902344 test_loss:78.002685546875\n",
      "2643/3000 train_loss: 28.22022819519043 test_loss:71.03186798095703\n",
      "2644/3000 train_loss: 31.78838348388672 test_loss:81.19865417480469\n",
      "2645/3000 train_loss: 26.884061813354492 test_loss:78.52462005615234\n",
      "2646/3000 train_loss: 30.349578857421875 test_loss:89.24537658691406\n",
      "2647/3000 train_loss: 30.158607482910156 test_loss:80.05937194824219\n",
      "2648/3000 train_loss: 28.767568588256836 test_loss:73.71326446533203\n",
      "2649/3000 train_loss: 33.427886962890625 test_loss:83.74787139892578\n",
      "2650/3000 train_loss: 32.201324462890625 test_loss:83.90664672851562\n",
      "2651/3000 train_loss: 30.162240982055664 test_loss:85.52542877197266\n",
      "2652/3000 train_loss: 31.261919021606445 test_loss:79.61892700195312\n",
      "2653/3000 train_loss: 32.50111770629883 test_loss:84.03660583496094\n",
      "2654/3000 train_loss: 30.017724990844727 test_loss:74.73430633544922\n",
      "2655/3000 train_loss: 31.988723754882812 test_loss:77.8183822631836\n",
      "2656/3000 train_loss: 32.281280517578125 test_loss:72.45376586914062\n",
      "2657/3000 train_loss: 32.09435272216797 test_loss:79.17658996582031\n",
      "2658/3000 train_loss: 32.706119537353516 test_loss:93.53305053710938\n",
      "2659/3000 train_loss: 26.448694229125977 test_loss:84.76451873779297\n",
      "2660/3000 train_loss: 27.7594051361084 test_loss:72.79866027832031\n",
      "2661/3000 train_loss: 29.774093627929688 test_loss:76.91030883789062\n",
      "2662/3000 train_loss: 28.56084632873535 test_loss:70.87460327148438\n",
      "2663/3000 train_loss: 34.43453598022461 test_loss:72.98477935791016\n",
      "2664/3000 train_loss: 30.99640655517578 test_loss:77.78134155273438\n",
      "2665/3000 train_loss: 31.352188110351562 test_loss:73.84515380859375\n",
      "2666/3000 train_loss: 27.785242080688477 test_loss:72.86270904541016\n",
      "2667/3000 train_loss: 24.19447135925293 test_loss:74.41217803955078\n",
      "2668/3000 train_loss: 28.557735443115234 test_loss:79.16304016113281\n",
      "2669/3000 train_loss: 29.95935821533203 test_loss:72.34354400634766\n",
      "2670/3000 train_loss: 33.4666748046875 test_loss:88.67716979980469\n",
      "2671/3000 train_loss: 32.222190856933594 test_loss:79.54739379882812\n",
      "2672/3000 train_loss: 28.213285446166992 test_loss:71.01163482666016\n",
      "2673/3000 train_loss: 34.33707809448242 test_loss:81.28703308105469\n",
      "2674/3000 train_loss: 31.27530860900879 test_loss:78.81369018554688\n",
      "2675/3000 train_loss: 26.377126693725586 test_loss:73.19208526611328\n",
      "2676/3000 train_loss: 33.219696044921875 test_loss:87.73995971679688\n",
      "2677/3000 train_loss: 31.666078567504883 test_loss:67.83479309082031\n",
      "2678/3000 train_loss: 31.130664825439453 test_loss:74.36750030517578\n",
      "2679/3000 train_loss: 35.467411041259766 test_loss:77.82682800292969\n",
      "2680/3000 train_loss: 31.106849670410156 test_loss:72.77605438232422\n",
      "2681/3000 train_loss: 40.35566711425781 test_loss:85.06140899658203\n",
      "2682/3000 train_loss: 29.05042266845703 test_loss:69.78951263427734\n",
      "2683/3000 train_loss: 29.896007537841797 test_loss:81.46311950683594\n",
      "2684/3000 train_loss: 32.04598617553711 test_loss:71.6434326171875\n",
      "2685/3000 train_loss: 36.352699279785156 test_loss:94.33141326904297\n",
      "2686/3000 train_loss: 37.10398864746094 test_loss:81.73905944824219\n",
      "2687/3000 train_loss: 32.49803924560547 test_loss:79.90752410888672\n",
      "2688/3000 train_loss: 31.885360717773438 test_loss:89.88758850097656\n",
      "2689/3000 train_loss: 30.460405349731445 test_loss:75.71012115478516\n",
      "2690/3000 train_loss: 34.01407241821289 test_loss:80.95802307128906\n",
      "2691/3000 train_loss: 35.08665466308594 test_loss:84.76641845703125\n",
      "2692/3000 train_loss: 30.051090240478516 test_loss:71.94725036621094\n",
      "2693/3000 train_loss: 31.622039794921875 test_loss:71.8255615234375\n",
      "2694/3000 train_loss: 29.393619537353516 test_loss:76.4625015258789\n",
      "2695/3000 train_loss: 28.838491439819336 test_loss:84.29706573486328\n",
      "2696/3000 train_loss: 27.976524353027344 test_loss:74.25533294677734\n",
      "2697/3000 train_loss: 28.532594680786133 test_loss:83.66302490234375\n",
      "2698/3000 train_loss: 27.92453956604004 test_loss:81.04330444335938\n",
      "2699/3000 train_loss: 27.56724739074707 test_loss:68.69564819335938\n",
      "2700/3000 train_loss: 27.567644119262695 test_loss:71.78411102294922\n",
      "2701/3000 train_loss: 28.695083618164062 test_loss:84.43160247802734\n",
      "2702/3000 train_loss: 29.656660079956055 test_loss:84.61327362060547\n",
      "2703/3000 train_loss: 34.129085540771484 test_loss:91.98230743408203\n",
      "2704/3000 train_loss: 32.57524108886719 test_loss:72.1414794921875\n",
      "2705/3000 train_loss: 31.395009994506836 test_loss:74.97946166992188\n",
      "2706/3000 train_loss: 34.699485778808594 test_loss:79.4486083984375\n",
      "2707/3000 train_loss: 36.5925407409668 test_loss:84.78145599365234\n",
      "2708/3000 train_loss: 32.723392486572266 test_loss:66.70574188232422\n",
      "2709/3000 train_loss: 27.62615966796875 test_loss:80.85401916503906\n",
      "2710/3000 train_loss: 31.21590805053711 test_loss:72.26758575439453\n",
      "2711/3000 train_loss: 32.85380554199219 test_loss:70.43589782714844\n",
      "2712/3000 train_loss: 32.7369270324707 test_loss:72.84649658203125\n",
      "2713/3000 train_loss: 32.80134582519531 test_loss:72.105712890625\n",
      "2714/3000 train_loss: 32.351951599121094 test_loss:76.41000366210938\n",
      "2715/3000 train_loss: 30.570220947265625 test_loss:73.98136138916016\n",
      "2716/3000 train_loss: 31.52690315246582 test_loss:81.70693969726562\n",
      "2717/3000 train_loss: 32.15766143798828 test_loss:68.1707763671875\n",
      "2718/3000 train_loss: 31.656843185424805 test_loss:83.4251937866211\n",
      "2719/3000 train_loss: 34.76601791381836 test_loss:72.98016357421875\n",
      "2720/3000 train_loss: 32.145591735839844 test_loss:77.06625366210938\n",
      "2721/3000 train_loss: 28.89378547668457 test_loss:76.4917221069336\n",
      "2722/3000 train_loss: 28.246599197387695 test_loss:75.40310668945312\n",
      "2723/3000 train_loss: 30.59302520751953 test_loss:77.18119812011719\n",
      "2724/3000 train_loss: 30.12067985534668 test_loss:84.58708953857422\n",
      "2725/3000 train_loss: 32.12651824951172 test_loss:66.509521484375\n",
      "2726/3000 train_loss: 25.698076248168945 test_loss:65.68119049072266\n",
      "2727/3000 train_loss: 28.16386604309082 test_loss:88.55302429199219\n",
      "2728/3000 train_loss: 30.02094078063965 test_loss:70.85055541992188\n",
      "2729/3000 train_loss: 28.550931930541992 test_loss:77.31627655029297\n",
      "2730/3000 train_loss: 28.767017364501953 test_loss:78.63903045654297\n",
      "2731/3000 train_loss: 35.46475601196289 test_loss:73.24874114990234\n",
      "2732/3000 train_loss: 32.94214630126953 test_loss:65.4608383178711\n",
      "2733/3000 train_loss: 29.181833267211914 test_loss:81.13541412353516\n",
      "2734/3000 train_loss: 30.43830680847168 test_loss:74.83146667480469\n",
      "2735/3000 train_loss: 28.091045379638672 test_loss:73.31478881835938\n",
      "2736/3000 train_loss: 30.435632705688477 test_loss:85.25521850585938\n",
      "2737/3000 train_loss: 31.856769561767578 test_loss:85.35386657714844\n",
      "2738/3000 train_loss: 29.902647018432617 test_loss:69.93994140625\n",
      "2739/3000 train_loss: 26.981792449951172 test_loss:79.66874694824219\n",
      "2740/3000 train_loss: 32.26350402832031 test_loss:70.39225006103516\n",
      "2741/3000 train_loss: 28.728191375732422 test_loss:75.4371109008789\n",
      "2742/3000 train_loss: 32.04396438598633 test_loss:75.76429748535156\n",
      "2743/3000 train_loss: 33.761512756347656 test_loss:101.20818328857422\n",
      "2744/3000 train_loss: 35.1568717956543 test_loss:74.42218017578125\n",
      "2745/3000 train_loss: 26.22821617126465 test_loss:77.32815551757812\n",
      "2746/3000 train_loss: 33.94221496582031 test_loss:74.76268768310547\n",
      "2747/3000 train_loss: 27.5790958404541 test_loss:72.70726776123047\n",
      "2748/3000 train_loss: 32.66594314575195 test_loss:75.15347290039062\n",
      "2749/3000 train_loss: 33.75407409667969 test_loss:66.48768615722656\n",
      "2750/3000 train_loss: 32.61449432373047 test_loss:72.3468246459961\n",
      "2751/3000 train_loss: 26.386474609375 test_loss:73.60345458984375\n",
      "2752/3000 train_loss: 28.636859893798828 test_loss:68.11114501953125\n",
      "2753/3000 train_loss: 34.239097595214844 test_loss:96.20832061767578\n",
      "2754/3000 train_loss: 28.024171829223633 test_loss:68.88272857666016\n",
      "2755/3000 train_loss: 28.473800659179688 test_loss:78.73711395263672\n",
      "2756/3000 train_loss: 41.84119415283203 test_loss:82.03924560546875\n",
      "2757/3000 train_loss: 33.829925537109375 test_loss:76.00996398925781\n",
      "2758/3000 train_loss: 33.09157943725586 test_loss:70.89450073242188\n",
      "2759/3000 train_loss: 27.946434020996094 test_loss:74.92987060546875\n",
      "2760/3000 train_loss: 29.49781036376953 test_loss:75.95889282226562\n",
      "2761/3000 train_loss: 30.251224517822266 test_loss:73.80982971191406\n",
      "2762/3000 train_loss: 30.688066482543945 test_loss:88.2453842163086\n",
      "2763/3000 train_loss: 34.069122314453125 test_loss:79.96601867675781\n",
      "2764/3000 train_loss: 34.22587966918945 test_loss:68.2302017211914\n",
      "2765/3000 train_loss: 35.3810920715332 test_loss:103.34858703613281\n",
      "2766/3000 train_loss: 41.297935485839844 test_loss:80.09266662597656\n",
      "2767/3000 train_loss: 33.23176193237305 test_loss:73.45684051513672\n",
      "2768/3000 train_loss: 29.909929275512695 test_loss:73.85001373291016\n",
      "2769/3000 train_loss: 27.23912239074707 test_loss:73.7337417602539\n",
      "2770/3000 train_loss: 28.297651290893555 test_loss:78.44451141357422\n",
      "2771/3000 train_loss: 32.66457748413086 test_loss:82.33184051513672\n",
      "2772/3000 train_loss: 33.521202087402344 test_loss:78.77743530273438\n",
      "2773/3000 train_loss: 33.89591598510742 test_loss:78.18984985351562\n",
      "2774/3000 train_loss: 33.54240417480469 test_loss:72.6068344116211\n",
      "2775/3000 train_loss: 32.03739929199219 test_loss:86.03711700439453\n",
      "2776/3000 train_loss: 28.54522132873535 test_loss:74.97035217285156\n",
      "2777/3000 train_loss: 32.56587600708008 test_loss:72.53140258789062\n",
      "2778/3000 train_loss: 30.239383697509766 test_loss:79.03863525390625\n",
      "2779/3000 train_loss: 27.849590301513672 test_loss:71.51761627197266\n",
      "2780/3000 train_loss: 33.360565185546875 test_loss:76.6202163696289\n",
      "2781/3000 train_loss: 27.082679748535156 test_loss:74.3104019165039\n",
      "2782/3000 train_loss: 32.420501708984375 test_loss:72.03263092041016\n",
      "2783/3000 train_loss: 35.3421630859375 test_loss:73.20600128173828\n",
      "2784/3000 train_loss: 24.326560974121094 test_loss:71.67323303222656\n",
      "2785/3000 train_loss: 28.68778419494629 test_loss:77.07425689697266\n",
      "2786/3000 train_loss: 29.198461532592773 test_loss:82.46363830566406\n",
      "2787/3000 train_loss: 28.954870223999023 test_loss:79.56470489501953\n",
      "2788/3000 train_loss: 30.074674606323242 test_loss:70.72549438476562\n",
      "2789/3000 train_loss: 25.549463272094727 test_loss:70.86372375488281\n",
      "2790/3000 train_loss: 28.04804229736328 test_loss:69.63536834716797\n",
      "2791/3000 train_loss: 29.324371337890625 test_loss:76.95633697509766\n",
      "2792/3000 train_loss: 36.246639251708984 test_loss:84.39295959472656\n",
      "2793/3000 train_loss: 33.05718231201172 test_loss:85.04339599609375\n",
      "2794/3000 train_loss: 30.3941593170166 test_loss:85.78587341308594\n",
      "2795/3000 train_loss: 32.90665054321289 test_loss:67.79945373535156\n",
      "2796/3000 train_loss: 31.885522842407227 test_loss:75.75350189208984\n",
      "2797/3000 train_loss: 28.460237503051758 test_loss:83.31891632080078\n",
      "2798/3000 train_loss: 35.557003021240234 test_loss:77.58485412597656\n",
      "2799/3000 train_loss: 29.701126098632812 test_loss:73.87895965576172\n",
      "2800/3000 train_loss: 37.596920013427734 test_loss:72.24003601074219\n",
      "2801/3000 train_loss: 41.34093475341797 test_loss:73.95979309082031\n",
      "2802/3000 train_loss: 29.889127731323242 test_loss:81.6443862915039\n",
      "2803/3000 train_loss: 32.21889114379883 test_loss:65.20634460449219\n",
      "2804/3000 train_loss: 26.731740951538086 test_loss:66.70563507080078\n",
      "2805/3000 train_loss: 27.123207092285156 test_loss:71.67892456054688\n",
      "2806/3000 train_loss: 28.84984588623047 test_loss:70.36100006103516\n",
      "2807/3000 train_loss: 28.21349334716797 test_loss:72.39482879638672\n",
      "2808/3000 train_loss: 24.7496280670166 test_loss:78.70394897460938\n",
      "2809/3000 train_loss: 28.198633193969727 test_loss:71.53094482421875\n",
      "2810/3000 train_loss: 31.80525016784668 test_loss:75.35630798339844\n",
      "2811/3000 train_loss: 36.98563766479492 test_loss:73.1353988647461\n",
      "2812/3000 train_loss: 32.08674621582031 test_loss:69.1108627319336\n",
      "2813/3000 train_loss: 29.896221160888672 test_loss:72.09444427490234\n",
      "2814/3000 train_loss: 26.372943878173828 test_loss:80.12608337402344\n",
      "2815/3000 train_loss: 31.413925170898438 test_loss:66.20166015625\n",
      "2816/3000 train_loss: 27.624916076660156 test_loss:75.2095947265625\n",
      "2817/3000 train_loss: 26.67099952697754 test_loss:81.65387725830078\n",
      "2818/3000 train_loss: 28.984315872192383 test_loss:72.5050277709961\n",
      "2819/3000 train_loss: 28.480304718017578 test_loss:73.71234893798828\n",
      "2820/3000 train_loss: 27.49176788330078 test_loss:75.55430603027344\n",
      "2821/3000 train_loss: 27.970258712768555 test_loss:81.54763793945312\n",
      "2822/3000 train_loss: 32.25461959838867 test_loss:69.27378845214844\n",
      "2823/3000 train_loss: 30.616382598876953 test_loss:67.49405670166016\n",
      "2824/3000 train_loss: 25.70388412475586 test_loss:72.9819564819336\n",
      "2825/3000 train_loss: 30.939865112304688 test_loss:86.76368713378906\n",
      "2826/3000 train_loss: 31.843305587768555 test_loss:69.38429260253906\n",
      "2827/3000 train_loss: 32.97540283203125 test_loss:77.78597259521484\n",
      "2828/3000 train_loss: 29.855056762695312 test_loss:73.4249038696289\n",
      "2829/3000 train_loss: 28.041955947875977 test_loss:75.93148803710938\n",
      "2830/3000 train_loss: 26.055828094482422 test_loss:73.1480941772461\n",
      "2831/3000 train_loss: 27.677310943603516 test_loss:67.32595825195312\n",
      "2832/3000 train_loss: 32.475120544433594 test_loss:73.60079193115234\n",
      "2833/3000 train_loss: 26.791284561157227 test_loss:72.03414154052734\n",
      "2834/3000 train_loss: 33.25017547607422 test_loss:81.92234802246094\n",
      "2835/3000 train_loss: 34.5040397644043 test_loss:68.47589874267578\n",
      "2836/3000 train_loss: 34.583953857421875 test_loss:74.96538543701172\n",
      "2837/3000 train_loss: 29.114585876464844 test_loss:66.18831634521484\n",
      "2838/3000 train_loss: 28.371585845947266 test_loss:63.86708450317383\n",
      "2839/3000 train_loss: 27.789350509643555 test_loss:66.56425476074219\n",
      "2840/3000 train_loss: 32.295249938964844 test_loss:75.66373443603516\n",
      "2841/3000 train_loss: 27.26590347290039 test_loss:69.55776977539062\n",
      "2842/3000 train_loss: 28.884197235107422 test_loss:79.12937927246094\n",
      "2843/3000 train_loss: 28.7922420501709 test_loss:74.10465240478516\n",
      "2844/3000 train_loss: 30.811323165893555 test_loss:76.90019226074219\n",
      "2845/3000 train_loss: 29.41635513305664 test_loss:75.99895477294922\n",
      "2846/3000 train_loss: 25.466781616210938 test_loss:74.0649185180664\n",
      "2847/3000 train_loss: 25.641639709472656 test_loss:83.34825134277344\n",
      "2848/3000 train_loss: 28.08969497680664 test_loss:70.27722930908203\n",
      "2849/3000 train_loss: 27.30518913269043 test_loss:78.57609558105469\n",
      "2850/3000 train_loss: 30.722633361816406 test_loss:72.92271423339844\n",
      "2851/3000 train_loss: 28.88509178161621 test_loss:72.41606140136719\n",
      "2852/3000 train_loss: 29.235937118530273 test_loss:75.44420623779297\n",
      "2853/3000 train_loss: 27.377168655395508 test_loss:84.04660034179688\n",
      "2854/3000 train_loss: 29.763916015625 test_loss:69.29794311523438\n",
      "2855/3000 train_loss: 27.668617248535156 test_loss:80.05117797851562\n",
      "2856/3000 train_loss: 28.213090896606445 test_loss:77.26334381103516\n",
      "2857/3000 train_loss: 29.342121124267578 test_loss:81.70975494384766\n",
      "2858/3000 train_loss: 40.0107307434082 test_loss:96.6563949584961\n",
      "2859/3000 train_loss: 40.29647445678711 test_loss:82.66149139404297\n",
      "2860/3000 train_loss: 33.76546859741211 test_loss:86.637451171875\n",
      "2861/3000 train_loss: 36.72755432128906 test_loss:80.73452758789062\n",
      "2862/3000 train_loss: 31.26068878173828 test_loss:73.79150390625\n",
      "2863/3000 train_loss: 29.662830352783203 test_loss:67.19896697998047\n",
      "2864/3000 train_loss: 29.702133178710938 test_loss:68.8743667602539\n",
      "2865/3000 train_loss: 32.16946792602539 test_loss:85.69579315185547\n",
      "2866/3000 train_loss: 32.129478454589844 test_loss:92.48486328125\n",
      "2867/3000 train_loss: 34.82965087890625 test_loss:77.53184509277344\n",
      "2868/3000 train_loss: 30.019067764282227 test_loss:87.15111541748047\n",
      "2869/3000 train_loss: 28.919126510620117 test_loss:67.81841278076172\n",
      "2870/3000 train_loss: 28.219131469726562 test_loss:81.2298583984375\n",
      "2871/3000 train_loss: 27.10573387145996 test_loss:75.254638671875\n",
      "2872/3000 train_loss: 32.280982971191406 test_loss:74.79048919677734\n",
      "2873/3000 train_loss: 30.16986846923828 test_loss:70.66246795654297\n",
      "2874/3000 train_loss: 28.22732162475586 test_loss:69.69076538085938\n",
      "2875/3000 train_loss: 30.214040756225586 test_loss:76.33195495605469\n",
      "2876/3000 train_loss: 26.342849731445312 test_loss:81.07247924804688\n",
      "2877/3000 train_loss: 28.30605125427246 test_loss:73.47069549560547\n",
      "2878/3000 train_loss: 30.05501365661621 test_loss:79.64630889892578\n",
      "2879/3000 train_loss: 26.260896682739258 test_loss:69.6293716430664\n",
      "2880/3000 train_loss: 43.136234283447266 test_loss:81.23943328857422\n",
      "2881/3000 train_loss: 31.81104278564453 test_loss:81.1403579711914\n",
      "2882/3000 train_loss: 28.11153221130371 test_loss:66.65812683105469\n",
      "2883/3000 train_loss: 30.41952133178711 test_loss:72.6236801147461\n",
      "2884/3000 train_loss: 26.79581642150879 test_loss:76.01747131347656\n",
      "2885/3000 train_loss: 32.69156265258789 test_loss:79.6664810180664\n",
      "2886/3000 train_loss: 29.803606033325195 test_loss:74.2199935913086\n",
      "2887/3000 train_loss: 29.196701049804688 test_loss:78.79084777832031\n",
      "2888/3000 train_loss: 31.24471664428711 test_loss:92.97257232666016\n",
      "2889/3000 train_loss: 33.722015380859375 test_loss:81.48220825195312\n",
      "2890/3000 train_loss: 30.35738754272461 test_loss:72.48416137695312\n",
      "2891/3000 train_loss: 25.735153198242188 test_loss:83.88587951660156\n",
      "2892/3000 train_loss: 26.950979232788086 test_loss:78.48336791992188\n",
      "2893/3000 train_loss: 30.22856903076172 test_loss:67.69928741455078\n",
      "2894/3000 train_loss: 29.793895721435547 test_loss:78.64451599121094\n",
      "2895/3000 train_loss: 27.64714241027832 test_loss:68.2513656616211\n",
      "2896/3000 train_loss: 25.932830810546875 test_loss:76.8619384765625\n",
      "2897/3000 train_loss: 30.342342376708984 test_loss:78.87120056152344\n",
      "2898/3000 train_loss: 30.571426391601562 test_loss:84.00860595703125\n",
      "2899/3000 train_loss: 29.88027572631836 test_loss:76.25135803222656\n",
      "2900/3000 train_loss: 26.921119689941406 test_loss:70.62159729003906\n",
      "2901/3000 train_loss: 34.28948974609375 test_loss:75.70592498779297\n",
      "2902/3000 train_loss: 36.325923919677734 test_loss:73.96863555908203\n",
      "2903/3000 train_loss: 27.532974243164062 test_loss:84.84394836425781\n",
      "2904/3000 train_loss: 30.011898040771484 test_loss:69.10394287109375\n",
      "2905/3000 train_loss: 26.322416305541992 test_loss:67.98694610595703\n",
      "2906/3000 train_loss: 29.34395980834961 test_loss:82.0465087890625\n",
      "2907/3000 train_loss: 27.912385940551758 test_loss:66.62417602539062\n",
      "2908/3000 train_loss: 31.962736129760742 test_loss:72.49983215332031\n",
      "2909/3000 train_loss: 30.695262908935547 test_loss:81.05033111572266\n",
      "2910/3000 train_loss: 28.41775894165039 test_loss:69.37660217285156\n",
      "2911/3000 train_loss: 24.781492233276367 test_loss:70.519775390625\n",
      "2912/3000 train_loss: 28.456703186035156 test_loss:70.99675750732422\n",
      "2913/3000 train_loss: 27.892803192138672 test_loss:71.72830963134766\n",
      "2914/3000 train_loss: 29.175586700439453 test_loss:68.40901184082031\n",
      "2915/3000 train_loss: 32.760108947753906 test_loss:71.78811645507812\n",
      "2916/3000 train_loss: 26.7600154876709 test_loss:78.64605712890625\n",
      "2917/3000 train_loss: 26.68866729736328 test_loss:70.03376007080078\n",
      "2918/3000 train_loss: 24.58491325378418 test_loss:68.73603057861328\n",
      "2919/3000 train_loss: 29.36290740966797 test_loss:78.49683380126953\n",
      "2920/3000 train_loss: 45.435794830322266 test_loss:77.01936340332031\n",
      "2921/3000 train_loss: 35.646156311035156 test_loss:76.86514282226562\n",
      "2922/3000 train_loss: 32.123600006103516 test_loss:70.77938842773438\n",
      "2923/3000 train_loss: 34.70991516113281 test_loss:80.34632110595703\n",
      "2924/3000 train_loss: 39.38254928588867 test_loss:87.21155548095703\n",
      "2925/3000 train_loss: 36.95518493652344 test_loss:87.361083984375\n",
      "2926/3000 train_loss: 35.5797119140625 test_loss:73.13347625732422\n",
      "2927/3000 train_loss: 31.473175048828125 test_loss:85.33680725097656\n",
      "2928/3000 train_loss: 32.98678207397461 test_loss:68.60258483886719\n",
      "2929/3000 train_loss: 29.735902786254883 test_loss:67.20003509521484\n",
      "2930/3000 train_loss: 32.98847579956055 test_loss:81.23857879638672\n",
      "2931/3000 train_loss: 28.46689796447754 test_loss:66.43643951416016\n",
      "2932/3000 train_loss: 28.36297607421875 test_loss:80.46247100830078\n",
      "2933/3000 train_loss: 30.455928802490234 test_loss:79.40773010253906\n",
      "2934/3000 train_loss: 29.510852813720703 test_loss:75.8266830444336\n",
      "2935/3000 train_loss: 32.275028228759766 test_loss:76.27934265136719\n",
      "2936/3000 train_loss: 31.555944442749023 test_loss:73.0048599243164\n",
      "2937/3000 train_loss: 31.511512756347656 test_loss:77.4124526977539\n",
      "2938/3000 train_loss: 28.903820037841797 test_loss:67.6961669921875\n",
      "2939/3000 train_loss: 29.170352935791016 test_loss:70.38780975341797\n",
      "2940/3000 train_loss: 24.049501419067383 test_loss:75.61736297607422\n",
      "2941/3000 train_loss: 28.643470764160156 test_loss:78.41814422607422\n",
      "2942/3000 train_loss: 33.38423156738281 test_loss:79.62261199951172\n",
      "2943/3000 train_loss: 34.79459762573242 test_loss:79.4743881225586\n",
      "2944/3000 train_loss: 31.317859649658203 test_loss:77.63533782958984\n",
      "2945/3000 train_loss: 26.6917724609375 test_loss:72.52738952636719\n",
      "2946/3000 train_loss: 30.039506912231445 test_loss:85.26048278808594\n",
      "2947/3000 train_loss: 29.32961654663086 test_loss:80.03575897216797\n",
      "2948/3000 train_loss: 30.066476821899414 test_loss:73.92235565185547\n",
      "2949/3000 train_loss: 30.87419891357422 test_loss:77.95985412597656\n",
      "2950/3000 train_loss: 25.826417922973633 test_loss:68.18822479248047\n",
      "2951/3000 train_loss: 33.34229278564453 test_loss:75.66348266601562\n",
      "2952/3000 train_loss: 33.31897735595703 test_loss:79.99021911621094\n",
      "2953/3000 train_loss: 26.64479637145996 test_loss:69.58008575439453\n",
      "2954/3000 train_loss: 33.824859619140625 test_loss:76.37097930908203\n",
      "2955/3000 train_loss: 29.843828201293945 test_loss:71.9661636352539\n",
      "2956/3000 train_loss: 27.231067657470703 test_loss:75.78721618652344\n",
      "2957/3000 train_loss: 28.609928131103516 test_loss:68.2585220336914\n",
      "2958/3000 train_loss: 26.464120864868164 test_loss:88.0321044921875\n",
      "2959/3000 train_loss: 30.174951553344727 test_loss:72.25516510009766\n",
      "2960/3000 train_loss: 37.49857711791992 test_loss:70.66291809082031\n",
      "2961/3000 train_loss: 28.737133026123047 test_loss:70.8743896484375\n",
      "2962/3000 train_loss: 29.202545166015625 test_loss:79.56199645996094\n",
      "2963/3000 train_loss: 26.26909828186035 test_loss:88.14102172851562\n",
      "2964/3000 train_loss: 31.462663650512695 test_loss:73.35596466064453\n",
      "2965/3000 train_loss: 28.481964111328125 test_loss:77.11087799072266\n",
      "2966/3000 train_loss: 31.243515014648438 test_loss:77.41383361816406\n",
      "2967/3000 train_loss: 32.348045349121094 test_loss:65.37725067138672\n",
      "2968/3000 train_loss: 30.440040588378906 test_loss:67.87757110595703\n",
      "2969/3000 train_loss: 30.228031158447266 test_loss:77.09005737304688\n",
      "2970/3000 train_loss: 33.94340896606445 test_loss:86.63284301757812\n",
      "2971/3000 train_loss: 27.935022354125977 test_loss:71.9889144897461\n",
      "2972/3000 train_loss: 31.40468978881836 test_loss:69.36177062988281\n",
      "2973/3000 train_loss: 28.10053253173828 test_loss:71.34768676757812\n",
      "2974/3000 train_loss: 32.78343963623047 test_loss:84.14051818847656\n",
      "2975/3000 train_loss: 38.70279312133789 test_loss:78.50381469726562\n",
      "2976/3000 train_loss: 30.958454132080078 test_loss:67.79115295410156\n",
      "2977/3000 train_loss: 24.693986892700195 test_loss:67.77070617675781\n",
      "2978/3000 train_loss: 23.998416900634766 test_loss:69.01661682128906\n",
      "2979/3000 train_loss: 27.981225967407227 test_loss:74.03337097167969\n",
      "2980/3000 train_loss: 30.470829010009766 test_loss:77.21094512939453\n",
      "2981/3000 train_loss: 36.11940383911133 test_loss:72.26533508300781\n",
      "2982/3000 train_loss: 31.644559860229492 test_loss:82.99207305908203\n",
      "2983/3000 train_loss: 30.564292907714844 test_loss:68.90079498291016\n",
      "2984/3000 train_loss: 27.563350677490234 test_loss:67.9361572265625\n",
      "2985/3000 train_loss: 25.496261596679688 test_loss:70.53231811523438\n",
      "2986/3000 train_loss: 24.31302833557129 test_loss:89.08056640625\n",
      "2987/3000 train_loss: 30.2164249420166 test_loss:85.88294982910156\n",
      "2988/3000 train_loss: 27.176347732543945 test_loss:69.4684829711914\n",
      "2989/3000 train_loss: 27.730392456054688 test_loss:73.02096557617188\n",
      "2990/3000 train_loss: 30.000492095947266 test_loss:67.17768859863281\n",
      "2991/3000 train_loss: 32.13410949707031 test_loss:74.11634063720703\n",
      "2992/3000 train_loss: 33.061737060546875 test_loss:80.67153930664062\n",
      "2993/3000 train_loss: 28.342390060424805 test_loss:75.8995132446289\n",
      "2994/3000 train_loss: 28.009668350219727 test_loss:68.23026275634766\n",
      "2995/3000 train_loss: 27.76222038269043 test_loss:74.98297119140625\n",
      "2996/3000 train_loss: 25.814735412597656 test_loss:69.01277923583984\n",
      "2997/3000 train_loss: 29.19418716430664 test_loss:78.64787292480469\n",
      "2998/3000 train_loss: 31.685956954956055 test_loss:70.93063354492188\n",
      "2999/3000 train_loss: 28.399913787841797 test_loss:84.0958251953125\n",
      "3000/3000 train_loss: 30.125473022460938 test_loss:69.03705596923828\n"
     ]
    }
   ],
   "source": [
    "losses = train(model = unet, optimizer = optimizer, criterion=criterion, data_tr=train_data,\n",
    "               data_val = test_data, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "6Ew7_F0-q7aL",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "3d44a60e-50ad-467b-dce5-b488697e12cd"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(69.0371)"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "avg_loss = 0\n",
    "# unet.eval()\n",
    "# unet.train()\n",
    "preds = []\n",
    "i = 0\n",
    "test_anomaly_losses = []\n",
    "test_normal_losses = []\n",
    "test_losses = []\n",
    "test_real = y_test.tolist()\n",
    "for batch in test_data:\n",
    "  with torch.no_grad():\n",
    "    # unet.train()\n",
    "    predictions = unet(batch.to(device)).cpu()\n",
    "    preds.append(predictions)\n",
    "  loss = criterion(predictions, batch.cpu())\n",
    "  for j in range(len(predictions)):\n",
    "    if int(y_test[i]) == 1:\n",
    "      test_anomaly_losses.append(float(criterion(predictions[j], batch[j])))\n",
    "    else:\n",
    "      test_normal_losses.append(float(criterion(predictions[j], batch[j])))\n",
    "    i += 1\n",
    "    test_losses.append(criterion(predictions[j], batch[j]))\n",
    "  # print(loss)\n",
    "  # print(loss)\n",
    "  avg_loss += loss / len(test_data)\n",
    "# avg_loss\n",
    "\n",
    "avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "VpDKorrRso9o",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "04b7205f-2b80-4d92-9ad8-0244828a9f40"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(72.97048539670152, 51.66336555480957)"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "sum(test_anomaly_losses)/len(test_anomaly_losses) , sum(test_normal_losses)/len(test_normal_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5LEvbZKYuh7J",
    "outputId": "5539ffa3-81a9-4350-eb73-7fac9924cff5"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.6888329238329238\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "vals = np.arange(40, 100, 0.1).tolist()\n",
    "for threshold in vals:\n",
    "  preds = []\n",
    "  for j in range(len(test_losses)):\n",
    "    if test_losses[j] > threshold:\n",
    "      preds.append(1)\n",
    "    else:\n",
    "      preds.append(0)\n",
    "  \n",
    "  results.append(roc_auc_score(test_real,preds))\n",
    "\n",
    "print(max(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QaSSqG8SbAw2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
