{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9SStKf4G0V5H"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "import torchaudio\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from skimage.util import img_as_ubyte\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import io\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_XtxbKLZq5KX",
    "outputId": "b7682926-4117-41da-c4ae-eeb2a5394005"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CYxHegIM0Z4i",
    "outputId": "5beea3d9-70c6-447b-b9b2-ded06a824263"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive/\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "h9DATQwS0ivD"
   },
   "outputs": [],
   "source": [
    "class MimiiDataset(Dataset):\n",
    "    def __init__(self,audio_dir, n_fft = 1024, win_length = 1024,\n",
    "                 hop_length = 512,power = 2,n_mels = 128,pad_mode = 'reflect',\n",
    "                 sr = 16000,center = True,norm = None):\n",
    "      \n",
    "        super(MimiiDataset, self).__init__()\n",
    "        self.audio_dir = audio_dir\n",
    "        self.n_mels = n_mels\n",
    "        self.n_fft = n_fft\n",
    "        self.win_length = win_length\n",
    "        self.hop_length = hop_length\n",
    "        self.power = power\n",
    "        self.pad_mode = pad_mode\n",
    "        self.sr = sr\n",
    "        self.center = center\n",
    "        self.norm = norm\n",
    "\n",
    "    def get_files(self):\n",
    "       return self.train_files, self.test_files\n",
    "    \n",
    "    def get_data(self,device, id):\n",
    "        \n",
    "        self.train_files, self.train_labels = self._train_file_list(device, id)\n",
    "        self.test_files, self.test_labels = self._test_file_list(device, id)\n",
    "        \n",
    "        self.train_data = self.get_audios(self.train_files)\n",
    "        self.test_data = self.get_audios(self.test_files)\n",
    "        \n",
    "        return self.train_data, self.test_data, self.train_labels, self.test_labels\n",
    "    \n",
    "    def _train_file_list(self, device, id):\n",
    "        query = os.path.abspath(\n",
    "            f\"{self.audio_dir}/{device}/train/normal_id_0{id}*.wav\"\n",
    "        )\n",
    "        train_normal_files = sorted(glob.glob(query))\n",
    "        train_normal_labels = np.zeros(len(train_normal_files))\n",
    "        \n",
    "        query = os.path.abspath(\n",
    "                f\"{self.audio_dir}/{device}/train/anomaly_id_0{id}*.wav\"\n",
    "            )\n",
    "        train_anomaly_files = sorted(glob.glob(query))\n",
    "        train_anomaly_labels = np.ones(len(train_anomaly_files))\n",
    "        \n",
    "        train_file_list = np.concatenate((train_normal_files, train_anomaly_files), axis=0)\n",
    "        train_labels = np.concatenate((train_normal_labels, train_anomaly_labels), axis=0)\n",
    "        \n",
    "        return train_file_list, train_labels\n",
    "    \n",
    "    def _test_file_list(self, device, id):     \n",
    "        query = os.path.abspath(\n",
    "            f\"{self.audio_dir}/{device}/test/normal_id_0{id}*.wav\"\n",
    "            )\n",
    "        test_normal_files = sorted(glob.glob(query))\n",
    "        test_normal_labels = np.zeros(len(test_normal_files))\n",
    "        \n",
    "        query = os.path.abspath(\n",
    "            f\"{self.audio_dir}/{device}/test/anomaly_id_0{id}*.wav\"\n",
    "            )\n",
    "        test_anomaly_files = sorted(glob.glob(query))\n",
    "        test_anomaly_labels = np.ones(len(test_anomaly_files))\n",
    "        \n",
    "        test_file_list = np.concatenate((test_normal_files, \n",
    "                                          test_anomaly_files), axis=0)\n",
    "        test_labels = np.concatenate((test_normal_labels,\n",
    "                                      test_anomaly_labels), axis=0)\n",
    "          \n",
    "        return test_file_list, test_labels\n",
    "\n",
    "    def normalize(self,tensor):\n",
    "        tensor_minusmean = tensor - tensor.mean()\n",
    "        return tensor_minusmean/np.absolute(tensor_minusmean).max()\n",
    "\n",
    "    def make0min(self,tensornd):\n",
    "        tensor = tensornd.numpy()\n",
    "        res = np.where(tensor == 0, 1E-19 , tensor)\n",
    "        return torch.from_numpy(res)\n",
    "\n",
    "    def spectrogrameToImage(self,specgram):\n",
    "        # specgram = torchaudio.transforms.MelSpectrogram(n_fft=1024, win_length=1024, \n",
    "        #                                                 hop_length=512, power=2, \n",
    "        #                                                 normalized=True, n_mels=128)(waveform )\n",
    "        specgram= self.make0min(specgram)\n",
    "        specgram = specgram.log2()[0,:,:].numpy()\n",
    "        \n",
    "        tr2image = transforms.Compose([transforms.ToPILImage()])\n",
    "\n",
    "        specgram= self.normalize(specgram)\n",
    "        # specgram = img_as_ubyte(specgram)\n",
    "        specgramImage = tr2image(specgram)\n",
    "        return specgramImage\n",
    "\n",
    "    def get_logmelspectrogram(self, waveform):\n",
    "        melspec = librosa.feature.melspectrogram(\n",
    "          n_fft=self.n_fft, win_length=self.win_length, hop_length=self.hop_length,\n",
    "          power=self.power,n_mels=self.n_mels,pad_mode=self.pad_mode,sr=self.sr,\n",
    "          center=self.center,norm=self.norm,htk=True,\n",
    "          y = waveform.numpy()\n",
    "        )\n",
    "\n",
    "        logmelspec = librosa.power_to_db(melspec)\n",
    "\n",
    "        return logmelspec\n",
    "\n",
    "    def get_melspectrogram(self,waveform):\n",
    "        melspec = librosa.feature.melspectrogram(\n",
    "            n_fft=self.n_fft, win_length=self.win_length, hop_length=self.hop_length,\n",
    "            power=self.power,n_mels=self.n_mels,pad_mode=self.pad_mode,sr=self.sr,\n",
    "            center=self.center,norm=self.norm,htk=True,\n",
    "            y = waveform.numpy()\n",
    "        )\n",
    "\n",
    "        return melspec\n",
    "    \n",
    "    def get_mfcc(self,waveform):\n",
    "        mfcc = librosa.feature.mfcc(    \n",
    "            n_fft=self.n_fft, win_length=self.win_length, \n",
    "            hop_length=self.hop_length,pad_mode=self.pad_mode,sr=self.sr,\n",
    "            center=self.center,norm=self.norm,n_mfcc=40,\n",
    "            y = waveform.numpy()\n",
    "        )\n",
    "\n",
    "        return mfcc\n",
    "\n",
    "    def get_chroma_stft(self,waveform):\n",
    "        stft = librosa.feature.chroma_stft(\n",
    "            n_fft=self.n_fft, win_length=self.win_length, \n",
    "            hop_length=self.hop_length,pad_mode=self.pad_mode,sr=self.sr,\n",
    "            center=self.center,norm=self.norm,n_chroma=12,\n",
    "            y=waveform.numpy()\n",
    "        )\n",
    "\n",
    "        return stft\n",
    "\n",
    "    def get_spectral_contrast(self,waveform):\n",
    "        spec_contrast = librosa.feature.spectral_contrast(    \n",
    "            n_fft=self.n_fft, win_length=self.win_length,center=self.center,\n",
    "            hop_length=self.hop_length,pad_mode=self.pad_mode,sr=self.sr,\n",
    "            y = waveform.numpy()\n",
    "        )\n",
    "\n",
    "        return spec_contrast\n",
    "    \n",
    "    def get_tonnetz(self,waveform):\n",
    "        harmonic = librosa.effects.harmonic(waveform.numpy())\n",
    "        tonnetz = librosa.feature.tonnetz(y=harmonic,sr=self.sr)\n",
    "\n",
    "        return tonnetz\n",
    "\n",
    "    def get_audios(self, file_list):\n",
    "        data = []\n",
    "        for i in range(len(file_list)):\n",
    "          y, sr = torchaudio.load(file_list[i])  \n",
    "          data.append(y)\n",
    "\n",
    "        return data\n",
    "    def _derive_data(self, file_list):\n",
    "        train_data = []\n",
    "        test_data = []\n",
    "        train_mode = True\n",
    "        for file_list in [self.train_files, self.test_files]:\n",
    "          tr2tensor = transforms.Compose([transforms.PILToTensor()])\n",
    "          data = []\n",
    "          for j in range(len(file_list)):\n",
    "            y, sr = torchaudio.load(file_list[j])  \n",
    "            spec = self.get_melspectrogram(y)\n",
    "            spec = self.spectrogrameToImage(spec)\n",
    "            spec = spec.convert('RGB')\n",
    "            vectors = tr2tensor(spec)\n",
    "            if train_mode:     \n",
    "              train_data.append(vectors)\n",
    "            else:\n",
    "              test_data.append(vectors)\n",
    "            \n",
    "          train_mode = False\n",
    "                \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "S96soeIc0o13"
   },
   "outputs": [],
   "source": [
    "dataset = MimiiDataset('/content/drive/MyDrive/mimii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Gn2zdn92doi1"
   },
   "outputs": [],
   "source": [
    "_, _, y_train, y_test = dataset.get_data('fan', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "SgjpeWy_RV1C"
   },
   "outputs": [],
   "source": [
    "train_mixed_f = torch.load('/content/drive/MyDrive/mixed_features/train_mf_fan2.pt')\n",
    "test_mixed_f = torch.load('/content/drive/MyDrive/mixed_features/test_mf_fan2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "jWMPVGu1qiEq"
   },
   "outputs": [],
   "source": [
    "train_data = DataLoader(train_mixed_f, batch_size=32, shuffle = False)\n",
    "test_data = DataLoader(test_mixed_f, batch_size = 32, shuffle= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "vNTBTRe6qnBq"
   },
   "outputs": [],
   "source": [
    "class UNet_FC(nn.Module):\n",
    "\n",
    "  def __init__(self, in_features):\n",
    "    super().__init__()\n",
    "    \n",
    "    self.bn = nn.BatchNorm1d(128)\n",
    "    self.relu = nn.ReLU()\n",
    "\n",
    "    self.fc0 = nn.Linear(in_features=in_features,out_features=in_features)\n",
    "\n",
    "    # encoder\n",
    "    self.fc1 = nn.Linear(in_features=in_features, out_features=128)\n",
    "    self.fc2 = nn.Linear(in_features=128,out_features=128)\n",
    "    self.fc3 = nn.Linear(in_features=128, out_features=128)\n",
    "    self.fc4 = nn.Linear(in_features=128, out_features=128)\n",
    "    self.fc5 = nn.Linear(in_features=128, out_features=8)\n",
    "\n",
    "    # decoder\n",
    "    self.fc6 = nn.Linear(in_features=8, out_features=128)\n",
    "    self.fc7 = nn.Linear(in_features=128*2, out_features=128)\n",
    "    self.fc8 = nn.Linear(in_features=128*2, out_features=128)\n",
    "    self.fc9 = nn.Linear(in_features=128*2, out_features=128)\n",
    "\n",
    "    self.out = nn.Linear(in_features=128*2, out_features=in_features)\n",
    "\n",
    "  def encoder(self, x):\n",
    "    input = self.fc0(x)\n",
    "\n",
    "    x1 = self.relu(self.bn(self.fc1(input)))\n",
    "    x2 = self.relu(self.bn(self.fc2(x1)))\n",
    "    x3 = self.relu(self.bn(self.fc3(x2)))\n",
    "    x4 = self.relu(self.bn(self.fc4(x3)))\n",
    "    x5 = self.relu(self.fc5(x4))\n",
    "\n",
    "    return [x5, x4, x3, x2, x1]\n",
    "\n",
    "  def decoder(self, x):\n",
    "    x6 = self.relu(self.fc6(x[0]))\n",
    "    con1 = torch.cat((x6,x[1]), 1) \n",
    "    x7 = self.relu(self.bn(self.fc7(con1)))\n",
    "    con2 = torch.cat((x7,x[2]), 1)\n",
    "    x8 = self.relu(self.bn(self.fc8(con2)))\n",
    "    con3 = torch.cat((x8,x[3]), 1)\n",
    "    x9 = self.relu(self.bn(self.fc9(con3)))\n",
    "    con4 = torch.cat((x9,x[4]), 1)\n",
    "\n",
    "    x10 = self.out(con4)\n",
    "\n",
    "    return x10\n",
    "  \n",
    "  def forward(self, x):\n",
    "    # encoded = self.encoder(x)\n",
    "\n",
    "    # decoded = self.decoder(encoded)\n",
    "    input = self.fc0(x)\n",
    "\n",
    "    x1 = self.relu(self.bn(self.fc1(input)))\n",
    "    x2 = self.relu(self.bn(self.fc2(x1)))\n",
    "    x3 = self.relu(self.bn(self.fc3(x2)))\n",
    "    x4 = self.relu(self.bn(self.fc4(x3)))\n",
    "    x5 = self.relu(self.fc5(x4))\n",
    "\n",
    "    xy = [x5, x4, x3, x2, x1]\n",
    "\n",
    "    x6 = self.relu(self.fc6(xy[0]))\n",
    "    con1 = torch.cat((x6,xy[1]), 1) \n",
    "    x7 = self.relu(self.bn(self.fc7(con1)))\n",
    "    con2 = torch.cat((x7,xy[2]), 1)\n",
    "    x8 = self.relu(self.bn(self.fc8(con2)))\n",
    "    con3 = torch.cat((x8,xy[3]), 1)\n",
    "    x9 = self.relu(self.bn(self.fc9(con3)))\n",
    "    con4 = torch.cat((x9,xy[4]), 1)\n",
    "\n",
    "    x10 = self.out(con4)\n",
    "\n",
    "    # return decoded\n",
    "    return x10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "ZfgcBtQ3qn5l"
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, data_tr, data_val, scheduler = None,\n",
    "          epochs = 3000, device = 'cpu'):\n",
    "    # X_val, Y_val = next(iter(data_val))\n",
    "    losses = []\n",
    "    prev_avg_loss = 100000\n",
    "    for epoch in range(epochs):\n",
    "        train_avg_loss = 0\n",
    "        test_avg_loss = 0\n",
    "        # model.train()  # train mode\n",
    "        for batch in data_tr:\n",
    "          # data to device\n",
    "          batch = batch.to(device)\n",
    "          # set parameter gradients to zero\n",
    "          optimizer.zero_grad()\n",
    "          # forward\n",
    "          # print(Y_batch.shape)\n",
    "          predictions = model(batch)\n",
    "          loss = criterion(predictions, batch)\n",
    "          loss.backward() # backward-pass\n",
    "          optimizer.step()  # update weights\n",
    "          # calculate loss to show the user\n",
    "          if scheduler:\n",
    "            scheduler.step(loss)\n",
    "          train_avg_loss += loss / len(data_tr)\n",
    "\n",
    "        # model.eval()\n",
    "        for batch in data_val:\n",
    "          with torch.no_grad():\n",
    "            preds = model(batch.to(device)).cpu()\n",
    "            loss = criterion(preds,batch)\n",
    "            test_avg_loss += loss / len(data_val)\n",
    "                    \n",
    "        losses.append(train_avg_loss.item())\n",
    "        # if (epoch+1)%50 == 0:\n",
    "        print(\"{}/{} train_loss: {} test_loss:{}\".format(epoch+1, epochs, train_avg_loss, test_avg_loss))\n",
    "        # if test_avg_loss < 70:\n",
    "        #   break\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "ptkVTF55quOL"
   },
   "outputs": [],
   "source": [
    "unet = UNet_FC(in_features=193).to(device)\n",
    "optimizer = Adam(params = unet.parameters(), lr = 10e-3)\n",
    "# optimizer = Adam(params = unet.parameters())\n",
    "criterion = nn.MSELoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, \n",
    "                                                       min_lr=10e-4, mode = 'min',\n",
    "                                                       patience = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QkfmYl9oXhcB",
    "outputId": "49370a95-fbca-4126-b95e-f21aabf8cbe9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/3000 train_loss: 293492.53125 test_loss:298281.3125\n",
      "2/3000 train_loss: 284962.625 test_loss:286132.65625\n",
      "3/3000 train_loss: 269196.09375 test_loss:266207.5625\n",
      "4/3000 train_loss: 244527.296875 test_loss:236619.375\n",
      "5/3000 train_loss: 212982.25 test_loss:202809.984375\n",
      "6/3000 train_loss: 180769.703125 test_loss:170289.0\n",
      "7/3000 train_loss: 145943.90625 test_loss:134222.46875\n",
      "8/3000 train_loss: 110645.890625 test_loss:99656.96875\n",
      "9/3000 train_loss: 78534.046875 test_loss:69166.21875\n",
      "10/3000 train_loss: 51949.62890625 test_loss:45702.7109375\n",
      "11/3000 train_loss: 32351.810546875 test_loss:28278.552734375\n",
      "12/3000 train_loss: 18557.923828125 test_loss:16621.830078125\n",
      "13/3000 train_loss: 9807.845703125 test_loss:9410.8564453125\n",
      "14/3000 train_loss: 5262.61279296875 test_loss:5782.5419921875\n",
      "15/3000 train_loss: 2729.24365234375 test_loss:3260.08837890625\n",
      "16/3000 train_loss: 1579.0833740234375 test_loss:2642.519287109375\n",
      "17/3000 train_loss: 1004.0106201171875 test_loss:1935.063232421875\n",
      "18/3000 train_loss: 774.5839233398438 test_loss:1646.395751953125\n",
      "19/3000 train_loss: 687.288818359375 test_loss:1506.70849609375\n",
      "20/3000 train_loss: 592.6265258789062 test_loss:1323.3726806640625\n",
      "21/3000 train_loss: 534.4886474609375 test_loss:1223.451416015625\n",
      "22/3000 train_loss: 477.2830505371094 test_loss:1151.709228515625\n",
      "23/3000 train_loss: 448.5827941894531 test_loss:1087.222412109375\n",
      "24/3000 train_loss: 424.6861877441406 test_loss:1047.4154052734375\n",
      "25/3000 train_loss: 405.75189208984375 test_loss:1020.1793823242188\n",
      "26/3000 train_loss: 387.5267028808594 test_loss:995.1605834960938\n",
      "27/3000 train_loss: 370.1709289550781 test_loss:972.2225341796875\n",
      "28/3000 train_loss: 354.12237548828125 test_loss:950.3839111328125\n",
      "29/3000 train_loss: 338.3586730957031 test_loss:928.4265747070312\n",
      "30/3000 train_loss: 323.035888671875 test_loss:910.3392944335938\n",
      "31/3000 train_loss: 309.0568542480469 test_loss:891.5849609375\n",
      "32/3000 train_loss: 296.0306396484375 test_loss:868.1525268554688\n",
      "33/3000 train_loss: 283.68292236328125 test_loss:848.9409790039062\n",
      "34/3000 train_loss: 273.2384033203125 test_loss:837.004150390625\n",
      "35/3000 train_loss: 268.23663330078125 test_loss:832.0142822265625\n",
      "36/3000 train_loss: 268.9451904296875 test_loss:856.451904296875\n",
      "37/3000 train_loss: 275.9587707519531 test_loss:925.6784057617188\n",
      "38/3000 train_loss: 278.8646240234375 test_loss:978.96630859375\n",
      "39/3000 train_loss: 269.0918273925781 test_loss:935.50634765625\n",
      "40/3000 train_loss: 246.78128051757812 test_loss:845.4788208007812\n",
      "41/3000 train_loss: 237.29934692382812 test_loss:816.0369873046875\n",
      "42/3000 train_loss: 224.200927734375 test_loss:801.958251953125\n",
      "43/3000 train_loss: 217.59072875976562 test_loss:785.0916137695312\n",
      "44/3000 train_loss: 211.9421844482422 test_loss:783.5515747070312\n",
      "45/3000 train_loss: 205.58970642089844 test_loss:788.1185302734375\n",
      "46/3000 train_loss: 201.77845764160156 test_loss:790.8555908203125\n",
      "47/3000 train_loss: 197.77676391601562 test_loss:775.9178466796875\n",
      "48/3000 train_loss: 191.91651916503906 test_loss:761.0152587890625\n",
      "49/3000 train_loss: 187.8990020751953 test_loss:748.4791259765625\n",
      "50/3000 train_loss: 185.33822631835938 test_loss:741.0338745117188\n",
      "51/3000 train_loss: 183.1967315673828 test_loss:729.0245361328125\n",
      "52/3000 train_loss: 180.9126739501953 test_loss:719.9341430664062\n",
      "53/3000 train_loss: 178.9972381591797 test_loss:710.9452514648438\n",
      "54/3000 train_loss: 175.67735290527344 test_loss:702.497314453125\n",
      "55/3000 train_loss: 171.70867919921875 test_loss:686.943603515625\n",
      "56/3000 train_loss: 168.70680236816406 test_loss:685.2278442382812\n",
      "57/3000 train_loss: 165.93321228027344 test_loss:670.9946899414062\n",
      "58/3000 train_loss: 163.3123016357422 test_loss:668.5970458984375\n",
      "59/3000 train_loss: 160.33827209472656 test_loss:656.892822265625\n",
      "60/3000 train_loss: 157.3809814453125 test_loss:655.44287109375\n",
      "61/3000 train_loss: 153.93667602539062 test_loss:648.5162353515625\n",
      "62/3000 train_loss: 151.7643585205078 test_loss:646.8826293945312\n",
      "63/3000 train_loss: 149.21287536621094 test_loss:642.3489379882812\n",
      "64/3000 train_loss: 146.75357055664062 test_loss:641.1771240234375\n",
      "65/3000 train_loss: 145.55429077148438 test_loss:632.856689453125\n",
      "66/3000 train_loss: 143.9447021484375 test_loss:634.587158203125\n",
      "67/3000 train_loss: 143.01026916503906 test_loss:629.3999633789062\n",
      "68/3000 train_loss: 141.80038452148438 test_loss:624.4819946289062\n",
      "69/3000 train_loss: 141.29576110839844 test_loss:623.8212280273438\n",
      "70/3000 train_loss: 141.1229248046875 test_loss:618.1051025390625\n",
      "71/3000 train_loss: 140.93722534179688 test_loss:614.5570068359375\n",
      "72/3000 train_loss: 141.2471466064453 test_loss:613.7256469726562\n",
      "73/3000 train_loss: 142.9460906982422 test_loss:611.385986328125\n",
      "74/3000 train_loss: 145.20846557617188 test_loss:616.8742065429688\n",
      "75/3000 train_loss: 151.990234375 test_loss:634.8441162109375\n",
      "76/3000 train_loss: 162.73678588867188 test_loss:673.5203247070312\n",
      "77/3000 train_loss: 172.2383270263672 test_loss:643.4031982421875\n",
      "78/3000 train_loss: 162.290771484375 test_loss:627.1301879882812\n",
      "79/3000 train_loss: 136.6795196533203 test_loss:603.8003540039062\n",
      "80/3000 train_loss: 127.88401794433594 test_loss:602.7496948242188\n",
      "81/3000 train_loss: 126.8226547241211 test_loss:590.957763671875\n",
      "82/3000 train_loss: 126.94715118408203 test_loss:596.1978759765625\n",
      "83/3000 train_loss: 127.96540069580078 test_loss:604.4819946289062\n",
      "84/3000 train_loss: 132.99703979492188 test_loss:622.51904296875\n",
      "85/3000 train_loss: 138.7942657470703 test_loss:637.3131103515625\n",
      "86/3000 train_loss: 139.0198516845703 test_loss:634.2283325195312\n",
      "87/3000 train_loss: 142.74984741210938 test_loss:618.1699829101562\n",
      "88/3000 train_loss: 140.01907348632812 test_loss:612.0410766601562\n",
      "89/3000 train_loss: 127.00811004638672 test_loss:566.659423828125\n",
      "90/3000 train_loss: 118.04545593261719 test_loss:569.5503540039062\n",
      "91/3000 train_loss: 112.90303802490234 test_loss:570.0982055664062\n",
      "92/3000 train_loss: 113.19241333007812 test_loss:567.22607421875\n",
      "93/3000 train_loss: 113.30301666259766 test_loss:565.6861572265625\n",
      "94/3000 train_loss: 108.40924072265625 test_loss:558.8609619140625\n",
      "95/3000 train_loss: 104.46208190917969 test_loss:552.2998657226562\n",
      "96/3000 train_loss: 102.78495788574219 test_loss:551.6702880859375\n",
      "97/3000 train_loss: 101.91107177734375 test_loss:550.5669555664062\n",
      "98/3000 train_loss: 101.88459777832031 test_loss:554.312255859375\n",
      "99/3000 train_loss: 102.86278533935547 test_loss:556.186767578125\n",
      "100/3000 train_loss: 102.42745208740234 test_loss:562.91064453125\n",
      "101/3000 train_loss: 102.3741455078125 test_loss:549.78369140625\n",
      "102/3000 train_loss: 100.41665649414062 test_loss:546.6387329101562\n",
      "103/3000 train_loss: 99.13206481933594 test_loss:533.8383178710938\n",
      "104/3000 train_loss: 98.65824890136719 test_loss:540.3873291015625\n",
      "105/3000 train_loss: 99.68145751953125 test_loss:550.2071533203125\n",
      "106/3000 train_loss: 99.61870574951172 test_loss:565.0934448242188\n",
      "107/3000 train_loss: 103.42240142822266 test_loss:568.0202026367188\n",
      "108/3000 train_loss: 109.23529052734375 test_loss:565.2681274414062\n",
      "109/3000 train_loss: 106.60610961914062 test_loss:540.3160400390625\n",
      "110/3000 train_loss: 104.44532012939453 test_loss:528.75927734375\n",
      "111/3000 train_loss: 107.12679290771484 test_loss:555.1229248046875\n",
      "112/3000 train_loss: 107.48162078857422 test_loss:548.5792236328125\n",
      "113/3000 train_loss: 100.13890075683594 test_loss:543.7158813476562\n",
      "114/3000 train_loss: 103.58634185791016 test_loss:520.5747680664062\n",
      "115/3000 train_loss: 104.40628814697266 test_loss:528.7353515625\n",
      "116/3000 train_loss: 104.20008850097656 test_loss:554.6507568359375\n",
      "117/3000 train_loss: 101.78534698486328 test_loss:568.4513549804688\n",
      "118/3000 train_loss: 109.96526336669922 test_loss:567.1441650390625\n",
      "119/3000 train_loss: 105.17794036865234 test_loss:529.4022216796875\n",
      "120/3000 train_loss: 98.5908432006836 test_loss:526.6815795898438\n",
      "121/3000 train_loss: 94.49410247802734 test_loss:544.1809692382812\n",
      "122/3000 train_loss: 94.31535339355469 test_loss:547.59814453125\n",
      "123/3000 train_loss: 93.86890411376953 test_loss:533.261962890625\n",
      "124/3000 train_loss: 94.58857727050781 test_loss:518.102294921875\n",
      "125/3000 train_loss: 95.16394805908203 test_loss:520.305908203125\n",
      "126/3000 train_loss: 97.12081909179688 test_loss:522.986572265625\n",
      "127/3000 train_loss: 100.3387222290039 test_loss:528.4525756835938\n",
      "128/3000 train_loss: 102.0873031616211 test_loss:524.3262329101562\n",
      "129/3000 train_loss: 105.7569351196289 test_loss:529.0494995117188\n",
      "130/3000 train_loss: 105.23477172851562 test_loss:514.21826171875\n",
      "131/3000 train_loss: 107.13851928710938 test_loss:535.5875854492188\n",
      "132/3000 train_loss: 114.63362121582031 test_loss:544.1290893554688\n",
      "133/3000 train_loss: 136.44265747070312 test_loss:558.8577880859375\n",
      "134/3000 train_loss: 139.17665100097656 test_loss:526.6427612304688\n",
      "135/3000 train_loss: 139.12136840820312 test_loss:588.088134765625\n",
      "136/3000 train_loss: 141.6708221435547 test_loss:597.7567749023438\n",
      "137/3000 train_loss: 149.53228759765625 test_loss:584.7855834960938\n",
      "138/3000 train_loss: 133.85377502441406 test_loss:550.5964965820312\n",
      "139/3000 train_loss: 145.61398315429688 test_loss:489.4855041503906\n",
      "140/3000 train_loss: 134.91259765625 test_loss:459.8814392089844\n",
      "141/3000 train_loss: 112.47319793701172 test_loss:450.92291259765625\n",
      "142/3000 train_loss: 106.89387512207031 test_loss:483.6695556640625\n",
      "143/3000 train_loss: 105.47206115722656 test_loss:500.4457092285156\n",
      "144/3000 train_loss: 106.62169647216797 test_loss:492.148681640625\n",
      "145/3000 train_loss: 107.5719223022461 test_loss:493.3441467285156\n",
      "146/3000 train_loss: 105.16519927978516 test_loss:479.2771301269531\n",
      "147/3000 train_loss: 100.76618194580078 test_loss:455.294921875\n",
      "148/3000 train_loss: 106.59835815429688 test_loss:446.485107421875\n",
      "149/3000 train_loss: 112.44091796875 test_loss:466.1832275390625\n",
      "150/3000 train_loss: 106.38182830810547 test_loss:485.8530578613281\n",
      "151/3000 train_loss: 104.76338195800781 test_loss:488.1465759277344\n",
      "152/3000 train_loss: 101.77555847167969 test_loss:461.4707336425781\n",
      "153/3000 train_loss: 91.82521057128906 test_loss:454.27392578125\n",
      "154/3000 train_loss: 85.86815643310547 test_loss:464.1033935546875\n",
      "155/3000 train_loss: 79.89274597167969 test_loss:447.0267028808594\n",
      "156/3000 train_loss: 79.80867004394531 test_loss:438.5198059082031\n",
      "157/3000 train_loss: 77.23700714111328 test_loss:432.93817138671875\n",
      "158/3000 train_loss: 76.23564147949219 test_loss:429.0025634765625\n",
      "159/3000 train_loss: 74.86678314208984 test_loss:431.48773193359375\n",
      "160/3000 train_loss: 73.37741088867188 test_loss:430.7050476074219\n",
      "161/3000 train_loss: 72.16181945800781 test_loss:422.33221435546875\n",
      "162/3000 train_loss: 71.6040267944336 test_loss:428.2347106933594\n",
      "163/3000 train_loss: 71.45101165771484 test_loss:428.24176025390625\n",
      "164/3000 train_loss: 70.97892761230469 test_loss:434.0194091796875\n",
      "165/3000 train_loss: 70.51132202148438 test_loss:419.30242919921875\n",
      "166/3000 train_loss: 69.10165405273438 test_loss:423.54302978515625\n",
      "167/3000 train_loss: 69.24573516845703 test_loss:419.6813659667969\n",
      "168/3000 train_loss: 69.14900207519531 test_loss:413.3983154296875\n",
      "169/3000 train_loss: 69.36042785644531 test_loss:409.6779479980469\n",
      "170/3000 train_loss: 69.14373016357422 test_loss:404.50634765625\n",
      "171/3000 train_loss: 68.82927703857422 test_loss:411.3860778808594\n",
      "172/3000 train_loss: 68.17115783691406 test_loss:414.8716125488281\n",
      "173/3000 train_loss: 67.7962875366211 test_loss:417.605712890625\n",
      "174/3000 train_loss: 68.1005630493164 test_loss:419.3080139160156\n",
      "175/3000 train_loss: 69.5126953125 test_loss:412.11749267578125\n",
      "176/3000 train_loss: 70.99918365478516 test_loss:431.8802185058594\n",
      "177/3000 train_loss: 74.21253204345703 test_loss:423.74713134765625\n",
      "178/3000 train_loss: 73.69356536865234 test_loss:451.94122314453125\n",
      "179/3000 train_loss: 72.23621368408203 test_loss:425.03741455078125\n",
      "180/3000 train_loss: 71.94927215576172 test_loss:436.69189453125\n",
      "181/3000 train_loss: 71.74747467041016 test_loss:452.5551452636719\n",
      "182/3000 train_loss: 72.27439880371094 test_loss:422.2442626953125\n",
      "183/3000 train_loss: 73.50653839111328 test_loss:409.218994140625\n",
      "184/3000 train_loss: 72.70219421386719 test_loss:419.8934020996094\n",
      "185/3000 train_loss: 70.68010711669922 test_loss:425.9941101074219\n",
      "186/3000 train_loss: 67.35295104980469 test_loss:425.2156677246094\n",
      "187/3000 train_loss: 64.64225769042969 test_loss:411.36370849609375\n",
      "188/3000 train_loss: 64.24759674072266 test_loss:430.2413024902344\n",
      "189/3000 train_loss: 65.24870300292969 test_loss:423.932861328125\n",
      "190/3000 train_loss: 66.79753875732422 test_loss:421.06695556640625\n",
      "191/3000 train_loss: 66.27412414550781 test_loss:397.7978820800781\n",
      "192/3000 train_loss: 68.10633087158203 test_loss:413.303466796875\n",
      "193/3000 train_loss: 66.50489807128906 test_loss:432.3356018066406\n",
      "194/3000 train_loss: 66.16758728027344 test_loss:430.36273193359375\n",
      "195/3000 train_loss: 69.74943542480469 test_loss:398.71307373046875\n",
      "196/3000 train_loss: 70.70293426513672 test_loss:391.4280090332031\n",
      "197/3000 train_loss: 69.99623107910156 test_loss:417.9783935546875\n",
      "198/3000 train_loss: 71.60546875 test_loss:407.05010986328125\n",
      "199/3000 train_loss: 69.32039642333984 test_loss:437.75823974609375\n",
      "200/3000 train_loss: 65.69242095947266 test_loss:437.1096496582031\n",
      "201/3000 train_loss: 66.56051635742188 test_loss:428.31732177734375\n",
      "202/3000 train_loss: 69.8775863647461 test_loss:429.63348388671875\n",
      "203/3000 train_loss: 73.8504409790039 test_loss:470.25091552734375\n",
      "204/3000 train_loss: 78.03539276123047 test_loss:410.9227294921875\n",
      "205/3000 train_loss: 73.35152435302734 test_loss:434.5451354980469\n",
      "206/3000 train_loss: 67.50821685791016 test_loss:395.38427734375\n",
      "207/3000 train_loss: 66.81499481201172 test_loss:395.6517639160156\n",
      "208/3000 train_loss: 65.79625701904297 test_loss:419.44659423828125\n",
      "209/3000 train_loss: 63.67155838012695 test_loss:358.927978515625\n",
      "210/3000 train_loss: 63.60346221923828 test_loss:398.6964416503906\n",
      "211/3000 train_loss: 63.72077941894531 test_loss:385.176025390625\n",
      "212/3000 train_loss: 61.06083297729492 test_loss:420.5655212402344\n",
      "213/3000 train_loss: 61.30293655395508 test_loss:420.3936767578125\n",
      "214/3000 train_loss: 64.77284240722656 test_loss:409.07073974609375\n",
      "215/3000 train_loss: 63.712928771972656 test_loss:423.2929382324219\n",
      "216/3000 train_loss: 66.10031127929688 test_loss:413.66192626953125\n",
      "217/3000 train_loss: 69.9834976196289 test_loss:409.4354248046875\n",
      "218/3000 train_loss: 67.48481750488281 test_loss:455.7143859863281\n",
      "219/3000 train_loss: 63.64580535888672 test_loss:403.0696105957031\n",
      "220/3000 train_loss: 66.3934326171875 test_loss:388.5936279296875\n",
      "221/3000 train_loss: 63.03569412231445 test_loss:387.2412109375\n",
      "222/3000 train_loss: 62.52940368652344 test_loss:382.2101745605469\n",
      "223/3000 train_loss: 62.330570220947266 test_loss:384.6062316894531\n",
      "224/3000 train_loss: 63.864112854003906 test_loss:397.028076171875\n",
      "225/3000 train_loss: 64.77809143066406 test_loss:394.1333923339844\n",
      "226/3000 train_loss: 68.70915985107422 test_loss:408.52862548828125\n",
      "227/3000 train_loss: 77.41146087646484 test_loss:407.21978759765625\n",
      "228/3000 train_loss: 84.1606674194336 test_loss:397.8013916015625\n",
      "229/3000 train_loss: 94.32164764404297 test_loss:385.63739013671875\n",
      "230/3000 train_loss: 96.73783111572266 test_loss:416.3426818847656\n",
      "231/3000 train_loss: 95.50398254394531 test_loss:402.4244384765625\n",
      "232/3000 train_loss: 81.02438354492188 test_loss:387.9234313964844\n",
      "233/3000 train_loss: 77.01266479492188 test_loss:384.6121520996094\n",
      "234/3000 train_loss: 80.90283966064453 test_loss:363.2486877441406\n",
      "235/3000 train_loss: 64.98477172851562 test_loss:350.0157775878906\n",
      "236/3000 train_loss: 60.28022384643555 test_loss:356.5334167480469\n",
      "237/3000 train_loss: 58.522247314453125 test_loss:368.76751708984375\n",
      "238/3000 train_loss: 57.52113723754883 test_loss:361.1918029785156\n",
      "239/3000 train_loss: 56.34384536743164 test_loss:350.87310791015625\n",
      "240/3000 train_loss: 56.769287109375 test_loss:350.4309387207031\n",
      "241/3000 train_loss: 58.226661682128906 test_loss:345.62628173828125\n",
      "242/3000 train_loss: 60.31890869140625 test_loss:332.1048278808594\n",
      "243/3000 train_loss: 58.34871292114258 test_loss:363.0404968261719\n",
      "244/3000 train_loss: 56.821571350097656 test_loss:366.8704833984375\n",
      "245/3000 train_loss: 54.18729782104492 test_loss:380.054443359375\n",
      "246/3000 train_loss: 55.280670166015625 test_loss:377.9276428222656\n",
      "247/3000 train_loss: 58.320735931396484 test_loss:389.03802490234375\n",
      "248/3000 train_loss: 59.722206115722656 test_loss:380.2159423828125\n",
      "249/3000 train_loss: 57.6878662109375 test_loss:400.1047668457031\n",
      "250/3000 train_loss: 57.338382720947266 test_loss:408.2195739746094\n",
      "251/3000 train_loss: 57.675662994384766 test_loss:361.9750061035156\n",
      "252/3000 train_loss: 56.711212158203125 test_loss:382.60498046875\n",
      "253/3000 train_loss: 53.35297775268555 test_loss:374.9516906738281\n",
      "254/3000 train_loss: 49.63637161254883 test_loss:362.86663818359375\n",
      "255/3000 train_loss: 47.90062713623047 test_loss:359.7873840332031\n",
      "256/3000 train_loss: 48.03459930419922 test_loss:355.00970458984375\n",
      "257/3000 train_loss: 47.76065444946289 test_loss:361.5149841308594\n",
      "258/3000 train_loss: 46.91268539428711 test_loss:358.9432373046875\n",
      "259/3000 train_loss: 46.97127151489258 test_loss:356.10235595703125\n",
      "260/3000 train_loss: 47.73824691772461 test_loss:362.01300048828125\n",
      "261/3000 train_loss: 47.60841751098633 test_loss:352.9349365234375\n",
      "262/3000 train_loss: 49.464847564697266 test_loss:370.49566650390625\n",
      "263/3000 train_loss: 51.27888488769531 test_loss:392.9248046875\n",
      "264/3000 train_loss: 55.67831039428711 test_loss:382.6346130371094\n",
      "265/3000 train_loss: 59.02604675292969 test_loss:370.26171875\n",
      "266/3000 train_loss: 58.49519729614258 test_loss:385.7176208496094\n",
      "267/3000 train_loss: 58.779579162597656 test_loss:380.60418701171875\n",
      "268/3000 train_loss: 60.85496520996094 test_loss:404.6334533691406\n",
      "269/3000 train_loss: 60.97330856323242 test_loss:335.9275817871094\n",
      "270/3000 train_loss: 58.19652557373047 test_loss:387.09490966796875\n",
      "271/3000 train_loss: 58.38860321044922 test_loss:423.095703125\n",
      "272/3000 train_loss: 65.02841186523438 test_loss:391.046875\n",
      "273/3000 train_loss: 63.26047134399414 test_loss:395.9887390136719\n",
      "274/3000 train_loss: 61.69023895263672 test_loss:428.6756896972656\n",
      "275/3000 train_loss: 64.44384765625 test_loss:423.33837890625\n",
      "276/3000 train_loss: 67.30796813964844 test_loss:382.1894226074219\n",
      "277/3000 train_loss: 62.652523040771484 test_loss:410.6265869140625\n",
      "278/3000 train_loss: 56.9501838684082 test_loss:391.9510498046875\n",
      "279/3000 train_loss: 63.15495300292969 test_loss:396.3402099609375\n",
      "280/3000 train_loss: 77.34898376464844 test_loss:358.4585266113281\n",
      "281/3000 train_loss: 68.57827758789062 test_loss:327.6802978515625\n",
      "282/3000 train_loss: 57.71555709838867 test_loss:364.7028503417969\n",
      "283/3000 train_loss: 55.60606002807617 test_loss:374.9144287109375\n",
      "284/3000 train_loss: 53.679054260253906 test_loss:371.4405517578125\n",
      "285/3000 train_loss: 52.4803466796875 test_loss:380.84625244140625\n",
      "286/3000 train_loss: 51.103485107421875 test_loss:359.4099426269531\n",
      "287/3000 train_loss: 51.30604934692383 test_loss:347.993896484375\n",
      "288/3000 train_loss: 51.59614944458008 test_loss:354.6510925292969\n",
      "289/3000 train_loss: 48.372047424316406 test_loss:341.79345703125\n",
      "290/3000 train_loss: 47.03984069824219 test_loss:348.30889892578125\n",
      "291/3000 train_loss: 46.77241897583008 test_loss:341.318603515625\n",
      "292/3000 train_loss: 46.04011535644531 test_loss:351.92742919921875\n",
      "293/3000 train_loss: 46.24310302734375 test_loss:358.4450378417969\n",
      "294/3000 train_loss: 46.708282470703125 test_loss:358.765380859375\n",
      "295/3000 train_loss: 45.263553619384766 test_loss:353.0730895996094\n",
      "296/3000 train_loss: 44.00345230102539 test_loss:343.07958984375\n",
      "297/3000 train_loss: 43.90932083129883 test_loss:349.0201721191406\n",
      "298/3000 train_loss: 44.72728729248047 test_loss:349.4976806640625\n",
      "299/3000 train_loss: 44.59120178222656 test_loss:347.22998046875\n",
      "300/3000 train_loss: 43.91555404663086 test_loss:346.6917724609375\n",
      "301/3000 train_loss: 43.42319869995117 test_loss:343.5760498046875\n",
      "302/3000 train_loss: 43.64752960205078 test_loss:347.7014465332031\n",
      "303/3000 train_loss: 43.5053825378418 test_loss:348.9316711425781\n",
      "304/3000 train_loss: 43.67124938964844 test_loss:354.4134216308594\n",
      "305/3000 train_loss: 44.515472412109375 test_loss:345.08001708984375\n",
      "306/3000 train_loss: 46.27002716064453 test_loss:342.1851806640625\n",
      "307/3000 train_loss: 47.14072036743164 test_loss:336.9091491699219\n",
      "308/3000 train_loss: 47.475250244140625 test_loss:353.3935852050781\n",
      "309/3000 train_loss: 46.741172790527344 test_loss:346.33544921875\n",
      "310/3000 train_loss: 46.40608215332031 test_loss:323.614990234375\n",
      "311/3000 train_loss: 45.141937255859375 test_loss:319.11102294921875\n",
      "312/3000 train_loss: 45.27688217163086 test_loss:328.33587646484375\n",
      "313/3000 train_loss: 45.11585998535156 test_loss:338.03839111328125\n",
      "314/3000 train_loss: 45.924373626708984 test_loss:335.0504455566406\n",
      "315/3000 train_loss: 51.35000228881836 test_loss:337.57806396484375\n",
      "316/3000 train_loss: 55.716922760009766 test_loss:325.11376953125\n",
      "317/3000 train_loss: 57.690738677978516 test_loss:337.1796569824219\n",
      "318/3000 train_loss: 63.49915313720703 test_loss:356.56689453125\n",
      "319/3000 train_loss: 57.078025817871094 test_loss:343.6291198730469\n",
      "320/3000 train_loss: 51.524417877197266 test_loss:306.8035888671875\n",
      "321/3000 train_loss: 49.114341735839844 test_loss:305.3365783691406\n",
      "322/3000 train_loss: 52.35185241699219 test_loss:355.5720520019531\n",
      "323/3000 train_loss: 49.5405387878418 test_loss:350.4339904785156\n",
      "324/3000 train_loss: 48.61465835571289 test_loss:327.51226806640625\n",
      "325/3000 train_loss: 48.53382873535156 test_loss:313.1625671386719\n",
      "326/3000 train_loss: 48.41011047363281 test_loss:355.7780456542969\n",
      "327/3000 train_loss: 54.66230773925781 test_loss:390.0427551269531\n",
      "328/3000 train_loss: 52.04774475097656 test_loss:357.86370849609375\n",
      "329/3000 train_loss: 53.35224914550781 test_loss:361.67535400390625\n",
      "330/3000 train_loss: 55.54895782470703 test_loss:390.3165283203125\n",
      "331/3000 train_loss: 53.64496612548828 test_loss:424.95257568359375\n",
      "332/3000 train_loss: 68.1965560913086 test_loss:416.0770263671875\n",
      "333/3000 train_loss: 68.32759857177734 test_loss:324.1192932128906\n",
      "334/3000 train_loss: 71.11210632324219 test_loss:343.02337646484375\n",
      "335/3000 train_loss: 65.26667022705078 test_loss:381.714111328125\n",
      "336/3000 train_loss: 78.15230560302734 test_loss:317.9349670410156\n",
      "337/3000 train_loss: 74.04132843017578 test_loss:299.8185729980469\n",
      "338/3000 train_loss: 57.23456573486328 test_loss:338.9501037597656\n",
      "339/3000 train_loss: 49.767738342285156 test_loss:333.5036926269531\n",
      "340/3000 train_loss: 44.37372589111328 test_loss:332.3691711425781\n",
      "341/3000 train_loss: 42.375282287597656 test_loss:337.5704345703125\n",
      "342/3000 train_loss: 43.795196533203125 test_loss:319.879150390625\n",
      "343/3000 train_loss: 42.724769592285156 test_loss:312.828857421875\n",
      "344/3000 train_loss: 44.36700439453125 test_loss:330.11553955078125\n",
      "345/3000 train_loss: 42.929901123046875 test_loss:311.7827453613281\n",
      "346/3000 train_loss: 39.20862579345703 test_loss:317.2749328613281\n",
      "347/3000 train_loss: 38.892086029052734 test_loss:322.124755859375\n",
      "348/3000 train_loss: 38.675872802734375 test_loss:314.2923583984375\n",
      "349/3000 train_loss: 38.93102264404297 test_loss:304.6209716796875\n",
      "350/3000 train_loss: 38.785953521728516 test_loss:305.86077880859375\n",
      "351/3000 train_loss: 41.472713470458984 test_loss:307.9128112792969\n",
      "352/3000 train_loss: 43.018192291259766 test_loss:304.5423889160156\n",
      "353/3000 train_loss: 41.1994514465332 test_loss:308.7247619628906\n",
      "354/3000 train_loss: 40.361061096191406 test_loss:319.25677490234375\n",
      "355/3000 train_loss: 41.7677001953125 test_loss:314.66888427734375\n",
      "356/3000 train_loss: 42.43989181518555 test_loss:301.53155517578125\n",
      "357/3000 train_loss: 43.02503204345703 test_loss:318.1004638671875\n",
      "358/3000 train_loss: 42.910804748535156 test_loss:297.27471923828125\n",
      "359/3000 train_loss: 44.61087417602539 test_loss:300.770263671875\n",
      "360/3000 train_loss: 49.207313537597656 test_loss:296.7294921875\n",
      "361/3000 train_loss: 45.13835906982422 test_loss:302.1759033203125\n",
      "362/3000 train_loss: 43.0799560546875 test_loss:301.59405517578125\n",
      "363/3000 train_loss: 41.89997863769531 test_loss:300.3440856933594\n",
      "364/3000 train_loss: 41.64814376831055 test_loss:307.3101501464844\n",
      "365/3000 train_loss: 39.86918258666992 test_loss:305.15289306640625\n",
      "366/3000 train_loss: 39.04365539550781 test_loss:297.1842346191406\n",
      "367/3000 train_loss: 40.1634635925293 test_loss:289.3064880371094\n",
      "368/3000 train_loss: 39.59458923339844 test_loss:269.363037109375\n",
      "369/3000 train_loss: 38.73884582519531 test_loss:279.1518249511719\n",
      "370/3000 train_loss: 39.15050506591797 test_loss:281.77301025390625\n",
      "371/3000 train_loss: 39.13448715209961 test_loss:287.71826171875\n",
      "372/3000 train_loss: 38.653785705566406 test_loss:283.7146911621094\n",
      "373/3000 train_loss: 38.466453552246094 test_loss:275.6142578125\n",
      "374/3000 train_loss: 38.226585388183594 test_loss:276.46148681640625\n",
      "375/3000 train_loss: 37.42507553100586 test_loss:273.94677734375\n",
      "376/3000 train_loss: 38.68434143066406 test_loss:266.1490173339844\n",
      "377/3000 train_loss: 38.24347686767578 test_loss:270.4333190917969\n",
      "378/3000 train_loss: 37.62721252441406 test_loss:278.3680114746094\n",
      "379/3000 train_loss: 37.16511917114258 test_loss:284.4967346191406\n",
      "380/3000 train_loss: 38.576622009277344 test_loss:276.60247802734375\n",
      "381/3000 train_loss: 38.95075607299805 test_loss:278.3436584472656\n",
      "382/3000 train_loss: 39.7138671875 test_loss:280.69970703125\n",
      "383/3000 train_loss: 42.663055419921875 test_loss:268.918212890625\n",
      "384/3000 train_loss: 45.491397857666016 test_loss:272.96588134765625\n",
      "385/3000 train_loss: 44.31437301635742 test_loss:298.44287109375\n",
      "386/3000 train_loss: 49.49220657348633 test_loss:285.36810302734375\n",
      "387/3000 train_loss: 54.76567840576172 test_loss:247.98300170898438\n",
      "388/3000 train_loss: 73.61920928955078 test_loss:271.0865478515625\n",
      "389/3000 train_loss: 54.89692306518555 test_loss:290.7367248535156\n",
      "390/3000 train_loss: 51.01094055175781 test_loss:283.4503479003906\n",
      "391/3000 train_loss: 49.776336669921875 test_loss:272.6738586425781\n",
      "392/3000 train_loss: 48.075294494628906 test_loss:284.6963195800781\n",
      "393/3000 train_loss: 42.44427490234375 test_loss:289.296875\n",
      "394/3000 train_loss: 42.163578033447266 test_loss:286.1941833496094\n",
      "395/3000 train_loss: 41.584651947021484 test_loss:290.5574645996094\n",
      "396/3000 train_loss: 40.807579040527344 test_loss:301.7906494140625\n",
      "397/3000 train_loss: 42.01166534423828 test_loss:294.0948486328125\n",
      "398/3000 train_loss: 42.47593307495117 test_loss:289.9925842285156\n",
      "399/3000 train_loss: 44.21046829223633 test_loss:292.0847473144531\n",
      "400/3000 train_loss: 45.76106262207031 test_loss:286.3453369140625\n",
      "401/3000 train_loss: 49.997745513916016 test_loss:274.1864318847656\n",
      "402/3000 train_loss: 52.262577056884766 test_loss:265.8695373535156\n",
      "403/3000 train_loss: 50.888004302978516 test_loss:273.3782653808594\n",
      "404/3000 train_loss: 44.17613220214844 test_loss:294.62322998046875\n",
      "405/3000 train_loss: 44.917327880859375 test_loss:300.25970458984375\n",
      "406/3000 train_loss: 42.781375885009766 test_loss:302.376708984375\n",
      "407/3000 train_loss: 41.6906623840332 test_loss:299.9666748046875\n",
      "408/3000 train_loss: 43.903045654296875 test_loss:293.9313049316406\n",
      "409/3000 train_loss: 44.06346130371094 test_loss:293.5079040527344\n",
      "410/3000 train_loss: 42.98603820800781 test_loss:299.19580078125\n",
      "411/3000 train_loss: 44.42612075805664 test_loss:304.72308349609375\n",
      "412/3000 train_loss: 45.01520538330078 test_loss:305.3533935546875\n",
      "413/3000 train_loss: 46.91619873046875 test_loss:298.6409606933594\n",
      "414/3000 train_loss: 46.358314514160156 test_loss:324.09356689453125\n",
      "415/3000 train_loss: 47.71396255493164 test_loss:356.4474792480469\n",
      "416/3000 train_loss: 53.35737228393555 test_loss:334.9205017089844\n",
      "417/3000 train_loss: 55.32341766357422 test_loss:342.7345886230469\n",
      "418/3000 train_loss: 48.95936965942383 test_loss:339.5230407714844\n",
      "419/3000 train_loss: 46.206295013427734 test_loss:344.1514892578125\n",
      "420/3000 train_loss: 49.4880485534668 test_loss:321.1141662597656\n",
      "421/3000 train_loss: 45.40922927856445 test_loss:323.5071716308594\n",
      "422/3000 train_loss: 40.80508804321289 test_loss:357.1462097167969\n",
      "423/3000 train_loss: 41.44910430908203 test_loss:363.3064270019531\n",
      "424/3000 train_loss: 43.63069534301758 test_loss:369.1202697753906\n",
      "425/3000 train_loss: 43.0687255859375 test_loss:380.05767822265625\n",
      "426/3000 train_loss: 44.1943359375 test_loss:369.78533935546875\n",
      "427/3000 train_loss: 45.86188888549805 test_loss:359.8592224121094\n",
      "428/3000 train_loss: 44.08264923095703 test_loss:383.505126953125\n",
      "429/3000 train_loss: 44.74346923828125 test_loss:409.6353454589844\n",
      "430/3000 train_loss: 49.959197998046875 test_loss:385.5484924316406\n",
      "431/3000 train_loss: 49.40347671508789 test_loss:356.9700622558594\n",
      "432/3000 train_loss: 46.992225646972656 test_loss:362.29620361328125\n",
      "433/3000 train_loss: 53.82361602783203 test_loss:372.7928771972656\n",
      "434/3000 train_loss: 53.91978454589844 test_loss:413.66522216796875\n",
      "435/3000 train_loss: 52.476219177246094 test_loss:459.0560607910156\n",
      "436/3000 train_loss: 54.665977478027344 test_loss:339.481201171875\n",
      "437/3000 train_loss: 45.26185989379883 test_loss:392.158203125\n",
      "438/3000 train_loss: 43.30462646484375 test_loss:408.4149169921875\n",
      "439/3000 train_loss: 42.01477813720703 test_loss:421.0513610839844\n",
      "440/3000 train_loss: 44.12394714355469 test_loss:415.8398742675781\n",
      "441/3000 train_loss: 42.82435989379883 test_loss:428.0298156738281\n",
      "442/3000 train_loss: 42.91438674926758 test_loss:434.5672607421875\n",
      "443/3000 train_loss: 45.389869689941406 test_loss:406.66009521484375\n",
      "444/3000 train_loss: 44.98772430419922 test_loss:397.7259521484375\n",
      "445/3000 train_loss: 45.56719207763672 test_loss:426.9219665527344\n",
      "446/3000 train_loss: 44.61536407470703 test_loss:410.08453369140625\n",
      "447/3000 train_loss: 50.16023635864258 test_loss:315.76947021484375\n",
      "448/3000 train_loss: 48.95194625854492 test_loss:333.5556335449219\n",
      "449/3000 train_loss: 53.28888702392578 test_loss:332.05340576171875\n",
      "450/3000 train_loss: 51.64445114135742 test_loss:373.577880859375\n",
      "451/3000 train_loss: 46.24161911010742 test_loss:357.6694030761719\n",
      "452/3000 train_loss: 40.43011474609375 test_loss:352.377685546875\n",
      "453/3000 train_loss: 39.69957733154297 test_loss:354.7809143066406\n",
      "454/3000 train_loss: 38.57708740234375 test_loss:358.2884826660156\n",
      "455/3000 train_loss: 36.649967193603516 test_loss:382.5910339355469\n",
      "456/3000 train_loss: 36.0828971862793 test_loss:384.13018798828125\n",
      "457/3000 train_loss: 35.827598571777344 test_loss:368.1062316894531\n",
      "458/3000 train_loss: 36.31597900390625 test_loss:363.5621643066406\n",
      "459/3000 train_loss: 35.832454681396484 test_loss:360.6155700683594\n",
      "460/3000 train_loss: 35.00554656982422 test_loss:366.6595764160156\n",
      "461/3000 train_loss: 34.2869987487793 test_loss:368.1460266113281\n",
      "462/3000 train_loss: 34.27350997924805 test_loss:373.4982604980469\n",
      "463/3000 train_loss: 34.32562255859375 test_loss:396.3351745605469\n",
      "464/3000 train_loss: 34.611000061035156 test_loss:410.11309814453125\n",
      "465/3000 train_loss: 34.774810791015625 test_loss:415.2106018066406\n",
      "466/3000 train_loss: 35.345001220703125 test_loss:415.90447998046875\n",
      "467/3000 train_loss: 36.26871109008789 test_loss:409.4677429199219\n",
      "468/3000 train_loss: 36.64323425292969 test_loss:401.3127136230469\n",
      "469/3000 train_loss: 37.43346405029297 test_loss:379.281005859375\n",
      "470/3000 train_loss: 40.38908004760742 test_loss:363.6368713378906\n",
      "471/3000 train_loss: 42.392005920410156 test_loss:371.3843078613281\n",
      "472/3000 train_loss: 39.371620178222656 test_loss:428.58782958984375\n",
      "473/3000 train_loss: 36.04683303833008 test_loss:430.09722900390625\n",
      "474/3000 train_loss: 34.244659423828125 test_loss:419.4820556640625\n",
      "475/3000 train_loss: 35.42123031616211 test_loss:425.4082946777344\n",
      "476/3000 train_loss: 35.61343002319336 test_loss:432.6368103027344\n",
      "477/3000 train_loss: 35.57451629638672 test_loss:417.0020751953125\n",
      "478/3000 train_loss: 37.83932876586914 test_loss:381.3805847167969\n",
      "479/3000 train_loss: 38.975467681884766 test_loss:387.74383544921875\n",
      "480/3000 train_loss: 39.66054916381836 test_loss:402.53485107421875\n",
      "481/3000 train_loss: 42.10063934326172 test_loss:421.307861328125\n",
      "482/3000 train_loss: 42.09775924682617 test_loss:384.61834716796875\n",
      "483/3000 train_loss: 46.781105041503906 test_loss:384.19134521484375\n",
      "484/3000 train_loss: 42.522640228271484 test_loss:434.9142150878906\n",
      "485/3000 train_loss: 38.28466796875 test_loss:466.73333740234375\n",
      "486/3000 train_loss: 38.24030303955078 test_loss:443.794921875\n",
      "487/3000 train_loss: 38.31075668334961 test_loss:401.8016357421875\n",
      "488/3000 train_loss: 36.77661895751953 test_loss:391.0836181640625\n",
      "489/3000 train_loss: 35.89472579956055 test_loss:394.0657043457031\n",
      "490/3000 train_loss: 37.86997604370117 test_loss:370.7516174316406\n",
      "491/3000 train_loss: 43.71328353881836 test_loss:377.45458984375\n",
      "492/3000 train_loss: 43.96357345581055 test_loss:416.61376953125\n",
      "493/3000 train_loss: 41.35572814941406 test_loss:455.134521484375\n",
      "494/3000 train_loss: 43.82571029663086 test_loss:407.9927673339844\n",
      "495/3000 train_loss: 38.48872375488281 test_loss:383.0202941894531\n",
      "496/3000 train_loss: 37.35700607299805 test_loss:354.8133544921875\n",
      "497/3000 train_loss: 40.20126724243164 test_loss:360.636962890625\n",
      "498/3000 train_loss: 39.67734909057617 test_loss:390.90850830078125\n",
      "499/3000 train_loss: 39.62705993652344 test_loss:417.2657165527344\n",
      "500/3000 train_loss: 39.6502571105957 test_loss:412.55096435546875\n",
      "501/3000 train_loss: 39.77163314819336 test_loss:376.64703369140625\n",
      "502/3000 train_loss: 40.402713775634766 test_loss:350.5134582519531\n",
      "503/3000 train_loss: 41.63444900512695 test_loss:341.13543701171875\n",
      "504/3000 train_loss: 45.28103256225586 test_loss:350.83221435546875\n",
      "505/3000 train_loss: 41.50720977783203 test_loss:379.0722961425781\n",
      "506/3000 train_loss: 40.640445709228516 test_loss:377.2225341796875\n",
      "507/3000 train_loss: 40.54581069946289 test_loss:373.4767150878906\n",
      "508/3000 train_loss: 41.461124420166016 test_loss:377.6769104003906\n",
      "509/3000 train_loss: 44.19977569580078 test_loss:373.55389404296875\n",
      "510/3000 train_loss: 42.575096130371094 test_loss:383.32147216796875\n",
      "511/3000 train_loss: 43.776615142822266 test_loss:401.8744201660156\n",
      "512/3000 train_loss: 41.3346061706543 test_loss:404.1551513671875\n",
      "513/3000 train_loss: 39.47463607788086 test_loss:432.41937255859375\n",
      "514/3000 train_loss: 40.99891662597656 test_loss:480.94061279296875\n",
      "515/3000 train_loss: 44.313133239746094 test_loss:443.891845703125\n",
      "516/3000 train_loss: 42.21967315673828 test_loss:451.7775573730469\n",
      "517/3000 train_loss: 44.90596389770508 test_loss:367.8981628417969\n",
      "518/3000 train_loss: 44.38971710205078 test_loss:402.77911376953125\n",
      "519/3000 train_loss: 45.38985824584961 test_loss:391.76898193359375\n",
      "520/3000 train_loss: 47.90141677856445 test_loss:375.6492919921875\n",
      "521/3000 train_loss: 52.55434799194336 test_loss:399.04815673828125\n",
      "522/3000 train_loss: 44.94760513305664 test_loss:374.00439453125\n",
      "523/3000 train_loss: 44.3634033203125 test_loss:353.0400085449219\n",
      "524/3000 train_loss: 43.30930709838867 test_loss:363.3252868652344\n",
      "525/3000 train_loss: 45.73866653442383 test_loss:358.73480224609375\n",
      "526/3000 train_loss: 44.918739318847656 test_loss:378.46533203125\n",
      "527/3000 train_loss: 42.655609130859375 test_loss:340.8877258300781\n",
      "528/3000 train_loss: 42.341575622558594 test_loss:300.9001159667969\n",
      "529/3000 train_loss: 42.25302505493164 test_loss:329.43902587890625\n",
      "530/3000 train_loss: 40.18654251098633 test_loss:319.7472839355469\n",
      "531/3000 train_loss: 40.96928024291992 test_loss:325.41632080078125\n",
      "532/3000 train_loss: 39.40790557861328 test_loss:332.0634765625\n",
      "533/3000 train_loss: 44.742374420166016 test_loss:363.3935546875\n",
      "534/3000 train_loss: 47.45126724243164 test_loss:407.62554931640625\n",
      "535/3000 train_loss: 47.73493957519531 test_loss:355.9134521484375\n",
      "536/3000 train_loss: 41.89955520629883 test_loss:328.3720397949219\n",
      "537/3000 train_loss: 36.561893463134766 test_loss:351.98944091796875\n",
      "538/3000 train_loss: 35.17361831665039 test_loss:348.6095886230469\n",
      "539/3000 train_loss: 33.85536193847656 test_loss:356.6019287109375\n",
      "540/3000 train_loss: 33.95574951171875 test_loss:357.9715881347656\n",
      "541/3000 train_loss: 34.12026596069336 test_loss:344.1033020019531\n",
      "542/3000 train_loss: 33.989009857177734 test_loss:322.3383483886719\n",
      "543/3000 train_loss: 34.139747619628906 test_loss:322.11651611328125\n",
      "544/3000 train_loss: 35.536712646484375 test_loss:326.19488525390625\n",
      "545/3000 train_loss: 34.40061950683594 test_loss:341.4752197265625\n",
      "546/3000 train_loss: 35.320682525634766 test_loss:345.224853515625\n",
      "547/3000 train_loss: 34.41765594482422 test_loss:344.2634582519531\n",
      "548/3000 train_loss: 34.352561950683594 test_loss:313.52191162109375\n",
      "549/3000 train_loss: 36.02421188354492 test_loss:299.4650573730469\n",
      "550/3000 train_loss: 35.46175765991211 test_loss:334.9795227050781\n",
      "551/3000 train_loss: 34.611385345458984 test_loss:350.6524658203125\n",
      "552/3000 train_loss: 33.613521575927734 test_loss:349.1565856933594\n",
      "553/3000 train_loss: 32.98795700073242 test_loss:322.25677490234375\n",
      "554/3000 train_loss: 32.837650299072266 test_loss:301.5833740234375\n",
      "555/3000 train_loss: 33.22325134277344 test_loss:317.7969665527344\n",
      "556/3000 train_loss: 32.04914093017578 test_loss:346.01556396484375\n",
      "557/3000 train_loss: 32.80589294433594 test_loss:344.931640625\n",
      "558/3000 train_loss: 32.402435302734375 test_loss:334.4651184082031\n",
      "559/3000 train_loss: 31.964942932128906 test_loss:307.6301574707031\n",
      "560/3000 train_loss: 33.09992599487305 test_loss:311.45562744140625\n",
      "561/3000 train_loss: 33.480926513671875 test_loss:340.0093078613281\n",
      "562/3000 train_loss: 33.60437774658203 test_loss:347.5367126464844\n",
      "563/3000 train_loss: 33.25239944458008 test_loss:345.8996276855469\n",
      "564/3000 train_loss: 34.355098724365234 test_loss:313.8642883300781\n",
      "565/3000 train_loss: 36.73334884643555 test_loss:292.72027587890625\n",
      "566/3000 train_loss: 36.95816421508789 test_loss:316.691162109375\n",
      "567/3000 train_loss: 35.906410217285156 test_loss:355.3904724121094\n",
      "568/3000 train_loss: 35.7317008972168 test_loss:367.7306213378906\n",
      "569/3000 train_loss: 35.00090026855469 test_loss:331.91424560546875\n",
      "570/3000 train_loss: 36.261940002441406 test_loss:314.542236328125\n",
      "571/3000 train_loss: 34.3725700378418 test_loss:311.9721984863281\n",
      "572/3000 train_loss: 34.181129455566406 test_loss:324.4462890625\n",
      "573/3000 train_loss: 36.83074188232422 test_loss:337.3877258300781\n",
      "574/3000 train_loss: 39.993324279785156 test_loss:341.35455322265625\n",
      "575/3000 train_loss: 42.30047607421875 test_loss:312.3598327636719\n",
      "576/3000 train_loss: 38.71820831298828 test_loss:366.7367858886719\n",
      "577/3000 train_loss: 42.06071472167969 test_loss:326.0588073730469\n",
      "578/3000 train_loss: 40.9361572265625 test_loss:352.303466796875\n",
      "579/3000 train_loss: 41.39862060546875 test_loss:332.8982849121094\n",
      "580/3000 train_loss: 41.918800354003906 test_loss:387.9201354980469\n",
      "581/3000 train_loss: 46.169715881347656 test_loss:336.8041076660156\n",
      "582/3000 train_loss: 42.93779754638672 test_loss:359.73876953125\n",
      "583/3000 train_loss: 45.27385330200195 test_loss:342.2209777832031\n",
      "584/3000 train_loss: 42.740867614746094 test_loss:395.5299377441406\n",
      "585/3000 train_loss: 45.315269470214844 test_loss:366.97674560546875\n",
      "586/3000 train_loss: 44.77925491333008 test_loss:387.7843017578125\n",
      "587/3000 train_loss: 45.0914192199707 test_loss:376.078125\n",
      "588/3000 train_loss: 46.93174743652344 test_loss:346.97076416015625\n",
      "589/3000 train_loss: 55.367610931396484 test_loss:331.2605285644531\n",
      "590/3000 train_loss: 57.617088317871094 test_loss:373.13446044921875\n",
      "591/3000 train_loss: 48.338043212890625 test_loss:328.41650390625\n",
      "592/3000 train_loss: 46.01441955566406 test_loss:337.9378356933594\n",
      "593/3000 train_loss: 46.71428680419922 test_loss:358.1262512207031\n",
      "594/3000 train_loss: 47.91594314575195 test_loss:343.6715087890625\n",
      "595/3000 train_loss: 50.025325775146484 test_loss:393.63818359375\n",
      "596/3000 train_loss: 45.02522277832031 test_loss:361.64251708984375\n",
      "597/3000 train_loss: 47.386112213134766 test_loss:372.9103088378906\n",
      "598/3000 train_loss: 42.295074462890625 test_loss:374.6573791503906\n",
      "599/3000 train_loss: 40.78041076660156 test_loss:363.214111328125\n",
      "600/3000 train_loss: 40.83073043823242 test_loss:365.4520263671875\n",
      "601/3000 train_loss: 41.66596221923828 test_loss:374.4199523925781\n",
      "602/3000 train_loss: 40.31831359863281 test_loss:381.79779052734375\n",
      "603/3000 train_loss: 39.0036506652832 test_loss:360.260986328125\n",
      "604/3000 train_loss: 36.1230354309082 test_loss:332.3390197753906\n",
      "605/3000 train_loss: 33.936607360839844 test_loss:356.8194885253906\n",
      "606/3000 train_loss: 31.95254898071289 test_loss:359.84637451171875\n",
      "607/3000 train_loss: 31.32558822631836 test_loss:355.5089111328125\n",
      "608/3000 train_loss: 30.44935417175293 test_loss:348.7776184082031\n",
      "609/3000 train_loss: 29.84347152709961 test_loss:344.2665710449219\n",
      "610/3000 train_loss: 29.509464263916016 test_loss:348.37249755859375\n",
      "611/3000 train_loss: 29.415103912353516 test_loss:350.2880859375\n",
      "612/3000 train_loss: 29.472875595092773 test_loss:367.7642517089844\n",
      "613/3000 train_loss: 29.403724670410156 test_loss:363.43084716796875\n",
      "614/3000 train_loss: 29.172327041625977 test_loss:349.84539794921875\n",
      "615/3000 train_loss: 28.901735305786133 test_loss:364.1064147949219\n",
      "616/3000 train_loss: 28.4642333984375 test_loss:356.70782470703125\n",
      "617/3000 train_loss: 27.880020141601562 test_loss:362.98931884765625\n",
      "618/3000 train_loss: 28.373655319213867 test_loss:360.950927734375\n",
      "619/3000 train_loss: 27.790212631225586 test_loss:360.5148620605469\n",
      "620/3000 train_loss: 28.242961883544922 test_loss:347.39947509765625\n",
      "621/3000 train_loss: 28.72943878173828 test_loss:335.6933288574219\n",
      "622/3000 train_loss: 28.94920539855957 test_loss:347.79486083984375\n",
      "623/3000 train_loss: 29.549470901489258 test_loss:382.57891845703125\n",
      "624/3000 train_loss: 29.78643035888672 test_loss:364.0528259277344\n",
      "625/3000 train_loss: 31.24052619934082 test_loss:365.24896240234375\n",
      "626/3000 train_loss: 30.483198165893555 test_loss:334.88885498046875\n",
      "627/3000 train_loss: 30.338581085205078 test_loss:341.10540771484375\n",
      "628/3000 train_loss: 32.660369873046875 test_loss:354.9111022949219\n",
      "629/3000 train_loss: 30.015897750854492 test_loss:386.9902648925781\n",
      "630/3000 train_loss: 32.572654724121094 test_loss:340.0767822265625\n",
      "631/3000 train_loss: 30.148393630981445 test_loss:320.0448303222656\n",
      "632/3000 train_loss: 29.90326690673828 test_loss:325.5486145019531\n",
      "633/3000 train_loss: 30.267852783203125 test_loss:324.9186096191406\n",
      "634/3000 train_loss: 30.34764862060547 test_loss:328.46478271484375\n",
      "635/3000 train_loss: 29.49359703063965 test_loss:367.96307373046875\n",
      "636/3000 train_loss: 32.4434928894043 test_loss:377.0601806640625\n",
      "637/3000 train_loss: 31.69775390625 test_loss:314.68975830078125\n",
      "638/3000 train_loss: 32.365596771240234 test_loss:311.1829833984375\n",
      "639/3000 train_loss: 29.778270721435547 test_loss:310.6033020019531\n",
      "640/3000 train_loss: 31.180625915527344 test_loss:333.4533996582031\n",
      "641/3000 train_loss: 30.68256950378418 test_loss:377.05157470703125\n",
      "642/3000 train_loss: 30.71409797668457 test_loss:336.51995849609375\n",
      "643/3000 train_loss: 29.331897735595703 test_loss:317.3710632324219\n",
      "644/3000 train_loss: 28.293771743774414 test_loss:292.0596923828125\n",
      "645/3000 train_loss: 29.452360153198242 test_loss:328.3771667480469\n",
      "646/3000 train_loss: 29.919857025146484 test_loss:379.9781799316406\n",
      "647/3000 train_loss: 32.62864303588867 test_loss:338.704345703125\n",
      "648/3000 train_loss: 31.384361267089844 test_loss:287.70318603515625\n",
      "649/3000 train_loss: 33.19284439086914 test_loss:315.2013244628906\n",
      "650/3000 train_loss: 31.453176498413086 test_loss:366.99346923828125\n",
      "651/3000 train_loss: 31.45948600769043 test_loss:350.3647155761719\n",
      "652/3000 train_loss: 31.312408447265625 test_loss:294.7392578125\n",
      "653/3000 train_loss: 30.458290100097656 test_loss:308.5379638671875\n",
      "654/3000 train_loss: 30.131576538085938 test_loss:363.2091369628906\n",
      "655/3000 train_loss: 30.875038146972656 test_loss:336.51812744140625\n",
      "656/3000 train_loss: 29.371601104736328 test_loss:299.1083984375\n",
      "657/3000 train_loss: 31.02377700805664 test_loss:294.8561096191406\n",
      "658/3000 train_loss: 32.77296447753906 test_loss:326.9730529785156\n",
      "659/3000 train_loss: 36.08395004272461 test_loss:364.1632995605469\n",
      "660/3000 train_loss: 32.3656005859375 test_loss:311.3740234375\n",
      "661/3000 train_loss: 33.60508346557617 test_loss:299.0352478027344\n",
      "662/3000 train_loss: 30.406049728393555 test_loss:318.6414794921875\n",
      "663/3000 train_loss: 28.673686981201172 test_loss:316.33441162109375\n",
      "664/3000 train_loss: 29.4599666595459 test_loss:321.0304260253906\n",
      "665/3000 train_loss: 29.84016990661621 test_loss:312.7510986328125\n",
      "666/3000 train_loss: 30.71834373474121 test_loss:303.68499755859375\n",
      "667/3000 train_loss: 34.192996978759766 test_loss:294.7403869628906\n",
      "668/3000 train_loss: 35.6942138671875 test_loss:325.7521057128906\n",
      "669/3000 train_loss: 34.911888122558594 test_loss:332.23236083984375\n",
      "670/3000 train_loss: 32.69724655151367 test_loss:306.7669677734375\n",
      "671/3000 train_loss: 30.89375114440918 test_loss:297.6127624511719\n",
      "672/3000 train_loss: 30.157272338867188 test_loss:309.05767822265625\n",
      "673/3000 train_loss: 31.477567672729492 test_loss:316.51153564453125\n",
      "674/3000 train_loss: 31.084449768066406 test_loss:304.5972595214844\n",
      "675/3000 train_loss: 30.980459213256836 test_loss:301.37835693359375\n",
      "676/3000 train_loss: 31.013708114624023 test_loss:299.53875732421875\n",
      "677/3000 train_loss: 31.408090591430664 test_loss:306.88507080078125\n",
      "678/3000 train_loss: 31.838851928710938 test_loss:302.0103759765625\n",
      "679/3000 train_loss: 30.54859733581543 test_loss:309.3532409667969\n",
      "680/3000 train_loss: 32.15973663330078 test_loss:309.257568359375\n",
      "681/3000 train_loss: 32.6795654296875 test_loss:295.0439453125\n",
      "682/3000 train_loss: 34.187286376953125 test_loss:296.5997009277344\n",
      "683/3000 train_loss: 33.732269287109375 test_loss:317.6959228515625\n",
      "684/3000 train_loss: 31.100648880004883 test_loss:329.82830810546875\n",
      "685/3000 train_loss: 33.29594802856445 test_loss:314.3665771484375\n",
      "686/3000 train_loss: 33.754112243652344 test_loss:297.616455078125\n",
      "687/3000 train_loss: 31.36043357849121 test_loss:310.71533203125\n",
      "688/3000 train_loss: 29.04103660583496 test_loss:319.37835693359375\n",
      "689/3000 train_loss: 28.681310653686523 test_loss:314.3845520019531\n",
      "690/3000 train_loss: 28.52836036682129 test_loss:323.2510681152344\n",
      "691/3000 train_loss: 31.05211639404297 test_loss:328.176025390625\n",
      "692/3000 train_loss: 29.85991096496582 test_loss:317.158203125\n",
      "693/3000 train_loss: 29.17679786682129 test_loss:321.10986328125\n",
      "694/3000 train_loss: 28.943490982055664 test_loss:339.4760437011719\n",
      "695/3000 train_loss: 29.513174057006836 test_loss:348.42315673828125\n",
      "696/3000 train_loss: 31.936080932617188 test_loss:333.11181640625\n",
      "697/3000 train_loss: 30.88239288330078 test_loss:356.63043212890625\n",
      "698/3000 train_loss: 30.90695571899414 test_loss:346.8989562988281\n",
      "699/3000 train_loss: 31.65953254699707 test_loss:336.5725402832031\n",
      "700/3000 train_loss: 31.37024688720703 test_loss:349.8015441894531\n",
      "701/3000 train_loss: 28.275583267211914 test_loss:367.3741455078125\n",
      "702/3000 train_loss: 29.628755569458008 test_loss:355.3625793457031\n",
      "703/3000 train_loss: 29.21403694152832 test_loss:351.15570068359375\n",
      "704/3000 train_loss: 28.162172317504883 test_loss:340.427734375\n",
      "705/3000 train_loss: 26.821195602416992 test_loss:339.552978515625\n",
      "706/3000 train_loss: 26.6450252532959 test_loss:358.791748046875\n",
      "707/3000 train_loss: 26.988401412963867 test_loss:377.20098876953125\n",
      "708/3000 train_loss: 27.554889678955078 test_loss:361.3962097167969\n",
      "709/3000 train_loss: 29.113296508789062 test_loss:351.613037109375\n",
      "710/3000 train_loss: 29.51940155029297 test_loss:356.8507080078125\n",
      "711/3000 train_loss: 30.97063446044922 test_loss:341.4381103515625\n",
      "712/3000 train_loss: 31.409772872924805 test_loss:314.20294189453125\n",
      "713/3000 train_loss: 31.309011459350586 test_loss:347.18035888671875\n",
      "714/3000 train_loss: 26.513668060302734 test_loss:363.763916015625\n",
      "715/3000 train_loss: 24.28008270263672 test_loss:365.94891357421875\n",
      "716/3000 train_loss: 24.56966781616211 test_loss:349.02301025390625\n",
      "717/3000 train_loss: 24.549179077148438 test_loss:326.62347412109375\n",
      "718/3000 train_loss: 24.77994155883789 test_loss:338.159423828125\n",
      "719/3000 train_loss: 24.86643409729004 test_loss:369.9882507324219\n",
      "720/3000 train_loss: 24.516311645507812 test_loss:374.20379638671875\n",
      "721/3000 train_loss: 24.59672737121582 test_loss:372.00457763671875\n",
      "722/3000 train_loss: 25.120332717895508 test_loss:350.22552490234375\n",
      "723/3000 train_loss: 25.870704650878906 test_loss:345.6589660644531\n",
      "724/3000 train_loss: 27.058128356933594 test_loss:345.1882019042969\n",
      "725/3000 train_loss: 28.84049415588379 test_loss:365.044921875\n",
      "726/3000 train_loss: 27.346973419189453 test_loss:430.62103271484375\n",
      "727/3000 train_loss: 28.414342880249023 test_loss:383.6826171875\n",
      "728/3000 train_loss: 27.60882568359375 test_loss:363.2203063964844\n",
      "729/3000 train_loss: 27.772428512573242 test_loss:340.5442199707031\n",
      "730/3000 train_loss: 27.393918991088867 test_loss:368.788330078125\n",
      "731/3000 train_loss: 26.09768295288086 test_loss:429.1828308105469\n",
      "732/3000 train_loss: 28.51945686340332 test_loss:394.9776306152344\n",
      "733/3000 train_loss: 31.31586265563965 test_loss:354.2216796875\n",
      "734/3000 train_loss: 31.890836715698242 test_loss:369.0664367675781\n",
      "735/3000 train_loss: 28.332653045654297 test_loss:417.5419616699219\n",
      "736/3000 train_loss: 27.126724243164062 test_loss:409.9006652832031\n",
      "737/3000 train_loss: 26.786245346069336 test_loss:403.7914123535156\n",
      "738/3000 train_loss: 29.253173828125 test_loss:392.5782165527344\n",
      "739/3000 train_loss: 27.987640380859375 test_loss:387.78955078125\n",
      "740/3000 train_loss: 32.10032272338867 test_loss:347.3597717285156\n",
      "741/3000 train_loss: 31.160888671875 test_loss:390.5910339355469\n",
      "742/3000 train_loss: 31.95370864868164 test_loss:382.72491455078125\n",
      "743/3000 train_loss: 28.442907333374023 test_loss:404.9459533691406\n",
      "744/3000 train_loss: 26.443511962890625 test_loss:396.0516662597656\n",
      "745/3000 train_loss: 25.63005828857422 test_loss:420.6051025390625\n",
      "746/3000 train_loss: 32.03778076171875 test_loss:391.0198974609375\n",
      "747/3000 train_loss: 28.47049331665039 test_loss:383.2708435058594\n",
      "748/3000 train_loss: 28.850963592529297 test_loss:425.0894775390625\n",
      "749/3000 train_loss: 30.684789657592773 test_loss:466.0751953125\n",
      "750/3000 train_loss: 31.335201263427734 test_loss:407.4716796875\n",
      "751/3000 train_loss: 30.691755294799805 test_loss:375.811279296875\n",
      "752/3000 train_loss: 29.38981056213379 test_loss:401.7466125488281\n",
      "753/3000 train_loss: 30.111093521118164 test_loss:423.18695068359375\n",
      "754/3000 train_loss: 28.82717514038086 test_loss:437.32733154296875\n",
      "755/3000 train_loss: 28.25183868408203 test_loss:408.1411437988281\n",
      "756/3000 train_loss: 27.438650131225586 test_loss:405.8401794433594\n",
      "757/3000 train_loss: 26.998672485351562 test_loss:406.8377380371094\n",
      "758/3000 train_loss: 27.2382869720459 test_loss:399.291015625\n",
      "759/3000 train_loss: 27.5004940032959 test_loss:405.8687438964844\n",
      "760/3000 train_loss: 27.13719367980957 test_loss:392.8103942871094\n",
      "761/3000 train_loss: 29.180063247680664 test_loss:404.8988037109375\n",
      "762/3000 train_loss: 28.012718200683594 test_loss:351.3038635253906\n",
      "763/3000 train_loss: 28.6101016998291 test_loss:359.33203125\n",
      "764/3000 train_loss: 29.637929916381836 test_loss:375.80682373046875\n",
      "765/3000 train_loss: 31.305713653564453 test_loss:400.0957336425781\n",
      "766/3000 train_loss: 30.525976181030273 test_loss:391.485595703125\n",
      "767/3000 train_loss: 32.4211540222168 test_loss:377.9783935546875\n",
      "768/3000 train_loss: 30.112993240356445 test_loss:329.0588073730469\n",
      "769/3000 train_loss: 32.393253326416016 test_loss:360.5773620605469\n",
      "770/3000 train_loss: 28.747690200805664 test_loss:370.69512939453125\n",
      "771/3000 train_loss: 28.967819213867188 test_loss:400.628173828125\n",
      "772/3000 train_loss: 27.985689163208008 test_loss:367.2044982910156\n",
      "773/3000 train_loss: 27.504613876342773 test_loss:341.599853515625\n",
      "774/3000 train_loss: 30.08560562133789 test_loss:360.5155334472656\n",
      "775/3000 train_loss: 30.90165901184082 test_loss:344.031982421875\n",
      "776/3000 train_loss: 32.1484375 test_loss:359.7283630371094\n",
      "777/3000 train_loss: 28.714529037475586 test_loss:373.9554443359375\n",
      "778/3000 train_loss: 27.517620086669922 test_loss:342.4307556152344\n",
      "779/3000 train_loss: 27.943403244018555 test_loss:332.880615234375\n",
      "780/3000 train_loss: 30.57642364501953 test_loss:325.3721008300781\n",
      "781/3000 train_loss: 31.20606803894043 test_loss:320.00592041015625\n",
      "782/3000 train_loss: 37.556304931640625 test_loss:424.22222900390625\n",
      "783/3000 train_loss: 36.296485900878906 test_loss:313.9579772949219\n",
      "784/3000 train_loss: 36.51094055175781 test_loss:304.012939453125\n",
      "785/3000 train_loss: 33.33448791503906 test_loss:309.85308837890625\n",
      "786/3000 train_loss: 31.00992202758789 test_loss:325.66357421875\n",
      "787/3000 train_loss: 29.211158752441406 test_loss:303.0227355957031\n",
      "788/3000 train_loss: 30.009031295776367 test_loss:295.5614929199219\n",
      "789/3000 train_loss: 28.911476135253906 test_loss:284.6820068359375\n",
      "790/3000 train_loss: 31.95142936706543 test_loss:299.5222473144531\n",
      "791/3000 train_loss: 31.076751708984375 test_loss:287.2005310058594\n",
      "792/3000 train_loss: 31.09812355041504 test_loss:288.238037109375\n",
      "793/3000 train_loss: 34.63386917114258 test_loss:292.7536315917969\n",
      "794/3000 train_loss: 35.052310943603516 test_loss:287.82879638671875\n",
      "795/3000 train_loss: 34.75562286376953 test_loss:298.3623962402344\n",
      "796/3000 train_loss: 40.013736724853516 test_loss:315.4085693359375\n",
      "797/3000 train_loss: 37.41638946533203 test_loss:302.3983154296875\n",
      "798/3000 train_loss: 34.75022506713867 test_loss:292.49609375\n",
      "799/3000 train_loss: 31.794483184814453 test_loss:299.58880615234375\n",
      "800/3000 train_loss: 36.74937057495117 test_loss:319.6551513671875\n",
      "801/3000 train_loss: 35.69287109375 test_loss:290.38824462890625\n",
      "802/3000 train_loss: 32.992942810058594 test_loss:285.8055114746094\n",
      "803/3000 train_loss: 35.28043746948242 test_loss:281.6455993652344\n",
      "804/3000 train_loss: 34.49449157714844 test_loss:273.5575866699219\n",
      "805/3000 train_loss: 35.37929916381836 test_loss:272.95050048828125\n",
      "806/3000 train_loss: 36.9035530090332 test_loss:278.09576416015625\n",
      "807/3000 train_loss: 36.216064453125 test_loss:282.5205993652344\n",
      "808/3000 train_loss: 39.03548049926758 test_loss:290.5907897949219\n",
      "809/3000 train_loss: 42.51033401489258 test_loss:314.79925537109375\n",
      "810/3000 train_loss: 52.83793258666992 test_loss:322.9354248046875\n",
      "811/3000 train_loss: 46.24220275878906 test_loss:311.12664794921875\n",
      "812/3000 train_loss: 49.50868606567383 test_loss:317.73321533203125\n",
      "813/3000 train_loss: 49.4489631652832 test_loss:331.45654296875\n",
      "814/3000 train_loss: 41.78718566894531 test_loss:328.7217712402344\n",
      "815/3000 train_loss: 47.42584991455078 test_loss:330.96722412109375\n",
      "816/3000 train_loss: 49.72010803222656 test_loss:345.544921875\n",
      "817/3000 train_loss: 51.229949951171875 test_loss:314.6947937011719\n",
      "818/3000 train_loss: 48.721866607666016 test_loss:286.731201171875\n",
      "819/3000 train_loss: 48.33012390136719 test_loss:288.2925109863281\n",
      "820/3000 train_loss: 47.333518981933594 test_loss:338.3205261230469\n",
      "821/3000 train_loss: 45.895145416259766 test_loss:308.93658447265625\n",
      "822/3000 train_loss: 42.92920684814453 test_loss:337.28741455078125\n",
      "823/3000 train_loss: 39.86337661743164 test_loss:333.37225341796875\n",
      "824/3000 train_loss: 39.14711380004883 test_loss:334.5679016113281\n",
      "825/3000 train_loss: 38.34548568725586 test_loss:335.5786437988281\n",
      "826/3000 train_loss: 37.178184509277344 test_loss:306.3343811035156\n",
      "827/3000 train_loss: 35.57255554199219 test_loss:314.18896484375\n",
      "828/3000 train_loss: 35.94255828857422 test_loss:305.7623596191406\n",
      "829/3000 train_loss: 38.684776306152344 test_loss:297.39056396484375\n",
      "830/3000 train_loss: 34.70745849609375 test_loss:333.99053955078125\n",
      "831/3000 train_loss: 34.23473358154297 test_loss:330.18310546875\n",
      "832/3000 train_loss: 35.58074951171875 test_loss:337.0271301269531\n",
      "833/3000 train_loss: 37.32891082763672 test_loss:361.33819580078125\n",
      "834/3000 train_loss: 42.03111267089844 test_loss:358.9129943847656\n",
      "835/3000 train_loss: 40.53080749511719 test_loss:378.1385498046875\n",
      "836/3000 train_loss: 41.7093505859375 test_loss:394.45196533203125\n",
      "837/3000 train_loss: 41.47514343261719 test_loss:380.1802673339844\n",
      "838/3000 train_loss: 44.92228317260742 test_loss:378.4870910644531\n",
      "839/3000 train_loss: 46.1129035949707 test_loss:426.2525939941406\n",
      "840/3000 train_loss: 42.827632904052734 test_loss:457.1290283203125\n",
      "841/3000 train_loss: 45.820716857910156 test_loss:475.8402404785156\n",
      "842/3000 train_loss: 49.77400588989258 test_loss:459.3575744628906\n",
      "843/3000 train_loss: 46.58045959472656 test_loss:439.44244384765625\n",
      "844/3000 train_loss: 46.71677780151367 test_loss:456.40911865234375\n",
      "845/3000 train_loss: 46.502838134765625 test_loss:521.0701904296875\n",
      "846/3000 train_loss: 48.875396728515625 test_loss:532.2667846679688\n",
      "847/3000 train_loss: 48.585391998291016 test_loss:457.8497314453125\n",
      "848/3000 train_loss: 44.46976852416992 test_loss:465.5298156738281\n",
      "849/3000 train_loss: 45.571693420410156 test_loss:476.93768310546875\n",
      "850/3000 train_loss: 52.208614349365234 test_loss:457.0333251953125\n",
      "851/3000 train_loss: 45.5297966003418 test_loss:466.2578430175781\n",
      "852/3000 train_loss: 47.39632034301758 test_loss:418.17596435546875\n",
      "853/3000 train_loss: 45.60483169555664 test_loss:414.8830871582031\n",
      "854/3000 train_loss: 44.89022445678711 test_loss:443.094970703125\n",
      "855/3000 train_loss: 44.35845947265625 test_loss:393.4202575683594\n",
      "856/3000 train_loss: 46.506900787353516 test_loss:377.58245849609375\n",
      "857/3000 train_loss: 47.54331970214844 test_loss:341.620361328125\n",
      "858/3000 train_loss: 40.99700927734375 test_loss:328.98272705078125\n",
      "859/3000 train_loss: 42.1964225769043 test_loss:319.1774597167969\n",
      "860/3000 train_loss: 40.13800048828125 test_loss:327.2568359375\n",
      "861/3000 train_loss: 37.85822677612305 test_loss:308.6262512207031\n",
      "862/3000 train_loss: 37.81798553466797 test_loss:293.68121337890625\n",
      "863/3000 train_loss: 35.94388961791992 test_loss:274.1590576171875\n",
      "864/3000 train_loss: 36.09687423706055 test_loss:276.419189453125\n",
      "865/3000 train_loss: 36.328697204589844 test_loss:254.50526428222656\n",
      "866/3000 train_loss: 41.447509765625 test_loss:254.4884033203125\n",
      "867/3000 train_loss: 37.75892639160156 test_loss:260.5513916015625\n",
      "868/3000 train_loss: 41.65960693359375 test_loss:265.4745178222656\n",
      "869/3000 train_loss: 39.91859436035156 test_loss:275.64447021484375\n",
      "870/3000 train_loss: 42.09864807128906 test_loss:285.5461730957031\n",
      "871/3000 train_loss: 42.0998649597168 test_loss:318.5885925292969\n",
      "872/3000 train_loss: 38.138633728027344 test_loss:328.98516845703125\n",
      "873/3000 train_loss: 36.19720458984375 test_loss:358.9606628417969\n",
      "874/3000 train_loss: 35.588836669921875 test_loss:378.591552734375\n",
      "875/3000 train_loss: 33.20635223388672 test_loss:401.8599853515625\n",
      "876/3000 train_loss: 32.74012756347656 test_loss:408.8424987792969\n",
      "877/3000 train_loss: 30.257238388061523 test_loss:391.14373779296875\n",
      "878/3000 train_loss: 30.245800018310547 test_loss:360.70953369140625\n",
      "879/3000 train_loss: 32.40890884399414 test_loss:369.1182556152344\n",
      "880/3000 train_loss: 34.94548034667969 test_loss:367.52935791015625\n",
      "881/3000 train_loss: 35.258827209472656 test_loss:343.67132568359375\n",
      "882/3000 train_loss: 37.0287971496582 test_loss:346.0087890625\n",
      "883/3000 train_loss: 36.23272705078125 test_loss:313.76373291015625\n",
      "884/3000 train_loss: 37.019813537597656 test_loss:293.19476318359375\n",
      "885/3000 train_loss: 36.86701202392578 test_loss:300.62957763671875\n",
      "886/3000 train_loss: 44.146453857421875 test_loss:318.5586242675781\n",
      "887/3000 train_loss: 39.457881927490234 test_loss:294.5834045410156\n",
      "888/3000 train_loss: 35.85383605957031 test_loss:294.6969909667969\n",
      "889/3000 train_loss: 38.90437698364258 test_loss:289.53839111328125\n",
      "890/3000 train_loss: 37.20454788208008 test_loss:313.1258544921875\n",
      "891/3000 train_loss: 38.00200653076172 test_loss:318.8916931152344\n",
      "892/3000 train_loss: 35.77586364746094 test_loss:315.7962341308594\n",
      "893/3000 train_loss: 34.204647064208984 test_loss:326.9423522949219\n",
      "894/3000 train_loss: 38.29353332519531 test_loss:328.7400207519531\n",
      "895/3000 train_loss: 34.65110778808594 test_loss:331.5758056640625\n",
      "896/3000 train_loss: 36.7892951965332 test_loss:325.8236083984375\n",
      "897/3000 train_loss: 40.70655059814453 test_loss:340.2784729003906\n",
      "898/3000 train_loss: 41.2525634765625 test_loss:360.9899597167969\n",
      "899/3000 train_loss: 39.78468322753906 test_loss:390.35296630859375\n",
      "900/3000 train_loss: 47.52278518676758 test_loss:346.2249755859375\n",
      "901/3000 train_loss: 36.34706115722656 test_loss:427.2625427246094\n",
      "902/3000 train_loss: 36.94502639770508 test_loss:415.1987609863281\n",
      "903/3000 train_loss: 33.510372161865234 test_loss:427.5697937011719\n",
      "904/3000 train_loss: 34.09328079223633 test_loss:430.59222412109375\n",
      "905/3000 train_loss: 33.290313720703125 test_loss:379.2342224121094\n",
      "906/3000 train_loss: 33.02590560913086 test_loss:425.5283203125\n",
      "907/3000 train_loss: 32.07164001464844 test_loss:390.1847839355469\n",
      "908/3000 train_loss: 29.32150650024414 test_loss:400.1192321777344\n",
      "909/3000 train_loss: 31.327068328857422 test_loss:395.6767578125\n",
      "910/3000 train_loss: 32.35424041748047 test_loss:385.27996826171875\n",
      "911/3000 train_loss: 29.959205627441406 test_loss:399.41302490234375\n",
      "912/3000 train_loss: 28.410799026489258 test_loss:372.0177917480469\n",
      "913/3000 train_loss: 29.243505477905273 test_loss:364.04669189453125\n",
      "914/3000 train_loss: 31.239320755004883 test_loss:335.6329345703125\n",
      "915/3000 train_loss: 32.37400817871094 test_loss:368.5613708496094\n",
      "916/3000 train_loss: 31.8259220123291 test_loss:345.1001892089844\n",
      "917/3000 train_loss: 28.171958923339844 test_loss:336.221435546875\n",
      "918/3000 train_loss: 31.16145896911621 test_loss:337.2914123535156\n",
      "919/3000 train_loss: 30.19978141784668 test_loss:314.20306396484375\n",
      "920/3000 train_loss: 30.73680877685547 test_loss:306.61456298828125\n",
      "921/3000 train_loss: 30.726139068603516 test_loss:290.9571228027344\n",
      "922/3000 train_loss: 31.33599281311035 test_loss:274.995849609375\n",
      "923/3000 train_loss: 32.002647399902344 test_loss:276.1912536621094\n",
      "924/3000 train_loss: 30.52079200744629 test_loss:277.50164794921875\n",
      "925/3000 train_loss: 31.170042037963867 test_loss:271.1664733886719\n",
      "926/3000 train_loss: 30.435989379882812 test_loss:258.5124206542969\n",
      "927/3000 train_loss: 29.362550735473633 test_loss:264.0126953125\n",
      "928/3000 train_loss: 28.316246032714844 test_loss:266.1871337890625\n",
      "929/3000 train_loss: 27.791852951049805 test_loss:280.3871154785156\n",
      "930/3000 train_loss: 28.586225509643555 test_loss:305.4381103515625\n",
      "931/3000 train_loss: 29.14972686767578 test_loss:319.43487548828125\n",
      "932/3000 train_loss: 29.5651798248291 test_loss:342.341064453125\n",
      "933/3000 train_loss: 29.91360855102539 test_loss:367.8332824707031\n",
      "934/3000 train_loss: 28.6577091217041 test_loss:372.9912414550781\n",
      "935/3000 train_loss: 27.98752212524414 test_loss:392.1889343261719\n",
      "936/3000 train_loss: 26.80453872680664 test_loss:397.6876525878906\n",
      "937/3000 train_loss: 27.094148635864258 test_loss:397.2268981933594\n",
      "938/3000 train_loss: 28.568824768066406 test_loss:362.2440490722656\n",
      "939/3000 train_loss: 26.799198150634766 test_loss:380.1440734863281\n",
      "940/3000 train_loss: 26.63282585144043 test_loss:373.3075256347656\n",
      "941/3000 train_loss: 27.972915649414062 test_loss:354.68890380859375\n",
      "942/3000 train_loss: 27.56342887878418 test_loss:331.2262878417969\n",
      "943/3000 train_loss: 30.18938636779785 test_loss:307.24908447265625\n",
      "944/3000 train_loss: 27.88446617126465 test_loss:308.86151123046875\n",
      "945/3000 train_loss: 32.583091735839844 test_loss:282.76025390625\n",
      "946/3000 train_loss: 30.93999481201172 test_loss:341.3200988769531\n",
      "947/3000 train_loss: 27.64904022216797 test_loss:299.2168273925781\n",
      "948/3000 train_loss: 27.545751571655273 test_loss:296.72344970703125\n",
      "949/3000 train_loss: 29.67967414855957 test_loss:282.39990234375\n",
      "950/3000 train_loss: 29.11946678161621 test_loss:273.11712646484375\n",
      "951/3000 train_loss: 28.306781768798828 test_loss:255.72801208496094\n",
      "952/3000 train_loss: 28.472259521484375 test_loss:263.98077392578125\n",
      "953/3000 train_loss: 28.585657119750977 test_loss:256.0000915527344\n",
      "954/3000 train_loss: 28.683025360107422 test_loss:265.4592590332031\n",
      "955/3000 train_loss: 30.306318283081055 test_loss:287.94647216796875\n",
      "956/3000 train_loss: 31.141565322875977 test_loss:296.78802490234375\n",
      "957/3000 train_loss: 31.86455535888672 test_loss:300.0778503417969\n",
      "958/3000 train_loss: 34.87489700317383 test_loss:313.2260437011719\n",
      "959/3000 train_loss: 31.791593551635742 test_loss:324.75286865234375\n",
      "960/3000 train_loss: 31.482086181640625 test_loss:343.14556884765625\n",
      "961/3000 train_loss: 30.415388107299805 test_loss:367.8496398925781\n",
      "962/3000 train_loss: 34.25142288208008 test_loss:352.1348571777344\n",
      "963/3000 train_loss: 32.67043685913086 test_loss:417.61346435546875\n",
      "964/3000 train_loss: 37.16752243041992 test_loss:415.9737854003906\n",
      "965/3000 train_loss: 37.340641021728516 test_loss:438.2103576660156\n",
      "966/3000 train_loss: 35.970088958740234 test_loss:415.0814514160156\n",
      "967/3000 train_loss: 38.92375946044922 test_loss:405.85113525390625\n",
      "968/3000 train_loss: 33.988582611083984 test_loss:392.5030517578125\n",
      "969/3000 train_loss: 32.44385528564453 test_loss:414.0645446777344\n",
      "970/3000 train_loss: 37.38042449951172 test_loss:417.2435302734375\n",
      "971/3000 train_loss: 33.41547775268555 test_loss:401.9958801269531\n",
      "972/3000 train_loss: 30.659334182739258 test_loss:412.4929504394531\n",
      "973/3000 train_loss: 30.135046005249023 test_loss:350.80291748046875\n",
      "974/3000 train_loss: 31.966075897216797 test_loss:361.7922668457031\n",
      "975/3000 train_loss: 28.766691207885742 test_loss:323.0809020996094\n",
      "976/3000 train_loss: 32.5162467956543 test_loss:301.1217041015625\n",
      "977/3000 train_loss: 30.063013076782227 test_loss:318.44586181640625\n",
      "978/3000 train_loss: 30.691539764404297 test_loss:298.5494079589844\n",
      "979/3000 train_loss: 28.962751388549805 test_loss:289.3865966796875\n",
      "980/3000 train_loss: 31.025348663330078 test_loss:281.33978271484375\n",
      "981/3000 train_loss: 29.884675979614258 test_loss:267.8486633300781\n",
      "982/3000 train_loss: 30.910001754760742 test_loss:260.47064208984375\n",
      "983/3000 train_loss: 29.720962524414062 test_loss:262.0648498535156\n",
      "984/3000 train_loss: 32.231834411621094 test_loss:266.2381591796875\n",
      "985/3000 train_loss: 31.748952865600586 test_loss:289.5701904296875\n",
      "986/3000 train_loss: 34.59508514404297 test_loss:284.3309326171875\n",
      "987/3000 train_loss: 30.640108108520508 test_loss:293.19085693359375\n",
      "988/3000 train_loss: 32.150657653808594 test_loss:303.73175048828125\n",
      "989/3000 train_loss: 33.14542770385742 test_loss:322.3482971191406\n",
      "990/3000 train_loss: 32.536685943603516 test_loss:366.3752136230469\n",
      "991/3000 train_loss: 32.58687973022461 test_loss:379.3363952636719\n",
      "992/3000 train_loss: 32.371273040771484 test_loss:380.66796875\n",
      "993/3000 train_loss: 31.068653106689453 test_loss:411.7585754394531\n",
      "994/3000 train_loss: 31.530729293823242 test_loss:410.087646484375\n",
      "995/3000 train_loss: 33.57733917236328 test_loss:416.1675720214844\n",
      "996/3000 train_loss: 30.377843856811523 test_loss:401.6416015625\n",
      "997/3000 train_loss: 28.336952209472656 test_loss:320.3885803222656\n",
      "998/3000 train_loss: 25.759586334228516 test_loss:306.1697998046875\n",
      "999/3000 train_loss: 26.552337646484375 test_loss:280.4357604980469\n",
      "1000/3000 train_loss: 27.1308650970459 test_loss:274.6788330078125\n",
      "1001/3000 train_loss: 28.361671447753906 test_loss:278.56085205078125\n",
      "1002/3000 train_loss: 28.612442016601562 test_loss:284.8636779785156\n",
      "1003/3000 train_loss: 27.955293655395508 test_loss:273.98187255859375\n",
      "1004/3000 train_loss: 28.513938903808594 test_loss:272.27166748046875\n",
      "1005/3000 train_loss: 28.49536895751953 test_loss:273.8943176269531\n",
      "1006/3000 train_loss: 28.192564010620117 test_loss:298.5983581542969\n",
      "1007/3000 train_loss: 29.751239776611328 test_loss:310.4853210449219\n",
      "1008/3000 train_loss: 29.86775016784668 test_loss:337.9271240234375\n",
      "1009/3000 train_loss: 27.350923538208008 test_loss:357.8926086425781\n",
      "1010/3000 train_loss: 28.64608383178711 test_loss:375.7562561035156\n",
      "1011/3000 train_loss: 27.374170303344727 test_loss:408.1448669433594\n",
      "1012/3000 train_loss: 28.906967163085938 test_loss:398.2679138183594\n",
      "1013/3000 train_loss: 27.97998809814453 test_loss:417.9090881347656\n",
      "1014/3000 train_loss: 31.137569427490234 test_loss:388.94317626953125\n",
      "1015/3000 train_loss: 29.442588806152344 test_loss:366.6185607910156\n",
      "1016/3000 train_loss: 25.389575958251953 test_loss:310.4147033691406\n",
      "1017/3000 train_loss: 27.5515193939209 test_loss:297.4205017089844\n",
      "1018/3000 train_loss: 28.217693328857422 test_loss:282.89813232421875\n",
      "1019/3000 train_loss: 28.180578231811523 test_loss:274.5126953125\n",
      "1020/3000 train_loss: 29.699867248535156 test_loss:277.5712585449219\n",
      "1021/3000 train_loss: 30.022130966186523 test_loss:279.45159912109375\n",
      "1022/3000 train_loss: 28.8232479095459 test_loss:273.5450744628906\n",
      "1023/3000 train_loss: 29.129661560058594 test_loss:265.942626953125\n",
      "1024/3000 train_loss: 31.292739868164062 test_loss:289.4959716796875\n",
      "1025/3000 train_loss: 32.37190628051758 test_loss:298.3133544921875\n",
      "1026/3000 train_loss: 30.298240661621094 test_loss:328.5519104003906\n",
      "1027/3000 train_loss: 28.420612335205078 test_loss:321.6794128417969\n",
      "1028/3000 train_loss: 27.835012435913086 test_loss:344.138427734375\n",
      "1029/3000 train_loss: 29.452878952026367 test_loss:352.3367614746094\n",
      "1030/3000 train_loss: 31.066307067871094 test_loss:366.26507568359375\n",
      "1031/3000 train_loss: 30.73575210571289 test_loss:401.59674072265625\n",
      "1032/3000 train_loss: 28.09463119506836 test_loss:435.1344909667969\n",
      "1033/3000 train_loss: 27.955928802490234 test_loss:441.0658264160156\n",
      "1034/3000 train_loss: 37.90444564819336 test_loss:340.0840148925781\n",
      "1035/3000 train_loss: 31.326929092407227 test_loss:424.6676330566406\n",
      "1036/3000 train_loss: 35.5205192565918 test_loss:421.6960754394531\n",
      "1037/3000 train_loss: 29.048858642578125 test_loss:362.4399719238281\n",
      "1038/3000 train_loss: 28.360443115234375 test_loss:316.0417785644531\n",
      "1039/3000 train_loss: 29.25647735595703 test_loss:285.876708984375\n",
      "1040/3000 train_loss: 30.288835525512695 test_loss:279.3168640136719\n",
      "1041/3000 train_loss: 28.171001434326172 test_loss:264.9170837402344\n",
      "1042/3000 train_loss: 27.33753776550293 test_loss:251.75833129882812\n",
      "1043/3000 train_loss: 26.100828170776367 test_loss:261.39227294921875\n",
      "1044/3000 train_loss: 29.03940773010254 test_loss:267.4023132324219\n",
      "1045/3000 train_loss: 28.856651306152344 test_loss:260.560791015625\n",
      "1046/3000 train_loss: 32.08721160888672 test_loss:257.5217590332031\n",
      "1047/3000 train_loss: 29.417387008666992 test_loss:271.9530334472656\n",
      "1048/3000 train_loss: 29.875408172607422 test_loss:297.0846862792969\n",
      "1049/3000 train_loss: 29.073562622070312 test_loss:307.1439514160156\n",
      "1050/3000 train_loss: 28.735612869262695 test_loss:322.2893981933594\n",
      "1051/3000 train_loss: 27.823680877685547 test_loss:375.870361328125\n",
      "1052/3000 train_loss: 29.802919387817383 test_loss:367.829833984375\n",
      "1053/3000 train_loss: 29.041757583618164 test_loss:456.87384033203125\n",
      "1054/3000 train_loss: 32.877479553222656 test_loss:431.207275390625\n",
      "1055/3000 train_loss: 30.224349975585938 test_loss:405.35498046875\n",
      "1056/3000 train_loss: 33.19953918457031 test_loss:370.6116027832031\n",
      "1057/3000 train_loss: 30.229190826416016 test_loss:409.7501220703125\n",
      "1058/3000 train_loss: 35.97306442260742 test_loss:354.6479187011719\n",
      "1059/3000 train_loss: 36.060176849365234 test_loss:394.77630615234375\n",
      "1060/3000 train_loss: 30.365327835083008 test_loss:350.2279357910156\n",
      "1061/3000 train_loss: 27.627016067504883 test_loss:325.0339660644531\n",
      "1062/3000 train_loss: 28.571590423583984 test_loss:300.5041198730469\n",
      "1063/3000 train_loss: 31.418447494506836 test_loss:298.0338439941406\n",
      "1064/3000 train_loss: 31.250579833984375 test_loss:286.3994445800781\n",
      "1065/3000 train_loss: 29.08225440979004 test_loss:286.9149169921875\n",
      "1066/3000 train_loss: 30.632734298706055 test_loss:281.85028076171875\n",
      "1067/3000 train_loss: 27.099132537841797 test_loss:274.0970764160156\n",
      "1068/3000 train_loss: 29.629541397094727 test_loss:271.0120849609375\n",
      "1069/3000 train_loss: 30.43694305419922 test_loss:264.1687927246094\n",
      "1070/3000 train_loss: 30.805692672729492 test_loss:289.426025390625\n",
      "1071/3000 train_loss: 31.170795440673828 test_loss:304.229248046875\n",
      "1072/3000 train_loss: 32.248435974121094 test_loss:323.76336669921875\n",
      "1073/3000 train_loss: 29.876861572265625 test_loss:338.7447509765625\n",
      "1074/3000 train_loss: 31.0820255279541 test_loss:367.5946350097656\n",
      "1075/3000 train_loss: 32.91647720336914 test_loss:392.0472717285156\n",
      "1076/3000 train_loss: 32.898983001708984 test_loss:425.1499328613281\n",
      "1077/3000 train_loss: 33.310455322265625 test_loss:425.3316650390625\n",
      "1078/3000 train_loss: 34.49318313598633 test_loss:462.7255554199219\n",
      "1079/3000 train_loss: 33.4308967590332 test_loss:404.83099365234375\n",
      "1080/3000 train_loss: 30.265748977661133 test_loss:388.5760498046875\n",
      "1081/3000 train_loss: 28.555727005004883 test_loss:353.8533630371094\n",
      "1082/3000 train_loss: 27.327133178710938 test_loss:292.319091796875\n",
      "1083/3000 train_loss: 27.92323112487793 test_loss:267.76953125\n",
      "1084/3000 train_loss: 33.01668167114258 test_loss:252.57301330566406\n",
      "1085/3000 train_loss: 28.926836013793945 test_loss:257.0453796386719\n",
      "1086/3000 train_loss: 29.102893829345703 test_loss:265.05157470703125\n",
      "1087/3000 train_loss: 30.852684020996094 test_loss:260.3088073730469\n",
      "1088/3000 train_loss: 29.10032081604004 test_loss:271.2257995605469\n",
      "1089/3000 train_loss: 32.87223815917969 test_loss:273.3002014160156\n",
      "1090/3000 train_loss: 31.7012996673584 test_loss:270.1846618652344\n",
      "1091/3000 train_loss: 31.755107879638672 test_loss:289.1216125488281\n",
      "1092/3000 train_loss: 28.507835388183594 test_loss:284.89990234375\n",
      "1093/3000 train_loss: 29.44135093688965 test_loss:310.31243896484375\n",
      "1094/3000 train_loss: 30.87107276916504 test_loss:343.3960876464844\n",
      "1095/3000 train_loss: 31.209379196166992 test_loss:361.3830261230469\n",
      "1096/3000 train_loss: 30.732101440429688 test_loss:396.933837890625\n",
      "1097/3000 train_loss: 32.321266174316406 test_loss:432.340576171875\n",
      "1098/3000 train_loss: 29.650943756103516 test_loss:436.4619140625\n",
      "1099/3000 train_loss: 32.52192687988281 test_loss:400.6878662109375\n",
      "1100/3000 train_loss: 32.50479507446289 test_loss:366.04364013671875\n",
      "1101/3000 train_loss: 34.2703971862793 test_loss:389.8431396484375\n",
      "1102/3000 train_loss: 31.289220809936523 test_loss:315.9648742675781\n",
      "1103/3000 train_loss: 30.289411544799805 test_loss:270.1738586425781\n",
      "1104/3000 train_loss: 26.93857765197754 test_loss:266.7909851074219\n",
      "1105/3000 train_loss: 24.774364471435547 test_loss:266.6318054199219\n",
      "1106/3000 train_loss: 29.37021827697754 test_loss:264.95672607421875\n",
      "1107/3000 train_loss: 29.525911331176758 test_loss:267.9311218261719\n",
      "1108/3000 train_loss: 29.289134979248047 test_loss:294.03704833984375\n",
      "1109/3000 train_loss: 26.47645378112793 test_loss:320.3074035644531\n",
      "1110/3000 train_loss: 25.60491180419922 test_loss:354.6615295410156\n",
      "1111/3000 train_loss: 24.810245513916016 test_loss:382.74395751953125\n",
      "1112/3000 train_loss: 25.497217178344727 test_loss:369.8453369140625\n",
      "1113/3000 train_loss: 23.874197006225586 test_loss:368.47601318359375\n",
      "1114/3000 train_loss: 24.4150447845459 test_loss:345.15191650390625\n",
      "1115/3000 train_loss: 25.97504997253418 test_loss:309.38897705078125\n",
      "1116/3000 train_loss: 23.36870002746582 test_loss:261.910400390625\n",
      "1117/3000 train_loss: 25.126157760620117 test_loss:248.64012145996094\n",
      "1118/3000 train_loss: 27.664024353027344 test_loss:250.25015258789062\n",
      "1119/3000 train_loss: 26.657846450805664 test_loss:286.6710510253906\n",
      "1120/3000 train_loss: 26.64320945739746 test_loss:264.54486083984375\n",
      "1121/3000 train_loss: 24.136751174926758 test_loss:270.0029296875\n",
      "1122/3000 train_loss: 24.933183670043945 test_loss:310.6542663574219\n",
      "1123/3000 train_loss: 22.523218154907227 test_loss:334.72332763671875\n",
      "1124/3000 train_loss: 24.80841636657715 test_loss:360.7153625488281\n",
      "1125/3000 train_loss: 27.00440216064453 test_loss:391.8618469238281\n",
      "1126/3000 train_loss: 23.281959533691406 test_loss:383.5782470703125\n",
      "1127/3000 train_loss: 27.77047348022461 test_loss:360.6580810546875\n",
      "1128/3000 train_loss: 26.713869094848633 test_loss:298.4710998535156\n",
      "1129/3000 train_loss: 25.020633697509766 test_loss:270.6656494140625\n",
      "1130/3000 train_loss: 24.416913986206055 test_loss:254.9131622314453\n",
      "1131/3000 train_loss: 27.999515533447266 test_loss:261.68243408203125\n",
      "1132/3000 train_loss: 30.549680709838867 test_loss:262.2225646972656\n",
      "1133/3000 train_loss: 27.644744873046875 test_loss:275.58892822265625\n",
      "1134/3000 train_loss: 27.43842124938965 test_loss:273.017333984375\n",
      "1135/3000 train_loss: 27.338184356689453 test_loss:255.00584411621094\n",
      "1136/3000 train_loss: 26.2197322845459 test_loss:270.14410400390625\n",
      "1137/3000 train_loss: 27.005687713623047 test_loss:306.3804016113281\n",
      "1138/3000 train_loss: 25.597476959228516 test_loss:339.46282958984375\n",
      "1139/3000 train_loss: 25.462434768676758 test_loss:391.7478332519531\n",
      "1140/3000 train_loss: 27.553159713745117 test_loss:424.46514892578125\n",
      "1141/3000 train_loss: 26.029685974121094 test_loss:485.8638916015625\n",
      "1142/3000 train_loss: 27.09514617919922 test_loss:398.8140563964844\n",
      "1143/3000 train_loss: 29.772993087768555 test_loss:369.67022705078125\n",
      "1144/3000 train_loss: 24.579010009765625 test_loss:335.6645202636719\n",
      "1145/3000 train_loss: 24.291637420654297 test_loss:278.8401184082031\n",
      "1146/3000 train_loss: 23.416296005249023 test_loss:254.11117553710938\n",
      "1147/3000 train_loss: 23.493154525756836 test_loss:255.94564819335938\n",
      "1148/3000 train_loss: 23.797229766845703 test_loss:261.93377685546875\n",
      "1149/3000 train_loss: 22.918968200683594 test_loss:270.33013916015625\n",
      "1150/3000 train_loss: 28.522262573242188 test_loss:264.5078125\n",
      "1151/3000 train_loss: 26.628637313842773 test_loss:275.9898376464844\n",
      "1152/3000 train_loss: 25.48271942138672 test_loss:298.9352722167969\n",
      "1153/3000 train_loss: 23.268033981323242 test_loss:355.5867004394531\n",
      "1154/3000 train_loss: 27.357471466064453 test_loss:357.74737548828125\n",
      "1155/3000 train_loss: 25.82120704650879 test_loss:406.8158874511719\n",
      "1156/3000 train_loss: 26.502683639526367 test_loss:388.8421630859375\n",
      "1157/3000 train_loss: 25.627952575683594 test_loss:391.7170715332031\n",
      "1158/3000 train_loss: 24.53651237487793 test_loss:363.64801025390625\n",
      "1159/3000 train_loss: 24.830482482910156 test_loss:297.6269836425781\n",
      "1160/3000 train_loss: 24.17664337158203 test_loss:260.2938232421875\n",
      "1161/3000 train_loss: 27.14562225341797 test_loss:261.6394958496094\n",
      "1162/3000 train_loss: 22.359609603881836 test_loss:266.080810546875\n",
      "1163/3000 train_loss: 23.5980167388916 test_loss:258.4537353515625\n",
      "1164/3000 train_loss: 23.214567184448242 test_loss:271.7360534667969\n",
      "1165/3000 train_loss: 24.915889739990234 test_loss:282.125732421875\n",
      "1166/3000 train_loss: 26.09617805480957 test_loss:295.44677734375\n",
      "1167/3000 train_loss: 23.320526123046875 test_loss:351.4419250488281\n",
      "1168/3000 train_loss: 22.75908088684082 test_loss:394.7070617675781\n",
      "1169/3000 train_loss: 23.53501319885254 test_loss:434.77813720703125\n",
      "1170/3000 train_loss: 24.065366744995117 test_loss:423.1985778808594\n",
      "1171/3000 train_loss: 26.503509521484375 test_loss:412.1532897949219\n",
      "1172/3000 train_loss: 25.58797264099121 test_loss:374.05059814453125\n",
      "1173/3000 train_loss: 26.245315551757812 test_loss:353.8407287597656\n",
      "1174/3000 train_loss: 27.85440444946289 test_loss:355.7027282714844\n",
      "1175/3000 train_loss: 28.745893478393555 test_loss:334.6807861328125\n",
      "1176/3000 train_loss: 25.51306915283203 test_loss:329.552490234375\n",
      "1177/3000 train_loss: 27.725780487060547 test_loss:306.3877258300781\n",
      "1178/3000 train_loss: 27.5083065032959 test_loss:313.5911560058594\n",
      "1179/3000 train_loss: 24.95101547241211 test_loss:277.1290588378906\n",
      "1180/3000 train_loss: 29.807594299316406 test_loss:267.8186950683594\n",
      "1181/3000 train_loss: 31.67073631286621 test_loss:265.5186462402344\n",
      "1182/3000 train_loss: 29.01443862915039 test_loss:281.669921875\n",
      "1183/3000 train_loss: 29.446598052978516 test_loss:281.38360595703125\n",
      "1184/3000 train_loss: 25.996410369873047 test_loss:293.9246826171875\n",
      "1185/3000 train_loss: 28.683170318603516 test_loss:270.6480712890625\n",
      "1186/3000 train_loss: 28.519887924194336 test_loss:272.0362548828125\n",
      "1187/3000 train_loss: 26.899551391601562 test_loss:292.0832824707031\n",
      "1188/3000 train_loss: 29.604469299316406 test_loss:272.4058837890625\n",
      "1189/3000 train_loss: 28.13811492919922 test_loss:297.7052917480469\n",
      "1190/3000 train_loss: 28.129127502441406 test_loss:304.3671569824219\n",
      "1191/3000 train_loss: 24.767534255981445 test_loss:306.5510559082031\n",
      "1192/3000 train_loss: 23.685436248779297 test_loss:335.026611328125\n",
      "1193/3000 train_loss: 25.218412399291992 test_loss:343.3571472167969\n",
      "1194/3000 train_loss: 24.17287254333496 test_loss:370.7482604980469\n",
      "1195/3000 train_loss: 24.666440963745117 test_loss:396.656494140625\n",
      "1196/3000 train_loss: 25.401660919189453 test_loss:419.450439453125\n",
      "1197/3000 train_loss: 24.446659088134766 test_loss:417.84588623046875\n",
      "1198/3000 train_loss: 26.689664840698242 test_loss:446.5245361328125\n",
      "1199/3000 train_loss: 26.81608772277832 test_loss:459.7148132324219\n",
      "1200/3000 train_loss: 28.05545425415039 test_loss:462.3730163574219\n",
      "1201/3000 train_loss: 29.23614501953125 test_loss:447.60089111328125\n",
      "1202/3000 train_loss: 28.945493698120117 test_loss:436.4875183105469\n",
      "1203/3000 train_loss: 27.495065689086914 test_loss:416.806884765625\n",
      "1204/3000 train_loss: 27.392696380615234 test_loss:357.9030456542969\n",
      "1205/3000 train_loss: 27.604936599731445 test_loss:318.2617492675781\n",
      "1206/3000 train_loss: 26.410968780517578 test_loss:287.2651672363281\n",
      "1207/3000 train_loss: 27.766620635986328 test_loss:268.59423828125\n",
      "1208/3000 train_loss: 26.577625274658203 test_loss:278.86712646484375\n",
      "1209/3000 train_loss: 29.631484985351562 test_loss:275.56939697265625\n",
      "1210/3000 train_loss: 26.750812530517578 test_loss:288.1535339355469\n",
      "1211/3000 train_loss: 29.1988525390625 test_loss:298.595703125\n",
      "1212/3000 train_loss: 28.411813735961914 test_loss:278.84844970703125\n",
      "1213/3000 train_loss: 30.02393341064453 test_loss:281.11944580078125\n",
      "1214/3000 train_loss: 31.472822189331055 test_loss:290.9725341796875\n",
      "1215/3000 train_loss: 28.597064971923828 test_loss:291.5892639160156\n",
      "1216/3000 train_loss: 26.20486068725586 test_loss:309.2889709472656\n",
      "1217/3000 train_loss: 28.067352294921875 test_loss:306.6879577636719\n",
      "1218/3000 train_loss: 25.618623733520508 test_loss:387.846435546875\n",
      "1219/3000 train_loss: 29.154268264770508 test_loss:405.2282409667969\n",
      "1220/3000 train_loss: 28.809911727905273 test_loss:425.1175537109375\n",
      "1221/3000 train_loss: 27.337642669677734 test_loss:482.8995056152344\n",
      "1222/3000 train_loss: 30.698062896728516 test_loss:473.00982666015625\n",
      "1223/3000 train_loss: 31.942424774169922 test_loss:491.9520568847656\n",
      "1224/3000 train_loss: 29.366790771484375 test_loss:486.5962829589844\n",
      "1225/3000 train_loss: 31.328271865844727 test_loss:483.12451171875\n",
      "1226/3000 train_loss: 32.25938415527344 test_loss:466.13458251953125\n",
      "1227/3000 train_loss: 30.322355270385742 test_loss:410.6304931640625\n",
      "1228/3000 train_loss: 28.614904403686523 test_loss:400.95623779296875\n",
      "1229/3000 train_loss: 31.28680992126465 test_loss:384.90716552734375\n",
      "1230/3000 train_loss: 32.65482711791992 test_loss:322.47088623046875\n",
      "1231/3000 train_loss: 26.613569259643555 test_loss:285.66070556640625\n",
      "1232/3000 train_loss: 30.2207088470459 test_loss:287.24957275390625\n",
      "1233/3000 train_loss: 32.10238265991211 test_loss:280.4502258300781\n",
      "1234/3000 train_loss: 29.263656616210938 test_loss:276.58599853515625\n",
      "1235/3000 train_loss: 28.61594009399414 test_loss:280.4435729980469\n",
      "1236/3000 train_loss: 32.993507385253906 test_loss:294.4908752441406\n",
      "1237/3000 train_loss: 29.496313095092773 test_loss:283.2090148925781\n",
      "1238/3000 train_loss: 30.21222686767578 test_loss:272.024169921875\n",
      "1239/3000 train_loss: 30.206214904785156 test_loss:272.8639221191406\n",
      "1240/3000 train_loss: 32.0596809387207 test_loss:290.2056579589844\n",
      "1241/3000 train_loss: 31.93271827697754 test_loss:290.6634826660156\n",
      "1242/3000 train_loss: 33.6636962890625 test_loss:314.05987548828125\n",
      "1243/3000 train_loss: 34.89388656616211 test_loss:339.86773681640625\n",
      "1244/3000 train_loss: 29.173906326293945 test_loss:393.1280822753906\n",
      "1245/3000 train_loss: 27.940494537353516 test_loss:461.6509704589844\n",
      "1246/3000 train_loss: 29.564414978027344 test_loss:443.8263854980469\n",
      "1247/3000 train_loss: 29.003293991088867 test_loss:465.13250732421875\n",
      "1248/3000 train_loss: 28.14920997619629 test_loss:452.92877197265625\n",
      "1249/3000 train_loss: 28.385173797607422 test_loss:411.2815246582031\n",
      "1250/3000 train_loss: 30.108543395996094 test_loss:381.07916259765625\n",
      "1251/3000 train_loss: 28.280122756958008 test_loss:335.36590576171875\n",
      "1252/3000 train_loss: 28.123313903808594 test_loss:321.6969909667969\n",
      "1253/3000 train_loss: 29.196001052856445 test_loss:299.8187561035156\n",
      "1254/3000 train_loss: 30.667177200317383 test_loss:299.7928466796875\n",
      "1255/3000 train_loss: 32.047176361083984 test_loss:285.02911376953125\n",
      "1256/3000 train_loss: 29.139543533325195 test_loss:281.7698974609375\n",
      "1257/3000 train_loss: 30.176090240478516 test_loss:282.2100524902344\n",
      "1258/3000 train_loss: 31.273012161254883 test_loss:283.6914978027344\n",
      "1259/3000 train_loss: 31.85376739501953 test_loss:275.67425537109375\n",
      "1260/3000 train_loss: 30.923616409301758 test_loss:252.27337646484375\n",
      "1261/3000 train_loss: 29.24816131591797 test_loss:244.8283233642578\n",
      "1262/3000 train_loss: 25.981239318847656 test_loss:249.844970703125\n",
      "1263/3000 train_loss: 26.649911880493164 test_loss:249.62353515625\n",
      "1264/3000 train_loss: 25.365453720092773 test_loss:254.58111572265625\n",
      "1265/3000 train_loss: 27.043134689331055 test_loss:267.27996826171875\n",
      "1266/3000 train_loss: 30.428844451904297 test_loss:279.11346435546875\n",
      "1267/3000 train_loss: 31.236513137817383 test_loss:313.96697998046875\n",
      "1268/3000 train_loss: 28.691322326660156 test_loss:337.0442199707031\n",
      "1269/3000 train_loss: 33.16012191772461 test_loss:356.80682373046875\n",
      "1270/3000 train_loss: 31.02764892578125 test_loss:404.5606689453125\n",
      "1271/3000 train_loss: 34.799407958984375 test_loss:393.0912170410156\n",
      "1272/3000 train_loss: 31.749588012695312 test_loss:410.54522705078125\n",
      "1273/3000 train_loss: 35.802101135253906 test_loss:413.6146545410156\n",
      "1274/3000 train_loss: 32.27584457397461 test_loss:400.67742919921875\n",
      "1275/3000 train_loss: 29.243812561035156 test_loss:365.0181884765625\n",
      "1276/3000 train_loss: 29.573347091674805 test_loss:372.21630859375\n",
      "1277/3000 train_loss: 30.409420013427734 test_loss:352.0671691894531\n",
      "1278/3000 train_loss: 30.786521911621094 test_loss:326.7469482421875\n",
      "1279/3000 train_loss: 31.241043090820312 test_loss:296.139892578125\n",
      "1280/3000 train_loss: 28.71295928955078 test_loss:276.8771667480469\n",
      "1281/3000 train_loss: 33.32483673095703 test_loss:268.646484375\n",
      "1282/3000 train_loss: 28.508872985839844 test_loss:271.8189392089844\n",
      "1283/3000 train_loss: 37.25096893310547 test_loss:272.58978271484375\n",
      "1284/3000 train_loss: 29.580854415893555 test_loss:253.107666015625\n",
      "1285/3000 train_loss: 33.33106231689453 test_loss:238.7821502685547\n",
      "1286/3000 train_loss: 31.324609756469727 test_loss:234.76597595214844\n",
      "1287/3000 train_loss: 31.1820011138916 test_loss:229.69674682617188\n",
      "1288/3000 train_loss: 33.05984115600586 test_loss:238.8214111328125\n",
      "1289/3000 train_loss: 33.270870208740234 test_loss:258.7326965332031\n",
      "1290/3000 train_loss: 35.32707214355469 test_loss:272.30108642578125\n",
      "1291/3000 train_loss: 32.030338287353516 test_loss:313.59295654296875\n",
      "1292/3000 train_loss: 32.85228729248047 test_loss:335.523681640625\n",
      "1293/3000 train_loss: 31.513103485107422 test_loss:365.86981201171875\n",
      "1294/3000 train_loss: 31.103391647338867 test_loss:389.4873352050781\n",
      "1295/3000 train_loss: 29.420963287353516 test_loss:395.54888916015625\n",
      "1296/3000 train_loss: 29.828886032104492 test_loss:436.7373046875\n",
      "1297/3000 train_loss: 32.96559524536133 test_loss:402.7299499511719\n",
      "1298/3000 train_loss: 27.07330322265625 test_loss:420.0343322753906\n",
      "1299/3000 train_loss: 28.363636016845703 test_loss:384.197509765625\n",
      "1300/3000 train_loss: 29.925996780395508 test_loss:359.5119934082031\n",
      "1301/3000 train_loss: 31.182373046875 test_loss:341.72918701171875\n",
      "1302/3000 train_loss: 27.521953582763672 test_loss:305.8088073730469\n",
      "1303/3000 train_loss: 30.887670516967773 test_loss:313.3988342285156\n",
      "1304/3000 train_loss: 29.03553581237793 test_loss:306.7990417480469\n",
      "1305/3000 train_loss: 27.93784523010254 test_loss:301.5217590332031\n",
      "1306/3000 train_loss: 30.459449768066406 test_loss:281.6582336425781\n",
      "1307/3000 train_loss: 28.88410186767578 test_loss:274.9216003417969\n",
      "1308/3000 train_loss: 27.31067657470703 test_loss:273.20745849609375\n",
      "1309/3000 train_loss: 28.871030807495117 test_loss:259.63238525390625\n",
      "1310/3000 train_loss: 28.63872718811035 test_loss:286.9924011230469\n",
      "1311/3000 train_loss: 26.409231185913086 test_loss:311.1925354003906\n",
      "1312/3000 train_loss: 25.57411003112793 test_loss:324.4079284667969\n",
      "1313/3000 train_loss: 26.238847732543945 test_loss:348.5112609863281\n",
      "1314/3000 train_loss: 28.366056442260742 test_loss:347.24737548828125\n",
      "1315/3000 train_loss: 27.31912612915039 test_loss:327.8663635253906\n",
      "1316/3000 train_loss: 30.6208553314209 test_loss:307.309814453125\n",
      "1317/3000 train_loss: 30.432151794433594 test_loss:324.061767578125\n",
      "1318/3000 train_loss: 31.047155380249023 test_loss:323.2203369140625\n",
      "1319/3000 train_loss: 30.33585548400879 test_loss:344.00311279296875\n",
      "1320/3000 train_loss: 27.56478500366211 test_loss:338.47515869140625\n",
      "1321/3000 train_loss: 26.697107315063477 test_loss:333.654541015625\n",
      "1322/3000 train_loss: 24.980073928833008 test_loss:301.5304870605469\n",
      "1323/3000 train_loss: 22.928123474121094 test_loss:294.29022216796875\n",
      "1324/3000 train_loss: 21.442325592041016 test_loss:302.26568603515625\n",
      "1325/3000 train_loss: 22.592567443847656 test_loss:314.34381103515625\n",
      "1326/3000 train_loss: 19.530794143676758 test_loss:333.31488037109375\n",
      "1327/3000 train_loss: 19.236112594604492 test_loss:341.7320861816406\n",
      "1328/3000 train_loss: 18.614315032958984 test_loss:343.8694152832031\n",
      "1329/3000 train_loss: 18.327255249023438 test_loss:326.8363952636719\n",
      "1330/3000 train_loss: 18.20305633544922 test_loss:332.2400817871094\n",
      "1331/3000 train_loss: 18.599637985229492 test_loss:317.1839599609375\n",
      "1332/3000 train_loss: 17.97786521911621 test_loss:305.92864990234375\n",
      "1333/3000 train_loss: 18.39452362060547 test_loss:290.3778076171875\n",
      "1334/3000 train_loss: 18.051664352416992 test_loss:275.0060729980469\n",
      "1335/3000 train_loss: 18.777204513549805 test_loss:261.1890869140625\n",
      "1336/3000 train_loss: 20.0931453704834 test_loss:261.10516357421875\n",
      "1337/3000 train_loss: 20.74403190612793 test_loss:265.85321044921875\n",
      "1338/3000 train_loss: 21.68349266052246 test_loss:273.7313232421875\n",
      "1339/3000 train_loss: 22.076810836791992 test_loss:277.9454650878906\n",
      "1340/3000 train_loss: 23.596363067626953 test_loss:267.866943359375\n",
      "1341/3000 train_loss: 22.771381378173828 test_loss:260.79388427734375\n",
      "1342/3000 train_loss: 25.079336166381836 test_loss:260.69683837890625\n",
      "1343/3000 train_loss: 23.385034561157227 test_loss:272.4256286621094\n",
      "1344/3000 train_loss: 23.65290641784668 test_loss:288.3703308105469\n",
      "1345/3000 train_loss: 23.396745681762695 test_loss:291.68328857421875\n",
      "1346/3000 train_loss: 24.723846435546875 test_loss:296.07952880859375\n",
      "1347/3000 train_loss: 24.23798942565918 test_loss:305.70086669921875\n",
      "1348/3000 train_loss: 22.83125114440918 test_loss:312.56951904296875\n",
      "1349/3000 train_loss: 23.719026565551758 test_loss:316.2541809082031\n",
      "1350/3000 train_loss: 22.589757919311523 test_loss:363.1518249511719\n",
      "1351/3000 train_loss: 25.108516693115234 test_loss:349.311767578125\n",
      "1352/3000 train_loss: 23.73215103149414 test_loss:380.51165771484375\n",
      "1353/3000 train_loss: 24.876049041748047 test_loss:395.2005615234375\n",
      "1354/3000 train_loss: 25.496103286743164 test_loss:385.5875549316406\n",
      "1355/3000 train_loss: 22.927330017089844 test_loss:409.69268798828125\n",
      "1356/3000 train_loss: 26.74309539794922 test_loss:433.1923828125\n",
      "1357/3000 train_loss: 22.676746368408203 test_loss:422.10986328125\n",
      "1358/3000 train_loss: 26.93073081970215 test_loss:449.9324035644531\n",
      "1359/3000 train_loss: 27.285320281982422 test_loss:411.13726806640625\n",
      "1360/3000 train_loss: 25.542951583862305 test_loss:423.0906982421875\n",
      "1361/3000 train_loss: 24.688833236694336 test_loss:499.5482177734375\n",
      "1362/3000 train_loss: 26.57413673400879 test_loss:464.1097717285156\n",
      "1363/3000 train_loss: 28.14677619934082 test_loss:413.16119384765625\n",
      "1364/3000 train_loss: 27.10291290283203 test_loss:423.8831481933594\n",
      "1365/3000 train_loss: 27.712692260742188 test_loss:465.1242980957031\n",
      "1366/3000 train_loss: 32.42982864379883 test_loss:464.5667724609375\n",
      "1367/3000 train_loss: 29.158376693725586 test_loss:442.7041931152344\n",
      "1368/3000 train_loss: 31.43980598449707 test_loss:424.512451171875\n",
      "1369/3000 train_loss: 27.833782196044922 test_loss:397.27203369140625\n",
      "1370/3000 train_loss: 28.87750244140625 test_loss:358.891357421875\n",
      "1371/3000 train_loss: 29.891511917114258 test_loss:347.04241943359375\n",
      "1372/3000 train_loss: 29.324819564819336 test_loss:296.27105712890625\n",
      "1373/3000 train_loss: 33.348731994628906 test_loss:299.1180725097656\n",
      "1374/3000 train_loss: 29.724000930786133 test_loss:301.6649169921875\n",
      "1375/3000 train_loss: 31.689565658569336 test_loss:279.3889465332031\n",
      "1376/3000 train_loss: 30.421409606933594 test_loss:273.11865234375\n",
      "1377/3000 train_loss: 32.0756950378418 test_loss:289.56524658203125\n",
      "1378/3000 train_loss: 30.85904312133789 test_loss:284.85321044921875\n",
      "1379/3000 train_loss: 30.840417861938477 test_loss:282.1809387207031\n",
      "1380/3000 train_loss: 27.724102020263672 test_loss:267.80474853515625\n",
      "1381/3000 train_loss: 30.70841407775879 test_loss:282.64923095703125\n",
      "1382/3000 train_loss: 29.160917282104492 test_loss:290.3930969238281\n",
      "1383/3000 train_loss: 32.27982711791992 test_loss:290.01800537109375\n",
      "1384/3000 train_loss: 33.65747833251953 test_loss:294.8177185058594\n",
      "1385/3000 train_loss: 28.98004150390625 test_loss:305.67779541015625\n",
      "1386/3000 train_loss: 31.874950408935547 test_loss:310.955810546875\n",
      "1387/3000 train_loss: 30.19811248779297 test_loss:327.46209716796875\n",
      "1388/3000 train_loss: 29.71318244934082 test_loss:342.12713623046875\n",
      "1389/3000 train_loss: 28.491748809814453 test_loss:362.8333435058594\n",
      "1390/3000 train_loss: 27.369308471679688 test_loss:374.35125732421875\n",
      "1391/3000 train_loss: 29.58927345275879 test_loss:396.025146484375\n",
      "1392/3000 train_loss: 27.144046783447266 test_loss:431.896484375\n",
      "1393/3000 train_loss: 27.191226959228516 test_loss:401.36016845703125\n",
      "1394/3000 train_loss: 30.445844650268555 test_loss:465.29833984375\n",
      "1395/3000 train_loss: 27.183578491210938 test_loss:504.92120361328125\n",
      "1396/3000 train_loss: 29.96415901184082 test_loss:426.57470703125\n",
      "1397/3000 train_loss: 31.0748233795166 test_loss:441.50531005859375\n",
      "1398/3000 train_loss: 27.52599334716797 test_loss:409.1011047363281\n",
      "1399/3000 train_loss: 31.1496639251709 test_loss:397.14508056640625\n",
      "1400/3000 train_loss: 27.748027801513672 test_loss:382.1714782714844\n",
      "1401/3000 train_loss: 29.85434341430664 test_loss:413.2696533203125\n",
      "1402/3000 train_loss: 29.07809829711914 test_loss:385.30548095703125\n",
      "1403/3000 train_loss: 27.879467010498047 test_loss:344.3315124511719\n",
      "1404/3000 train_loss: 29.994667053222656 test_loss:336.9310607910156\n",
      "1405/3000 train_loss: 29.009559631347656 test_loss:313.8057861328125\n",
      "1406/3000 train_loss: 29.86968421936035 test_loss:300.7641296386719\n",
      "1407/3000 train_loss: 30.341106414794922 test_loss:286.6578063964844\n",
      "1408/3000 train_loss: 28.95156478881836 test_loss:268.26605224609375\n",
      "1409/3000 train_loss: 30.115436553955078 test_loss:252.0691375732422\n",
      "1410/3000 train_loss: 29.978071212768555 test_loss:243.95399475097656\n",
      "1411/3000 train_loss: 32.09449768066406 test_loss:249.71983337402344\n",
      "1412/3000 train_loss: 33.04790496826172 test_loss:259.1125793457031\n",
      "1413/3000 train_loss: 32.8353271484375 test_loss:265.8395080566406\n",
      "1414/3000 train_loss: 28.531856536865234 test_loss:257.80950927734375\n",
      "1415/3000 train_loss: 34.06855773925781 test_loss:267.3175964355469\n",
      "1416/3000 train_loss: 32.742530822753906 test_loss:278.6872863769531\n",
      "1417/3000 train_loss: 29.805522918701172 test_loss:291.90753173828125\n",
      "1418/3000 train_loss: 30.988977432250977 test_loss:302.0633544921875\n",
      "1419/3000 train_loss: 25.33893585205078 test_loss:337.2684020996094\n",
      "1420/3000 train_loss: 30.420284271240234 test_loss:341.25518798828125\n",
      "1421/3000 train_loss: 27.665021896362305 test_loss:359.8289794921875\n",
      "1422/3000 train_loss: 27.602060317993164 test_loss:388.303955078125\n",
      "1423/3000 train_loss: 26.00109100341797 test_loss:407.8779296875\n",
      "1424/3000 train_loss: 25.982881546020508 test_loss:413.7626953125\n",
      "1425/3000 train_loss: 25.24070167541504 test_loss:427.09674072265625\n",
      "1426/3000 train_loss: 27.651548385620117 test_loss:425.79119873046875\n",
      "1427/3000 train_loss: 23.885765075683594 test_loss:435.005859375\n",
      "1428/3000 train_loss: 26.035476684570312 test_loss:425.2345275878906\n",
      "1429/3000 train_loss: 24.88637924194336 test_loss:455.0215759277344\n",
      "1430/3000 train_loss: 26.745365142822266 test_loss:395.91705322265625\n",
      "1431/3000 train_loss: 27.476421356201172 test_loss:384.2585754394531\n",
      "1432/3000 train_loss: 28.24536895751953 test_loss:379.28106689453125\n",
      "1433/3000 train_loss: 30.09099578857422 test_loss:351.75146484375\n",
      "1434/3000 train_loss: 26.0122127532959 test_loss:328.9675598144531\n",
      "1435/3000 train_loss: 28.19851303100586 test_loss:314.087646484375\n",
      "1436/3000 train_loss: 27.100988388061523 test_loss:307.6977844238281\n",
      "1437/3000 train_loss: 26.665212631225586 test_loss:270.8128967285156\n",
      "1438/3000 train_loss: 28.04084014892578 test_loss:249.91622924804688\n",
      "1439/3000 train_loss: 30.824256896972656 test_loss:242.22613525390625\n",
      "1440/3000 train_loss: 28.99460220336914 test_loss:232.57974243164062\n",
      "1441/3000 train_loss: 26.033159255981445 test_loss:229.2901611328125\n",
      "1442/3000 train_loss: 29.374271392822266 test_loss:216.91590881347656\n",
      "1443/3000 train_loss: 32.69268798828125 test_loss:221.62892150878906\n",
      "1444/3000 train_loss: 29.493438720703125 test_loss:212.75035095214844\n",
      "1445/3000 train_loss: 30.409610748291016 test_loss:215.63784790039062\n",
      "1446/3000 train_loss: 33.64286804199219 test_loss:220.08460998535156\n",
      "1447/3000 train_loss: 31.61573028564453 test_loss:233.4114990234375\n",
      "1448/3000 train_loss: 33.14863586425781 test_loss:245.2042694091797\n",
      "1449/3000 train_loss: 32.78487777709961 test_loss:267.4379577636719\n",
      "1450/3000 train_loss: 33.65190124511719 test_loss:291.4791564941406\n",
      "1451/3000 train_loss: 29.467723846435547 test_loss:319.8547668457031\n",
      "1452/3000 train_loss: 31.84916114807129 test_loss:334.279052734375\n",
      "1453/3000 train_loss: 27.46274757385254 test_loss:378.3849182128906\n",
      "1454/3000 train_loss: 31.986839294433594 test_loss:388.8230895996094\n",
      "1455/3000 train_loss: 27.00543975830078 test_loss:391.938232421875\n",
      "1456/3000 train_loss: 30.97085189819336 test_loss:425.89532470703125\n",
      "1457/3000 train_loss: 31.107711791992188 test_loss:440.8231506347656\n",
      "1458/3000 train_loss: 27.309728622436523 test_loss:423.0473937988281\n",
      "1459/3000 train_loss: 30.679279327392578 test_loss:421.91693115234375\n",
      "1460/3000 train_loss: 34.26759338378906 test_loss:411.3338623046875\n",
      "1461/3000 train_loss: 29.912242889404297 test_loss:360.98773193359375\n",
      "1462/3000 train_loss: 28.567720413208008 test_loss:300.08563232421875\n",
      "1463/3000 train_loss: 31.98507308959961 test_loss:291.3354187011719\n",
      "1464/3000 train_loss: 26.642887115478516 test_loss:276.5874938964844\n",
      "1465/3000 train_loss: 28.38347816467285 test_loss:267.86273193359375\n",
      "1466/3000 train_loss: 25.5887393951416 test_loss:280.802978515625\n",
      "1467/3000 train_loss: 25.540847778320312 test_loss:284.07366943359375\n",
      "1468/3000 train_loss: 23.61880111694336 test_loss:284.8604736328125\n",
      "1469/3000 train_loss: 25.92803192138672 test_loss:281.7759704589844\n",
      "1470/3000 train_loss: 24.099773406982422 test_loss:271.6285705566406\n",
      "1471/3000 train_loss: 27.07676887512207 test_loss:266.4916076660156\n",
      "1472/3000 train_loss: 24.494070053100586 test_loss:258.8449401855469\n",
      "1473/3000 train_loss: 24.8472843170166 test_loss:265.4770202636719\n",
      "1474/3000 train_loss: 23.25577163696289 test_loss:284.7467346191406\n",
      "1475/3000 train_loss: 22.488357543945312 test_loss:310.12689208984375\n",
      "1476/3000 train_loss: 23.07036018371582 test_loss:328.2064514160156\n",
      "1477/3000 train_loss: 23.358842849731445 test_loss:383.4077453613281\n",
      "1478/3000 train_loss: 25.14899253845215 test_loss:393.2056579589844\n",
      "1479/3000 train_loss: 25.93609619140625 test_loss:407.2433776855469\n",
      "1480/3000 train_loss: 27.36273193359375 test_loss:402.83392333984375\n",
      "1481/3000 train_loss: 29.753087997436523 test_loss:370.965087890625\n",
      "1482/3000 train_loss: 27.668306350708008 test_loss:388.38397216796875\n",
      "1483/3000 train_loss: 26.252641677856445 test_loss:371.15386962890625\n",
      "1484/3000 train_loss: 26.856056213378906 test_loss:344.2391662597656\n",
      "1485/3000 train_loss: 25.46189308166504 test_loss:338.2358703613281\n",
      "1486/3000 train_loss: 23.78951644897461 test_loss:307.55157470703125\n",
      "1487/3000 train_loss: 23.430435180664062 test_loss:277.3175964355469\n",
      "1488/3000 train_loss: 26.28998565673828 test_loss:258.1283264160156\n",
      "1489/3000 train_loss: 26.513416290283203 test_loss:264.30438232421875\n",
      "1490/3000 train_loss: 26.76639747619629 test_loss:275.33306884765625\n",
      "1491/3000 train_loss: 24.087711334228516 test_loss:263.4698486328125\n",
      "1492/3000 train_loss: 20.856765747070312 test_loss:263.59857177734375\n",
      "1493/3000 train_loss: 18.14221954345703 test_loss:268.78118896484375\n",
      "1494/3000 train_loss: 17.237951278686523 test_loss:278.4369201660156\n",
      "1495/3000 train_loss: 16.9656982421875 test_loss:292.4538269042969\n",
      "1496/3000 train_loss: 17.944469451904297 test_loss:295.6437072753906\n",
      "1497/3000 train_loss: 17.754674911499023 test_loss:300.9654235839844\n",
      "1498/3000 train_loss: 17.696388244628906 test_loss:297.5065612792969\n",
      "1499/3000 train_loss: 17.83481788635254 test_loss:292.22015380859375\n",
      "1500/3000 train_loss: 18.72199821472168 test_loss:299.329345703125\n",
      "1501/3000 train_loss: 18.58002281188965 test_loss:305.4674377441406\n",
      "1502/3000 train_loss: 17.882278442382812 test_loss:323.56085205078125\n",
      "1503/3000 train_loss: 19.77906608581543 test_loss:310.8981018066406\n",
      "1504/3000 train_loss: 20.620878219604492 test_loss:320.35394287109375\n",
      "1505/3000 train_loss: 21.208133697509766 test_loss:355.3385009765625\n",
      "1506/3000 train_loss: 19.87548828125 test_loss:363.95465087890625\n",
      "1507/3000 train_loss: 21.258535385131836 test_loss:376.0994567871094\n",
      "1508/3000 train_loss: 21.134103775024414 test_loss:390.7905578613281\n",
      "1509/3000 train_loss: 20.11594581604004 test_loss:357.5896301269531\n",
      "1510/3000 train_loss: 20.399932861328125 test_loss:344.52593994140625\n",
      "1511/3000 train_loss: 20.641698837280273 test_loss:321.8664855957031\n",
      "1512/3000 train_loss: 19.223453521728516 test_loss:306.48736572265625\n",
      "1513/3000 train_loss: 21.41501235961914 test_loss:295.6199645996094\n",
      "1514/3000 train_loss: 22.771915435791016 test_loss:303.0995788574219\n",
      "1515/3000 train_loss: 23.24066925048828 test_loss:292.07196044921875\n",
      "1516/3000 train_loss: 24.595918655395508 test_loss:265.923095703125\n",
      "1517/3000 train_loss: 22.68353271484375 test_loss:268.3400573730469\n",
      "1518/3000 train_loss: 25.165603637695312 test_loss:279.8492126464844\n",
      "1519/3000 train_loss: 22.55727195739746 test_loss:284.1257019042969\n",
      "1520/3000 train_loss: 22.215303421020508 test_loss:293.5995178222656\n",
      "1521/3000 train_loss: 20.65440559387207 test_loss:306.8060302734375\n",
      "1522/3000 train_loss: 24.2396183013916 test_loss:300.6809387207031\n",
      "1523/3000 train_loss: 20.342609405517578 test_loss:291.0613708496094\n",
      "1524/3000 train_loss: 22.555200576782227 test_loss:274.267333984375\n",
      "1525/3000 train_loss: 20.726011276245117 test_loss:266.4064025878906\n",
      "1526/3000 train_loss: 20.195026397705078 test_loss:244.01193237304688\n",
      "1527/3000 train_loss: 19.897069931030273 test_loss:254.6080780029297\n",
      "1528/3000 train_loss: 20.285112380981445 test_loss:259.47186279296875\n",
      "1529/3000 train_loss: 23.06723976135254 test_loss:264.8101501464844\n",
      "1530/3000 train_loss: 22.500696182250977 test_loss:266.0721435546875\n",
      "1531/3000 train_loss: 20.970165252685547 test_loss:261.96728515625\n",
      "1532/3000 train_loss: 20.275266647338867 test_loss:271.3652038574219\n",
      "1533/3000 train_loss: 20.961654663085938 test_loss:306.20989990234375\n",
      "1534/3000 train_loss: 21.758346557617188 test_loss:296.84521484375\n",
      "1535/3000 train_loss: 20.526514053344727 test_loss:310.2733154296875\n",
      "1536/3000 train_loss: 20.251405715942383 test_loss:296.514404296875\n",
      "1537/3000 train_loss: 22.612321853637695 test_loss:310.7062072753906\n",
      "1538/3000 train_loss: 23.512271881103516 test_loss:283.7762756347656\n",
      "1539/3000 train_loss: 20.599632263183594 test_loss:297.8454895019531\n",
      "1540/3000 train_loss: 19.713138580322266 test_loss:304.7631530761719\n",
      "1541/3000 train_loss: 19.062997817993164 test_loss:331.62432861328125\n",
      "1542/3000 train_loss: 20.25516128540039 test_loss:344.16754150390625\n",
      "1543/3000 train_loss: 20.464187622070312 test_loss:344.52484130859375\n",
      "1544/3000 train_loss: 22.294218063354492 test_loss:358.60498046875\n",
      "1545/3000 train_loss: 22.562772750854492 test_loss:331.71905517578125\n",
      "1546/3000 train_loss: 21.26761245727539 test_loss:399.14154052734375\n",
      "1547/3000 train_loss: 22.0565242767334 test_loss:385.6352233886719\n",
      "1548/3000 train_loss: 21.66368293762207 test_loss:394.37225341796875\n",
      "1549/3000 train_loss: 21.740224838256836 test_loss:407.77252197265625\n",
      "1550/3000 train_loss: 20.93520736694336 test_loss:423.4698486328125\n",
      "1551/3000 train_loss: 21.199872970581055 test_loss:451.6817321777344\n",
      "1552/3000 train_loss: 24.467052459716797 test_loss:369.0389709472656\n",
      "1553/3000 train_loss: 22.79351043701172 test_loss:426.47491455078125\n",
      "1554/3000 train_loss: 21.77615737915039 test_loss:452.28277587890625\n",
      "1555/3000 train_loss: 21.47376251220703 test_loss:414.99517822265625\n",
      "1556/3000 train_loss: 21.876605987548828 test_loss:463.471923828125\n",
      "1557/3000 train_loss: 24.315044403076172 test_loss:398.8520202636719\n",
      "1558/3000 train_loss: 22.241483688354492 test_loss:427.0294494628906\n",
      "1559/3000 train_loss: 22.26888656616211 test_loss:447.79180908203125\n",
      "1560/3000 train_loss: 21.536130905151367 test_loss:424.13067626953125\n",
      "1561/3000 train_loss: 20.89588165283203 test_loss:403.6442565917969\n",
      "1562/3000 train_loss: 20.18795394897461 test_loss:413.636962890625\n",
      "1563/3000 train_loss: 21.078813552856445 test_loss:419.2821350097656\n",
      "1564/3000 train_loss: 21.20766258239746 test_loss:376.6792297363281\n",
      "1565/3000 train_loss: 21.431713104248047 test_loss:364.09912109375\n",
      "1566/3000 train_loss: 21.230804443359375 test_loss:311.29315185546875\n",
      "1567/3000 train_loss: 24.94428825378418 test_loss:327.80322265625\n",
      "1568/3000 train_loss: 23.1405029296875 test_loss:345.00946044921875\n",
      "1569/3000 train_loss: 22.073774337768555 test_loss:309.2073059082031\n",
      "1570/3000 train_loss: 21.700292587280273 test_loss:294.6206359863281\n",
      "1571/3000 train_loss: 22.89702606201172 test_loss:284.28790283203125\n",
      "1572/3000 train_loss: 22.730100631713867 test_loss:282.1703796386719\n",
      "1573/3000 train_loss: 25.38251495361328 test_loss:273.8292236328125\n",
      "1574/3000 train_loss: 22.730737686157227 test_loss:274.26934814453125\n",
      "1575/3000 train_loss: 22.7831974029541 test_loss:268.9505920410156\n",
      "1576/3000 train_loss: 21.22511100769043 test_loss:276.0241394042969\n",
      "1577/3000 train_loss: 27.169599533081055 test_loss:277.1932373046875\n",
      "1578/3000 train_loss: 26.61830711364746 test_loss:272.49481201171875\n",
      "1579/3000 train_loss: 25.55772590637207 test_loss:270.68011474609375\n",
      "1580/3000 train_loss: 27.043716430664062 test_loss:260.1153259277344\n",
      "1581/3000 train_loss: 26.313976287841797 test_loss:251.30422973632812\n",
      "1582/3000 train_loss: 25.763164520263672 test_loss:248.39901733398438\n",
      "1583/3000 train_loss: 27.507251739501953 test_loss:251.64007568359375\n",
      "1584/3000 train_loss: 29.68228530883789 test_loss:261.8171081542969\n",
      "1585/3000 train_loss: 30.05744171142578 test_loss:252.95033264160156\n",
      "1586/3000 train_loss: 29.624500274658203 test_loss:253.20211791992188\n",
      "1587/3000 train_loss: 29.92573356628418 test_loss:251.76376342773438\n",
      "1588/3000 train_loss: 28.194002151489258 test_loss:243.36053466796875\n",
      "1589/3000 train_loss: 25.81066131591797 test_loss:261.74005126953125\n",
      "1590/3000 train_loss: 27.494352340698242 test_loss:269.7826843261719\n",
      "1591/3000 train_loss: 28.71698570251465 test_loss:272.037841796875\n",
      "1592/3000 train_loss: 29.545652389526367 test_loss:275.50177001953125\n",
      "1593/3000 train_loss: 29.645647048950195 test_loss:312.8975830078125\n",
      "1594/3000 train_loss: 32.04868698120117 test_loss:315.28741455078125\n",
      "1595/3000 train_loss: 29.150724411010742 test_loss:307.090576171875\n",
      "1596/3000 train_loss: 25.7844295501709 test_loss:338.7638244628906\n",
      "1597/3000 train_loss: 27.276565551757812 test_loss:367.08367919921875\n",
      "1598/3000 train_loss: 28.12537384033203 test_loss:394.827880859375\n",
      "1599/3000 train_loss: 27.97927474975586 test_loss:403.96563720703125\n",
      "1600/3000 train_loss: 25.0832576751709 test_loss:413.47735595703125\n",
      "1601/3000 train_loss: 28.51718521118164 test_loss:466.05474853515625\n",
      "1602/3000 train_loss: 29.46985626220703 test_loss:487.91583251953125\n",
      "1603/3000 train_loss: 28.852134704589844 test_loss:485.6304931640625\n",
      "1604/3000 train_loss: 28.986347198486328 test_loss:482.5066223144531\n",
      "1605/3000 train_loss: 31.013465881347656 test_loss:452.444091796875\n",
      "1606/3000 train_loss: 29.187463760375977 test_loss:415.5803527832031\n",
      "1607/3000 train_loss: 26.135684967041016 test_loss:418.3240966796875\n",
      "1608/3000 train_loss: 26.95436668395996 test_loss:385.7085266113281\n",
      "1609/3000 train_loss: 24.69391632080078 test_loss:365.1350402832031\n",
      "1610/3000 train_loss: 26.29044532775879 test_loss:352.1428527832031\n",
      "1611/3000 train_loss: 25.618635177612305 test_loss:316.7086486816406\n",
      "1612/3000 train_loss: 26.58011245727539 test_loss:291.5821228027344\n",
      "1613/3000 train_loss: 25.703699111938477 test_loss:292.5443420410156\n",
      "1614/3000 train_loss: 24.47136878967285 test_loss:278.67626953125\n",
      "1615/3000 train_loss: 26.665802001953125 test_loss:256.74835205078125\n",
      "1616/3000 train_loss: 25.547454833984375 test_loss:241.7700958251953\n",
      "1617/3000 train_loss: 27.093713760375977 test_loss:236.30801391601562\n",
      "1618/3000 train_loss: 27.152082443237305 test_loss:237.79335021972656\n",
      "1619/3000 train_loss: 24.639970779418945 test_loss:237.84910583496094\n",
      "1620/3000 train_loss: 24.190868377685547 test_loss:231.40081787109375\n",
      "1621/3000 train_loss: 25.711563110351562 test_loss:232.0897674560547\n",
      "1622/3000 train_loss: 27.3398494720459 test_loss:229.00588989257812\n",
      "1623/3000 train_loss: 27.74282455444336 test_loss:238.13870239257812\n",
      "1624/3000 train_loss: 27.21453857421875 test_loss:253.71441650390625\n",
      "1625/3000 train_loss: 27.719043731689453 test_loss:256.3575439453125\n",
      "1626/3000 train_loss: 27.041120529174805 test_loss:256.2670593261719\n",
      "1627/3000 train_loss: 28.55495834350586 test_loss:269.4053039550781\n",
      "1628/3000 train_loss: 28.318923950195312 test_loss:288.22998046875\n",
      "1629/3000 train_loss: 31.61715316772461 test_loss:312.4261779785156\n",
      "1630/3000 train_loss: 33.38212203979492 test_loss:328.9549865722656\n",
      "1631/3000 train_loss: 29.248493194580078 test_loss:342.4375915527344\n",
      "1632/3000 train_loss: 29.191822052001953 test_loss:389.46356201171875\n",
      "1633/3000 train_loss: 30.589941024780273 test_loss:400.774658203125\n",
      "1634/3000 train_loss: 29.63317108154297 test_loss:444.7982482910156\n",
      "1635/3000 train_loss: 33.50654220581055 test_loss:453.2566223144531\n",
      "1636/3000 train_loss: 31.119417190551758 test_loss:438.93402099609375\n",
      "1637/3000 train_loss: 30.667932510375977 test_loss:476.03924560546875\n",
      "1638/3000 train_loss: 29.559741973876953 test_loss:430.8993225097656\n",
      "1639/3000 train_loss: 30.11786460876465 test_loss:479.911376953125\n",
      "1640/3000 train_loss: 31.217727661132812 test_loss:425.5308532714844\n",
      "1641/3000 train_loss: 28.77651023864746 test_loss:396.3480529785156\n",
      "1642/3000 train_loss: 28.30679702758789 test_loss:388.4665222167969\n",
      "1643/3000 train_loss: 27.71430015563965 test_loss:374.0996398925781\n",
      "1644/3000 train_loss: 27.505949020385742 test_loss:332.08233642578125\n",
      "1645/3000 train_loss: 30.26507568359375 test_loss:338.9797668457031\n",
      "1646/3000 train_loss: 29.440711975097656 test_loss:306.6829528808594\n",
      "1647/3000 train_loss: 28.03129005432129 test_loss:289.55206298828125\n",
      "1648/3000 train_loss: 28.99570655822754 test_loss:274.9495544433594\n",
      "1649/3000 train_loss: 28.87591552734375 test_loss:268.89666748046875\n",
      "1650/3000 train_loss: 27.970458984375 test_loss:261.9507141113281\n",
      "1651/3000 train_loss: 25.796384811401367 test_loss:258.0272521972656\n",
      "1652/3000 train_loss: 27.456327438354492 test_loss:259.7521057128906\n",
      "1653/3000 train_loss: 26.33098602294922 test_loss:260.5780029296875\n",
      "1654/3000 train_loss: 25.489839553833008 test_loss:258.7187194824219\n",
      "1655/3000 train_loss: 27.623485565185547 test_loss:258.3727722167969\n",
      "1656/3000 train_loss: 22.643016815185547 test_loss:291.51165771484375\n",
      "1657/3000 train_loss: 26.24551773071289 test_loss:308.4386901855469\n",
      "1658/3000 train_loss: 26.61791229248047 test_loss:336.81561279296875\n",
      "1659/3000 train_loss: 25.2228946685791 test_loss:381.82525634765625\n",
      "1660/3000 train_loss: 24.955101013183594 test_loss:400.7542724609375\n",
      "1661/3000 train_loss: 26.53656005859375 test_loss:407.78369140625\n",
      "1662/3000 train_loss: 24.83838653564453 test_loss:435.88848876953125\n",
      "1663/3000 train_loss: 25.929914474487305 test_loss:424.87615966796875\n",
      "1664/3000 train_loss: 23.90670394897461 test_loss:446.8304138183594\n",
      "1665/3000 train_loss: 27.033016204833984 test_loss:427.6793212890625\n",
      "1666/3000 train_loss: 26.497865676879883 test_loss:350.846435546875\n",
      "1667/3000 train_loss: 23.89472770690918 test_loss:336.6213684082031\n",
      "1668/3000 train_loss: 23.306711196899414 test_loss:313.6728210449219\n",
      "1669/3000 train_loss: 22.51006317138672 test_loss:311.0435791015625\n",
      "1670/3000 train_loss: 22.567838668823242 test_loss:289.8101501464844\n",
      "1671/3000 train_loss: 23.940799713134766 test_loss:284.4114685058594\n",
      "1672/3000 train_loss: 23.773202896118164 test_loss:273.324462890625\n",
      "1673/3000 train_loss: 28.847137451171875 test_loss:270.0165710449219\n",
      "1674/3000 train_loss: 28.017255783081055 test_loss:276.2944641113281\n",
      "1675/3000 train_loss: 30.79503059387207 test_loss:276.93865966796875\n",
      "1676/3000 train_loss: 30.115434646606445 test_loss:277.95660400390625\n",
      "1677/3000 train_loss: 34.46278381347656 test_loss:272.6880798339844\n",
      "1678/3000 train_loss: 29.1977596282959 test_loss:285.2081298828125\n",
      "1679/3000 train_loss: 25.431798934936523 test_loss:303.6700439453125\n",
      "1680/3000 train_loss: 26.548931121826172 test_loss:308.077880859375\n",
      "1681/3000 train_loss: 25.31595802307129 test_loss:351.3111572265625\n",
      "1682/3000 train_loss: 22.713123321533203 test_loss:365.6760559082031\n",
      "1683/3000 train_loss: 22.899394989013672 test_loss:358.5455627441406\n",
      "1684/3000 train_loss: 23.297780990600586 test_loss:365.8525390625\n",
      "1685/3000 train_loss: 22.96598243713379 test_loss:363.4635925292969\n",
      "1686/3000 train_loss: 21.135414123535156 test_loss:369.0946960449219\n",
      "1687/3000 train_loss: 21.354177474975586 test_loss:364.93792724609375\n",
      "1688/3000 train_loss: 21.70075035095215 test_loss:347.4813232421875\n",
      "1689/3000 train_loss: 23.20283317565918 test_loss:357.2690124511719\n",
      "1690/3000 train_loss: 21.802825927734375 test_loss:331.2896728515625\n",
      "1691/3000 train_loss: 20.762115478515625 test_loss:310.6777648925781\n",
      "1692/3000 train_loss: 19.970897674560547 test_loss:319.4173583984375\n",
      "1693/3000 train_loss: 19.404552459716797 test_loss:309.7242126464844\n",
      "1694/3000 train_loss: 19.434307098388672 test_loss:306.9350891113281\n",
      "1695/3000 train_loss: 18.170921325683594 test_loss:284.94818115234375\n",
      "1696/3000 train_loss: 18.613285064697266 test_loss:276.6908874511719\n",
      "1697/3000 train_loss: 19.199983596801758 test_loss:264.3473815917969\n",
      "1698/3000 train_loss: 20.69414520263672 test_loss:256.9095764160156\n",
      "1699/3000 train_loss: 20.295583724975586 test_loss:255.42501831054688\n",
      "1700/3000 train_loss: 18.88214874267578 test_loss:264.3654479980469\n",
      "1701/3000 train_loss: 19.239269256591797 test_loss:270.56707763671875\n",
      "1702/3000 train_loss: 19.64695930480957 test_loss:285.2158508300781\n",
      "1703/3000 train_loss: 20.63059425354004 test_loss:302.2137756347656\n",
      "1704/3000 train_loss: 19.787282943725586 test_loss:327.5870666503906\n",
      "1705/3000 train_loss: 21.713232040405273 test_loss:332.20001220703125\n",
      "1706/3000 train_loss: 21.33281707763672 test_loss:371.55889892578125\n",
      "1707/3000 train_loss: 20.948627471923828 test_loss:421.12908935546875\n",
      "1708/3000 train_loss: 23.382116317749023 test_loss:419.5826416015625\n",
      "1709/3000 train_loss: 23.13818359375 test_loss:454.3628845214844\n",
      "1710/3000 train_loss: 24.011850357055664 test_loss:478.0833740234375\n",
      "1711/3000 train_loss: 25.295347213745117 test_loss:436.62127685546875\n",
      "1712/3000 train_loss: 26.71865463256836 test_loss:372.01361083984375\n",
      "1713/3000 train_loss: 21.01108169555664 test_loss:426.9290771484375\n",
      "1714/3000 train_loss: 21.143341064453125 test_loss:375.53448486328125\n",
      "1715/3000 train_loss: 20.991607666015625 test_loss:368.655029296875\n",
      "1716/3000 train_loss: 22.64642333984375 test_loss:369.9803161621094\n",
      "1717/3000 train_loss: 19.532297134399414 test_loss:361.9146728515625\n",
      "1718/3000 train_loss: 21.74881362915039 test_loss:341.1669006347656\n",
      "1719/3000 train_loss: 21.446128845214844 test_loss:341.99407958984375\n",
      "1720/3000 train_loss: 22.79305648803711 test_loss:328.47119140625\n",
      "1721/3000 train_loss: 23.215904235839844 test_loss:314.2486572265625\n",
      "1722/3000 train_loss: 23.484107971191406 test_loss:290.96319580078125\n",
      "1723/3000 train_loss: 24.459840774536133 test_loss:293.6628723144531\n",
      "1724/3000 train_loss: 24.12415885925293 test_loss:282.9157409667969\n",
      "1725/3000 train_loss: 25.386817932128906 test_loss:269.12908935546875\n",
      "1726/3000 train_loss: 25.38076400756836 test_loss:266.36260986328125\n",
      "1727/3000 train_loss: 25.7426700592041 test_loss:249.64390563964844\n",
      "1728/3000 train_loss: 26.74359893798828 test_loss:253.6453399658203\n",
      "1729/3000 train_loss: 28.488401412963867 test_loss:262.8787841796875\n",
      "1730/3000 train_loss: 31.53334617614746 test_loss:255.54461669921875\n",
      "1731/3000 train_loss: 28.968469619750977 test_loss:238.13282775878906\n",
      "1732/3000 train_loss: 30.702608108520508 test_loss:241.8248291015625\n",
      "1733/3000 train_loss: 29.904605865478516 test_loss:251.32606506347656\n",
      "1734/3000 train_loss: 32.33262252807617 test_loss:254.14520263671875\n",
      "1735/3000 train_loss: 29.81639862060547 test_loss:285.8988342285156\n",
      "1736/3000 train_loss: 33.03882598876953 test_loss:288.9891662597656\n",
      "1737/3000 train_loss: 30.903345108032227 test_loss:312.9772644042969\n",
      "1738/3000 train_loss: 34.820621490478516 test_loss:315.1961975097656\n",
      "1739/3000 train_loss: 33.780826568603516 test_loss:336.27166748046875\n",
      "1740/3000 train_loss: 36.188602447509766 test_loss:367.7182312011719\n",
      "1741/3000 train_loss: 31.886079788208008 test_loss:381.3700256347656\n",
      "1742/3000 train_loss: 34.89641189575195 test_loss:375.63165283203125\n",
      "1743/3000 train_loss: 32.27859115600586 test_loss:397.54241943359375\n",
      "1744/3000 train_loss: 32.7012939453125 test_loss:412.6303405761719\n",
      "1745/3000 train_loss: 30.187654495239258 test_loss:455.45135498046875\n",
      "1746/3000 train_loss: 31.614791870117188 test_loss:475.7657470703125\n",
      "1747/3000 train_loss: 31.138011932373047 test_loss:468.7342224121094\n",
      "1748/3000 train_loss: 31.68876838684082 test_loss:521.0462646484375\n",
      "1749/3000 train_loss: 37.829349517822266 test_loss:446.4007873535156\n",
      "1750/3000 train_loss: 34.947017669677734 test_loss:449.4718017578125\n",
      "1751/3000 train_loss: 31.678266525268555 test_loss:401.51129150390625\n",
      "1752/3000 train_loss: 34.31787872314453 test_loss:358.8675231933594\n",
      "1753/3000 train_loss: 29.3981876373291 test_loss:343.04736328125\n",
      "1754/3000 train_loss: 25.22136878967285 test_loss:323.32672119140625\n",
      "1755/3000 train_loss: 25.82787322998047 test_loss:283.4006652832031\n",
      "1756/3000 train_loss: 27.390073776245117 test_loss:275.39996337890625\n",
      "1757/3000 train_loss: 25.147377014160156 test_loss:263.8419494628906\n",
      "1758/3000 train_loss: 24.535749435424805 test_loss:256.4645690917969\n",
      "1759/3000 train_loss: 22.998130798339844 test_loss:247.759521484375\n",
      "1760/3000 train_loss: 22.411283493041992 test_loss:257.4764709472656\n",
      "1761/3000 train_loss: 24.520580291748047 test_loss:257.56646728515625\n",
      "1762/3000 train_loss: 23.56403160095215 test_loss:256.2298583984375\n",
      "1763/3000 train_loss: 24.910676956176758 test_loss:272.9503479003906\n",
      "1764/3000 train_loss: 22.261903762817383 test_loss:280.5089111328125\n",
      "1765/3000 train_loss: 21.68329429626465 test_loss:306.34490966796875\n",
      "1766/3000 train_loss: 22.237207412719727 test_loss:320.387939453125\n",
      "1767/3000 train_loss: 21.718521118164062 test_loss:345.00439453125\n",
      "1768/3000 train_loss: 20.854665756225586 test_loss:372.779052734375\n",
      "1769/3000 train_loss: 22.670534133911133 test_loss:380.4321594238281\n",
      "1770/3000 train_loss: 23.212846755981445 test_loss:392.7974853515625\n",
      "1771/3000 train_loss: 22.70955467224121 test_loss:409.71551513671875\n",
      "1772/3000 train_loss: 22.308713912963867 test_loss:388.0290222167969\n",
      "1773/3000 train_loss: 23.637514114379883 test_loss:387.2992248535156\n",
      "1774/3000 train_loss: 23.119487762451172 test_loss:351.7426452636719\n",
      "1775/3000 train_loss: 23.78732681274414 test_loss:347.564208984375\n",
      "1776/3000 train_loss: 22.64356803894043 test_loss:329.2246398925781\n",
      "1777/3000 train_loss: 21.700605392456055 test_loss:283.0487060546875\n",
      "1778/3000 train_loss: 24.494930267333984 test_loss:275.6564025878906\n",
      "1779/3000 train_loss: 22.61321258544922 test_loss:248.82762145996094\n",
      "1780/3000 train_loss: 25.139434814453125 test_loss:245.37074279785156\n",
      "1781/3000 train_loss: 26.713764190673828 test_loss:251.8208465576172\n",
      "1782/3000 train_loss: 24.720136642456055 test_loss:248.70391845703125\n",
      "1783/3000 train_loss: 26.200237274169922 test_loss:252.02947998046875\n",
      "1784/3000 train_loss: 27.889490127563477 test_loss:252.2399139404297\n",
      "1785/3000 train_loss: 31.023462295532227 test_loss:256.72125244140625\n",
      "1786/3000 train_loss: 28.58660888671875 test_loss:241.65185546875\n",
      "1787/3000 train_loss: 29.88812828063965 test_loss:256.99542236328125\n",
      "1788/3000 train_loss: 25.507003784179688 test_loss:266.1798095703125\n",
      "1789/3000 train_loss: 24.19275665283203 test_loss:267.52117919921875\n",
      "1790/3000 train_loss: 23.8963565826416 test_loss:318.1931457519531\n",
      "1791/3000 train_loss: 26.081317901611328 test_loss:353.96734619140625\n",
      "1792/3000 train_loss: 25.51544189453125 test_loss:379.4478454589844\n",
      "1793/3000 train_loss: 25.875385284423828 test_loss:386.9883117675781\n",
      "1794/3000 train_loss: 25.73811912536621 test_loss:421.8437194824219\n",
      "1795/3000 train_loss: 26.078657150268555 test_loss:446.6424865722656\n",
      "1796/3000 train_loss: 27.620576858520508 test_loss:447.3833312988281\n",
      "1797/3000 train_loss: 30.465869903564453 test_loss:499.4180908203125\n",
      "1798/3000 train_loss: 26.566696166992188 test_loss:462.17047119140625\n",
      "1799/3000 train_loss: 28.864694595336914 test_loss:453.07275390625\n",
      "1800/3000 train_loss: 29.03557777404785 test_loss:443.847412109375\n",
      "1801/3000 train_loss: 26.17483901977539 test_loss:393.04510498046875\n",
      "1802/3000 train_loss: 28.977842330932617 test_loss:357.88140869140625\n",
      "1803/3000 train_loss: 27.76177215576172 test_loss:341.6728515625\n",
      "1804/3000 train_loss: 25.7290096282959 test_loss:290.46832275390625\n",
      "1805/3000 train_loss: 27.239017486572266 test_loss:286.5254211425781\n",
      "1806/3000 train_loss: 23.71729278564453 test_loss:282.9400939941406\n",
      "1807/3000 train_loss: 23.470617294311523 test_loss:273.2239074707031\n",
      "1808/3000 train_loss: 26.242849349975586 test_loss:272.5582275390625\n",
      "1809/3000 train_loss: 24.662317276000977 test_loss:275.9241943359375\n",
      "1810/3000 train_loss: 29.406402587890625 test_loss:277.97613525390625\n",
      "1811/3000 train_loss: 26.179113388061523 test_loss:274.5517272949219\n",
      "1812/3000 train_loss: 28.504255294799805 test_loss:268.0909118652344\n",
      "1813/3000 train_loss: 26.735353469848633 test_loss:277.22662353515625\n",
      "1814/3000 train_loss: 28.759960174560547 test_loss:294.9801330566406\n",
      "1815/3000 train_loss: 29.844240188598633 test_loss:319.5992431640625\n",
      "1816/3000 train_loss: 29.815963745117188 test_loss:327.85003662109375\n",
      "1817/3000 train_loss: 24.71617317199707 test_loss:364.01751708984375\n",
      "1818/3000 train_loss: 27.386905670166016 test_loss:408.263671875\n",
      "1819/3000 train_loss: 30.386701583862305 test_loss:409.7435607910156\n",
      "1820/3000 train_loss: 30.59687042236328 test_loss:430.53045654296875\n",
      "1821/3000 train_loss: 28.9499454498291 test_loss:458.783203125\n",
      "1822/3000 train_loss: 29.169668197631836 test_loss:437.62860107421875\n",
      "1823/3000 train_loss: 29.702821731567383 test_loss:439.9952392578125\n",
      "1824/3000 train_loss: 30.203697204589844 test_loss:399.62969970703125\n",
      "1825/3000 train_loss: 31.59114646911621 test_loss:356.91552734375\n",
      "1826/3000 train_loss: 29.42529296875 test_loss:315.7850341796875\n",
      "1827/3000 train_loss: 30.495716094970703 test_loss:287.5172424316406\n",
      "1828/3000 train_loss: 28.497493743896484 test_loss:264.7467956542969\n",
      "1829/3000 train_loss: 28.966995239257812 test_loss:252.78848266601562\n",
      "1830/3000 train_loss: 27.68701171875 test_loss:240.92877197265625\n",
      "1831/3000 train_loss: 29.617162704467773 test_loss:238.30287170410156\n",
      "1832/3000 train_loss: 26.466583251953125 test_loss:244.75445556640625\n",
      "1833/3000 train_loss: 30.499574661254883 test_loss:237.50677490234375\n",
      "1834/3000 train_loss: 30.12838363647461 test_loss:241.4418487548828\n",
      "1835/3000 train_loss: 29.93832015991211 test_loss:244.4130859375\n",
      "1836/3000 train_loss: 30.72920799255371 test_loss:246.98924255371094\n",
      "1837/3000 train_loss: 31.46906089782715 test_loss:258.9006652832031\n",
      "1838/3000 train_loss: 30.991525650024414 test_loss:278.8790588378906\n",
      "1839/3000 train_loss: 29.20981216430664 test_loss:324.3150939941406\n",
      "1840/3000 train_loss: 29.512653350830078 test_loss:365.2928466796875\n",
      "1841/3000 train_loss: 33.26274871826172 test_loss:386.4117126464844\n",
      "1842/3000 train_loss: 26.576889038085938 test_loss:415.47979736328125\n",
      "1843/3000 train_loss: 31.39944076538086 test_loss:449.9117431640625\n",
      "1844/3000 train_loss: 30.391096115112305 test_loss:479.96392822265625\n",
      "1845/3000 train_loss: 32.948020935058594 test_loss:449.5326232910156\n",
      "1846/3000 train_loss: 32.94735336303711 test_loss:394.3297424316406\n",
      "1847/3000 train_loss: 32.51062774658203 test_loss:361.1664733886719\n",
      "1848/3000 train_loss: 29.50087547302246 test_loss:351.5816345214844\n",
      "1849/3000 train_loss: 26.64375114440918 test_loss:312.2313232421875\n",
      "1850/3000 train_loss: 26.381816864013672 test_loss:291.2732238769531\n",
      "1851/3000 train_loss: 24.973848342895508 test_loss:302.5511169433594\n",
      "1852/3000 train_loss: 27.70601463317871 test_loss:289.7146911621094\n",
      "1853/3000 train_loss: 27.38121795654297 test_loss:295.14691162109375\n",
      "1854/3000 train_loss: 28.301515579223633 test_loss:295.853759765625\n",
      "1855/3000 train_loss: 27.261430740356445 test_loss:288.9248352050781\n",
      "1856/3000 train_loss: 25.053407669067383 test_loss:300.402587890625\n",
      "1857/3000 train_loss: 24.69211769104004 test_loss:314.2544860839844\n",
      "1858/3000 train_loss: 23.630239486694336 test_loss:337.4666748046875\n",
      "1859/3000 train_loss: 23.61726188659668 test_loss:360.647705078125\n",
      "1860/3000 train_loss: 22.468177795410156 test_loss:380.81512451171875\n",
      "1861/3000 train_loss: 22.027257919311523 test_loss:393.66058349609375\n",
      "1862/3000 train_loss: 22.32868766784668 test_loss:417.2243347167969\n",
      "1863/3000 train_loss: 21.868776321411133 test_loss:426.9295349121094\n",
      "1864/3000 train_loss: 22.408918380737305 test_loss:423.65411376953125\n",
      "1865/3000 train_loss: 23.600040435791016 test_loss:411.70977783203125\n",
      "1866/3000 train_loss: 20.901987075805664 test_loss:423.9007873535156\n",
      "1867/3000 train_loss: 21.325401306152344 test_loss:404.306396484375\n",
      "1868/3000 train_loss: 23.419443130493164 test_loss:385.2515563964844\n",
      "1869/3000 train_loss: 21.61356544494629 test_loss:382.2246398925781\n",
      "1870/3000 train_loss: 21.97682762145996 test_loss:371.4942626953125\n",
      "1871/3000 train_loss: 22.216960906982422 test_loss:355.4580383300781\n",
      "1872/3000 train_loss: 20.672409057617188 test_loss:320.21600341796875\n",
      "1873/3000 train_loss: 22.092748641967773 test_loss:310.1593933105469\n",
      "1874/3000 train_loss: 22.54803466796875 test_loss:273.23974609375\n",
      "1875/3000 train_loss: 23.114221572875977 test_loss:248.65567016601562\n",
      "1876/3000 train_loss: 24.78498077392578 test_loss:236.3748321533203\n",
      "1877/3000 train_loss: 25.384309768676758 test_loss:249.92355346679688\n",
      "1878/3000 train_loss: 24.967287063598633 test_loss:247.38021850585938\n",
      "1879/3000 train_loss: 29.17229652404785 test_loss:246.02618408203125\n",
      "1880/3000 train_loss: 26.612531661987305 test_loss:251.2868194580078\n",
      "1881/3000 train_loss: 26.309911727905273 test_loss:251.9365692138672\n",
      "1882/3000 train_loss: 29.958826065063477 test_loss:255.56295776367188\n",
      "1883/3000 train_loss: 26.189476013183594 test_loss:261.79852294921875\n",
      "1884/3000 train_loss: 31.014402389526367 test_loss:262.918212890625\n",
      "1885/3000 train_loss: 31.97504234313965 test_loss:270.0212097167969\n",
      "1886/3000 train_loss: 36.201148986816406 test_loss:284.0041198730469\n",
      "1887/3000 train_loss: 30.493778228759766 test_loss:278.6419982910156\n",
      "1888/3000 train_loss: 33.86704635620117 test_loss:289.9706115722656\n",
      "1889/3000 train_loss: 32.179664611816406 test_loss:318.59710693359375\n",
      "1890/3000 train_loss: 29.32803726196289 test_loss:357.7117614746094\n",
      "1891/3000 train_loss: 27.52544593811035 test_loss:399.3769836425781\n",
      "1892/3000 train_loss: 29.193443298339844 test_loss:404.63671875\n",
      "1893/3000 train_loss: 25.472558975219727 test_loss:436.1673278808594\n",
      "1894/3000 train_loss: 25.409360885620117 test_loss:423.1600341796875\n",
      "1895/3000 train_loss: 24.466768264770508 test_loss:437.3736877441406\n",
      "1896/3000 train_loss: 30.548721313476562 test_loss:416.202880859375\n",
      "1897/3000 train_loss: 27.987957000732422 test_loss:435.26641845703125\n",
      "1898/3000 train_loss: 30.906169891357422 test_loss:409.3837585449219\n",
      "1899/3000 train_loss: 28.736879348754883 test_loss:403.0699462890625\n",
      "1900/3000 train_loss: 25.358844757080078 test_loss:369.2078857421875\n",
      "1901/3000 train_loss: 27.976882934570312 test_loss:346.26776123046875\n",
      "1902/3000 train_loss: 27.77819061279297 test_loss:310.539794921875\n",
      "1903/3000 train_loss: 23.26314353942871 test_loss:282.78594970703125\n",
      "1904/3000 train_loss: 21.86174774169922 test_loss:272.8619384765625\n",
      "1905/3000 train_loss: 19.65509605407715 test_loss:242.3707733154297\n",
      "1906/3000 train_loss: 20.940401077270508 test_loss:233.3676300048828\n",
      "1907/3000 train_loss: 22.981910705566406 test_loss:235.49380493164062\n",
      "1908/3000 train_loss: 23.612979888916016 test_loss:228.90440368652344\n",
      "1909/3000 train_loss: 23.648319244384766 test_loss:239.55316162109375\n",
      "1910/3000 train_loss: 23.775545120239258 test_loss:259.8052673339844\n",
      "1911/3000 train_loss: 24.103939056396484 test_loss:274.8894348144531\n",
      "1912/3000 train_loss: 25.219728469848633 test_loss:293.2320861816406\n",
      "1913/3000 train_loss: 20.898117065429688 test_loss:341.9007263183594\n",
      "1914/3000 train_loss: 21.446617126464844 test_loss:374.3173522949219\n",
      "1915/3000 train_loss: 21.75088882446289 test_loss:415.40191650390625\n",
      "1916/3000 train_loss: 25.066776275634766 test_loss:388.10028076171875\n",
      "1917/3000 train_loss: 21.829181671142578 test_loss:395.3786926269531\n",
      "1918/3000 train_loss: 27.143444061279297 test_loss:428.952392578125\n",
      "1919/3000 train_loss: 24.125534057617188 test_loss:372.6165771484375\n",
      "1920/3000 train_loss: 24.332849502563477 test_loss:390.3400573730469\n",
      "1921/3000 train_loss: 26.50083351135254 test_loss:382.7242736816406\n",
      "1922/3000 train_loss: 30.31734848022461 test_loss:369.9501647949219\n",
      "1923/3000 train_loss: 32.663997650146484 test_loss:371.86083984375\n",
      "1924/3000 train_loss: 28.10714340209961 test_loss:322.9429626464844\n",
      "1925/3000 train_loss: 28.71829605102539 test_loss:317.00897216796875\n",
      "1926/3000 train_loss: 25.3918399810791 test_loss:298.80633544921875\n",
      "1927/3000 train_loss: 26.17559242248535 test_loss:284.2768249511719\n",
      "1928/3000 train_loss: 24.594528198242188 test_loss:269.39154052734375\n",
      "1929/3000 train_loss: 27.83376693725586 test_loss:255.21495056152344\n",
      "1930/3000 train_loss: 22.39505386352539 test_loss:247.5605010986328\n",
      "1931/3000 train_loss: 27.889692306518555 test_loss:248.44717407226562\n",
      "1932/3000 train_loss: 25.214792251586914 test_loss:255.35769653320312\n",
      "1933/3000 train_loss: 23.51617431640625 test_loss:245.89761352539062\n",
      "1934/3000 train_loss: 22.712677001953125 test_loss:259.7064514160156\n",
      "1935/3000 train_loss: 23.944108963012695 test_loss:267.11004638671875\n",
      "1936/3000 train_loss: 23.217164993286133 test_loss:286.3204345703125\n",
      "1937/3000 train_loss: 22.691360473632812 test_loss:309.9619445800781\n",
      "1938/3000 train_loss: 20.019758224487305 test_loss:318.7103271484375\n",
      "1939/3000 train_loss: 20.81208038330078 test_loss:344.98272705078125\n",
      "1940/3000 train_loss: 22.782371520996094 test_loss:347.5651550292969\n",
      "1941/3000 train_loss: 20.201457977294922 test_loss:357.7561950683594\n",
      "1942/3000 train_loss: 19.31678009033203 test_loss:331.1246337890625\n",
      "1943/3000 train_loss: 19.40387725830078 test_loss:330.7186279296875\n",
      "1944/3000 train_loss: 17.818239212036133 test_loss:289.1100158691406\n",
      "1945/3000 train_loss: 17.907569885253906 test_loss:262.28546142578125\n",
      "1946/3000 train_loss: 18.54836082458496 test_loss:256.7674560546875\n",
      "1947/3000 train_loss: 18.842287063598633 test_loss:253.21151733398438\n",
      "1948/3000 train_loss: 16.910493850708008 test_loss:236.17559814453125\n",
      "1949/3000 train_loss: 17.84788703918457 test_loss:226.7484130859375\n",
      "1950/3000 train_loss: 20.90009307861328 test_loss:226.4821319580078\n",
      "1951/3000 train_loss: 20.6237850189209 test_loss:224.27011108398438\n",
      "1952/3000 train_loss: 21.513572692871094 test_loss:227.6812286376953\n",
      "1953/3000 train_loss: 23.243227005004883 test_loss:235.16456604003906\n",
      "1954/3000 train_loss: 23.843765258789062 test_loss:253.139892578125\n",
      "1955/3000 train_loss: 22.870447158813477 test_loss:270.0553894042969\n",
      "1956/3000 train_loss: 23.31427764892578 test_loss:302.50860595703125\n",
      "1957/3000 train_loss: 24.602285385131836 test_loss:293.9327392578125\n",
      "1958/3000 train_loss: 22.38686180114746 test_loss:345.05877685546875\n",
      "1959/3000 train_loss: 26.71854019165039 test_loss:345.2898864746094\n",
      "1960/3000 train_loss: 19.02935218811035 test_loss:335.60443115234375\n",
      "1961/3000 train_loss: 18.847543716430664 test_loss:349.7501525878906\n",
      "1962/3000 train_loss: 18.712608337402344 test_loss:330.7122802734375\n",
      "1963/3000 train_loss: 19.876121520996094 test_loss:321.5244445800781\n",
      "1964/3000 train_loss: 20.66781997680664 test_loss:333.3766784667969\n",
      "1965/3000 train_loss: 19.189266204833984 test_loss:311.8283386230469\n",
      "1966/3000 train_loss: 18.645837783813477 test_loss:275.23388671875\n",
      "1967/3000 train_loss: 18.256145477294922 test_loss:262.75238037109375\n",
      "1968/3000 train_loss: 18.221099853515625 test_loss:269.5248718261719\n",
      "1969/3000 train_loss: 17.04669761657715 test_loss:250.4992218017578\n",
      "1970/3000 train_loss: 17.51837921142578 test_loss:246.40814208984375\n",
      "1971/3000 train_loss: 17.091598510742188 test_loss:243.75064086914062\n",
      "1972/3000 train_loss: 17.172260284423828 test_loss:244.8624267578125\n",
      "1973/3000 train_loss: 17.17335319519043 test_loss:247.93617248535156\n",
      "1974/3000 train_loss: 17.176128387451172 test_loss:247.43704223632812\n",
      "1975/3000 train_loss: 16.66282081604004 test_loss:250.95069885253906\n",
      "1976/3000 train_loss: 19.2017879486084 test_loss:246.742919921875\n",
      "1977/3000 train_loss: 17.970632553100586 test_loss:256.4188537597656\n",
      "1978/3000 train_loss: 20.736934661865234 test_loss:256.3400573730469\n",
      "1979/3000 train_loss: 20.94773292541504 test_loss:260.66119384765625\n",
      "1980/3000 train_loss: 20.017196655273438 test_loss:287.5726318359375\n",
      "1981/3000 train_loss: 20.31813621520996 test_loss:309.2698669433594\n",
      "1982/3000 train_loss: 19.81316375732422 test_loss:355.8539733886719\n",
      "1983/3000 train_loss: 21.471057891845703 test_loss:373.1175231933594\n",
      "1984/3000 train_loss: 26.916542053222656 test_loss:375.47943115234375\n",
      "1985/3000 train_loss: 19.20519256591797 test_loss:425.2001647949219\n",
      "1986/3000 train_loss: 22.23541259765625 test_loss:415.64398193359375\n",
      "1987/3000 train_loss: 21.672149658203125 test_loss:440.0274353027344\n",
      "1988/3000 train_loss: 20.628337860107422 test_loss:459.4541931152344\n",
      "1989/3000 train_loss: 27.423744201660156 test_loss:427.23553466796875\n",
      "1990/3000 train_loss: 22.077592849731445 test_loss:436.4495849609375\n",
      "1991/3000 train_loss: 20.413646697998047 test_loss:401.527099609375\n",
      "1992/3000 train_loss: 20.459623336791992 test_loss:414.0830993652344\n",
      "1993/3000 train_loss: 21.175649642944336 test_loss:405.64300537109375\n",
      "1994/3000 train_loss: 20.436532974243164 test_loss:364.2497253417969\n",
      "1995/3000 train_loss: 21.123193740844727 test_loss:328.087890625\n",
      "1996/3000 train_loss: 21.13826560974121 test_loss:308.27392578125\n",
      "1997/3000 train_loss: 22.10931968688965 test_loss:289.3238525390625\n",
      "1998/3000 train_loss: 21.591358184814453 test_loss:283.575439453125\n",
      "1999/3000 train_loss: 21.25821304321289 test_loss:254.56756591796875\n",
      "2000/3000 train_loss: 21.081151962280273 test_loss:247.319580078125\n",
      "2001/3000 train_loss: 19.553922653198242 test_loss:238.39910888671875\n",
      "2002/3000 train_loss: 22.46550750732422 test_loss:250.83175659179688\n",
      "2003/3000 train_loss: 20.575483322143555 test_loss:265.24664306640625\n",
      "2004/3000 train_loss: 20.468294143676758 test_loss:280.9888000488281\n",
      "2005/3000 train_loss: 21.959300994873047 test_loss:303.0339050292969\n",
      "2006/3000 train_loss: 26.624065399169922 test_loss:275.1821594238281\n",
      "2007/3000 train_loss: 29.257497787475586 test_loss:313.2649230957031\n",
      "2008/3000 train_loss: 24.385215759277344 test_loss:337.2135314941406\n",
      "2009/3000 train_loss: 18.11532974243164 test_loss:374.56719970703125\n",
      "2010/3000 train_loss: 17.740074157714844 test_loss:392.983154296875\n",
      "2011/3000 train_loss: 18.73635482788086 test_loss:412.9658203125\n",
      "2012/3000 train_loss: 20.932016372680664 test_loss:414.6563720703125\n",
      "2013/3000 train_loss: 21.209877014160156 test_loss:403.5536193847656\n",
      "2014/3000 train_loss: 19.79105567932129 test_loss:451.8978271484375\n",
      "2015/3000 train_loss: 24.005512237548828 test_loss:422.7740783691406\n",
      "2016/3000 train_loss: 23.69049644470215 test_loss:442.85736083984375\n",
      "2017/3000 train_loss: 21.791255950927734 test_loss:422.22882080078125\n",
      "2018/3000 train_loss: 25.450902938842773 test_loss:424.7019958496094\n",
      "2019/3000 train_loss: 23.667638778686523 test_loss:400.4707336425781\n",
      "2020/3000 train_loss: 25.959623336791992 test_loss:368.927490234375\n",
      "2021/3000 train_loss: 23.462465286254883 test_loss:324.9486999511719\n",
      "2022/3000 train_loss: 25.47041893005371 test_loss:336.30267333984375\n",
      "2023/3000 train_loss: 20.855224609375 test_loss:321.1697082519531\n",
      "2024/3000 train_loss: 22.223745346069336 test_loss:290.9002990722656\n",
      "2025/3000 train_loss: 21.595251083374023 test_loss:272.5007629394531\n",
      "2026/3000 train_loss: 20.143465042114258 test_loss:252.66561889648438\n",
      "2027/3000 train_loss: 18.818565368652344 test_loss:249.809326171875\n",
      "2028/3000 train_loss: 20.425161361694336 test_loss:241.1295623779297\n",
      "2029/3000 train_loss: 21.54317283630371 test_loss:241.72354125976562\n",
      "2030/3000 train_loss: 21.752880096435547 test_loss:253.76150512695312\n",
      "2031/3000 train_loss: 22.979598999023438 test_loss:233.06578063964844\n",
      "2032/3000 train_loss: 24.128305435180664 test_loss:251.10552978515625\n",
      "2033/3000 train_loss: 23.85200309753418 test_loss:248.9901123046875\n",
      "2034/3000 train_loss: 26.67170524597168 test_loss:264.6824645996094\n",
      "2035/3000 train_loss: 25.805633544921875 test_loss:271.1383056640625\n",
      "2036/3000 train_loss: 27.359560012817383 test_loss:298.4619445800781\n",
      "2037/3000 train_loss: 27.61329460144043 test_loss:320.8258972167969\n",
      "2038/3000 train_loss: 29.64900779724121 test_loss:322.79425048828125\n",
      "2039/3000 train_loss: 26.694971084594727 test_loss:343.7700500488281\n",
      "2040/3000 train_loss: 29.893518447875977 test_loss:367.9906311035156\n",
      "2041/3000 train_loss: 26.06489372253418 test_loss:396.13226318359375\n",
      "2042/3000 train_loss: 27.86142349243164 test_loss:406.0938720703125\n",
      "2043/3000 train_loss: 23.582082748413086 test_loss:438.8650817871094\n",
      "2044/3000 train_loss: 25.718746185302734 test_loss:423.1370849609375\n",
      "2045/3000 train_loss: 25.680761337280273 test_loss:403.9835510253906\n",
      "2046/3000 train_loss: 25.015750885009766 test_loss:353.848388671875\n",
      "2047/3000 train_loss: 24.866798400878906 test_loss:344.48626708984375\n",
      "2048/3000 train_loss: 23.14712905883789 test_loss:313.01165771484375\n",
      "2049/3000 train_loss: 23.82951545715332 test_loss:283.1880798339844\n",
      "2050/3000 train_loss: 22.97877311706543 test_loss:282.9841003417969\n",
      "2051/3000 train_loss: 23.948448181152344 test_loss:277.7510681152344\n",
      "2052/3000 train_loss: 20.156089782714844 test_loss:266.1971740722656\n",
      "2053/3000 train_loss: 20.691940307617188 test_loss:266.8670959472656\n",
      "2054/3000 train_loss: 21.61766242980957 test_loss:261.47442626953125\n",
      "2055/3000 train_loss: 23.284774780273438 test_loss:261.67108154296875\n",
      "2056/3000 train_loss: 23.14841079711914 test_loss:267.0309143066406\n",
      "2057/3000 train_loss: 22.438840866088867 test_loss:280.7561950683594\n",
      "2058/3000 train_loss: 23.023452758789062 test_loss:321.2597961425781\n",
      "2059/3000 train_loss: 28.903324127197266 test_loss:306.406005859375\n",
      "2060/3000 train_loss: 22.089353561401367 test_loss:304.4589538574219\n",
      "2061/3000 train_loss: 20.425155639648438 test_loss:301.6011962890625\n",
      "2062/3000 train_loss: 17.403575897216797 test_loss:311.89349365234375\n",
      "2063/3000 train_loss: 16.411033630371094 test_loss:315.39483642578125\n",
      "2064/3000 train_loss: 14.9408597946167 test_loss:323.2383117675781\n",
      "2065/3000 train_loss: 15.28099250793457 test_loss:332.2105712890625\n",
      "2066/3000 train_loss: 15.414548873901367 test_loss:339.8501281738281\n",
      "2067/3000 train_loss: 15.621902465820312 test_loss:359.2292785644531\n",
      "2068/3000 train_loss: 15.739309310913086 test_loss:352.573974609375\n",
      "2069/3000 train_loss: 17.06334114074707 test_loss:348.13763427734375\n",
      "2070/3000 train_loss: 17.784414291381836 test_loss:345.879150390625\n",
      "2071/3000 train_loss: 19.05711555480957 test_loss:354.1927185058594\n",
      "2072/3000 train_loss: 18.020278930664062 test_loss:324.0576171875\n",
      "2073/3000 train_loss: 17.476295471191406 test_loss:309.9737243652344\n",
      "2074/3000 train_loss: 16.471473693847656 test_loss:301.50311279296875\n",
      "2075/3000 train_loss: 15.730977058410645 test_loss:296.0516662597656\n",
      "2076/3000 train_loss: 16.352439880371094 test_loss:289.10687255859375\n",
      "2077/3000 train_loss: 15.873486518859863 test_loss:274.10614013671875\n",
      "2078/3000 train_loss: 15.122776985168457 test_loss:258.8623962402344\n",
      "2079/3000 train_loss: 16.334774017333984 test_loss:259.0593566894531\n",
      "2080/3000 train_loss: 19.318819046020508 test_loss:288.1543884277344\n",
      "2081/3000 train_loss: 20.67627716064453 test_loss:281.2863464355469\n",
      "2082/3000 train_loss: 19.350234985351562 test_loss:292.904296875\n",
      "2083/3000 train_loss: 18.64515495300293 test_loss:313.710693359375\n",
      "2084/3000 train_loss: 19.782503128051758 test_loss:329.0273742675781\n",
      "2085/3000 train_loss: 21.75786018371582 test_loss:329.94708251953125\n",
      "2086/3000 train_loss: 19.491243362426758 test_loss:358.3956298828125\n",
      "2087/3000 train_loss: 20.60406494140625 test_loss:366.1661682128906\n",
      "2088/3000 train_loss: 20.13520622253418 test_loss:435.212646484375\n",
      "2089/3000 train_loss: 20.42983055114746 test_loss:424.8086853027344\n",
      "2090/3000 train_loss: 20.88227081298828 test_loss:414.56854248046875\n",
      "2091/3000 train_loss: 21.31103515625 test_loss:389.331787109375\n",
      "2092/3000 train_loss: 22.282197952270508 test_loss:402.56219482421875\n",
      "2093/3000 train_loss: 20.24017906188965 test_loss:403.7933654785156\n",
      "2094/3000 train_loss: 22.6726016998291 test_loss:380.6996154785156\n",
      "2095/3000 train_loss: 23.64948844909668 test_loss:371.8468017578125\n",
      "2096/3000 train_loss: 23.23043441772461 test_loss:344.9744567871094\n",
      "2097/3000 train_loss: 23.86349868774414 test_loss:341.4520263671875\n",
      "2098/3000 train_loss: 26.154733657836914 test_loss:329.3358154296875\n",
      "2099/3000 train_loss: 23.59984588623047 test_loss:315.38446044921875\n",
      "2100/3000 train_loss: 25.438493728637695 test_loss:311.98883056640625\n",
      "2101/3000 train_loss: 24.13112449645996 test_loss:317.2822265625\n",
      "2102/3000 train_loss: 26.468067169189453 test_loss:286.95361328125\n",
      "2103/3000 train_loss: 24.388729095458984 test_loss:283.554443359375\n",
      "2104/3000 train_loss: 24.61729621887207 test_loss:270.19873046875\n",
      "2105/3000 train_loss: 27.96483612060547 test_loss:257.4687194824219\n",
      "2106/3000 train_loss: 26.903783798217773 test_loss:247.9567413330078\n",
      "2107/3000 train_loss: 29.476045608520508 test_loss:265.7346496582031\n",
      "2108/3000 train_loss: 23.650775909423828 test_loss:245.85211181640625\n",
      "2109/3000 train_loss: 23.32122802734375 test_loss:234.66445922851562\n",
      "2110/3000 train_loss: 22.711767196655273 test_loss:232.64376831054688\n",
      "2111/3000 train_loss: 24.625158309936523 test_loss:227.6775360107422\n",
      "2112/3000 train_loss: 26.93939971923828 test_loss:225.50082397460938\n",
      "2113/3000 train_loss: 28.905092239379883 test_loss:231.11395263671875\n",
      "2114/3000 train_loss: 26.1090145111084 test_loss:233.5217742919922\n",
      "2115/3000 train_loss: 28.908781051635742 test_loss:225.88658142089844\n",
      "2116/3000 train_loss: 28.411083221435547 test_loss:250.56629943847656\n",
      "2117/3000 train_loss: 26.159364700317383 test_loss:263.8069152832031\n",
      "2118/3000 train_loss: 25.285327911376953 test_loss:298.6483154296875\n",
      "2119/3000 train_loss: 25.42801284790039 test_loss:329.31591796875\n",
      "2120/3000 train_loss: 25.29839324951172 test_loss:342.3238830566406\n",
      "2121/3000 train_loss: 24.733762741088867 test_loss:366.9323425292969\n",
      "2122/3000 train_loss: 23.84949493408203 test_loss:395.28680419921875\n",
      "2123/3000 train_loss: 25.319211959838867 test_loss:407.5146484375\n",
      "2124/3000 train_loss: 24.502403259277344 test_loss:449.96844482421875\n",
      "2125/3000 train_loss: 23.17108917236328 test_loss:465.28985595703125\n",
      "2126/3000 train_loss: 27.595932006835938 test_loss:416.60888671875\n",
      "2127/3000 train_loss: 23.199703216552734 test_loss:429.4805908203125\n",
      "2128/3000 train_loss: 27.014663696289062 test_loss:427.2295837402344\n",
      "2129/3000 train_loss: 26.723285675048828 test_loss:405.3864440917969\n",
      "2130/3000 train_loss: 28.10435676574707 test_loss:365.9917297363281\n",
      "2131/3000 train_loss: 25.973691940307617 test_loss:356.14166259765625\n",
      "2132/3000 train_loss: 25.639846801757812 test_loss:320.34014892578125\n",
      "2133/3000 train_loss: 25.173276901245117 test_loss:301.72515869140625\n",
      "2134/3000 train_loss: 22.30440902709961 test_loss:264.62158203125\n",
      "2135/3000 train_loss: 21.609298706054688 test_loss:253.11541748046875\n",
      "2136/3000 train_loss: 23.224939346313477 test_loss:253.6156463623047\n",
      "2137/3000 train_loss: 21.619672775268555 test_loss:245.7079315185547\n",
      "2138/3000 train_loss: 24.027982711791992 test_loss:242.10104370117188\n",
      "2139/3000 train_loss: 20.932525634765625 test_loss:251.72042846679688\n",
      "2140/3000 train_loss: 22.463146209716797 test_loss:245.34762573242188\n",
      "2141/3000 train_loss: 25.34819793701172 test_loss:254.4618377685547\n",
      "2142/3000 train_loss: 25.408254623413086 test_loss:273.284423828125\n",
      "2143/3000 train_loss: 22.99137306213379 test_loss:293.6108703613281\n",
      "2144/3000 train_loss: 24.45642852783203 test_loss:317.08428955078125\n",
      "2145/3000 train_loss: 24.918956756591797 test_loss:354.76995849609375\n",
      "2146/3000 train_loss: 24.17685890197754 test_loss:385.51300048828125\n",
      "2147/3000 train_loss: 24.99098014831543 test_loss:393.5579528808594\n",
      "2148/3000 train_loss: 23.173728942871094 test_loss:421.53076171875\n",
      "2149/3000 train_loss: 24.577878952026367 test_loss:426.9075622558594\n",
      "2150/3000 train_loss: 25.234498977661133 test_loss:410.28240966796875\n",
      "2151/3000 train_loss: 29.56041145324707 test_loss:392.098388671875\n",
      "2152/3000 train_loss: 26.925613403320312 test_loss:340.3670654296875\n",
      "2153/3000 train_loss: 25.651166915893555 test_loss:346.27099609375\n",
      "2154/3000 train_loss: 27.509675979614258 test_loss:307.7005310058594\n",
      "2155/3000 train_loss: 27.346839904785156 test_loss:292.5648498535156\n",
      "2156/3000 train_loss: 22.013626098632812 test_loss:254.90650939941406\n",
      "2157/3000 train_loss: 24.025651931762695 test_loss:251.39892578125\n",
      "2158/3000 train_loss: 23.155250549316406 test_loss:249.53065490722656\n",
      "2159/3000 train_loss: 22.346763610839844 test_loss:243.67039489746094\n",
      "2160/3000 train_loss: 24.084592819213867 test_loss:246.1649169921875\n",
      "2161/3000 train_loss: 20.96489906311035 test_loss:247.3096923828125\n",
      "2162/3000 train_loss: 19.794050216674805 test_loss:264.800537109375\n",
      "2163/3000 train_loss: 23.150043487548828 test_loss:277.7520751953125\n",
      "2164/3000 train_loss: 21.195581436157227 test_loss:299.7702941894531\n",
      "2165/3000 train_loss: 22.68083381652832 test_loss:336.85662841796875\n",
      "2166/3000 train_loss: 21.846750259399414 test_loss:364.17987060546875\n",
      "2167/3000 train_loss: 20.58601188659668 test_loss:391.9771423339844\n",
      "2168/3000 train_loss: 18.744001388549805 test_loss:400.9609375\n",
      "2169/3000 train_loss: 19.385025024414062 test_loss:382.89056396484375\n",
      "2170/3000 train_loss: 20.65785789489746 test_loss:377.8425598144531\n",
      "2171/3000 train_loss: 22.29364776611328 test_loss:360.9450988769531\n",
      "2172/3000 train_loss: 22.49271011352539 test_loss:306.6185302734375\n",
      "2173/3000 train_loss: 24.5994930267334 test_loss:269.097412109375\n",
      "2174/3000 train_loss: 24.494417190551758 test_loss:246.6999053955078\n",
      "2175/3000 train_loss: 22.68427085876465 test_loss:237.59762573242188\n",
      "2176/3000 train_loss: 22.381139755249023 test_loss:237.01014709472656\n",
      "2177/3000 train_loss: 22.31270408630371 test_loss:234.47872924804688\n",
      "2178/3000 train_loss: 20.697099685668945 test_loss:232.38368225097656\n",
      "2179/3000 train_loss: 23.5153865814209 test_loss:227.8993682861328\n",
      "2180/3000 train_loss: 22.557825088500977 test_loss:232.91445922851562\n",
      "2181/3000 train_loss: 22.98871612548828 test_loss:232.54666137695312\n",
      "2182/3000 train_loss: 24.63604736328125 test_loss:239.73208618164062\n",
      "2183/3000 train_loss: 22.615966796875 test_loss:271.53741455078125\n",
      "2184/3000 train_loss: 22.70819854736328 test_loss:284.2781677246094\n",
      "2185/3000 train_loss: 27.28021812438965 test_loss:300.3957214355469\n",
      "2186/3000 train_loss: 19.48371696472168 test_loss:321.1597595214844\n",
      "2187/3000 train_loss: 18.855754852294922 test_loss:326.97711181640625\n",
      "2188/3000 train_loss: 17.72804832458496 test_loss:369.0099182128906\n",
      "2189/3000 train_loss: 18.488889694213867 test_loss:360.5118713378906\n",
      "2190/3000 train_loss: 18.951723098754883 test_loss:354.2967529296875\n",
      "2191/3000 train_loss: 22.126907348632812 test_loss:344.88446044921875\n",
      "2192/3000 train_loss: 21.193159103393555 test_loss:315.2761535644531\n",
      "2193/3000 train_loss: 20.40030288696289 test_loss:290.4549865722656\n",
      "2194/3000 train_loss: 24.668243408203125 test_loss:282.7995910644531\n",
      "2195/3000 train_loss: 18.865154266357422 test_loss:253.1649169921875\n",
      "2196/3000 train_loss: 21.98883819580078 test_loss:243.02615356445312\n",
      "2197/3000 train_loss: 18.21236801147461 test_loss:237.20025634765625\n",
      "2198/3000 train_loss: 19.95880126953125 test_loss:249.9316864013672\n",
      "2199/3000 train_loss: 18.29561424255371 test_loss:243.500732421875\n",
      "2200/3000 train_loss: 20.184444427490234 test_loss:250.1832275390625\n",
      "2201/3000 train_loss: 21.821266174316406 test_loss:257.2313232421875\n",
      "2202/3000 train_loss: 22.01770782470703 test_loss:274.13531494140625\n",
      "2203/3000 train_loss: 20.988279342651367 test_loss:312.2918701171875\n",
      "2204/3000 train_loss: 25.194190979003906 test_loss:315.3521728515625\n",
      "2205/3000 train_loss: 20.923202514648438 test_loss:344.2727355957031\n",
      "2206/3000 train_loss: 22.512157440185547 test_loss:333.5748291015625\n",
      "2207/3000 train_loss: 25.839000701904297 test_loss:392.66302490234375\n",
      "2208/3000 train_loss: 23.2398624420166 test_loss:377.414306640625\n",
      "2209/3000 train_loss: 22.22522735595703 test_loss:424.11492919921875\n",
      "2210/3000 train_loss: 24.61241912841797 test_loss:429.5502014160156\n",
      "2211/3000 train_loss: 22.423601150512695 test_loss:413.66961669921875\n",
      "2212/3000 train_loss: 24.18414878845215 test_loss:401.4046936035156\n",
      "2213/3000 train_loss: 24.624042510986328 test_loss:384.975830078125\n",
      "2214/3000 train_loss: 24.230669021606445 test_loss:344.2696228027344\n",
      "2215/3000 train_loss: 21.102102279663086 test_loss:291.6125183105469\n",
      "2216/3000 train_loss: 19.537050247192383 test_loss:260.17034912109375\n",
      "2217/3000 train_loss: 20.855966567993164 test_loss:245.8650360107422\n",
      "2218/3000 train_loss: 18.70285415649414 test_loss:248.54464721679688\n",
      "2219/3000 train_loss: 21.77012825012207 test_loss:244.1135711669922\n",
      "2220/3000 train_loss: 22.7231502532959 test_loss:246.93551635742188\n",
      "2221/3000 train_loss: 22.825096130371094 test_loss:271.70318603515625\n",
      "2222/3000 train_loss: 20.535064697265625 test_loss:281.85943603515625\n",
      "2223/3000 train_loss: 22.07991600036621 test_loss:274.3827209472656\n",
      "2224/3000 train_loss: 20.50951385498047 test_loss:307.9974670410156\n",
      "2225/3000 train_loss: 23.12204360961914 test_loss:331.512939453125\n",
      "2226/3000 train_loss: 21.69023323059082 test_loss:387.39813232421875\n",
      "2227/3000 train_loss: 26.48261070251465 test_loss:417.378662109375\n",
      "2228/3000 train_loss: 26.239547729492188 test_loss:482.36016845703125\n",
      "2229/3000 train_loss: 21.022682189941406 test_loss:399.3722839355469\n",
      "2230/3000 train_loss: 22.527645111083984 test_loss:378.5468444824219\n",
      "2231/3000 train_loss: 19.746749877929688 test_loss:357.49456787109375\n",
      "2232/3000 train_loss: 21.675077438354492 test_loss:329.5006408691406\n",
      "2233/3000 train_loss: 22.041881561279297 test_loss:281.3065490722656\n",
      "2234/3000 train_loss: 22.762601852416992 test_loss:277.4480895996094\n",
      "2235/3000 train_loss: 21.66340446472168 test_loss:271.309814453125\n",
      "2236/3000 train_loss: 18.444499969482422 test_loss:250.5535125732422\n",
      "2237/3000 train_loss: 21.80805015563965 test_loss:237.4856414794922\n",
      "2238/3000 train_loss: 21.310091018676758 test_loss:226.81954956054688\n",
      "2239/3000 train_loss: 23.405719757080078 test_loss:223.2423858642578\n",
      "2240/3000 train_loss: 23.863367080688477 test_loss:225.30921936035156\n",
      "2241/3000 train_loss: 24.877737045288086 test_loss:222.18370056152344\n",
      "2242/3000 train_loss: 26.0408878326416 test_loss:223.8507537841797\n",
      "2243/3000 train_loss: 29.372716903686523 test_loss:243.28842163085938\n",
      "2244/3000 train_loss: 21.724763870239258 test_loss:272.28631591796875\n",
      "2245/3000 train_loss: 24.114370346069336 test_loss:292.6746520996094\n",
      "2246/3000 train_loss: 24.47274398803711 test_loss:328.1439514160156\n",
      "2247/3000 train_loss: 25.9829044342041 test_loss:348.2431945800781\n",
      "2248/3000 train_loss: 23.876689910888672 test_loss:392.5495910644531\n",
      "2249/3000 train_loss: 23.444320678710938 test_loss:366.4679870605469\n",
      "2250/3000 train_loss: 21.41632080078125 test_loss:439.0277404785156\n",
      "2251/3000 train_loss: 26.057418823242188 test_loss:426.6020202636719\n",
      "2252/3000 train_loss: 22.795263290405273 test_loss:404.06170654296875\n",
      "2253/3000 train_loss: 24.885984420776367 test_loss:390.486328125\n",
      "2254/3000 train_loss: 22.700050354003906 test_loss:429.950927734375\n",
      "2255/3000 train_loss: 26.974611282348633 test_loss:375.6612548828125\n",
      "2256/3000 train_loss: 22.786163330078125 test_loss:352.83453369140625\n",
      "2257/3000 train_loss: 25.991016387939453 test_loss:351.17645263671875\n",
      "2258/3000 train_loss: 26.732322692871094 test_loss:319.2550964355469\n",
      "2259/3000 train_loss: 27.67654037475586 test_loss:302.0111389160156\n",
      "2260/3000 train_loss: 26.803932189941406 test_loss:259.00775146484375\n",
      "2261/3000 train_loss: 29.26007843017578 test_loss:252.39791870117188\n",
      "2262/3000 train_loss: 22.242290496826172 test_loss:237.03025817871094\n",
      "2263/3000 train_loss: 25.709558486938477 test_loss:238.7621612548828\n",
      "2264/3000 train_loss: 23.78546142578125 test_loss:237.09011840820312\n",
      "2265/3000 train_loss: 26.10922622680664 test_loss:230.0784149169922\n",
      "2266/3000 train_loss: 24.684587478637695 test_loss:234.10462951660156\n",
      "2267/3000 train_loss: 27.617755889892578 test_loss:244.21865844726562\n",
      "2268/3000 train_loss: 27.572635650634766 test_loss:251.0895233154297\n",
      "2269/3000 train_loss: 27.691225051879883 test_loss:266.5780334472656\n",
      "2270/3000 train_loss: 29.358043670654297 test_loss:308.9552307128906\n",
      "2271/3000 train_loss: 32.589561462402344 test_loss:337.4193115234375\n",
      "2272/3000 train_loss: 24.676612854003906 test_loss:385.608642578125\n",
      "2273/3000 train_loss: 30.48612403869629 test_loss:423.4111328125\n",
      "2274/3000 train_loss: 29.681163787841797 test_loss:451.67364501953125\n",
      "2275/3000 train_loss: 26.215723037719727 test_loss:468.1663818359375\n",
      "2276/3000 train_loss: 28.001943588256836 test_loss:458.5190734863281\n",
      "2277/3000 train_loss: 24.320388793945312 test_loss:427.3666076660156\n",
      "2278/3000 train_loss: 24.42961311340332 test_loss:389.6688537597656\n",
      "2279/3000 train_loss: 22.736278533935547 test_loss:358.362060546875\n",
      "2280/3000 train_loss: 22.66889190673828 test_loss:329.475341796875\n",
      "2281/3000 train_loss: 21.960050582885742 test_loss:306.2901611328125\n",
      "2282/3000 train_loss: 22.11624526977539 test_loss:276.073486328125\n",
      "2283/3000 train_loss: 21.18437957763672 test_loss:257.7939147949219\n",
      "2284/3000 train_loss: 21.339330673217773 test_loss:245.52076721191406\n",
      "2285/3000 train_loss: 23.461339950561523 test_loss:246.09922790527344\n",
      "2286/3000 train_loss: 25.663042068481445 test_loss:246.4552001953125\n",
      "2287/3000 train_loss: 26.395708084106445 test_loss:250.83029174804688\n",
      "2288/3000 train_loss: 28.458417892456055 test_loss:260.223876953125\n",
      "2289/3000 train_loss: 28.08913230895996 test_loss:281.7705383300781\n",
      "2290/3000 train_loss: 27.547622680664062 test_loss:304.8543701171875\n",
      "2291/3000 train_loss: 23.365829467773438 test_loss:356.59893798828125\n",
      "2292/3000 train_loss: 25.767690658569336 test_loss:313.0710754394531\n",
      "2293/3000 train_loss: 21.310747146606445 test_loss:381.31884765625\n",
      "2294/3000 train_loss: 20.67454719543457 test_loss:420.7413635253906\n",
      "2295/3000 train_loss: 23.322999954223633 test_loss:387.7425537109375\n",
      "2296/3000 train_loss: 22.047607421875 test_loss:393.04150390625\n",
      "2297/3000 train_loss: 24.091432571411133 test_loss:359.7733154296875\n",
      "2298/3000 train_loss: 23.970252990722656 test_loss:338.421630859375\n",
      "2299/3000 train_loss: 25.084531784057617 test_loss:325.8694152832031\n",
      "2300/3000 train_loss: 24.20500946044922 test_loss:296.17218017578125\n",
      "2301/3000 train_loss: 27.98507308959961 test_loss:261.20428466796875\n",
      "2302/3000 train_loss: 22.995059967041016 test_loss:251.80409240722656\n",
      "2303/3000 train_loss: 20.198917388916016 test_loss:240.29209899902344\n",
      "2304/3000 train_loss: 23.017925262451172 test_loss:234.79345703125\n",
      "2305/3000 train_loss: 20.850074768066406 test_loss:231.71112060546875\n",
      "2306/3000 train_loss: 20.532094955444336 test_loss:244.26327514648438\n",
      "2307/3000 train_loss: 19.455150604248047 test_loss:229.72055053710938\n",
      "2308/3000 train_loss: 21.60459327697754 test_loss:245.73773193359375\n",
      "2309/3000 train_loss: 22.065576553344727 test_loss:249.83428955078125\n",
      "2310/3000 train_loss: 21.957359313964844 test_loss:269.7078857421875\n",
      "2311/3000 train_loss: 23.319517135620117 test_loss:308.0972900390625\n",
      "2312/3000 train_loss: 19.06012725830078 test_loss:346.26251220703125\n",
      "2313/3000 train_loss: 22.750864028930664 test_loss:382.0179443359375\n",
      "2314/3000 train_loss: 18.286285400390625 test_loss:376.6606750488281\n",
      "2315/3000 train_loss: 17.6879940032959 test_loss:415.218505859375\n",
      "2316/3000 train_loss: 19.27694320678711 test_loss:413.1251525878906\n",
      "2317/3000 train_loss: 20.075653076171875 test_loss:375.5286865234375\n",
      "2318/3000 train_loss: 20.33644676208496 test_loss:359.857177734375\n",
      "2319/3000 train_loss: 20.781383514404297 test_loss:355.6070556640625\n",
      "2320/3000 train_loss: 20.461393356323242 test_loss:348.0654296875\n",
      "2321/3000 train_loss: 19.654714584350586 test_loss:340.7782897949219\n",
      "2322/3000 train_loss: 21.01993179321289 test_loss:309.65484619140625\n",
      "2323/3000 train_loss: 21.818742752075195 test_loss:304.016357421875\n",
      "2324/3000 train_loss: 20.187162399291992 test_loss:285.6202697753906\n",
      "2325/3000 train_loss: 20.2183895111084 test_loss:268.6902770996094\n",
      "2326/3000 train_loss: 19.235008239746094 test_loss:256.29296875\n",
      "2327/3000 train_loss: 18.704687118530273 test_loss:248.5206756591797\n",
      "2328/3000 train_loss: 22.604572296142578 test_loss:247.34896850585938\n",
      "2329/3000 train_loss: 22.184436798095703 test_loss:252.23899841308594\n",
      "2330/3000 train_loss: 22.82037353515625 test_loss:256.60418701171875\n",
      "2331/3000 train_loss: 25.200027465820312 test_loss:277.6513671875\n",
      "2332/3000 train_loss: 26.411352157592773 test_loss:294.20941162109375\n",
      "2333/3000 train_loss: 26.93461036682129 test_loss:324.143310546875\n",
      "2334/3000 train_loss: 21.05044174194336 test_loss:360.1419372558594\n",
      "2335/3000 train_loss: 21.74761199951172 test_loss:423.53106689453125\n",
      "2336/3000 train_loss: 20.210601806640625 test_loss:463.75079345703125\n",
      "2337/3000 train_loss: 23.226388931274414 test_loss:492.14642333984375\n",
      "2338/3000 train_loss: 23.593900680541992 test_loss:470.94342041015625\n",
      "2339/3000 train_loss: 23.719253540039062 test_loss:465.7891540527344\n",
      "2340/3000 train_loss: 27.06744384765625 test_loss:445.46429443359375\n",
      "2341/3000 train_loss: 26.092458724975586 test_loss:366.8233642578125\n",
      "2342/3000 train_loss: 22.40852928161621 test_loss:318.2667236328125\n",
      "2343/3000 train_loss: 24.585317611694336 test_loss:300.8744812011719\n",
      "2344/3000 train_loss: 24.546649932861328 test_loss:273.5226135253906\n",
      "2345/3000 train_loss: 23.068578720092773 test_loss:254.87086486816406\n",
      "2346/3000 train_loss: 22.477069854736328 test_loss:235.07748413085938\n",
      "2347/3000 train_loss: 21.83370590209961 test_loss:238.47557067871094\n",
      "2348/3000 train_loss: 24.631385803222656 test_loss:228.71038818359375\n",
      "2349/3000 train_loss: 26.820798873901367 test_loss:225.85606384277344\n",
      "2350/3000 train_loss: 26.700973510742188 test_loss:228.2574005126953\n",
      "2351/3000 train_loss: 22.777774810791016 test_loss:227.3053741455078\n",
      "2352/3000 train_loss: 22.53822898864746 test_loss:234.30886840820312\n",
      "2353/3000 train_loss: 24.306318283081055 test_loss:252.21849060058594\n",
      "2354/3000 train_loss: 23.25983428955078 test_loss:280.0520324707031\n",
      "2355/3000 train_loss: 26.21370506286621 test_loss:309.971923828125\n",
      "2356/3000 train_loss: 25.889501571655273 test_loss:334.88275146484375\n",
      "2357/3000 train_loss: 23.396923065185547 test_loss:359.4779968261719\n",
      "2358/3000 train_loss: 24.902103424072266 test_loss:337.68865966796875\n",
      "2359/3000 train_loss: 21.368146896362305 test_loss:376.9635009765625\n",
      "2360/3000 train_loss: 20.48897933959961 test_loss:394.2172546386719\n",
      "2361/3000 train_loss: 22.39232635498047 test_loss:380.2506408691406\n",
      "2362/3000 train_loss: 20.19493293762207 test_loss:349.4410095214844\n",
      "2363/3000 train_loss: 22.019147872924805 test_loss:344.54833984375\n",
      "2364/3000 train_loss: 20.198768615722656 test_loss:307.0945739746094\n",
      "2365/3000 train_loss: 19.772357940673828 test_loss:282.6187744140625\n",
      "2366/3000 train_loss: 20.018789291381836 test_loss:266.3333435058594\n",
      "2367/3000 train_loss: 18.02220916748047 test_loss:250.47308349609375\n",
      "2368/3000 train_loss: 19.81661605834961 test_loss:238.25350952148438\n",
      "2369/3000 train_loss: 21.852745056152344 test_loss:231.69239807128906\n",
      "2370/3000 train_loss: 21.319210052490234 test_loss:230.5174560546875\n",
      "2371/3000 train_loss: 23.11256980895996 test_loss:226.089599609375\n",
      "2372/3000 train_loss: 20.277496337890625 test_loss:221.55670166015625\n",
      "2373/3000 train_loss: 23.49892807006836 test_loss:231.28451538085938\n",
      "2374/3000 train_loss: 20.830469131469727 test_loss:230.4592742919922\n",
      "2375/3000 train_loss: 21.892396926879883 test_loss:243.2460479736328\n",
      "2376/3000 train_loss: 20.590295791625977 test_loss:260.473388671875\n",
      "2377/3000 train_loss: 21.203243255615234 test_loss:303.74383544921875\n",
      "2378/3000 train_loss: 22.64339828491211 test_loss:369.5669250488281\n",
      "2379/3000 train_loss: 24.40997886657715 test_loss:385.16595458984375\n",
      "2380/3000 train_loss: 27.262615203857422 test_loss:371.7086181640625\n",
      "2381/3000 train_loss: 22.05902099609375 test_loss:375.5157470703125\n",
      "2382/3000 train_loss: 20.775278091430664 test_loss:442.224365234375\n",
      "2383/3000 train_loss: 24.63717269897461 test_loss:437.72515869140625\n",
      "2384/3000 train_loss: 24.094934463500977 test_loss:399.2961730957031\n",
      "2385/3000 train_loss: 21.73920440673828 test_loss:409.027099609375\n",
      "2386/3000 train_loss: 22.58013153076172 test_loss:365.9749755859375\n",
      "2387/3000 train_loss: 23.04854393005371 test_loss:336.6033935546875\n",
      "2388/3000 train_loss: 21.94764518737793 test_loss:290.5792236328125\n",
      "2389/3000 train_loss: 19.637786865234375 test_loss:266.24090576171875\n",
      "2390/3000 train_loss: 21.250118255615234 test_loss:244.42611694335938\n",
      "2391/3000 train_loss: 20.595149993896484 test_loss:239.56765747070312\n",
      "2392/3000 train_loss: 18.94028663635254 test_loss:221.71676635742188\n",
      "2393/3000 train_loss: 21.129505157470703 test_loss:220.7584228515625\n",
      "2394/3000 train_loss: 20.078397750854492 test_loss:233.0338592529297\n",
      "2395/3000 train_loss: 19.93075180053711 test_loss:232.540283203125\n",
      "2396/3000 train_loss: 16.7473087310791 test_loss:233.58721923828125\n",
      "2397/3000 train_loss: 21.65486717224121 test_loss:233.3197021484375\n",
      "2398/3000 train_loss: 20.862396240234375 test_loss:260.7704162597656\n",
      "2399/3000 train_loss: 20.0916748046875 test_loss:287.9522399902344\n",
      "2400/3000 train_loss: 19.211292266845703 test_loss:300.9068603515625\n",
      "2401/3000 train_loss: 16.54416275024414 test_loss:328.76763916015625\n",
      "2402/3000 train_loss: 16.827451705932617 test_loss:361.9797058105469\n",
      "2403/3000 train_loss: 16.450857162475586 test_loss:391.1394348144531\n",
      "2404/3000 train_loss: 18.45441246032715 test_loss:377.53948974609375\n",
      "2405/3000 train_loss: 19.09463882446289 test_loss:336.382080078125\n",
      "2406/3000 train_loss: 17.99528694152832 test_loss:340.2956848144531\n",
      "2407/3000 train_loss: 19.754423141479492 test_loss:335.5147399902344\n",
      "2408/3000 train_loss: 17.210601806640625 test_loss:297.4335632324219\n",
      "2409/3000 train_loss: 20.734254837036133 test_loss:286.1297607421875\n",
      "2410/3000 train_loss: 20.705989837646484 test_loss:283.1092224121094\n",
      "2411/3000 train_loss: 20.24759864807129 test_loss:271.38812255859375\n",
      "2412/3000 train_loss: 16.631107330322266 test_loss:261.4432678222656\n",
      "2413/3000 train_loss: 17.953325271606445 test_loss:247.52566528320312\n",
      "2414/3000 train_loss: 15.394819259643555 test_loss:238.419921875\n",
      "2415/3000 train_loss: 17.311153411865234 test_loss:240.41131591796875\n",
      "2416/3000 train_loss: 19.342607498168945 test_loss:231.63658142089844\n",
      "2417/3000 train_loss: 17.362300872802734 test_loss:220.742919921875\n",
      "2418/3000 train_loss: 19.096158981323242 test_loss:232.18325805664062\n",
      "2419/3000 train_loss: 20.44932746887207 test_loss:252.9974365234375\n",
      "2420/3000 train_loss: 22.06084442138672 test_loss:261.3045654296875\n",
      "2421/3000 train_loss: 19.869129180908203 test_loss:262.4730224609375\n",
      "2422/3000 train_loss: 22.519716262817383 test_loss:269.8313293457031\n",
      "2423/3000 train_loss: 23.60921287536621 test_loss:319.9749755859375\n",
      "2424/3000 train_loss: 22.637767791748047 test_loss:288.3283386230469\n",
      "2425/3000 train_loss: 22.04764747619629 test_loss:304.8717956542969\n",
      "2426/3000 train_loss: 19.003023147583008 test_loss:316.8277282714844\n",
      "2427/3000 train_loss: 18.124177932739258 test_loss:360.810791015625\n",
      "2428/3000 train_loss: 20.339895248413086 test_loss:362.8386535644531\n",
      "2429/3000 train_loss: 21.983823776245117 test_loss:379.3722229003906\n",
      "2430/3000 train_loss: 19.099308013916016 test_loss:427.0011291503906\n",
      "2431/3000 train_loss: 21.08284568786621 test_loss:385.24530029296875\n",
      "2432/3000 train_loss: 18.763240814208984 test_loss:386.9507751464844\n",
      "2433/3000 train_loss: 20.924686431884766 test_loss:405.5400085449219\n",
      "2434/3000 train_loss: 23.847578048706055 test_loss:367.7397766113281\n",
      "2435/3000 train_loss: 21.550893783569336 test_loss:331.78790283203125\n",
      "2436/3000 train_loss: 21.451114654541016 test_loss:343.08843994140625\n",
      "2437/3000 train_loss: 25.20272445678711 test_loss:322.5461120605469\n",
      "2438/3000 train_loss: 20.733684539794922 test_loss:298.2095031738281\n",
      "2439/3000 train_loss: 19.377466201782227 test_loss:292.6888732910156\n",
      "2440/3000 train_loss: 17.35883903503418 test_loss:263.6416320800781\n",
      "2441/3000 train_loss: 18.03703498840332 test_loss:254.3884735107422\n",
      "2442/3000 train_loss: 19.5048770904541 test_loss:246.75912475585938\n",
      "2443/3000 train_loss: 19.882535934448242 test_loss:232.12506103515625\n",
      "2444/3000 train_loss: 18.691736221313477 test_loss:231.11781311035156\n",
      "2445/3000 train_loss: 19.761098861694336 test_loss:232.06692504882812\n",
      "2446/3000 train_loss: 17.644607543945312 test_loss:229.9182891845703\n",
      "2447/3000 train_loss: 18.34609031677246 test_loss:231.89479064941406\n",
      "2448/3000 train_loss: 20.45296859741211 test_loss:231.1009979248047\n",
      "2449/3000 train_loss: 18.499902725219727 test_loss:234.71694946289062\n",
      "2450/3000 train_loss: 22.865867614746094 test_loss:232.30384826660156\n",
      "2451/3000 train_loss: 18.704164505004883 test_loss:253.8104705810547\n",
      "2452/3000 train_loss: 20.342554092407227 test_loss:250.41143798828125\n",
      "2453/3000 train_loss: 20.585067749023438 test_loss:263.0606384277344\n",
      "2454/3000 train_loss: 17.196821212768555 test_loss:293.00714111328125\n",
      "2455/3000 train_loss: 20.050363540649414 test_loss:295.28582763671875\n",
      "2456/3000 train_loss: 18.43316078186035 test_loss:319.66162109375\n",
      "2457/3000 train_loss: 19.081445693969727 test_loss:337.36639404296875\n",
      "2458/3000 train_loss: 19.172840118408203 test_loss:343.8083190917969\n",
      "2459/3000 train_loss: 20.123743057250977 test_loss:361.1131591796875\n",
      "2460/3000 train_loss: 19.027463912963867 test_loss:419.1946105957031\n",
      "2461/3000 train_loss: 20.94199562072754 test_loss:406.1937255859375\n",
      "2462/3000 train_loss: 20.661022186279297 test_loss:388.42266845703125\n",
      "2463/3000 train_loss: 19.998685836791992 test_loss:378.1433410644531\n",
      "2464/3000 train_loss: 20.788776397705078 test_loss:360.59869384765625\n",
      "2465/3000 train_loss: 21.598421096801758 test_loss:332.592529296875\n",
      "2466/3000 train_loss: 21.88644027709961 test_loss:304.9573974609375\n",
      "2467/3000 train_loss: 23.562347412109375 test_loss:283.40277099609375\n",
      "2468/3000 train_loss: 20.953136444091797 test_loss:264.8340759277344\n",
      "2469/3000 train_loss: 20.362571716308594 test_loss:247.06155395507812\n",
      "2470/3000 train_loss: 24.282182693481445 test_loss:244.11109924316406\n",
      "2471/3000 train_loss: 20.655553817749023 test_loss:235.8173828125\n",
      "2472/3000 train_loss: 23.48534393310547 test_loss:231.0763702392578\n",
      "2473/3000 train_loss: 20.018108367919922 test_loss:221.2210693359375\n",
      "2474/3000 train_loss: 21.29610824584961 test_loss:218.48098754882812\n",
      "2475/3000 train_loss: 22.036972045898438 test_loss:215.9305877685547\n",
      "2476/3000 train_loss: 20.524620056152344 test_loss:227.8408966064453\n",
      "2477/3000 train_loss: 22.329666137695312 test_loss:226.46226501464844\n",
      "2478/3000 train_loss: 23.20163345336914 test_loss:243.3086395263672\n",
      "2479/3000 train_loss: 22.687332153320312 test_loss:270.87689208984375\n",
      "2480/3000 train_loss: 23.21957015991211 test_loss:288.54827880859375\n",
      "2481/3000 train_loss: 24.01788330078125 test_loss:302.3758850097656\n",
      "2482/3000 train_loss: 21.96767807006836 test_loss:358.197021484375\n",
      "2483/3000 train_loss: 25.297555923461914 test_loss:373.41558837890625\n",
      "2484/3000 train_loss: 21.266218185424805 test_loss:380.64373779296875\n",
      "2485/3000 train_loss: 24.249366760253906 test_loss:440.2960510253906\n",
      "2486/3000 train_loss: 26.048246383666992 test_loss:486.58380126953125\n",
      "2487/3000 train_loss: 24.917221069335938 test_loss:438.2007141113281\n",
      "2488/3000 train_loss: 23.83930206298828 test_loss:446.0057678222656\n",
      "2489/3000 train_loss: 25.628427505493164 test_loss:438.8041687011719\n",
      "2490/3000 train_loss: 24.699798583984375 test_loss:396.16131591796875\n",
      "2491/3000 train_loss: 30.06410026550293 test_loss:381.2986755371094\n",
      "2492/3000 train_loss: 26.73581314086914 test_loss:319.1601867675781\n",
      "2493/3000 train_loss: 22.490089416503906 test_loss:289.7802429199219\n",
      "2494/3000 train_loss: 23.336769104003906 test_loss:264.89324951171875\n",
      "2495/3000 train_loss: 22.328760147094727 test_loss:245.2400665283203\n",
      "2496/3000 train_loss: 21.61949348449707 test_loss:231.9052276611328\n",
      "2497/3000 train_loss: 27.18027687072754 test_loss:223.671630859375\n",
      "2498/3000 train_loss: 23.835729598999023 test_loss:223.41046142578125\n",
      "2499/3000 train_loss: 24.133804321289062 test_loss:229.02117919921875\n",
      "2500/3000 train_loss: 24.508769989013672 test_loss:219.01251220703125\n",
      "2501/3000 train_loss: 25.81005096435547 test_loss:228.1844482421875\n",
      "2502/3000 train_loss: 24.79268455505371 test_loss:239.27520751953125\n",
      "2503/3000 train_loss: 23.392955780029297 test_loss:254.18505859375\n",
      "2504/3000 train_loss: 23.239356994628906 test_loss:268.5513610839844\n",
      "2505/3000 train_loss: 23.236833572387695 test_loss:298.88287353515625\n",
      "2506/3000 train_loss: 27.290876388549805 test_loss:321.189208984375\n",
      "2507/3000 train_loss: 20.550859451293945 test_loss:314.50054931640625\n",
      "2508/3000 train_loss: 20.6460018157959 test_loss:371.60791015625\n",
      "2509/3000 train_loss: 25.604482650756836 test_loss:424.7778015136719\n",
      "2510/3000 train_loss: 21.704843521118164 test_loss:406.7532043457031\n",
      "2511/3000 train_loss: 26.9365234375 test_loss:377.38153076171875\n",
      "2512/3000 train_loss: 22.851673126220703 test_loss:385.1891174316406\n",
      "2513/3000 train_loss: 25.46805191040039 test_loss:382.7701110839844\n",
      "2514/3000 train_loss: 24.524898529052734 test_loss:355.7824401855469\n",
      "2515/3000 train_loss: 26.448686599731445 test_loss:328.77264404296875\n",
      "2516/3000 train_loss: 25.201181411743164 test_loss:282.59857177734375\n",
      "2517/3000 train_loss: 29.383563995361328 test_loss:249.40753173828125\n",
      "2518/3000 train_loss: 27.067047119140625 test_loss:248.7742919921875\n",
      "2519/3000 train_loss: 25.314422607421875 test_loss:251.14768981933594\n",
      "2520/3000 train_loss: 22.94902801513672 test_loss:229.9473876953125\n",
      "2521/3000 train_loss: 23.242504119873047 test_loss:227.21441650390625\n",
      "2522/3000 train_loss: 25.25067710876465 test_loss:224.64999389648438\n",
      "2523/3000 train_loss: 24.40697479248047 test_loss:220.6846160888672\n",
      "2524/3000 train_loss: 24.26751708984375 test_loss:229.74240112304688\n",
      "2525/3000 train_loss: 23.033201217651367 test_loss:235.50689697265625\n",
      "2526/3000 train_loss: 23.475414276123047 test_loss:271.12823486328125\n",
      "2527/3000 train_loss: 22.74542236328125 test_loss:323.49896240234375\n",
      "2528/3000 train_loss: 23.98109245300293 test_loss:373.0289001464844\n",
      "2529/3000 train_loss: 21.267438888549805 test_loss:363.1586608886719\n",
      "2530/3000 train_loss: 18.863107681274414 test_loss:392.1153564453125\n",
      "2531/3000 train_loss: 21.015108108520508 test_loss:418.35845947265625\n",
      "2532/3000 train_loss: 18.060466766357422 test_loss:400.2373046875\n",
      "2533/3000 train_loss: 19.64596939086914 test_loss:371.5252990722656\n",
      "2534/3000 train_loss: 21.282957077026367 test_loss:323.9507141113281\n",
      "2535/3000 train_loss: 20.985464096069336 test_loss:267.4184265136719\n",
      "2536/3000 train_loss: 21.817007064819336 test_loss:227.6277313232422\n",
      "2537/3000 train_loss: 22.175539016723633 test_loss:237.92532348632812\n",
      "2538/3000 train_loss: 22.85761070251465 test_loss:234.88816833496094\n",
      "2539/3000 train_loss: 19.78669548034668 test_loss:219.95706176757812\n",
      "2540/3000 train_loss: 22.350358963012695 test_loss:245.33328247070312\n",
      "2541/3000 train_loss: 21.71785545349121 test_loss:241.1649169921875\n",
      "2542/3000 train_loss: 22.97896957397461 test_loss:277.9807434082031\n",
      "2543/3000 train_loss: 20.94924545288086 test_loss:296.40972900390625\n",
      "2544/3000 train_loss: 22.783466339111328 test_loss:310.2900695800781\n",
      "2545/3000 train_loss: 22.003957748413086 test_loss:338.2225646972656\n",
      "2546/3000 train_loss: 25.315731048583984 test_loss:340.9669494628906\n",
      "2547/3000 train_loss: 20.448225021362305 test_loss:367.6388854980469\n",
      "2548/3000 train_loss: 20.855224609375 test_loss:408.396484375\n",
      "2549/3000 train_loss: 21.731657028198242 test_loss:444.1577453613281\n",
      "2550/3000 train_loss: 22.126644134521484 test_loss:404.04254150390625\n",
      "2551/3000 train_loss: 23.531803131103516 test_loss:427.3780212402344\n",
      "2552/3000 train_loss: 22.993408203125 test_loss:425.2350769042969\n",
      "2553/3000 train_loss: 21.416852951049805 test_loss:380.4010009765625\n",
      "2554/3000 train_loss: 24.301441192626953 test_loss:334.7789611816406\n",
      "2555/3000 train_loss: 22.860557556152344 test_loss:317.0498962402344\n",
      "2556/3000 train_loss: 23.964885711669922 test_loss:297.4954833984375\n",
      "2557/3000 train_loss: 21.521923065185547 test_loss:269.23065185546875\n",
      "2558/3000 train_loss: 21.701187133789062 test_loss:248.427734375\n",
      "2559/3000 train_loss: 24.014196395874023 test_loss:238.6409912109375\n",
      "2560/3000 train_loss: 21.76942253112793 test_loss:233.93057250976562\n",
      "2561/3000 train_loss: 24.410959243774414 test_loss:233.18544006347656\n",
      "2562/3000 train_loss: 24.03716278076172 test_loss:247.66061401367188\n",
      "2563/3000 train_loss: 22.52348518371582 test_loss:254.46875\n",
      "2564/3000 train_loss: 20.719865798950195 test_loss:296.9688415527344\n",
      "2565/3000 train_loss: 21.638275146484375 test_loss:332.84002685546875\n",
      "2566/3000 train_loss: 21.795921325683594 test_loss:354.6663818359375\n",
      "2567/3000 train_loss: 25.01709747314453 test_loss:351.8042297363281\n",
      "2568/3000 train_loss: 22.25712013244629 test_loss:399.0538330078125\n",
      "2569/3000 train_loss: 19.975679397583008 test_loss:415.48553466796875\n",
      "2570/3000 train_loss: 17.595970153808594 test_loss:397.8973693847656\n",
      "2571/3000 train_loss: 24.004722595214844 test_loss:423.90557861328125\n",
      "2572/3000 train_loss: 21.228116989135742 test_loss:381.3909912109375\n",
      "2573/3000 train_loss: 23.398958206176758 test_loss:353.44903564453125\n",
      "2574/3000 train_loss: 23.355127334594727 test_loss:347.4191589355469\n",
      "2575/3000 train_loss: 21.4964656829834 test_loss:304.3695983886719\n",
      "2576/3000 train_loss: 24.257814407348633 test_loss:284.2159423828125\n",
      "2577/3000 train_loss: 20.786020278930664 test_loss:266.29766845703125\n",
      "2578/3000 train_loss: 21.529232025146484 test_loss:238.46998596191406\n",
      "2579/3000 train_loss: 22.732742309570312 test_loss:239.20480346679688\n",
      "2580/3000 train_loss: 20.578584671020508 test_loss:229.1874237060547\n",
      "2581/3000 train_loss: 22.331798553466797 test_loss:223.72793579101562\n",
      "2582/3000 train_loss: 24.001012802124023 test_loss:237.80419921875\n",
      "2583/3000 train_loss: 22.404848098754883 test_loss:244.68753051757812\n",
      "2584/3000 train_loss: 23.903945922851562 test_loss:265.8182373046875\n",
      "2585/3000 train_loss: 23.257951736450195 test_loss:299.2308654785156\n",
      "2586/3000 train_loss: 23.726118087768555 test_loss:321.87408447265625\n",
      "2587/3000 train_loss: 22.983179092407227 test_loss:351.5494689941406\n",
      "2588/3000 train_loss: 22.23906135559082 test_loss:404.6848449707031\n",
      "2589/3000 train_loss: 22.393814086914062 test_loss:402.175537109375\n",
      "2590/3000 train_loss: 23.3253173828125 test_loss:417.1002502441406\n",
      "2591/3000 train_loss: 24.272850036621094 test_loss:384.09222412109375\n",
      "2592/3000 train_loss: 21.669103622436523 test_loss:336.734619140625\n",
      "2593/3000 train_loss: 21.473615646362305 test_loss:329.6978759765625\n",
      "2594/3000 train_loss: 20.558319091796875 test_loss:313.9197998046875\n",
      "2595/3000 train_loss: 20.89752769470215 test_loss:302.87298583984375\n",
      "2596/3000 train_loss: 20.617488861083984 test_loss:253.5076446533203\n",
      "2597/3000 train_loss: 18.96116828918457 test_loss:243.23638916015625\n",
      "2598/3000 train_loss: 21.849166870117188 test_loss:236.3553466796875\n",
      "2599/3000 train_loss: 22.75678062438965 test_loss:224.4593963623047\n",
      "2600/3000 train_loss: 21.259349822998047 test_loss:227.5963592529297\n",
      "2601/3000 train_loss: 22.91270637512207 test_loss:222.09719848632812\n",
      "2602/3000 train_loss: 25.076128005981445 test_loss:228.65586853027344\n",
      "2603/3000 train_loss: 25.422887802124023 test_loss:243.61798095703125\n",
      "2604/3000 train_loss: 27.154911041259766 test_loss:253.13616943359375\n",
      "2605/3000 train_loss: 24.04035758972168 test_loss:293.68597412109375\n",
      "2606/3000 train_loss: 25.418397903442383 test_loss:317.972900390625\n",
      "2607/3000 train_loss: 27.572614669799805 test_loss:346.8249206542969\n",
      "2608/3000 train_loss: 22.064899444580078 test_loss:388.678955078125\n",
      "2609/3000 train_loss: 21.836292266845703 test_loss:402.8476257324219\n",
      "2610/3000 train_loss: 23.561845779418945 test_loss:435.73370361328125\n",
      "2611/3000 train_loss: 21.76826286315918 test_loss:425.0863342285156\n",
      "2612/3000 train_loss: 23.080476760864258 test_loss:430.5963134765625\n",
      "2613/3000 train_loss: 25.18431854248047 test_loss:400.1740417480469\n",
      "2614/3000 train_loss: 25.160438537597656 test_loss:409.1220703125\n",
      "2615/3000 train_loss: 22.82221794128418 test_loss:338.8767395019531\n",
      "2616/3000 train_loss: 23.746408462524414 test_loss:333.32147216796875\n",
      "2617/3000 train_loss: 19.81976318359375 test_loss:281.048583984375\n",
      "2618/3000 train_loss: 19.1724853515625 test_loss:253.8996124267578\n",
      "2619/3000 train_loss: 19.76032257080078 test_loss:248.19625854492188\n",
      "2620/3000 train_loss: 20.57400131225586 test_loss:237.4134521484375\n",
      "2621/3000 train_loss: 18.125322341918945 test_loss:226.17247009277344\n",
      "2622/3000 train_loss: 16.49701499938965 test_loss:249.56509399414062\n",
      "2623/3000 train_loss: 18.96832847595215 test_loss:243.7832794189453\n",
      "2624/3000 train_loss: 17.09181022644043 test_loss:255.14895629882812\n",
      "2625/3000 train_loss: 18.070384979248047 test_loss:258.9768371582031\n",
      "2626/3000 train_loss: 18.703596115112305 test_loss:275.89019775390625\n",
      "2627/3000 train_loss: 17.15290641784668 test_loss:316.3117370605469\n",
      "2628/3000 train_loss: 18.572816848754883 test_loss:340.3072204589844\n",
      "2629/3000 train_loss: 18.696533203125 test_loss:382.3128356933594\n",
      "2630/3000 train_loss: 17.22289276123047 test_loss:386.0906677246094\n",
      "2631/3000 train_loss: 21.253847122192383 test_loss:380.21588134765625\n",
      "2632/3000 train_loss: 18.71612548828125 test_loss:405.122314453125\n",
      "2633/3000 train_loss: 20.636409759521484 test_loss:388.69830322265625\n",
      "2634/3000 train_loss: 21.11037254333496 test_loss:379.9748840332031\n",
      "2635/3000 train_loss: 20.069061279296875 test_loss:351.548095703125\n",
      "2636/3000 train_loss: 19.775596618652344 test_loss:326.1000671386719\n",
      "2637/3000 train_loss: 20.6224365234375 test_loss:310.8831787109375\n",
      "2638/3000 train_loss: 19.078861236572266 test_loss:301.6754150390625\n",
      "2639/3000 train_loss: 20.955799102783203 test_loss:276.268798828125\n",
      "2640/3000 train_loss: 19.257247924804688 test_loss:264.0550231933594\n",
      "2641/3000 train_loss: 22.056398391723633 test_loss:259.2159729003906\n",
      "2642/3000 train_loss: 20.381488800048828 test_loss:252.72622680664062\n",
      "2643/3000 train_loss: 19.391490936279297 test_loss:249.8175811767578\n",
      "2644/3000 train_loss: 19.06488800048828 test_loss:253.40145874023438\n",
      "2645/3000 train_loss: 20.210975646972656 test_loss:267.34637451171875\n",
      "2646/3000 train_loss: 19.788169860839844 test_loss:262.9527282714844\n",
      "2647/3000 train_loss: 19.7690372467041 test_loss:261.44830322265625\n",
      "2648/3000 train_loss: 19.79245376586914 test_loss:270.7966003417969\n",
      "2649/3000 train_loss: 20.51552391052246 test_loss:273.89312744140625\n",
      "2650/3000 train_loss: 20.11556625366211 test_loss:277.6716613769531\n",
      "2651/3000 train_loss: 18.828083038330078 test_loss:320.2611389160156\n",
      "2652/3000 train_loss: 19.125791549682617 test_loss:375.776611328125\n",
      "2653/3000 train_loss: 18.255346298217773 test_loss:408.7139587402344\n",
      "2654/3000 train_loss: 19.931915283203125 test_loss:418.40191650390625\n",
      "2655/3000 train_loss: 21.118328094482422 test_loss:435.9140625\n",
      "2656/3000 train_loss: 20.434499740600586 test_loss:403.4742736816406\n",
      "2657/3000 train_loss: 22.71458625793457 test_loss:367.29595947265625\n",
      "2658/3000 train_loss: 22.341737747192383 test_loss:366.92083740234375\n",
      "2659/3000 train_loss: 19.54798126220703 test_loss:303.6447448730469\n",
      "2660/3000 train_loss: 22.10990333557129 test_loss:298.2313537597656\n",
      "2661/3000 train_loss: 21.95722770690918 test_loss:274.7049255371094\n",
      "2662/3000 train_loss: 20.812658309936523 test_loss:262.40899658203125\n",
      "2663/3000 train_loss: 21.111116409301758 test_loss:251.99813842773438\n",
      "2664/3000 train_loss: 18.486005783081055 test_loss:249.44847106933594\n",
      "2665/3000 train_loss: 21.116018295288086 test_loss:238.50497436523438\n",
      "2666/3000 train_loss: 22.369794845581055 test_loss:224.8814697265625\n",
      "2667/3000 train_loss: 22.084123611450195 test_loss:235.1559600830078\n",
      "2668/3000 train_loss: 21.8255615234375 test_loss:229.84799194335938\n",
      "2669/3000 train_loss: 21.649982452392578 test_loss:224.856689453125\n",
      "2670/3000 train_loss: 25.5745792388916 test_loss:222.19650268554688\n",
      "2671/3000 train_loss: 22.948558807373047 test_loss:232.56448364257812\n",
      "2672/3000 train_loss: 25.277095794677734 test_loss:246.07421875\n",
      "2673/3000 train_loss: 25.102550506591797 test_loss:264.9199523925781\n",
      "2674/3000 train_loss: 26.68543243408203 test_loss:300.3009033203125\n",
      "2675/3000 train_loss: 27.353660583496094 test_loss:336.63238525390625\n",
      "2676/3000 train_loss: 26.173322677612305 test_loss:356.9560546875\n",
      "2677/3000 train_loss: 27.25130844116211 test_loss:363.93804931640625\n",
      "2678/3000 train_loss: 24.467864990234375 test_loss:395.0656433105469\n",
      "2679/3000 train_loss: 24.786958694458008 test_loss:440.18170166015625\n",
      "2680/3000 train_loss: 25.745967864990234 test_loss:398.0386657714844\n",
      "2681/3000 train_loss: 23.318761825561523 test_loss:385.3947448730469\n",
      "2682/3000 train_loss: 22.40457534790039 test_loss:315.0090637207031\n",
      "2683/3000 train_loss: 22.477144241333008 test_loss:352.6300354003906\n",
      "2684/3000 train_loss: 19.420469284057617 test_loss:302.7618408203125\n",
      "2685/3000 train_loss: 21.056747436523438 test_loss:300.6297607421875\n",
      "2686/3000 train_loss: 18.966224670410156 test_loss:282.40380859375\n",
      "2687/3000 train_loss: 18.674291610717773 test_loss:272.6004943847656\n",
      "2688/3000 train_loss: 19.486337661743164 test_loss:247.783203125\n",
      "2689/3000 train_loss: 17.66582489013672 test_loss:240.83392333984375\n",
      "2690/3000 train_loss: 18.208343505859375 test_loss:248.60630798339844\n",
      "2691/3000 train_loss: 17.995813369750977 test_loss:233.85543823242188\n",
      "2692/3000 train_loss: 21.11373519897461 test_loss:237.56591796875\n",
      "2693/3000 train_loss: 18.549396514892578 test_loss:240.6746826171875\n",
      "2694/3000 train_loss: 20.67076301574707 test_loss:240.6956024169922\n",
      "2695/3000 train_loss: 20.246219635009766 test_loss:259.1820068359375\n",
      "2696/3000 train_loss: 19.675865173339844 test_loss:270.8644714355469\n",
      "2697/3000 train_loss: 21.582509994506836 test_loss:285.6176452636719\n",
      "2698/3000 train_loss: 19.45195198059082 test_loss:309.43194580078125\n",
      "2699/3000 train_loss: 19.72088623046875 test_loss:347.2969055175781\n",
      "2700/3000 train_loss: 22.989137649536133 test_loss:358.68310546875\n",
      "2701/3000 train_loss: 21.86028289794922 test_loss:404.6383056640625\n",
      "2702/3000 train_loss: 22.658710479736328 test_loss:413.1095886230469\n",
      "2703/3000 train_loss: 20.663869857788086 test_loss:402.02789306640625\n",
      "2704/3000 train_loss: 20.704978942871094 test_loss:402.4384765625\n",
      "2705/3000 train_loss: 22.593626022338867 test_loss:374.31634521484375\n",
      "2706/3000 train_loss: 23.97239112854004 test_loss:381.3765869140625\n",
      "2707/3000 train_loss: 20.62116813659668 test_loss:325.3175354003906\n",
      "2708/3000 train_loss: 21.66302490234375 test_loss:325.0929260253906\n",
      "2709/3000 train_loss: 21.538530349731445 test_loss:297.498046875\n",
      "2710/3000 train_loss: 21.093801498413086 test_loss:283.22528076171875\n",
      "2711/3000 train_loss: 18.69265365600586 test_loss:259.91387939453125\n",
      "2712/3000 train_loss: 21.088560104370117 test_loss:272.3174743652344\n",
      "2713/3000 train_loss: 16.94600486755371 test_loss:256.06298828125\n",
      "2714/3000 train_loss: 18.408863067626953 test_loss:259.12945556640625\n",
      "2715/3000 train_loss: 17.06720733642578 test_loss:251.04302978515625\n",
      "2716/3000 train_loss: 18.292150497436523 test_loss:264.5919189453125\n",
      "2717/3000 train_loss: 19.018604278564453 test_loss:284.3931579589844\n",
      "2718/3000 train_loss: 19.930917739868164 test_loss:271.75146484375\n",
      "2719/3000 train_loss: 18.934412002563477 test_loss:314.30181884765625\n",
      "2720/3000 train_loss: 19.410968780517578 test_loss:329.0738220214844\n",
      "2721/3000 train_loss: 19.242387771606445 test_loss:326.4164123535156\n",
      "2722/3000 train_loss: 20.15849494934082 test_loss:398.66107177734375\n",
      "2723/3000 train_loss: 18.82186508178711 test_loss:400.7286071777344\n",
      "2724/3000 train_loss: 20.262388229370117 test_loss:438.5729675292969\n",
      "2725/3000 train_loss: 22.439491271972656 test_loss:399.27166748046875\n",
      "2726/3000 train_loss: 18.36701011657715 test_loss:413.2840270996094\n",
      "2727/3000 train_loss: 20.959192276000977 test_loss:384.26678466796875\n",
      "2728/3000 train_loss: 18.899993896484375 test_loss:375.6096496582031\n",
      "2729/3000 train_loss: 20.74175262451172 test_loss:316.08575439453125\n",
      "2730/3000 train_loss: 21.359209060668945 test_loss:286.4182434082031\n",
      "2731/3000 train_loss: 20.214256286621094 test_loss:254.55032348632812\n",
      "2732/3000 train_loss: 22.83852195739746 test_loss:251.4982452392578\n",
      "2733/3000 train_loss: 21.519760131835938 test_loss:252.37677001953125\n",
      "2734/3000 train_loss: 23.356298446655273 test_loss:248.2957305908203\n",
      "2735/3000 train_loss: 24.416181564331055 test_loss:245.1580352783203\n",
      "2736/3000 train_loss: 26.209848403930664 test_loss:253.7583465576172\n",
      "2737/3000 train_loss: 27.072425842285156 test_loss:251.91111755371094\n",
      "2738/3000 train_loss: 23.929468154907227 test_loss:234.86618041992188\n",
      "2739/3000 train_loss: 26.611984252929688 test_loss:253.00779724121094\n",
      "2740/3000 train_loss: 27.488100051879883 test_loss:252.60789489746094\n",
      "2741/3000 train_loss: 25.988080978393555 test_loss:272.3459777832031\n",
      "2742/3000 train_loss: 26.30396842956543 test_loss:292.0311584472656\n",
      "2743/3000 train_loss: 29.223609924316406 test_loss:300.5598449707031\n",
      "2744/3000 train_loss: 24.55039405822754 test_loss:345.5833435058594\n",
      "2745/3000 train_loss: 28.357494354248047 test_loss:374.5981750488281\n",
      "2746/3000 train_loss: 26.864974975585938 test_loss:375.4796447753906\n",
      "2747/3000 train_loss: 24.610279083251953 test_loss:409.1230773925781\n",
      "2748/3000 train_loss: 25.02042579650879 test_loss:423.97454833984375\n",
      "2749/3000 train_loss: 24.795148849487305 test_loss:451.8902587890625\n",
      "2750/3000 train_loss: 30.468971252441406 test_loss:414.6361083984375\n",
      "2751/3000 train_loss: 25.738527297973633 test_loss:376.0010070800781\n",
      "2752/3000 train_loss: 22.777109146118164 test_loss:357.08673095703125\n",
      "2753/3000 train_loss: 26.033689498901367 test_loss:386.59326171875\n",
      "2754/3000 train_loss: 24.742582321166992 test_loss:333.9896545410156\n",
      "2755/3000 train_loss: 24.01702880859375 test_loss:308.1350402832031\n",
      "2756/3000 train_loss: 23.702592849731445 test_loss:296.1907043457031\n",
      "2757/3000 train_loss: 21.056201934814453 test_loss:270.6741943359375\n",
      "2758/3000 train_loss: 21.64569854736328 test_loss:249.85269165039062\n",
      "2759/3000 train_loss: 21.737260818481445 test_loss:238.40948486328125\n",
      "2760/3000 train_loss: 22.842735290527344 test_loss:246.10047912597656\n",
      "2761/3000 train_loss: 22.319786071777344 test_loss:220.13938903808594\n",
      "2762/3000 train_loss: 19.952999114990234 test_loss:235.0691680908203\n",
      "2763/3000 train_loss: 18.982622146606445 test_loss:227.4776611328125\n",
      "2764/3000 train_loss: 19.405855178833008 test_loss:227.23681640625\n",
      "2765/3000 train_loss: 21.056062698364258 test_loss:228.63006591796875\n",
      "2766/3000 train_loss: 22.375917434692383 test_loss:230.14300537109375\n",
      "2767/3000 train_loss: 23.844593048095703 test_loss:232.39633178710938\n",
      "2768/3000 train_loss: 22.951459884643555 test_loss:237.27639770507812\n",
      "2769/3000 train_loss: 25.73461151123047 test_loss:255.5743408203125\n",
      "2770/3000 train_loss: 21.675161361694336 test_loss:286.0222473144531\n",
      "2771/3000 train_loss: 22.066110610961914 test_loss:341.5408935546875\n",
      "2772/3000 train_loss: 22.382854461669922 test_loss:372.6556396484375\n",
      "2773/3000 train_loss: 27.234207153320312 test_loss:406.53289794921875\n",
      "2774/3000 train_loss: 20.67397117614746 test_loss:388.7818603515625\n",
      "2775/3000 train_loss: 25.405963897705078 test_loss:383.8058776855469\n",
      "2776/3000 train_loss: 21.71810531616211 test_loss:413.7949523925781\n",
      "2777/3000 train_loss: 24.03514289855957 test_loss:383.41888427734375\n",
      "2778/3000 train_loss: 23.755165100097656 test_loss:397.46826171875\n",
      "2779/3000 train_loss: 23.04399871826172 test_loss:379.4671630859375\n",
      "2780/3000 train_loss: 24.09334945678711 test_loss:351.6257019042969\n",
      "2781/3000 train_loss: 21.813987731933594 test_loss:295.0695495605469\n",
      "2782/3000 train_loss: 19.319568634033203 test_loss:258.883056640625\n",
      "2783/3000 train_loss: 20.481945037841797 test_loss:244.65809631347656\n",
      "2784/3000 train_loss: 20.709264755249023 test_loss:236.49407958984375\n",
      "2785/3000 train_loss: 18.743202209472656 test_loss:224.79437255859375\n",
      "2786/3000 train_loss: 18.885143280029297 test_loss:224.50399780273438\n",
      "2787/3000 train_loss: 21.006698608398438 test_loss:226.26795959472656\n",
      "2788/3000 train_loss: 19.933439254760742 test_loss:232.58016967773438\n",
      "2789/3000 train_loss: 21.955669403076172 test_loss:238.34490966796875\n",
      "2790/3000 train_loss: 21.899459838867188 test_loss:257.7784729003906\n",
      "2791/3000 train_loss: 21.65557098388672 test_loss:260.9847412109375\n",
      "2792/3000 train_loss: 23.55175018310547 test_loss:278.7967224121094\n",
      "2793/3000 train_loss: 26.37691307067871 test_loss:321.94921875\n",
      "2794/3000 train_loss: 21.972492218017578 test_loss:358.808349609375\n",
      "2795/3000 train_loss: 23.377553939819336 test_loss:408.06561279296875\n",
      "2796/3000 train_loss: 23.067611694335938 test_loss:411.1886901855469\n",
      "2797/3000 train_loss: 23.391613006591797 test_loss:451.5745849609375\n",
      "2798/3000 train_loss: 23.215614318847656 test_loss:427.11932373046875\n",
      "2799/3000 train_loss: 25.050296783447266 test_loss:419.8520202636719\n",
      "2800/3000 train_loss: 26.582733154296875 test_loss:387.87445068359375\n",
      "2801/3000 train_loss: 22.742155075073242 test_loss:347.7108459472656\n",
      "2802/3000 train_loss: 21.99495506286621 test_loss:303.31707763671875\n",
      "2803/3000 train_loss: 22.204242706298828 test_loss:289.2095642089844\n",
      "2804/3000 train_loss: 19.709592819213867 test_loss:263.38861083984375\n",
      "2805/3000 train_loss: 19.425750732421875 test_loss:246.42562866210938\n",
      "2806/3000 train_loss: 18.814796447753906 test_loss:237.2003631591797\n",
      "2807/3000 train_loss: 18.11805534362793 test_loss:237.91184997558594\n",
      "2808/3000 train_loss: 18.44708824157715 test_loss:252.90646362304688\n",
      "2809/3000 train_loss: 20.130184173583984 test_loss:241.87356567382812\n",
      "2810/3000 train_loss: 19.894248962402344 test_loss:277.5681457519531\n",
      "2811/3000 train_loss: 22.270719528198242 test_loss:286.6588439941406\n",
      "2812/3000 train_loss: 22.425994873046875 test_loss:321.0003967285156\n",
      "2813/3000 train_loss: 22.283016204833984 test_loss:339.48272705078125\n",
      "2814/3000 train_loss: 24.1920108795166 test_loss:357.23004150390625\n",
      "2815/3000 train_loss: 24.68195343017578 test_loss:342.01873779296875\n",
      "2816/3000 train_loss: 15.978778839111328 test_loss:326.9320983886719\n",
      "2817/3000 train_loss: 14.902310371398926 test_loss:343.01708984375\n",
      "2818/3000 train_loss: 14.766897201538086 test_loss:357.77435302734375\n",
      "2819/3000 train_loss: 15.39168930053711 test_loss:343.11798095703125\n",
      "2820/3000 train_loss: 17.591947555541992 test_loss:346.5152587890625\n",
      "2821/3000 train_loss: 16.2368106842041 test_loss:352.80126953125\n",
      "2822/3000 train_loss: 17.956233978271484 test_loss:336.8858337402344\n",
      "2823/3000 train_loss: 18.858383178710938 test_loss:312.19781494140625\n",
      "2824/3000 train_loss: 19.914674758911133 test_loss:287.68353271484375\n",
      "2825/3000 train_loss: 18.96863555908203 test_loss:282.11260986328125\n",
      "2826/3000 train_loss: 19.243196487426758 test_loss:274.8185119628906\n",
      "2827/3000 train_loss: 16.2471981048584 test_loss:265.7352600097656\n",
      "2828/3000 train_loss: 20.838682174682617 test_loss:264.1517028808594\n",
      "2829/3000 train_loss: 16.438940048217773 test_loss:263.6159362792969\n",
      "2830/3000 train_loss: 19.205598831176758 test_loss:262.5299987792969\n",
      "2831/3000 train_loss: 16.472036361694336 test_loss:258.8580017089844\n",
      "2832/3000 train_loss: 16.968887329101562 test_loss:259.7930603027344\n",
      "2833/3000 train_loss: 18.496747970581055 test_loss:257.7950744628906\n",
      "2834/3000 train_loss: 17.254213333129883 test_loss:284.43609619140625\n",
      "2835/3000 train_loss: 19.096162796020508 test_loss:293.7569885253906\n",
      "2836/3000 train_loss: 18.811695098876953 test_loss:331.56695556640625\n",
      "2837/3000 train_loss: 20.607519149780273 test_loss:361.1832580566406\n",
      "2838/3000 train_loss: 18.775989532470703 test_loss:368.53271484375\n",
      "2839/3000 train_loss: 19.229909896850586 test_loss:405.3569641113281\n",
      "2840/3000 train_loss: 17.7195987701416 test_loss:425.13226318359375\n",
      "2841/3000 train_loss: 19.641427993774414 test_loss:421.3218078613281\n",
      "2842/3000 train_loss: 19.69540786743164 test_loss:408.95648193359375\n",
      "2843/3000 train_loss: 19.1568660736084 test_loss:394.0950622558594\n",
      "2844/3000 train_loss: 23.84658432006836 test_loss:407.654541015625\n",
      "2845/3000 train_loss: 21.368816375732422 test_loss:380.8536376953125\n",
      "2846/3000 train_loss: 21.318798065185547 test_loss:359.7872619628906\n",
      "2847/3000 train_loss: 21.893138885498047 test_loss:328.87945556640625\n",
      "2848/3000 train_loss: 19.36016845703125 test_loss:284.9942932128906\n",
      "2849/3000 train_loss: 23.847301483154297 test_loss:278.6283874511719\n",
      "2850/3000 train_loss: 20.223461151123047 test_loss:262.9940490722656\n",
      "2851/3000 train_loss: 20.213150024414062 test_loss:254.4194793701172\n",
      "2852/3000 train_loss: 21.583396911621094 test_loss:242.33792114257812\n",
      "2853/3000 train_loss: 20.85068130493164 test_loss:245.3325958251953\n",
      "2854/3000 train_loss: 22.59136962890625 test_loss:248.10179138183594\n",
      "2855/3000 train_loss: 19.449039459228516 test_loss:253.50230407714844\n",
      "2856/3000 train_loss: 20.968177795410156 test_loss:247.51498413085938\n",
      "2857/3000 train_loss: 22.17742919921875 test_loss:236.13723754882812\n",
      "2858/3000 train_loss: 24.541595458984375 test_loss:261.66693115234375\n",
      "2859/3000 train_loss: 21.744176864624023 test_loss:258.5581359863281\n",
      "2860/3000 train_loss: 23.32059097290039 test_loss:280.6214904785156\n",
      "2861/3000 train_loss: 20.38977813720703 test_loss:312.5047302246094\n",
      "2862/3000 train_loss: 22.87758445739746 test_loss:325.1430969238281\n",
      "2863/3000 train_loss: 23.805509567260742 test_loss:347.5419006347656\n",
      "2864/3000 train_loss: 20.900400161743164 test_loss:384.4503479003906\n",
      "2865/3000 train_loss: 18.824247360229492 test_loss:377.24151611328125\n",
      "2866/3000 train_loss: 22.57640266418457 test_loss:400.578369140625\n",
      "2867/3000 train_loss: 21.58232879638672 test_loss:352.0919189453125\n",
      "2868/3000 train_loss: 18.331214904785156 test_loss:357.72802734375\n",
      "2869/3000 train_loss: 20.47321891784668 test_loss:337.6646423339844\n",
      "2870/3000 train_loss: 19.984514236450195 test_loss:290.3702392578125\n",
      "2871/3000 train_loss: 19.927093505859375 test_loss:311.89361572265625\n",
      "2872/3000 train_loss: 16.633081436157227 test_loss:269.0230712890625\n",
      "2873/3000 train_loss: 16.83930206298828 test_loss:267.2252502441406\n",
      "2874/3000 train_loss: 15.911677360534668 test_loss:247.5666961669922\n",
      "2875/3000 train_loss: 17.965755462646484 test_loss:236.55409240722656\n",
      "2876/3000 train_loss: 17.831897735595703 test_loss:248.5158233642578\n",
      "2877/3000 train_loss: 21.360694885253906 test_loss:239.75599670410156\n",
      "2878/3000 train_loss: 19.457639694213867 test_loss:246.96969604492188\n",
      "2879/3000 train_loss: 19.805845260620117 test_loss:254.62774658203125\n",
      "2880/3000 train_loss: 19.41408920288086 test_loss:267.3326416015625\n",
      "2881/3000 train_loss: 21.6155948638916 test_loss:289.01092529296875\n",
      "2882/3000 train_loss: 23.31987762451172 test_loss:291.39251708984375\n",
      "2883/3000 train_loss: 19.05925941467285 test_loss:338.8531494140625\n",
      "2884/3000 train_loss: 18.540197372436523 test_loss:351.49005126953125\n",
      "2885/3000 train_loss: 21.990385055541992 test_loss:372.8826904296875\n",
      "2886/3000 train_loss: 19.965206146240234 test_loss:412.2060546875\n",
      "2887/3000 train_loss: 22.56374168395996 test_loss:405.7127685546875\n",
      "2888/3000 train_loss: 20.579137802124023 test_loss:396.5581970214844\n",
      "2889/3000 train_loss: 18.709014892578125 test_loss:347.3072814941406\n",
      "2890/3000 train_loss: 19.860706329345703 test_loss:340.3559265136719\n",
      "2891/3000 train_loss: 18.835430145263672 test_loss:308.74810791015625\n",
      "2892/3000 train_loss: 19.482757568359375 test_loss:306.4267578125\n",
      "2893/3000 train_loss: 17.929285049438477 test_loss:294.4044189453125\n",
      "2894/3000 train_loss: 19.883039474487305 test_loss:272.3515625\n",
      "2895/3000 train_loss: 21.707351684570312 test_loss:259.1133117675781\n",
      "2896/3000 train_loss: 18.558780670166016 test_loss:244.88720703125\n",
      "2897/3000 train_loss: 21.37696647644043 test_loss:236.33050537109375\n",
      "2898/3000 train_loss: 20.555517196655273 test_loss:232.51344299316406\n",
      "2899/3000 train_loss: 18.82231330871582 test_loss:233.8708038330078\n",
      "2900/3000 train_loss: 19.416088104248047 test_loss:225.36215209960938\n",
      "2901/3000 train_loss: 19.05660057067871 test_loss:218.08421325683594\n",
      "2902/3000 train_loss: 19.037494659423828 test_loss:240.43255615234375\n",
      "2903/3000 train_loss: 19.092910766601562 test_loss:234.466552734375\n",
      "2904/3000 train_loss: 20.976621627807617 test_loss:252.479248046875\n",
      "2905/3000 train_loss: 20.514965057373047 test_loss:254.27847290039062\n",
      "2906/3000 train_loss: 14.580187797546387 test_loss:270.22943115234375\n",
      "2907/3000 train_loss: 13.472774505615234 test_loss:298.71270751953125\n",
      "2908/3000 train_loss: 13.810506820678711 test_loss:303.1933898925781\n",
      "2909/3000 train_loss: 15.363348960876465 test_loss:328.1129150390625\n",
      "2910/3000 train_loss: 14.003253936767578 test_loss:358.3074951171875\n",
      "2911/3000 train_loss: 14.625877380371094 test_loss:389.0560302734375\n",
      "2912/3000 train_loss: 18.54591178894043 test_loss:421.2738342285156\n",
      "2913/3000 train_loss: 18.13433074951172 test_loss:398.44842529296875\n",
      "2914/3000 train_loss: 17.78908920288086 test_loss:410.3959045410156\n",
      "2915/3000 train_loss: 22.268739700317383 test_loss:366.7666015625\n",
      "2916/3000 train_loss: 16.57942771911621 test_loss:345.8905944824219\n",
      "2917/3000 train_loss: 17.938581466674805 test_loss:325.3755187988281\n",
      "2918/3000 train_loss: 20.333826065063477 test_loss:327.1745910644531\n",
      "2919/3000 train_loss: 19.283597946166992 test_loss:302.61651611328125\n",
      "2920/3000 train_loss: 18.62943458557129 test_loss:272.9658203125\n",
      "2921/3000 train_loss: 18.174196243286133 test_loss:250.70651245117188\n",
      "2922/3000 train_loss: 16.62179183959961 test_loss:242.70579528808594\n",
      "2923/3000 train_loss: 18.28588104248047 test_loss:246.84954833984375\n",
      "2924/3000 train_loss: 17.817548751831055 test_loss:240.12673950195312\n",
      "2925/3000 train_loss: 16.813156127929688 test_loss:253.166259765625\n",
      "2926/3000 train_loss: 17.627201080322266 test_loss:249.49429321289062\n",
      "2927/3000 train_loss: 18.46229362487793 test_loss:253.083984375\n",
      "2928/3000 train_loss: 18.869487762451172 test_loss:255.3291015625\n",
      "2929/3000 train_loss: 18.02902603149414 test_loss:259.40802001953125\n",
      "2930/3000 train_loss: 18.54178237915039 test_loss:293.7168273925781\n",
      "2931/3000 train_loss: 18.931840896606445 test_loss:296.4808349609375\n",
      "2932/3000 train_loss: 19.160057067871094 test_loss:318.3600769042969\n",
      "2933/3000 train_loss: 21.027767181396484 test_loss:364.2545166015625\n",
      "2934/3000 train_loss: 22.498807907104492 test_loss:372.83685302734375\n",
      "2935/3000 train_loss: 23.11726188659668 test_loss:375.31146240234375\n",
      "2936/3000 train_loss: 23.978290557861328 test_loss:414.7350769042969\n",
      "2937/3000 train_loss: 23.948694229125977 test_loss:409.190673828125\n",
      "2938/3000 train_loss: 24.80411148071289 test_loss:462.72552490234375\n",
      "2939/3000 train_loss: 24.444814682006836 test_loss:439.2179260253906\n",
      "2940/3000 train_loss: 25.50414276123047 test_loss:476.10125732421875\n",
      "2941/3000 train_loss: 23.51040267944336 test_loss:470.7342529296875\n",
      "2942/3000 train_loss: 24.442026138305664 test_loss:454.3811340332031\n",
      "2943/3000 train_loss: 25.40068817138672 test_loss:411.40350341796875\n",
      "2944/3000 train_loss: 21.50857162475586 test_loss:364.7599182128906\n",
      "2945/3000 train_loss: 22.001203536987305 test_loss:357.0780029296875\n",
      "2946/3000 train_loss: 23.055335998535156 test_loss:343.07867431640625\n",
      "2947/3000 train_loss: 19.55497169494629 test_loss:301.5453186035156\n",
      "2948/3000 train_loss: 20.379867553710938 test_loss:288.1408386230469\n",
      "2949/3000 train_loss: 23.66034507751465 test_loss:290.56787109375\n",
      "2950/3000 train_loss: 20.38469123840332 test_loss:283.67071533203125\n",
      "2951/3000 train_loss: 18.41266441345215 test_loss:264.455810546875\n",
      "2952/3000 train_loss: 20.540401458740234 test_loss:256.3062438964844\n",
      "2953/3000 train_loss: 21.897743225097656 test_loss:254.57257080078125\n",
      "2954/3000 train_loss: 22.63508415222168 test_loss:248.30950927734375\n",
      "2955/3000 train_loss: 20.733013153076172 test_loss:254.26145935058594\n",
      "2956/3000 train_loss: 21.125099182128906 test_loss:251.30105590820312\n",
      "2957/3000 train_loss: 21.822656631469727 test_loss:260.9521179199219\n",
      "2958/3000 train_loss: 24.46118927001953 test_loss:287.36627197265625\n",
      "2959/3000 train_loss: 21.031557083129883 test_loss:295.6047668457031\n",
      "2960/3000 train_loss: 19.976808547973633 test_loss:324.5340576171875\n",
      "2961/3000 train_loss: 19.84707260131836 test_loss:354.5384521484375\n",
      "2962/3000 train_loss: 20.098068237304688 test_loss:360.5091247558594\n",
      "2963/3000 train_loss: 18.595680236816406 test_loss:353.87213134765625\n",
      "2964/3000 train_loss: 18.80567169189453 test_loss:399.0892333984375\n",
      "2965/3000 train_loss: 19.328256607055664 test_loss:401.3144226074219\n",
      "2966/3000 train_loss: 20.777679443359375 test_loss:345.7011413574219\n",
      "2967/3000 train_loss: 23.318552017211914 test_loss:344.771240234375\n",
      "2968/3000 train_loss: 18.5460205078125 test_loss:292.7181701660156\n",
      "2969/3000 train_loss: 19.05072784423828 test_loss:291.90570068359375\n",
      "2970/3000 train_loss: 17.47848129272461 test_loss:281.4168701171875\n",
      "2971/3000 train_loss: 16.86278533935547 test_loss:270.24664306640625\n",
      "2972/3000 train_loss: 17.025991439819336 test_loss:267.3642883300781\n",
      "2973/3000 train_loss: 17.573076248168945 test_loss:256.5615539550781\n",
      "2974/3000 train_loss: 18.802696228027344 test_loss:251.9267578125\n",
      "2975/3000 train_loss: 19.443225860595703 test_loss:262.38031005859375\n",
      "2976/3000 train_loss: 18.859464645385742 test_loss:259.40667724609375\n",
      "2977/3000 train_loss: 20.107452392578125 test_loss:261.9226379394531\n",
      "2978/3000 train_loss: 23.863910675048828 test_loss:271.8315124511719\n",
      "2979/3000 train_loss: 17.48276138305664 test_loss:295.855712890625\n",
      "2980/3000 train_loss: 19.954513549804688 test_loss:316.6101379394531\n",
      "2981/3000 train_loss: 21.720008850097656 test_loss:331.58038330078125\n",
      "2982/3000 train_loss: 17.96005630493164 test_loss:379.66571044921875\n",
      "2983/3000 train_loss: 20.725284576416016 test_loss:388.86602783203125\n",
      "2984/3000 train_loss: 19.106918334960938 test_loss:417.0289611816406\n",
      "2985/3000 train_loss: 20.34016990661621 test_loss:417.36456298828125\n",
      "2986/3000 train_loss: 22.088565826416016 test_loss:405.7041015625\n",
      "2987/3000 train_loss: 25.89010238647461 test_loss:374.5787353515625\n",
      "2988/3000 train_loss: 23.89863395690918 test_loss:373.48822021484375\n",
      "2989/3000 train_loss: 22.12571907043457 test_loss:336.1877136230469\n",
      "2990/3000 train_loss: 26.593238830566406 test_loss:347.64892578125\n",
      "2991/3000 train_loss: 20.774250030517578 test_loss:308.2954406738281\n",
      "2992/3000 train_loss: 22.01740837097168 test_loss:301.699951171875\n",
      "2993/3000 train_loss: 23.319992065429688 test_loss:280.88897705078125\n",
      "2994/3000 train_loss: 25.611053466796875 test_loss:300.9036865234375\n",
      "2995/3000 train_loss: 19.492656707763672 test_loss:276.93707275390625\n",
      "2996/3000 train_loss: 18.970125198364258 test_loss:277.7912902832031\n",
      "2997/3000 train_loss: 22.229894638061523 test_loss:286.32342529296875\n",
      "2998/3000 train_loss: 18.40082359313965 test_loss:266.5465087890625\n",
      "2999/3000 train_loss: 21.100788116455078 test_loss:271.2301330566406\n",
      "3000/3000 train_loss: 19.013120651245117 test_loss:251.8040313720703\n"
     ]
    }
   ],
   "source": [
    "losses = train(model = unet, optimizer = optimizer, criterion=criterion, data_tr=train_data,\n",
    "               data_val = test_data, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Ew7_F0-q7aL",
    "outputId": "fd12d567-b527-4dc7-cd13-8b9873564f12"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(251.8040)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_loss = 0\n",
    "# unet.eval()\n",
    "# unet.train()\n",
    "preds = []\n",
    "i = 0\n",
    "test_anomaly_losses = []\n",
    "test_normal_losses = []\n",
    "test_losses = []\n",
    "test_real = y_test.tolist()\n",
    "for batch in test_data:\n",
    "  with torch.no_grad():\n",
    "    # unet.train()\n",
    "    predictions = unet(batch.to(device)).cpu()\n",
    "    preds.append(predictions)\n",
    "  loss = criterion(predictions, batch.cpu())\n",
    "  for j in range(len(predictions)):\n",
    "    if int(y_test[i]) == 1:\n",
    "      test_anomaly_losses.append(float(criterion(predictions[j], batch[j])))\n",
    "    else:\n",
    "      test_normal_losses.append(float(criterion(predictions[j], batch[j])))\n",
    "    i += 1\n",
    "    test_losses.append(criterion(predictions[j], batch[j]))\n",
    "  # print(loss)\n",
    "  # print(loss)\n",
    "  avg_loss += loss / len(test_data)\n",
    "# avg_loss\n",
    "\n",
    "avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VpDKorrRso9o",
    "outputId": "204a8944-6790-4530-a60d-e5ca07152660"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(180.8031184577942, 6.868332479000092)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(test_anomaly_losses)/len(test_anomaly_losses) , sum(test_normal_losses)/len(test_normal_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5LEvbZKYuh7J",
    "outputId": "dbc93fa2-d1e7-4349-9449-c2805ddbca9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9301810584958217\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "vals = np.arange(10, 400, 0.5).tolist()\n",
    "for threshold in vals:\n",
    "  preds = []\n",
    "  for j in range(len(test_losses)):\n",
    "    if test_losses[j] > threshold:\n",
    "      preds.append(1)\n",
    "    else:\n",
    "      preds.append(0)\n",
    "  \n",
    "  results.append(roc_auc_score(test_real,preds))\n",
    "\n",
    "print(max(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QaSSqG8SbAw2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
