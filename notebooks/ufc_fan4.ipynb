{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9SStKf4G0V5H"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim import Adam\n",
        "import torchaudio\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from skimage.util import img_as_ubyte\n",
        "import pandas as pd\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import librosa\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import io\n",
        "from sklearn.metrics import roc_auc_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XtxbKLZq5KX",
        "outputId": "37023990-2b40-4e1f-e63e-1dba18c3acdb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYxHegIM0Z4i",
        "outputId": "f68f6fab-6d0f-4c1f-9d0a-527e166123f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "h9DATQwS0ivD"
      },
      "outputs": [],
      "source": [
        "class MimiiDataset(Dataset):\n",
        "    def __init__(self,audio_dir, n_fft = 1024, win_length = 1024,\n",
        "                 hop_length = 512,power = 2,n_mels = 128,pad_mode = 'reflect',\n",
        "                 sr = 16000,center = True,norm = None):\n",
        "      \n",
        "        super(MimiiDataset, self).__init__()\n",
        "        self.audio_dir = audio_dir\n",
        "        self.n_mels = n_mels\n",
        "        self.n_fft = n_fft\n",
        "        self.win_length = win_length\n",
        "        self.hop_length = hop_length\n",
        "        self.power = power\n",
        "        self.pad_mode = pad_mode\n",
        "        self.sr = sr\n",
        "        self.center = center\n",
        "        self.norm = norm\n",
        "\n",
        "    def get_files(self):\n",
        "       return self.train_files, self.test_files\n",
        "    \n",
        "    def get_data(self,device, id):\n",
        "        \n",
        "        self.train_files, self.train_labels = self._train_file_list(device, id)\n",
        "        self.test_files, self.test_labels = self._test_file_list(device, id)\n",
        "        \n",
        "        self.train_data = self.get_audios(self.train_files)\n",
        "        self.test_data = self.get_audios(self.test_files)\n",
        "        \n",
        "        return self.train_data, self.test_data, self.train_labels, self.test_labels\n",
        "    \n",
        "    def _train_file_list(self, device, id):\n",
        "        query = os.path.abspath(\n",
        "            f\"{self.audio_dir}/{device}/train/normal_id_0{id}*.wav\"\n",
        "        )\n",
        "        train_normal_files = sorted(glob.glob(query))\n",
        "        train_normal_labels = np.zeros(len(train_normal_files))\n",
        "        \n",
        "        query = os.path.abspath(\n",
        "                f\"{self.audio_dir}/{device}/train/anomaly_id_0{id}*.wav\"\n",
        "            )\n",
        "        train_anomaly_files = sorted(glob.glob(query))\n",
        "        train_anomaly_labels = np.ones(len(train_anomaly_files))\n",
        "        \n",
        "        train_file_list = np.concatenate((train_normal_files, train_anomaly_files), axis=0)\n",
        "        train_labels = np.concatenate((train_normal_labels, train_anomaly_labels), axis=0)\n",
        "        \n",
        "        return train_file_list, train_labels\n",
        "    \n",
        "    def _test_file_list(self, device, id):     \n",
        "        query = os.path.abspath(\n",
        "            f\"{self.audio_dir}/{device}/test/normal_id_0{id}*.wav\"\n",
        "            )\n",
        "        test_normal_files = sorted(glob.glob(query))\n",
        "        test_normal_labels = np.zeros(len(test_normal_files))\n",
        "        \n",
        "        query = os.path.abspath(\n",
        "            f\"{self.audio_dir}/{device}/test/anomaly_id_0{id}*.wav\"\n",
        "            )\n",
        "        test_anomaly_files = sorted(glob.glob(query))\n",
        "        test_anomaly_labels = np.ones(len(test_anomaly_files))\n",
        "        \n",
        "        test_file_list = np.concatenate((test_normal_files, \n",
        "                                          test_anomaly_files), axis=0)\n",
        "        test_labels = np.concatenate((test_normal_labels,\n",
        "                                      test_anomaly_labels), axis=0)\n",
        "          \n",
        "        return test_file_list, test_labels\n",
        "\n",
        "    def normalize(self,tensor):\n",
        "        tensor_minusmean = tensor - tensor.mean()\n",
        "        return tensor_minusmean/np.absolute(tensor_minusmean).max()\n",
        "\n",
        "    def make0min(self,tensornd):\n",
        "        tensor = tensornd.numpy()\n",
        "        res = np.where(tensor == 0, 1E-19 , tensor)\n",
        "        return torch.from_numpy(res)\n",
        "\n",
        "    def spectrogrameToImage(self,specgram):\n",
        "        # specgram = torchaudio.transforms.MelSpectrogram(n_fft=1024, win_length=1024, \n",
        "        #                                                 hop_length=512, power=2, \n",
        "        #                                                 normalized=True, n_mels=128)(waveform )\n",
        "        specgram= self.make0min(specgram)\n",
        "        specgram = specgram.log2()[0,:,:].numpy()\n",
        "        \n",
        "        tr2image = transforms.Compose([transforms.ToPILImage()])\n",
        "\n",
        "        specgram= self.normalize(specgram)\n",
        "        # specgram = img_as_ubyte(specgram)\n",
        "        specgramImage = tr2image(specgram)\n",
        "        return specgramImage\n",
        "\n",
        "    def get_logmelspectrogram(self, waveform):\n",
        "        melspec = librosa.feature.melspectrogram(\n",
        "          n_fft=self.n_fft, win_length=self.win_length, hop_length=self.hop_length,\n",
        "          power=self.power,n_mels=self.n_mels,pad_mode=self.pad_mode,sr=self.sr,\n",
        "          center=self.center,norm=self.norm,htk=True,\n",
        "          y = waveform.numpy()\n",
        "        )\n",
        "\n",
        "        logmelspec = librosa.power_to_db(melspec)\n",
        "\n",
        "        return logmelspec\n",
        "\n",
        "    def get_melspectrogram(self,waveform):\n",
        "        melspec = librosa.feature.melspectrogram(\n",
        "            n_fft=self.n_fft, win_length=self.win_length, hop_length=self.hop_length,\n",
        "            power=self.power,n_mels=self.n_mels,pad_mode=self.pad_mode,sr=self.sr,\n",
        "            center=self.center,norm=self.norm,htk=True,\n",
        "            y = waveform.numpy()\n",
        "        )\n",
        "\n",
        "        return melspec\n",
        "    \n",
        "    def get_mfcc(self,waveform):\n",
        "        mfcc = librosa.feature.mfcc(    \n",
        "            n_fft=self.n_fft, win_length=self.win_length, \n",
        "            hop_length=self.hop_length,pad_mode=self.pad_mode,sr=self.sr,\n",
        "            center=self.center,norm=self.norm,n_mfcc=40,\n",
        "            y = waveform.numpy()\n",
        "        )\n",
        "\n",
        "        return mfcc\n",
        "\n",
        "    def get_chroma_stft(self,waveform):\n",
        "        stft = librosa.feature.chroma_stft(\n",
        "            n_fft=self.n_fft, win_length=self.win_length, \n",
        "            hop_length=self.hop_length,pad_mode=self.pad_mode,sr=self.sr,\n",
        "            center=self.center,norm=self.norm,n_chroma=12,\n",
        "            y=waveform.numpy()\n",
        "        )\n",
        "\n",
        "        return stft\n",
        "\n",
        "    def get_spectral_contrast(self,waveform):\n",
        "        spec_contrast = librosa.feature.spectral_contrast(    \n",
        "            n_fft=self.n_fft, win_length=self.win_length,center=self.center,\n",
        "            hop_length=self.hop_length,pad_mode=self.pad_mode,sr=self.sr,\n",
        "            y = waveform.numpy()\n",
        "        )\n",
        "\n",
        "        return spec_contrast\n",
        "    \n",
        "    def get_tonnetz(self,waveform):\n",
        "        harmonic = librosa.effects.harmonic(waveform.numpy())\n",
        "        tonnetz = librosa.feature.tonnetz(y=harmonic,sr=self.sr)\n",
        "\n",
        "        return tonnetz\n",
        "\n",
        "    def get_audios(self, file_list):\n",
        "        data = []\n",
        "        for i in range(len(file_list)):\n",
        "          y, sr = torchaudio.load(file_list[i])  \n",
        "          data.append(y)\n",
        "\n",
        "        return data\n",
        "    def _derive_data(self, file_list):\n",
        "        train_data = []\n",
        "        test_data = []\n",
        "        train_mode = True\n",
        "        for file_list in [self.train_files, self.test_files]:\n",
        "          tr2tensor = transforms.Compose([transforms.PILToTensor()])\n",
        "          data = []\n",
        "          for j in range(len(file_list)):\n",
        "            y, sr = torchaudio.load(file_list[j])  \n",
        "            spec = self.get_melspectrogram(y)\n",
        "            spec = self.spectrogrameToImage(spec)\n",
        "            spec = spec.convert('RGB')\n",
        "            vectors = tr2tensor(spec)\n",
        "            if train_mode:     \n",
        "              train_data.append(vectors)\n",
        "            else:\n",
        "              test_data.append(vectors)\n",
        "            \n",
        "          train_mode = False\n",
        "                \n",
        "        return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "S96soeIc0o13"
      },
      "outputs": [],
      "source": [
        "dataset = MimiiDataset('/content/drive/MyDrive/mimii')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Gn2zdn92doi1"
      },
      "outputs": [],
      "source": [
        "_, _, y_train, y_test = dataset.get_data('fan', 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "2bi9uuBmiVBu"
      },
      "outputs": [],
      "source": [
        "def mean_mfccs(wave_list):\n",
        "  data = []\n",
        "  for wave in wave_list:\n",
        "    mfcc = np.mean(dataset.get_mfcc(wave)[0], axis = 1)\n",
        "    data.append(mfcc)\n",
        "  \n",
        "  return data\n",
        "\n",
        "def mean_stfts(wave_list):\n",
        "  data = []\n",
        "  for wave in wave_list:\n",
        "    stft = np.mean(dataset.get_chroma_stft(wave)[0], axis = 1)\n",
        "    data.append(stft)\n",
        "  \n",
        "  return data\n",
        "\n",
        "def mean_melspecs(wave_list):\n",
        "  data = []\n",
        "  for wave in wave_list:\n",
        "    melspec = np.mean(dataset.get_melspectrogram(wave)[0], axis = 1)\n",
        "    data.append(melspec)\n",
        "  \n",
        "  return data\n",
        "\n",
        "def mean_spec_contrasts(wave_list):\n",
        "  data = []\n",
        "  for wave in wave_list:\n",
        "    spec_contrast = np.mean(dataset.get_spectral_contrast(wave)[0], axis = 1)\n",
        "    data.append(spec_contrast)\n",
        "  \n",
        "  return data\n",
        "  \n",
        "def mean_tonnetzs(wave_list):\n",
        "  data = []\n",
        "  for wave in wave_list:\n",
        "    tonnetz = np.mean(dataset.get_tonnetz(wave)[0], axis = 1)\n",
        "    data.append(tonnetz)\n",
        "  \n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_i9ezYUFiU_b"
      },
      "outputs": [],
      "source": [
        "train_melspecs, test_melspecs = mean_melspecs(df_train), mean_melspecs(df_test)\n",
        "train_mfccs, test_mfccs = mean_mfccs(df_train), mean_mfccs(df_test)\n",
        "train_stfts, test_stfts = mean_stfts(df_train), mean_stfts(df_test)\n",
        "train_spec_contrasts, test_spec_contrasts = mean_spec_contrasts(df_train), mean_spec_contrasts(df_test)\n",
        "train_tonnetzs, test_tonnetzs = mean_tonnetzs(df_train), mean_tonnetzs(df_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfPtT0YdpttI"
      },
      "outputs": [],
      "source": [
        "train_mixed_f = []\n",
        "test_mixed_f = []\n",
        "\n",
        "for i in range(len(train_melspecs)):\n",
        "\n",
        "  train_mf = np.concatenate((train_melspecs[i],train_mfccs[i],train_stfts[i],\n",
        "                             train_spec_contrasts[i],train_tonnetzs[i])).tolist()\n",
        "\n",
        "  train_mixed_f.append(train_mf)\n",
        "\n",
        "for i in range(len(test_melspecs)):\n",
        "\n",
        "  test_mf = np.concatenate((test_melspecs[i],test_mfccs[i],test_stfts[i],\n",
        "                             test_spec_contrasts[i],test_tonnetzs[i])).tolist()\n",
        "\n",
        "  test_mixed_f.append(test_mf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8OT45sHpvBE"
      },
      "outputs": [],
      "source": [
        "train_mixed_f = torch.FloatTensor(train_mixed_f)\n",
        "test_mixed_f = torch.FloatTensor(test_mixed_f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VKtjykaqoIC",
        "outputId": "702b23d3-c09b-46ef-df74-9feca643c45d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1006, 193])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "train_mixed_f.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dafhP0mBpx21"
      },
      "outputs": [],
      "source": [
        "torch.save(train_mixed_f, '/content/drive/MyDrive/mixed_features/train_mf_toycar2.pt')\n",
        "torch.save( test_mixed_f, '/content/drive/MyDrive/mixed_features/test_mf_toycar2.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "SgjpeWy_RV1C"
      },
      "outputs": [],
      "source": [
        "train_mixed_f = torch.load('/content/drive/MyDrive/mixed_features/train_mf_fan4.pt')\n",
        "test_mixed_f = torch.load('/content/drive/MyDrive/mixed_features/test_mf_fan4.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEl3qOh-mZVK",
        "outputId": "09a4ce6f-49ac-4fda-acc0-9b0fdc089e20"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([933, 193])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "train_mixed_f.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "jWMPVGu1qiEq"
      },
      "outputs": [],
      "source": [
        "train_data = DataLoader(train_mixed_f, batch_size=64, shuffle = True)\n",
        "test_data = DataLoader(test_mixed_f, batch_size = 64, shuffle= False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "vNTBTRe6qnBq"
      },
      "outputs": [],
      "source": [
        "class UNet_FC(nn.Module):\n",
        "\n",
        "  def __init__(self, in_features):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.bn = nn.BatchNorm1d(128)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "    self.fc0 = nn.Linear(in_features=in_features,out_features=in_features)\n",
        "\n",
        "    # encoder\n",
        "    self.fc1 = nn.Linear(in_features=in_features, out_features=128)\n",
        "    self.fc2 = nn.Linear(in_features=128,out_features=128)\n",
        "    self.fc3 = nn.Linear(in_features=128, out_features=128)\n",
        "    self.fc4 = nn.Linear(in_features=128, out_features=128)\n",
        "    self.fc5 = nn.Linear(in_features=128, out_features=8)\n",
        "\n",
        "    # decoder\n",
        "    self.fc6 = nn.Linear(in_features=8, out_features=128)\n",
        "    self.fc7 = nn.Linear(in_features=128*2, out_features=128)\n",
        "    self.fc8 = nn.Linear(in_features=128*2, out_features=128)\n",
        "    self.fc9 = nn.Linear(in_features=128*2, out_features=128)\n",
        "\n",
        "    self.out = nn.Linear(in_features=128*2, out_features=in_features)\n",
        "\n",
        "  def encoder(self, x):\n",
        "    input = self.fc0(x)\n",
        "\n",
        "    x1 = self.relu(self.bn(self.fc1(input)))\n",
        "    x2 = self.relu(self.bn(self.fc2(x1)))\n",
        "    x3 = self.relu(self.bn(self.fc3(x2)))\n",
        "    x4 = self.relu(self.bn(self.fc4(x3)))\n",
        "    x5 = self.relu(self.fc5(x4))\n",
        "\n",
        "    return [x5, x4, x3, x2, x1]\n",
        "\n",
        "  def decoder(self, x):\n",
        "    x6 = self.relu(self.fc6(x[0]))\n",
        "    con1 = torch.cat((x6,x[1]), 1) \n",
        "    x7 = self.relu(self.bn(self.fc7(con1)))\n",
        "    con2 = torch.cat((x7,x[2]), 1)\n",
        "    x8 = self.relu(self.bn(self.fc8(con2)))\n",
        "    con3 = torch.cat((x8,x[3]), 1)\n",
        "    x9 = self.relu(self.bn(self.fc9(con3)))\n",
        "    con4 = torch.cat((x9,x[4]), 1)\n",
        "\n",
        "    x10 = self.out(con4)\n",
        "\n",
        "    return x10\n",
        "  \n",
        "  def forward(self, x):\n",
        "    # encoded = self.encoder(x)\n",
        "\n",
        "    # decoded = self.decoder(encoded)\n",
        "    input = self.fc0(x)\n",
        "\n",
        "    x1 = self.relu(self.bn(self.fc1(input)))\n",
        "    x2 = self.relu(self.bn(self.fc2(x1)))\n",
        "    x3 = self.relu(self.bn(self.fc3(x2)))\n",
        "    x4 = self.relu(self.bn(self.fc4(x3)))\n",
        "    x5 = self.relu(self.fc5(x4))\n",
        "\n",
        "    xy = [x5, x4, x3, x2, x1]\n",
        "\n",
        "    x6 = self.relu(self.fc6(xy[0]))\n",
        "    con1 = torch.cat((x6,xy[1]), 1) \n",
        "    x7 = self.relu(self.bn(self.fc7(con1)))\n",
        "    con2 = torch.cat((x7,xy[2]), 1)\n",
        "    x8 = self.relu(self.bn(self.fc8(con2)))\n",
        "    con3 = torch.cat((x8,xy[3]), 1)\n",
        "    x9 = self.relu(self.bn(self.fc9(con3)))\n",
        "    con4 = torch.cat((x9,xy[4]), 1)\n",
        "\n",
        "    x10 = self.out(con4)\n",
        "\n",
        "    # return decoded\n",
        "    return x10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "ZfgcBtQ3qn5l"
      },
      "outputs": [],
      "source": [
        "def train(model, optimizer, criterion, data_tr, data_val, scheduler = None,\n",
        "          epochs = 3000, device = 'cpu'):\n",
        "    # X_val, Y_val = next(iter(data_val))\n",
        "    losses = []\n",
        "    prev_avg_loss = 100000\n",
        "    for epoch in range(epochs):\n",
        "        train_avg_loss = 0\n",
        "        test_avg_loss = 0\n",
        "        # model.train()  # train mode\n",
        "        for batch in data_tr:\n",
        "          # data to device\n",
        "          batch = batch.to(device)\n",
        "          # set parameter gradients to zero\n",
        "          optimizer.zero_grad()\n",
        "          # forward\n",
        "          # print(Y_batch.shape)\n",
        "          predictions = model(batch)\n",
        "          loss = criterion(predictions, batch)\n",
        "          loss.backward() # backward-pass\n",
        "          optimizer.step()  # update weights\n",
        "          # calculate loss to show the user\n",
        "          if scheduler:\n",
        "            scheduler.step(loss)\n",
        "          train_avg_loss += loss / len(data_tr)\n",
        "\n",
        "        # model.eval()\n",
        "        for batch in data_val:\n",
        "          with torch.no_grad():\n",
        "            preds = model(batch.to(device)).cpu()\n",
        "            loss = criterion(preds,batch)\n",
        "            test_avg_loss += loss / len(data_val)\n",
        "                    \n",
        "        losses.append(train_avg_loss.item())\n",
        "        # if (epoch+1)%50 == 0:\n",
        "        print(\"{}/{} train_loss: {} test_loss:{}\".format(epoch+1, epochs, train_avg_loss, test_avg_loss))\n",
        "        # if test_avg_loss < 70:\n",
        "        #   break\n",
        "    return losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "ptkVTF55quOL"
      },
      "outputs": [],
      "source": [
        "unet = UNet_FC(in_features=193).to(device)\n",
        "optimizer = Adam(params = unet.parameters(), lr = 10e-3)\n",
        "# optimizer = Adam(params = unet.parameters())\n",
        "criterion = nn.MSELoss()\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, \n",
        "                                                       min_lr=10e-4, mode = 'min',\n",
        "                                                       patience = 30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkfmYl9oXhcB",
        "outputId": "63bf7ce3-3140-4806-bc57-304de4a82d80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/3000 train_loss: 315412.59375 test_loss:315568.40625\n",
            "2/3000 train_loss: 312800.25 test_loss:311701.84375\n",
            "3/3000 train_loss: 308060.90625 test_loss:307275.65625\n",
            "4/3000 train_loss: 304349.3125 test_loss:303816.59375\n",
            "5/3000 train_loss: 300477.59375 test_loss:299891.03125\n",
            "6/3000 train_loss: 296445.96875 test_loss:295570.90625\n",
            "7/3000 train_loss: 292139.28125 test_loss:290539.9375\n",
            "8/3000 train_loss: 286667.65625 test_loss:284816.625\n",
            "9/3000 train_loss: 280584.625 test_loss:278237.34375\n",
            "10/3000 train_loss: 273892.53125 test_loss:271284.21875\n",
            "11/3000 train_loss: 266845.78125 test_loss:263699.03125\n",
            "12/3000 train_loss: 259496.265625 test_loss:255663.84375\n",
            "13/3000 train_loss: 251325.46875 test_loss:247912.375\n",
            "14/3000 train_loss: 243580.71875 test_loss:239649.3125\n",
            "15/3000 train_loss: 234843.359375 test_loss:230873.984375\n",
            "16/3000 train_loss: 226384.546875 test_loss:222021.859375\n",
            "17/3000 train_loss: 217701.453125 test_loss:213897.90625\n",
            "18/3000 train_loss: 209249.296875 test_loss:204848.25\n",
            "19/3000 train_loss: 200415.671875 test_loss:195705.34375\n",
            "20/3000 train_loss: 191349.390625 test_loss:187251.515625\n",
            "21/3000 train_loss: 182563.59375 test_loss:177972.8125\n",
            "22/3000 train_loss: 172313.40625 test_loss:168080.890625\n",
            "23/3000 train_loss: 163254.921875 test_loss:158775.8125\n",
            "24/3000 train_loss: 153740.3125 test_loss:149263.703125\n",
            "25/3000 train_loss: 144656.203125 test_loss:140375.171875\n",
            "26/3000 train_loss: 135614.03125 test_loss:131261.8125\n",
            "27/3000 train_loss: 126922.6328125 test_loss:123059.984375\n",
            "28/3000 train_loss: 118907.2109375 test_loss:115213.328125\n",
            "29/3000 train_loss: 110178.2578125 test_loss:106990.015625\n",
            "30/3000 train_loss: 101504.9609375 test_loss:98123.9453125\n",
            "31/3000 train_loss: 93340.1015625 test_loss:89976.5703125\n",
            "32/3000 train_loss: 85513.4453125 test_loss:82385.53125\n",
            "33/3000 train_loss: 78181.0 test_loss:74674.4140625\n",
            "34/3000 train_loss: 71032.3203125 test_loss:68280.3984375\n",
            "35/3000 train_loss: 64965.83203125 test_loss:62745.0703125\n",
            "36/3000 train_loss: 58767.37890625 test_loss:55933.4296875\n",
            "37/3000 train_loss: 52936.75 test_loss:49864.71484375\n",
            "38/3000 train_loss: 46516.8359375 test_loss:44339.5546875\n",
            "39/3000 train_loss: 41280.1875 test_loss:39385.65625\n",
            "40/3000 train_loss: 37019.66796875 test_loss:35243.19921875\n",
            "41/3000 train_loss: 33088.8203125 test_loss:31420.296875\n",
            "42/3000 train_loss: 28987.017578125 test_loss:27828.48828125\n",
            "43/3000 train_loss: 25406.234375 test_loss:24687.091796875\n",
            "44/3000 train_loss: 22701.005859375 test_loss:21355.9921875\n",
            "45/3000 train_loss: 19339.548828125 test_loss:17851.927734375\n",
            "46/3000 train_loss: 16290.7841796875 test_loss:15710.1513671875\n",
            "47/3000 train_loss: 14028.5732421875 test_loss:13443.2294921875\n",
            "48/3000 train_loss: 12363.607421875 test_loss:11869.2978515625\n",
            "49/3000 train_loss: 10345.4501953125 test_loss:9805.58203125\n",
            "50/3000 train_loss: 8751.5146484375 test_loss:8308.7958984375\n",
            "51/3000 train_loss: 7332.0576171875 test_loss:7455.501953125\n",
            "52/3000 train_loss: 6625.35400390625 test_loss:6758.15869140625\n",
            "53/3000 train_loss: 5930.66796875 test_loss:5547.4462890625\n",
            "54/3000 train_loss: 4848.2685546875 test_loss:4595.64208984375\n",
            "55/3000 train_loss: 4060.603271484375 test_loss:3960.7490234375\n",
            "56/3000 train_loss: 3356.40771484375 test_loss:3222.154052734375\n",
            "57/3000 train_loss: 2846.74169921875 test_loss:2795.07958984375\n",
            "58/3000 train_loss: 2401.123046875 test_loss:2651.745849609375\n",
            "59/3000 train_loss: 2142.822998046875 test_loss:2133.5009765625\n",
            "60/3000 train_loss: 1877.652099609375 test_loss:1912.435302734375\n",
            "61/3000 train_loss: 1676.5313720703125 test_loss:1597.8857421875\n",
            "62/3000 train_loss: 1394.781982421875 test_loss:1381.7861328125\n",
            "63/3000 train_loss: 1213.0662841796875 test_loss:1259.39892578125\n",
            "64/3000 train_loss: 1082.2222900390625 test_loss:1161.7291259765625\n",
            "65/3000 train_loss: 984.728759765625 test_loss:1074.0921630859375\n",
            "66/3000 train_loss: 913.8948974609375 test_loss:939.5020141601562\n",
            "67/3000 train_loss: 857.1766357421875 test_loss:897.2354736328125\n",
            "68/3000 train_loss: 826.404541015625 test_loss:926.77197265625\n",
            "69/3000 train_loss: 748.7872314453125 test_loss:811.1624145507812\n",
            "70/3000 train_loss: 727.5131225585938 test_loss:753.8824462890625\n",
            "71/3000 train_loss: 696.9342041015625 test_loss:739.2861328125\n",
            "72/3000 train_loss: 651.8870849609375 test_loss:703.9904174804688\n",
            "73/3000 train_loss: 692.0294189453125 test_loss:705.6668701171875\n",
            "74/3000 train_loss: 625.1682739257812 test_loss:708.1565551757812\n",
            "75/3000 train_loss: 643.7691650390625 test_loss:696.9170532226562\n",
            "76/3000 train_loss: 614.7614135742188 test_loss:695.0426025390625\n",
            "77/3000 train_loss: 649.6426391601562 test_loss:695.0398559570312\n",
            "78/3000 train_loss: 645.6814575195312 test_loss:676.9132690429688\n",
            "79/3000 train_loss: 620.823486328125 test_loss:673.1767578125\n",
            "80/3000 train_loss: 637.0673217773438 test_loss:666.6918334960938\n",
            "81/3000 train_loss: 608.0538330078125 test_loss:659.20068359375\n",
            "82/3000 train_loss: 615.399169921875 test_loss:657.1889038085938\n",
            "83/3000 train_loss: 614.3234252929688 test_loss:647.8667602539062\n",
            "84/3000 train_loss: 588.7403564453125 test_loss:649.5783081054688\n",
            "85/3000 train_loss: 580.6119995117188 test_loss:641.4274291992188\n",
            "86/3000 train_loss: 582.9221801757812 test_loss:632.3543701171875\n",
            "87/3000 train_loss: 580.6095581054688 test_loss:633.6672973632812\n",
            "88/3000 train_loss: 575.2286376953125 test_loss:624.27734375\n",
            "89/3000 train_loss: 589.9791870117188 test_loss:628.2671508789062\n",
            "90/3000 train_loss: 573.9447631835938 test_loss:620.3840942382812\n",
            "91/3000 train_loss: 544.8170166015625 test_loss:624.0646362304688\n",
            "92/3000 train_loss: 572.448974609375 test_loss:619.25439453125\n",
            "93/3000 train_loss: 578.3134155273438 test_loss:610.870849609375\n",
            "94/3000 train_loss: 558.9544677734375 test_loss:610.5509033203125\n",
            "95/3000 train_loss: 558.243896484375 test_loss:606.874267578125\n",
            "96/3000 train_loss: 550.3661499023438 test_loss:600.7352294921875\n",
            "97/3000 train_loss: 567.125732421875 test_loss:610.0850830078125\n",
            "98/3000 train_loss: 584.3178100585938 test_loss:605.6505737304688\n",
            "99/3000 train_loss: 561.603759765625 test_loss:619.0459594726562\n",
            "100/3000 train_loss: 574.2896118164062 test_loss:593.0341186523438\n",
            "101/3000 train_loss: 540.7341918945312 test_loss:584.68310546875\n",
            "102/3000 train_loss: 524.0615844726562 test_loss:591.1806640625\n",
            "103/3000 train_loss: 515.4093017578125 test_loss:585.8728637695312\n",
            "104/3000 train_loss: 545.2420043945312 test_loss:580.128173828125\n",
            "105/3000 train_loss: 513.476318359375 test_loss:573.0606689453125\n",
            "106/3000 train_loss: 518.9826049804688 test_loss:573.8524169921875\n",
            "107/3000 train_loss: 559.2921142578125 test_loss:574.3416748046875\n",
            "108/3000 train_loss: 541.2155151367188 test_loss:573.5254516601562\n",
            "109/3000 train_loss: 510.0989685058594 test_loss:569.2847900390625\n",
            "110/3000 train_loss: 516.9053344726562 test_loss:558.5869140625\n",
            "111/3000 train_loss: 510.9803161621094 test_loss:557.830810546875\n",
            "112/3000 train_loss: 501.2923889160156 test_loss:561.8241577148438\n",
            "113/3000 train_loss: 508.03741455078125 test_loss:553.3504638671875\n",
            "114/3000 train_loss: 531.8794555664062 test_loss:559.4734497070312\n",
            "115/3000 train_loss: 541.1904296875 test_loss:546.6837158203125\n",
            "116/3000 train_loss: 514.3517456054688 test_loss:542.9296264648438\n",
            "117/3000 train_loss: 496.7839660644531 test_loss:543.933349609375\n",
            "118/3000 train_loss: 522.3865966796875 test_loss:539.7769775390625\n",
            "119/3000 train_loss: 500.8673095703125 test_loss:534.6508178710938\n",
            "120/3000 train_loss: 470.27642822265625 test_loss:536.3544921875\n",
            "121/3000 train_loss: 500.0830383300781 test_loss:529.8562622070312\n",
            "122/3000 train_loss: 494.2032165527344 test_loss:528.66455078125\n",
            "123/3000 train_loss: 487.15447998046875 test_loss:529.6011962890625\n",
            "124/3000 train_loss: 471.571533203125 test_loss:525.1793823242188\n",
            "125/3000 train_loss: 483.39007568359375 test_loss:521.6627197265625\n",
            "126/3000 train_loss: 498.800537109375 test_loss:530.260986328125\n",
            "127/3000 train_loss: 487.0155944824219 test_loss:530.6444091796875\n",
            "128/3000 train_loss: 466.8316955566406 test_loss:517.2691650390625\n",
            "129/3000 train_loss: 463.1127014160156 test_loss:519.8826293945312\n",
            "130/3000 train_loss: 467.48858642578125 test_loss:514.5735473632812\n",
            "131/3000 train_loss: 492.794189453125 test_loss:529.1846923828125\n",
            "132/3000 train_loss: 485.1445617675781 test_loss:509.5694274902344\n",
            "133/3000 train_loss: 464.0359802246094 test_loss:513.077392578125\n",
            "134/3000 train_loss: 475.6264343261719 test_loss:522.7438354492188\n",
            "135/3000 train_loss: 462.4517822265625 test_loss:513.4771118164062\n",
            "136/3000 train_loss: 458.427490234375 test_loss:507.0801696777344\n",
            "137/3000 train_loss: 449.5013732910156 test_loss:503.05218505859375\n",
            "138/3000 train_loss: 442.09161376953125 test_loss:497.80950927734375\n",
            "139/3000 train_loss: 455.7381591796875 test_loss:505.9055480957031\n",
            "140/3000 train_loss: 457.25 test_loss:493.0355224609375\n",
            "141/3000 train_loss: 453.1764221191406 test_loss:493.9990539550781\n",
            "142/3000 train_loss: 447.6073303222656 test_loss:497.3211669921875\n",
            "143/3000 train_loss: 439.894775390625 test_loss:490.7870178222656\n",
            "144/3000 train_loss: 442.2406921386719 test_loss:484.2054443359375\n",
            "145/3000 train_loss: 479.65966796875 test_loss:483.5773010253906\n",
            "146/3000 train_loss: 436.09368896484375 test_loss:481.93707275390625\n",
            "147/3000 train_loss: 450.3675842285156 test_loss:476.3577880859375\n",
            "148/3000 train_loss: 440.4844970703125 test_loss:470.3555908203125\n",
            "149/3000 train_loss: 449.38018798828125 test_loss:467.25274658203125\n",
            "150/3000 train_loss: 465.318115234375 test_loss:467.5461730957031\n",
            "151/3000 train_loss: 425.2071838378906 test_loss:473.18804931640625\n",
            "152/3000 train_loss: 441.757568359375 test_loss:469.9416809082031\n",
            "153/3000 train_loss: 429.65631103515625 test_loss:472.7880554199219\n",
            "154/3000 train_loss: 443.73388671875 test_loss:464.28485107421875\n",
            "155/3000 train_loss: 434.1556701660156 test_loss:463.92181396484375\n",
            "156/3000 train_loss: 440.86138916015625 test_loss:458.2333984375\n",
            "157/3000 train_loss: 423.7740783691406 test_loss:456.6396789550781\n",
            "158/3000 train_loss: 437.9367980957031 test_loss:456.47613525390625\n",
            "159/3000 train_loss: 410.7682800292969 test_loss:456.2378234863281\n",
            "160/3000 train_loss: 420.3130187988281 test_loss:462.3511657714844\n",
            "161/3000 train_loss: 451.047119140625 test_loss:454.064697265625\n",
            "162/3000 train_loss: 434.0658264160156 test_loss:458.47076416015625\n",
            "163/3000 train_loss: 416.93524169921875 test_loss:457.1973876953125\n",
            "164/3000 train_loss: 395.20758056640625 test_loss:454.38092041015625\n",
            "165/3000 train_loss: 425.20953369140625 test_loss:439.33758544921875\n",
            "166/3000 train_loss: 406.4147644042969 test_loss:456.16754150390625\n",
            "167/3000 train_loss: 415.85760498046875 test_loss:443.7367248535156\n",
            "168/3000 train_loss: 409.4782409667969 test_loss:442.57757568359375\n",
            "169/3000 train_loss: 405.2463684082031 test_loss:449.8585205078125\n",
            "170/3000 train_loss: 402.9328918457031 test_loss:439.4496154785156\n",
            "171/3000 train_loss: 410.76751708984375 test_loss:433.247314453125\n",
            "172/3000 train_loss: 386.4720764160156 test_loss:433.414794921875\n",
            "173/3000 train_loss: 406.4060363769531 test_loss:435.2465515136719\n",
            "174/3000 train_loss: 373.3009948730469 test_loss:434.5167236328125\n",
            "175/3000 train_loss: 407.0855712890625 test_loss:432.44317626953125\n",
            "176/3000 train_loss: 396.3902587890625 test_loss:433.65814208984375\n",
            "177/3000 train_loss: 374.955810546875 test_loss:437.1167297363281\n",
            "178/3000 train_loss: 381.51300048828125 test_loss:423.4569396972656\n",
            "179/3000 train_loss: 381.2077941894531 test_loss:426.32183837890625\n",
            "180/3000 train_loss: 387.7654724121094 test_loss:455.0215148925781\n",
            "181/3000 train_loss: 384.71392822265625 test_loss:424.916015625\n",
            "182/3000 train_loss: 359.73040771484375 test_loss:420.3936462402344\n",
            "183/3000 train_loss: 378.6923522949219 test_loss:412.87664794921875\n",
            "184/3000 train_loss: 384.6313171386719 test_loss:407.50994873046875\n",
            "185/3000 train_loss: 378.32440185546875 test_loss:400.8739318847656\n",
            "186/3000 train_loss: 386.36285400390625 test_loss:412.0511779785156\n",
            "187/3000 train_loss: 379.56573486328125 test_loss:405.8748474121094\n",
            "188/3000 train_loss: 344.0492248535156 test_loss:403.96197509765625\n",
            "189/3000 train_loss: 393.7189025878906 test_loss:398.7345886230469\n",
            "190/3000 train_loss: 365.1333312988281 test_loss:393.63006591796875\n",
            "191/3000 train_loss: 362.4145202636719 test_loss:397.0653991699219\n",
            "192/3000 train_loss: 356.1883239746094 test_loss:395.706787109375\n",
            "193/3000 train_loss: 356.2281188964844 test_loss:386.8739013671875\n",
            "194/3000 train_loss: 344.1755065917969 test_loss:387.4237060546875\n",
            "195/3000 train_loss: 344.8176574707031 test_loss:405.4688720703125\n",
            "196/3000 train_loss: 359.2792663574219 test_loss:392.79522705078125\n",
            "197/3000 train_loss: 350.0839538574219 test_loss:383.2938232421875\n",
            "198/3000 train_loss: 340.7110595703125 test_loss:379.42425537109375\n",
            "199/3000 train_loss: 349.0863952636719 test_loss:382.03131103515625\n",
            "200/3000 train_loss: 338.2550354003906 test_loss:389.6058349609375\n",
            "201/3000 train_loss: 341.37847900390625 test_loss:376.0211181640625\n",
            "202/3000 train_loss: 344.82415771484375 test_loss:376.268798828125\n",
            "203/3000 train_loss: 344.0681457519531 test_loss:373.8200988769531\n",
            "204/3000 train_loss: 371.5711669921875 test_loss:372.2930603027344\n",
            "205/3000 train_loss: 374.2297058105469 test_loss:372.6050720214844\n",
            "206/3000 train_loss: 341.6668395996094 test_loss:366.8374328613281\n",
            "207/3000 train_loss: 326.46942138671875 test_loss:365.3699035644531\n",
            "208/3000 train_loss: 328.95526123046875 test_loss:372.8603515625\n",
            "209/3000 train_loss: 326.91033935546875 test_loss:363.2744140625\n",
            "210/3000 train_loss: 326.0178527832031 test_loss:369.3055114746094\n",
            "211/3000 train_loss: 330.2909851074219 test_loss:371.6078186035156\n",
            "212/3000 train_loss: 339.1361389160156 test_loss:360.4933166503906\n",
            "213/3000 train_loss: 324.741455078125 test_loss:366.2989196777344\n",
            "214/3000 train_loss: 336.06781005859375 test_loss:355.5713195800781\n",
            "215/3000 train_loss: 323.5011291503906 test_loss:356.97613525390625\n",
            "216/3000 train_loss: 328.9770812988281 test_loss:356.41400146484375\n",
            "217/3000 train_loss: 324.6125793457031 test_loss:351.9848327636719\n",
            "218/3000 train_loss: 330.634033203125 test_loss:363.62298583984375\n",
            "219/3000 train_loss: 316.8573303222656 test_loss:349.9222717285156\n",
            "220/3000 train_loss: 317.86358642578125 test_loss:350.6156005859375\n",
            "221/3000 train_loss: 328.6878662109375 test_loss:356.0771179199219\n",
            "222/3000 train_loss: 318.64013671875 test_loss:365.94775390625\n",
            "223/3000 train_loss: 310.7432861328125 test_loss:351.5917663574219\n",
            "224/3000 train_loss: 314.2738342285156 test_loss:350.9051513671875\n",
            "225/3000 train_loss: 316.45147705078125 test_loss:346.9920349121094\n",
            "226/3000 train_loss: 317.29931640625 test_loss:344.9203186035156\n",
            "227/3000 train_loss: 307.3791198730469 test_loss:343.5245056152344\n",
            "228/3000 train_loss: 290.1134033203125 test_loss:339.2362365722656\n",
            "229/3000 train_loss: 304.2916564941406 test_loss:349.6351318359375\n",
            "230/3000 train_loss: 319.15765380859375 test_loss:350.8792724609375\n",
            "231/3000 train_loss: 312.9863586425781 test_loss:346.56781005859375\n",
            "232/3000 train_loss: 297.8528137207031 test_loss:352.8210144042969\n",
            "233/3000 train_loss: 318.240966796875 test_loss:339.7646179199219\n",
            "234/3000 train_loss: 291.8486022949219 test_loss:334.5035095214844\n",
            "235/3000 train_loss: 287.6921691894531 test_loss:336.76922607421875\n",
            "236/3000 train_loss: 302.8231201171875 test_loss:333.22589111328125\n",
            "237/3000 train_loss: 311.23394775390625 test_loss:339.3985290527344\n",
            "238/3000 train_loss: 285.20709228515625 test_loss:335.96502685546875\n",
            "239/3000 train_loss: 300.21478271484375 test_loss:332.2646179199219\n",
            "240/3000 train_loss: 298.11181640625 test_loss:331.0655517578125\n",
            "241/3000 train_loss: 294.2508239746094 test_loss:328.14202880859375\n",
            "242/3000 train_loss: 292.1028747558594 test_loss:326.74908447265625\n",
            "243/3000 train_loss: 288.64019775390625 test_loss:335.0495300292969\n",
            "244/3000 train_loss: 312.69439697265625 test_loss:327.3611755371094\n",
            "245/3000 train_loss: 300.9526672363281 test_loss:329.8267822265625\n",
            "246/3000 train_loss: 279.97540283203125 test_loss:324.36199951171875\n",
            "247/3000 train_loss: 281.50518798828125 test_loss:328.3955383300781\n",
            "248/3000 train_loss: 284.6106872558594 test_loss:327.2437744140625\n",
            "249/3000 train_loss: 278.9334716796875 test_loss:323.9246826171875\n",
            "250/3000 train_loss: 281.8548889160156 test_loss:325.6705322265625\n",
            "251/3000 train_loss: 280.2371520996094 test_loss:328.901123046875\n",
            "252/3000 train_loss: 309.6331481933594 test_loss:332.113525390625\n",
            "253/3000 train_loss: 289.9656982421875 test_loss:328.94171142578125\n",
            "254/3000 train_loss: 282.0548400878906 test_loss:317.34197998046875\n",
            "255/3000 train_loss: 290.33563232421875 test_loss:319.0625\n",
            "256/3000 train_loss: 276.97509765625 test_loss:316.6751708984375\n",
            "257/3000 train_loss: 265.9464111328125 test_loss:316.25897216796875\n",
            "258/3000 train_loss: 270.6566162109375 test_loss:326.25811767578125\n",
            "259/3000 train_loss: 275.3434143066406 test_loss:314.0896911621094\n",
            "260/3000 train_loss: 277.6036071777344 test_loss:313.98223876953125\n",
            "261/3000 train_loss: 280.5763854980469 test_loss:313.94287109375\n",
            "262/3000 train_loss: 275.8343505859375 test_loss:315.1586608886719\n",
            "263/3000 train_loss: 286.2176208496094 test_loss:313.10345458984375\n",
            "264/3000 train_loss: 284.57525634765625 test_loss:313.6319885253906\n",
            "265/3000 train_loss: 274.8350524902344 test_loss:306.1123962402344\n",
            "266/3000 train_loss: 276.9580383300781 test_loss:316.51605224609375\n",
            "267/3000 train_loss: 286.7499084472656 test_loss:322.5946044921875\n",
            "268/3000 train_loss: 278.3856506347656 test_loss:319.52777099609375\n",
            "269/3000 train_loss: 260.3113708496094 test_loss:310.2764587402344\n",
            "270/3000 train_loss: 264.2306213378906 test_loss:308.5090026855469\n",
            "271/3000 train_loss: 274.455322265625 test_loss:307.7962341308594\n",
            "272/3000 train_loss: 267.9227600097656 test_loss:310.3617248535156\n",
            "273/3000 train_loss: 268.69427490234375 test_loss:303.2160949707031\n",
            "274/3000 train_loss: 260.3525085449219 test_loss:309.5028991699219\n",
            "275/3000 train_loss: 266.0395812988281 test_loss:305.55706787109375\n",
            "276/3000 train_loss: 264.9712829589844 test_loss:303.8078308105469\n",
            "277/3000 train_loss: 269.1685791015625 test_loss:307.2915344238281\n",
            "278/3000 train_loss: 273.0928039550781 test_loss:306.87799072265625\n",
            "279/3000 train_loss: 264.4798889160156 test_loss:305.7117614746094\n",
            "280/3000 train_loss: 255.51376342773438 test_loss:303.489013671875\n",
            "281/3000 train_loss: 266.3262939453125 test_loss:306.957763671875\n",
            "282/3000 train_loss: 257.6102294921875 test_loss:301.32855224609375\n",
            "283/3000 train_loss: 249.62142944335938 test_loss:305.75067138671875\n",
            "284/3000 train_loss: 265.241943359375 test_loss:302.3464050292969\n",
            "285/3000 train_loss: 261.3728942871094 test_loss:301.2140808105469\n",
            "286/3000 train_loss: 257.8688049316406 test_loss:307.6813659667969\n",
            "287/3000 train_loss: 247.3863983154297 test_loss:304.68023681640625\n",
            "288/3000 train_loss: 255.11561584472656 test_loss:298.894287109375\n",
            "289/3000 train_loss: 237.03350830078125 test_loss:297.5205078125\n",
            "290/3000 train_loss: 249.89254760742188 test_loss:293.6614074707031\n",
            "291/3000 train_loss: 261.8204650878906 test_loss:299.1680603027344\n",
            "292/3000 train_loss: 242.8015594482422 test_loss:301.3143310546875\n",
            "293/3000 train_loss: 269.3746643066406 test_loss:296.3792724609375\n",
            "294/3000 train_loss: 254.8636016845703 test_loss:297.62176513671875\n",
            "295/3000 train_loss: 256.44256591796875 test_loss:294.9493408203125\n",
            "296/3000 train_loss: 256.05426025390625 test_loss:294.6925964355469\n",
            "297/3000 train_loss: 247.8756866455078 test_loss:302.20623779296875\n",
            "298/3000 train_loss: 249.06967163085938 test_loss:295.9171142578125\n",
            "299/3000 train_loss: 243.56475830078125 test_loss:292.1417541503906\n",
            "300/3000 train_loss: 246.83973693847656 test_loss:292.08062744140625\n",
            "301/3000 train_loss: 268.1732177734375 test_loss:296.77862548828125\n",
            "302/3000 train_loss: 245.29232788085938 test_loss:294.1365966796875\n",
            "303/3000 train_loss: 237.73675537109375 test_loss:292.86810302734375\n",
            "304/3000 train_loss: 243.36361694335938 test_loss:297.5798034667969\n",
            "305/3000 train_loss: 252.45114135742188 test_loss:288.49029541015625\n",
            "306/3000 train_loss: 252.86407470703125 test_loss:292.07159423828125\n",
            "307/3000 train_loss: 230.55886840820312 test_loss:293.78741455078125\n",
            "308/3000 train_loss: 236.84730529785156 test_loss:290.7004089355469\n",
            "309/3000 train_loss: 245.7117919921875 test_loss:287.98602294921875\n",
            "310/3000 train_loss: 237.36526489257812 test_loss:289.6891784667969\n",
            "311/3000 train_loss: 244.88877868652344 test_loss:282.7500915527344\n",
            "312/3000 train_loss: 255.73526000976562 test_loss:284.3243713378906\n",
            "313/3000 train_loss: 234.5417022705078 test_loss:289.56585693359375\n",
            "314/3000 train_loss: 265.2966613769531 test_loss:288.96240234375\n",
            "315/3000 train_loss: 239.84280395507812 test_loss:286.5860290527344\n",
            "316/3000 train_loss: 247.87521362304688 test_loss:283.82171630859375\n",
            "317/3000 train_loss: 236.58273315429688 test_loss:281.13189697265625\n",
            "318/3000 train_loss: 231.84210205078125 test_loss:282.3450622558594\n",
            "319/3000 train_loss: 244.97344970703125 test_loss:296.8787841796875\n",
            "320/3000 train_loss: 234.99014282226562 test_loss:278.3901672363281\n",
            "321/3000 train_loss: 231.74021911621094 test_loss:277.96343994140625\n",
            "322/3000 train_loss: 224.36180114746094 test_loss:287.9659118652344\n",
            "323/3000 train_loss: 235.75094604492188 test_loss:280.8153991699219\n",
            "324/3000 train_loss: 245.24156188964844 test_loss:283.408203125\n",
            "325/3000 train_loss: 251.23101806640625 test_loss:281.05047607421875\n",
            "326/3000 train_loss: 222.78341674804688 test_loss:282.3109130859375\n",
            "327/3000 train_loss: 234.27935791015625 test_loss:278.1680603027344\n",
            "328/3000 train_loss: 230.600341796875 test_loss:282.382080078125\n",
            "329/3000 train_loss: 228.95013427734375 test_loss:281.66021728515625\n",
            "330/3000 train_loss: 230.49789428710938 test_loss:275.4224853515625\n",
            "331/3000 train_loss: 239.1781463623047 test_loss:287.3165588378906\n",
            "332/3000 train_loss: 238.45919799804688 test_loss:281.4931640625\n",
            "333/3000 train_loss: 227.66751098632812 test_loss:274.6045227050781\n",
            "334/3000 train_loss: 225.88876342773438 test_loss:280.9269714355469\n",
            "335/3000 train_loss: 231.1550750732422 test_loss:278.51715087890625\n",
            "336/3000 train_loss: 211.74728393554688 test_loss:275.3201599121094\n",
            "337/3000 train_loss: 213.420654296875 test_loss:273.9462890625\n",
            "338/3000 train_loss: 216.97457885742188 test_loss:267.79742431640625\n",
            "339/3000 train_loss: 220.98374938964844 test_loss:275.8319091796875\n",
            "340/3000 train_loss: 209.2917022705078 test_loss:267.9725036621094\n",
            "341/3000 train_loss: 216.72915649414062 test_loss:271.90472412109375\n",
            "342/3000 train_loss: 217.3201446533203 test_loss:271.8582763671875\n",
            "343/3000 train_loss: 210.64405822753906 test_loss:275.5885314941406\n",
            "344/3000 train_loss: 212.25238037109375 test_loss:269.0509948730469\n",
            "345/3000 train_loss: 210.06259155273438 test_loss:263.4808044433594\n",
            "346/3000 train_loss: 210.10244750976562 test_loss:274.52337646484375\n",
            "347/3000 train_loss: 211.98812866210938 test_loss:262.9208984375\n",
            "348/3000 train_loss: 200.96327209472656 test_loss:266.14501953125\n",
            "349/3000 train_loss: 209.99485778808594 test_loss:266.6723327636719\n",
            "350/3000 train_loss: 205.84115600585938 test_loss:273.4501953125\n",
            "351/3000 train_loss: 226.09681701660156 test_loss:265.0299072265625\n",
            "352/3000 train_loss: 207.4431915283203 test_loss:275.36572265625\n",
            "353/3000 train_loss: 212.07647705078125 test_loss:266.4151916503906\n",
            "354/3000 train_loss: 214.0245361328125 test_loss:265.7620849609375\n",
            "355/3000 train_loss: 202.6881103515625 test_loss:260.25274658203125\n",
            "356/3000 train_loss: 210.01531982421875 test_loss:260.6424560546875\n",
            "357/3000 train_loss: 204.65118408203125 test_loss:267.92852783203125\n",
            "358/3000 train_loss: 207.69195556640625 test_loss:265.4041748046875\n",
            "359/3000 train_loss: 197.77699279785156 test_loss:262.2969970703125\n",
            "360/3000 train_loss: 195.7578887939453 test_loss:262.72491455078125\n",
            "361/3000 train_loss: 198.83242797851562 test_loss:257.92938232421875\n",
            "362/3000 train_loss: 188.9742431640625 test_loss:256.60009765625\n",
            "363/3000 train_loss: 200.3850555419922 test_loss:256.1551513671875\n",
            "364/3000 train_loss: 198.49057006835938 test_loss:259.5540771484375\n",
            "365/3000 train_loss: 206.0753173828125 test_loss:254.1083221435547\n",
            "366/3000 train_loss: 195.47113037109375 test_loss:254.88363647460938\n",
            "367/3000 train_loss: 195.5826416015625 test_loss:252.2604522705078\n",
            "368/3000 train_loss: 194.49798583984375 test_loss:263.6136779785156\n",
            "369/3000 train_loss: 209.11041259765625 test_loss:260.182861328125\n",
            "370/3000 train_loss: 204.4405517578125 test_loss:251.21243286132812\n",
            "371/3000 train_loss: 193.19337463378906 test_loss:250.67691040039062\n",
            "372/3000 train_loss: 204.8095245361328 test_loss:253.5791015625\n",
            "373/3000 train_loss: 187.3024139404297 test_loss:248.11880493164062\n",
            "374/3000 train_loss: 191.36721801757812 test_loss:251.31854248046875\n",
            "375/3000 train_loss: 199.35858154296875 test_loss:249.611572265625\n",
            "376/3000 train_loss: 182.5689697265625 test_loss:249.3850860595703\n",
            "377/3000 train_loss: 189.74288940429688 test_loss:248.21815490722656\n",
            "378/3000 train_loss: 214.2254180908203 test_loss:255.944580078125\n",
            "379/3000 train_loss: 188.16197204589844 test_loss:252.28170776367188\n",
            "380/3000 train_loss: 202.4773712158203 test_loss:253.99404907226562\n",
            "381/3000 train_loss: 203.3267364501953 test_loss:247.2628173828125\n",
            "382/3000 train_loss: 187.8223876953125 test_loss:246.35494995117188\n",
            "383/3000 train_loss: 185.48060607910156 test_loss:242.87918090820312\n",
            "384/3000 train_loss: 188.97447204589844 test_loss:245.54034423828125\n",
            "385/3000 train_loss: 191.57064819335938 test_loss:249.30926513671875\n",
            "386/3000 train_loss: 193.96258544921875 test_loss:241.21038818359375\n",
            "387/3000 train_loss: 184.80250549316406 test_loss:243.60391235351562\n",
            "388/3000 train_loss: 197.15737915039062 test_loss:248.03543090820312\n",
            "389/3000 train_loss: 185.2389373779297 test_loss:247.3948974609375\n",
            "390/3000 train_loss: 184.03587341308594 test_loss:243.32208251953125\n",
            "391/3000 train_loss: 188.99172973632812 test_loss:247.69940185546875\n",
            "392/3000 train_loss: 183.5520477294922 test_loss:240.97142028808594\n",
            "393/3000 train_loss: 186.47433471679688 test_loss:238.39385986328125\n",
            "394/3000 train_loss: 183.81585693359375 test_loss:237.98931884765625\n",
            "395/3000 train_loss: 176.98849487304688 test_loss:244.6658935546875\n",
            "396/3000 train_loss: 175.92247009277344 test_loss:246.48829650878906\n",
            "397/3000 train_loss: 184.64437866210938 test_loss:239.3076171875\n",
            "398/3000 train_loss: 174.17108154296875 test_loss:235.53363037109375\n",
            "399/3000 train_loss: 170.56396484375 test_loss:239.13543701171875\n",
            "400/3000 train_loss: 180.8529510498047 test_loss:237.58433532714844\n",
            "401/3000 train_loss: 175.26235961914062 test_loss:235.7657470703125\n",
            "402/3000 train_loss: 186.52967834472656 test_loss:248.5230255126953\n",
            "403/3000 train_loss: 187.48013305664062 test_loss:236.7399444580078\n",
            "404/3000 train_loss: 166.8446807861328 test_loss:235.28578186035156\n",
            "405/3000 train_loss: 182.0924530029297 test_loss:242.12432861328125\n",
            "406/3000 train_loss: 172.66665649414062 test_loss:233.06338500976562\n",
            "407/3000 train_loss: 175.7210693359375 test_loss:242.14163208007812\n",
            "408/3000 train_loss: 179.7014923095703 test_loss:236.61000061035156\n",
            "409/3000 train_loss: 172.67816162109375 test_loss:234.75393676757812\n",
            "410/3000 train_loss: 182.73556518554688 test_loss:237.6378631591797\n",
            "411/3000 train_loss: 180.43724060058594 test_loss:234.89173889160156\n",
            "412/3000 train_loss: 177.24266052246094 test_loss:233.45899963378906\n",
            "413/3000 train_loss: 171.6802978515625 test_loss:235.99240112304688\n",
            "414/3000 train_loss: 174.74974060058594 test_loss:231.01034545898438\n",
            "415/3000 train_loss: 171.03341674804688 test_loss:232.8330078125\n",
            "416/3000 train_loss: 176.51596069335938 test_loss:228.8900604248047\n",
            "417/3000 train_loss: 178.71298217773438 test_loss:234.2092742919922\n",
            "418/3000 train_loss: 167.59725952148438 test_loss:228.3596649169922\n",
            "419/3000 train_loss: 163.39627075195312 test_loss:227.47698974609375\n",
            "420/3000 train_loss: 159.27622985839844 test_loss:229.9579315185547\n",
            "421/3000 train_loss: 163.424072265625 test_loss:230.1002197265625\n",
            "422/3000 train_loss: 166.9752197265625 test_loss:235.227294921875\n",
            "423/3000 train_loss: 162.3302459716797 test_loss:229.8324432373047\n",
            "424/3000 train_loss: 167.79420471191406 test_loss:233.9795684814453\n",
            "425/3000 train_loss: 163.08670043945312 test_loss:231.60260009765625\n",
            "426/3000 train_loss: 171.3031005859375 test_loss:228.9549560546875\n",
            "427/3000 train_loss: 154.4380645751953 test_loss:223.67523193359375\n",
            "428/3000 train_loss: 162.0375213623047 test_loss:223.064453125\n",
            "429/3000 train_loss: 165.97398376464844 test_loss:223.99160766601562\n",
            "430/3000 train_loss: 159.40977478027344 test_loss:221.80569458007812\n",
            "431/3000 train_loss: 163.7925567626953 test_loss:228.71153259277344\n",
            "432/3000 train_loss: 158.89535522460938 test_loss:224.18276977539062\n",
            "433/3000 train_loss: 163.49200439453125 test_loss:220.5423583984375\n",
            "434/3000 train_loss: 159.394775390625 test_loss:216.74700927734375\n",
            "435/3000 train_loss: 159.52243041992188 test_loss:216.8190155029297\n",
            "436/3000 train_loss: 150.3180694580078 test_loss:220.17453002929688\n",
            "437/3000 train_loss: 161.18113708496094 test_loss:217.21156311035156\n",
            "438/3000 train_loss: 146.82525634765625 test_loss:218.24063110351562\n",
            "439/3000 train_loss: 147.03787231445312 test_loss:216.7393341064453\n",
            "440/3000 train_loss: 159.16549682617188 test_loss:219.79000854492188\n",
            "441/3000 train_loss: 156.31971740722656 test_loss:218.20938110351562\n",
            "442/3000 train_loss: 152.67205810546875 test_loss:220.6305389404297\n",
            "443/3000 train_loss: 149.84889221191406 test_loss:227.253662109375\n",
            "444/3000 train_loss: 146.67755126953125 test_loss:221.8918914794922\n",
            "445/3000 train_loss: 150.6167755126953 test_loss:224.79351806640625\n",
            "446/3000 train_loss: 152.45896911621094 test_loss:215.93087768554688\n",
            "447/3000 train_loss: 142.60142517089844 test_loss:219.39808654785156\n",
            "448/3000 train_loss: 150.52047729492188 test_loss:216.20228576660156\n",
            "449/3000 train_loss: 168.06326293945312 test_loss:222.3668212890625\n",
            "450/3000 train_loss: 150.7171630859375 test_loss:218.4840087890625\n",
            "451/3000 train_loss: 143.76235961914062 test_loss:214.3959197998047\n",
            "452/3000 train_loss: 152.85772705078125 test_loss:212.58587646484375\n",
            "453/3000 train_loss: 143.70269775390625 test_loss:220.14505004882812\n",
            "454/3000 train_loss: 152.62074279785156 test_loss:223.01004028320312\n",
            "455/3000 train_loss: 150.78866577148438 test_loss:213.30538940429688\n",
            "456/3000 train_loss: 140.54383850097656 test_loss:208.9730224609375\n",
            "457/3000 train_loss: 145.6412811279297 test_loss:212.4402618408203\n",
            "458/3000 train_loss: 138.6888427734375 test_loss:216.3839874267578\n",
            "459/3000 train_loss: 144.83030700683594 test_loss:208.57363891601562\n",
            "460/3000 train_loss: 145.1972198486328 test_loss:216.00123596191406\n",
            "461/3000 train_loss: 141.471435546875 test_loss:210.91111755371094\n",
            "462/3000 train_loss: 141.26641845703125 test_loss:212.15040588378906\n",
            "463/3000 train_loss: 138.42091369628906 test_loss:214.89129638671875\n",
            "464/3000 train_loss: 142.75511169433594 test_loss:211.220458984375\n",
            "465/3000 train_loss: 140.40313720703125 test_loss:207.70797729492188\n",
            "466/3000 train_loss: 137.1766357421875 test_loss:206.25926208496094\n",
            "467/3000 train_loss: 132.83494567871094 test_loss:208.4750213623047\n",
            "468/3000 train_loss: 143.666259765625 test_loss:211.95358276367188\n",
            "469/3000 train_loss: 134.69876098632812 test_loss:207.00143432617188\n",
            "470/3000 train_loss: 132.95436096191406 test_loss:206.51307678222656\n",
            "471/3000 train_loss: 139.90530395507812 test_loss:205.64988708496094\n",
            "472/3000 train_loss: 135.0144805908203 test_loss:211.54623413085938\n",
            "473/3000 train_loss: 137.3596954345703 test_loss:204.88436889648438\n",
            "474/3000 train_loss: 133.72117614746094 test_loss:205.80628967285156\n",
            "475/3000 train_loss: 140.83021545410156 test_loss:207.51419067382812\n",
            "476/3000 train_loss: 136.60025024414062 test_loss:219.8045196533203\n",
            "477/3000 train_loss: 136.0979461669922 test_loss:208.59521484375\n",
            "478/3000 train_loss: 139.71170043945312 test_loss:203.59242248535156\n",
            "479/3000 train_loss: 137.37571716308594 test_loss:206.18441772460938\n",
            "480/3000 train_loss: 139.88638305664062 test_loss:206.86749267578125\n",
            "481/3000 train_loss: 140.0290069580078 test_loss:205.24668884277344\n",
            "482/3000 train_loss: 134.35281372070312 test_loss:207.9004669189453\n",
            "483/3000 train_loss: 138.9604949951172 test_loss:214.61305236816406\n",
            "484/3000 train_loss: 138.3816375732422 test_loss:204.1552734375\n",
            "485/3000 train_loss: 131.1787567138672 test_loss:208.79928588867188\n",
            "486/3000 train_loss: 137.11947631835938 test_loss:202.5738983154297\n",
            "487/3000 train_loss: 129.43272399902344 test_loss:202.36923217773438\n",
            "488/3000 train_loss: 144.90835571289062 test_loss:207.28353881835938\n",
            "489/3000 train_loss: 134.5853271484375 test_loss:201.7086944580078\n",
            "490/3000 train_loss: 131.72926330566406 test_loss:199.7832489013672\n",
            "491/3000 train_loss: 124.87117004394531 test_loss:200.40806579589844\n",
            "492/3000 train_loss: 142.43589782714844 test_loss:205.45330810546875\n",
            "493/3000 train_loss: 134.380615234375 test_loss:201.20054626464844\n",
            "494/3000 train_loss: 128.14955139160156 test_loss:208.02590942382812\n",
            "495/3000 train_loss: 126.10993957519531 test_loss:200.04185485839844\n",
            "496/3000 train_loss: 129.0904541015625 test_loss:199.4439239501953\n",
            "497/3000 train_loss: 129.90872192382812 test_loss:206.02455139160156\n",
            "498/3000 train_loss: 131.30072021484375 test_loss:197.1918487548828\n",
            "499/3000 train_loss: 141.51083374023438 test_loss:198.66827392578125\n",
            "500/3000 train_loss: 127.55074310302734 test_loss:206.54644775390625\n",
            "501/3000 train_loss: 142.1856689453125 test_loss:204.1857452392578\n",
            "502/3000 train_loss: 136.98770141601562 test_loss:202.4100341796875\n",
            "503/3000 train_loss: 137.0258331298828 test_loss:202.0560760498047\n",
            "504/3000 train_loss: 136.00709533691406 test_loss:197.0217742919922\n",
            "505/3000 train_loss: 133.95272827148438 test_loss:198.10096740722656\n",
            "506/3000 train_loss: 127.37873840332031 test_loss:199.81773376464844\n",
            "507/3000 train_loss: 129.2191619873047 test_loss:193.62428283691406\n",
            "508/3000 train_loss: 124.0213851928711 test_loss:194.57260131835938\n",
            "509/3000 train_loss: 125.39990234375 test_loss:196.9913330078125\n",
            "510/3000 train_loss: 132.61846923828125 test_loss:198.48574829101562\n",
            "511/3000 train_loss: 129.82167053222656 test_loss:195.42422485351562\n",
            "512/3000 train_loss: 124.58329010009766 test_loss:196.49942016601562\n",
            "513/3000 train_loss: 122.87801361083984 test_loss:193.677001953125\n",
            "514/3000 train_loss: 119.78986358642578 test_loss:191.2203369140625\n",
            "515/3000 train_loss: 124.00232696533203 test_loss:193.0073699951172\n",
            "516/3000 train_loss: 121.22097778320312 test_loss:196.12704467773438\n",
            "517/3000 train_loss: 131.79054260253906 test_loss:192.73814392089844\n",
            "518/3000 train_loss: 125.81535339355469 test_loss:194.666259765625\n",
            "519/3000 train_loss: 126.01972961425781 test_loss:192.90562438964844\n",
            "520/3000 train_loss: 125.49172973632812 test_loss:189.63067626953125\n",
            "521/3000 train_loss: 119.98182678222656 test_loss:191.53761291503906\n",
            "522/3000 train_loss: 130.91970825195312 test_loss:190.54107666015625\n",
            "523/3000 train_loss: 127.36822509765625 test_loss:193.12059020996094\n",
            "524/3000 train_loss: 126.21705627441406 test_loss:194.35287475585938\n",
            "525/3000 train_loss: 120.24759674072266 test_loss:191.5623016357422\n",
            "526/3000 train_loss: 123.38407897949219 test_loss:189.30838012695312\n",
            "527/3000 train_loss: 124.21528625488281 test_loss:187.09112548828125\n",
            "528/3000 train_loss: 115.53678131103516 test_loss:189.27987670898438\n",
            "529/3000 train_loss: 127.0218276977539 test_loss:197.56736755371094\n",
            "530/3000 train_loss: 124.8992691040039 test_loss:192.3184356689453\n",
            "531/3000 train_loss: 117.15103149414062 test_loss:187.90855407714844\n",
            "532/3000 train_loss: 116.91315460205078 test_loss:189.88580322265625\n",
            "533/3000 train_loss: 120.36431884765625 test_loss:191.112548828125\n",
            "534/3000 train_loss: 126.80790710449219 test_loss:185.69778442382812\n",
            "535/3000 train_loss: 117.85824584960938 test_loss:187.2298126220703\n",
            "536/3000 train_loss: 117.8432846069336 test_loss:186.78465270996094\n",
            "537/3000 train_loss: 119.63253784179688 test_loss:185.54566955566406\n",
            "538/3000 train_loss: 117.2415542602539 test_loss:187.13328552246094\n",
            "539/3000 train_loss: 115.00820922851562 test_loss:193.66941833496094\n",
            "540/3000 train_loss: 124.84490203857422 test_loss:184.56271362304688\n",
            "541/3000 train_loss: 115.68911743164062 test_loss:188.52651977539062\n",
            "542/3000 train_loss: 129.29150390625 test_loss:180.2998504638672\n",
            "543/3000 train_loss: 122.13091278076172 test_loss:185.31857299804688\n",
            "544/3000 train_loss: 113.98587036132812 test_loss:186.82562255859375\n",
            "545/3000 train_loss: 129.04147338867188 test_loss:185.80499267578125\n",
            "546/3000 train_loss: 128.65029907226562 test_loss:188.4227752685547\n",
            "547/3000 train_loss: 119.55908203125 test_loss:193.35971069335938\n",
            "548/3000 train_loss: 118.06192779541016 test_loss:186.7580108642578\n",
            "549/3000 train_loss: 114.54435729980469 test_loss:185.76156616210938\n",
            "550/3000 train_loss: 112.43084716796875 test_loss:184.1751251220703\n",
            "551/3000 train_loss: 118.93534851074219 test_loss:183.95989990234375\n",
            "552/3000 train_loss: 113.85919189453125 test_loss:182.69412231445312\n",
            "553/3000 train_loss: 112.18991088867188 test_loss:181.97215270996094\n",
            "554/3000 train_loss: 111.0478515625 test_loss:186.0911102294922\n",
            "555/3000 train_loss: 109.86267852783203 test_loss:184.28070068359375\n",
            "556/3000 train_loss: 118.2582015991211 test_loss:187.01023864746094\n",
            "557/3000 train_loss: 121.1800308227539 test_loss:177.05844116210938\n",
            "558/3000 train_loss: 114.68864440917969 test_loss:183.37603759765625\n",
            "559/3000 train_loss: 124.22847747802734 test_loss:178.193359375\n",
            "560/3000 train_loss: 110.84325408935547 test_loss:183.13653564453125\n",
            "561/3000 train_loss: 109.96388244628906 test_loss:181.22645568847656\n",
            "562/3000 train_loss: 110.23175048828125 test_loss:180.5720977783203\n",
            "563/3000 train_loss: 112.47834777832031 test_loss:177.76171875\n",
            "564/3000 train_loss: 115.75346374511719 test_loss:177.32272338867188\n",
            "565/3000 train_loss: 113.51931762695312 test_loss:181.0272979736328\n",
            "566/3000 train_loss: 110.25997161865234 test_loss:180.4122772216797\n",
            "567/3000 train_loss: 103.27143096923828 test_loss:178.77195739746094\n",
            "568/3000 train_loss: 111.53634643554688 test_loss:178.6839141845703\n",
            "569/3000 train_loss: 113.55825805664062 test_loss:181.54803466796875\n",
            "570/3000 train_loss: 111.39106750488281 test_loss:182.44241333007812\n",
            "571/3000 train_loss: 112.06348419189453 test_loss:177.21554565429688\n",
            "572/3000 train_loss: 108.03630828857422 test_loss:187.90069580078125\n",
            "573/3000 train_loss: 114.3911361694336 test_loss:187.4293212890625\n",
            "574/3000 train_loss: 111.25701904296875 test_loss:181.02645874023438\n",
            "575/3000 train_loss: 111.1321029663086 test_loss:175.80128479003906\n",
            "576/3000 train_loss: 110.99016571044922 test_loss:173.19903564453125\n",
            "577/3000 train_loss: 104.8399429321289 test_loss:176.37179565429688\n",
            "578/3000 train_loss: 119.88214874267578 test_loss:183.42552185058594\n",
            "579/3000 train_loss: 106.90779113769531 test_loss:179.34677124023438\n",
            "580/3000 train_loss: 117.78008270263672 test_loss:176.74110412597656\n",
            "581/3000 train_loss: 111.73374938964844 test_loss:176.3267059326172\n",
            "582/3000 train_loss: 108.30239868164062 test_loss:177.1104278564453\n",
            "583/3000 train_loss: 106.41073608398438 test_loss:174.1522216796875\n",
            "584/3000 train_loss: 103.29535675048828 test_loss:174.02978515625\n",
            "585/3000 train_loss: 112.27460479736328 test_loss:179.57994079589844\n",
            "586/3000 train_loss: 110.92891693115234 test_loss:176.9776611328125\n",
            "587/3000 train_loss: 103.36207580566406 test_loss:173.52743530273438\n",
            "588/3000 train_loss: 102.25299835205078 test_loss:175.15635681152344\n",
            "589/3000 train_loss: 107.54997253417969 test_loss:176.40464782714844\n",
            "590/3000 train_loss: 112.57795715332031 test_loss:179.4950408935547\n",
            "591/3000 train_loss: 107.55316162109375 test_loss:176.384521484375\n",
            "592/3000 train_loss: 107.99979400634766 test_loss:173.12477111816406\n",
            "593/3000 train_loss: 103.93941497802734 test_loss:176.0297088623047\n",
            "594/3000 train_loss: 106.22805786132812 test_loss:174.22378540039062\n",
            "595/3000 train_loss: 106.3541259765625 test_loss:175.99560546875\n",
            "596/3000 train_loss: 99.40547943115234 test_loss:174.77244567871094\n",
            "597/3000 train_loss: 112.93452453613281 test_loss:172.43792724609375\n",
            "598/3000 train_loss: 108.30944061279297 test_loss:171.33633422851562\n",
            "599/3000 train_loss: 98.52264404296875 test_loss:175.00054931640625\n",
            "600/3000 train_loss: 105.7464599609375 test_loss:172.6929473876953\n",
            "601/3000 train_loss: 101.41524505615234 test_loss:170.2879180908203\n",
            "602/3000 train_loss: 102.94819641113281 test_loss:177.36697387695312\n",
            "603/3000 train_loss: 103.59300231933594 test_loss:178.08287048339844\n",
            "604/3000 train_loss: 112.6834487915039 test_loss:176.47361755371094\n",
            "605/3000 train_loss: 103.1361083984375 test_loss:172.7862091064453\n",
            "606/3000 train_loss: 107.75102996826172 test_loss:178.4063262939453\n",
            "607/3000 train_loss: 109.13178253173828 test_loss:173.8001251220703\n",
            "608/3000 train_loss: 97.09970092773438 test_loss:176.13690185546875\n",
            "609/3000 train_loss: 97.877197265625 test_loss:174.7386474609375\n",
            "610/3000 train_loss: 99.5692138671875 test_loss:175.04864501953125\n",
            "611/3000 train_loss: 105.22541809082031 test_loss:179.20993041992188\n",
            "612/3000 train_loss: 103.3188705444336 test_loss:174.24282836914062\n",
            "613/3000 train_loss: 101.05181884765625 test_loss:176.21067810058594\n",
            "614/3000 train_loss: 108.11795043945312 test_loss:165.8131866455078\n",
            "615/3000 train_loss: 102.35991668701172 test_loss:174.24725341796875\n",
            "616/3000 train_loss: 98.34190368652344 test_loss:175.64779663085938\n",
            "617/3000 train_loss: 104.21727752685547 test_loss:172.0743408203125\n",
            "618/3000 train_loss: 110.65460968017578 test_loss:180.4792938232422\n",
            "619/3000 train_loss: 108.6935806274414 test_loss:173.88784790039062\n",
            "620/3000 train_loss: 114.17127227783203 test_loss:173.38973999023438\n",
            "621/3000 train_loss: 98.8833236694336 test_loss:170.70098876953125\n",
            "622/3000 train_loss: 100.28923797607422 test_loss:169.7212677001953\n",
            "623/3000 train_loss: 109.72412872314453 test_loss:174.72525024414062\n",
            "624/3000 train_loss: 104.10159301757812 test_loss:167.97207641601562\n",
            "625/3000 train_loss: 95.2756576538086 test_loss:168.9432830810547\n",
            "626/3000 train_loss: 97.6059799194336 test_loss:173.76368713378906\n",
            "627/3000 train_loss: 100.38249206542969 test_loss:171.69007873535156\n",
            "628/3000 train_loss: 93.0523681640625 test_loss:171.43310546875\n",
            "629/3000 train_loss: 89.23471069335938 test_loss:166.099365234375\n",
            "630/3000 train_loss: 99.65492248535156 test_loss:163.9212646484375\n",
            "631/3000 train_loss: 93.136474609375 test_loss:166.63160705566406\n",
            "632/3000 train_loss: 104.68226623535156 test_loss:165.12191772460938\n",
            "633/3000 train_loss: 93.61788940429688 test_loss:169.14520263671875\n",
            "634/3000 train_loss: 94.25114440917969 test_loss:165.3017120361328\n",
            "635/3000 train_loss: 94.46546936035156 test_loss:162.51837158203125\n",
            "636/3000 train_loss: 93.9280776977539 test_loss:166.27210998535156\n",
            "637/3000 train_loss: 95.04585266113281 test_loss:165.31210327148438\n",
            "638/3000 train_loss: 96.73348236083984 test_loss:163.1046600341797\n",
            "639/3000 train_loss: 92.50531768798828 test_loss:161.05975341796875\n",
            "640/3000 train_loss: 89.00504302978516 test_loss:161.759033203125\n",
            "641/3000 train_loss: 91.13223266601562 test_loss:167.0044403076172\n",
            "642/3000 train_loss: 95.18595886230469 test_loss:177.13865661621094\n",
            "643/3000 train_loss: 96.49851989746094 test_loss:165.45114135742188\n",
            "644/3000 train_loss: 97.0624008178711 test_loss:165.14634704589844\n",
            "645/3000 train_loss: 92.38533020019531 test_loss:165.3748321533203\n",
            "646/3000 train_loss: 84.91500854492188 test_loss:165.549560546875\n",
            "647/3000 train_loss: 93.03512573242188 test_loss:165.069091796875\n",
            "648/3000 train_loss: 90.80590057373047 test_loss:168.8642120361328\n",
            "649/3000 train_loss: 96.4290542602539 test_loss:162.0493927001953\n",
            "650/3000 train_loss: 96.03585815429688 test_loss:165.43064880371094\n",
            "651/3000 train_loss: 98.05953979492188 test_loss:161.51426696777344\n",
            "652/3000 train_loss: 88.25273895263672 test_loss:160.06529235839844\n",
            "653/3000 train_loss: 95.7426528930664 test_loss:163.27182006835938\n",
            "654/3000 train_loss: 85.83068084716797 test_loss:165.90748596191406\n",
            "655/3000 train_loss: 114.33135986328125 test_loss:162.61891174316406\n",
            "656/3000 train_loss: 90.4830093383789 test_loss:169.02171325683594\n",
            "657/3000 train_loss: 90.63494873046875 test_loss:166.60450744628906\n",
            "658/3000 train_loss: 97.00715637207031 test_loss:169.68563842773438\n",
            "659/3000 train_loss: 87.21501922607422 test_loss:167.7261505126953\n",
            "660/3000 train_loss: 90.10926055908203 test_loss:164.3497314453125\n",
            "661/3000 train_loss: 98.67692565917969 test_loss:156.89479064941406\n",
            "662/3000 train_loss: 97.69999694824219 test_loss:167.83743286132812\n",
            "663/3000 train_loss: 85.8060531616211 test_loss:163.30056762695312\n",
            "664/3000 train_loss: 80.2900619506836 test_loss:162.05953979492188\n",
            "665/3000 train_loss: 86.03596496582031 test_loss:159.23690795898438\n",
            "666/3000 train_loss: 96.3253402709961 test_loss:163.24118041992188\n",
            "667/3000 train_loss: 87.94288635253906 test_loss:160.68234252929688\n",
            "668/3000 train_loss: 84.36797332763672 test_loss:160.54351806640625\n",
            "669/3000 train_loss: 87.65071868896484 test_loss:163.75682067871094\n",
            "670/3000 train_loss: 88.771728515625 test_loss:163.05050659179688\n",
            "671/3000 train_loss: 88.58070373535156 test_loss:166.47113037109375\n",
            "672/3000 train_loss: 83.85081481933594 test_loss:156.5039825439453\n",
            "673/3000 train_loss: 82.60594177246094 test_loss:160.3323211669922\n",
            "674/3000 train_loss: 82.00494384765625 test_loss:160.0507354736328\n",
            "675/3000 train_loss: 90.12740325927734 test_loss:157.26316833496094\n",
            "676/3000 train_loss: 92.76309204101562 test_loss:160.66600036621094\n",
            "677/3000 train_loss: 86.74565124511719 test_loss:167.88153076171875\n",
            "678/3000 train_loss: 88.55502319335938 test_loss:156.15206909179688\n",
            "679/3000 train_loss: 86.02588653564453 test_loss:156.32676696777344\n",
            "680/3000 train_loss: 88.17420959472656 test_loss:158.90223693847656\n",
            "681/3000 train_loss: 101.20468139648438 test_loss:169.17262268066406\n",
            "682/3000 train_loss: 91.5308609008789 test_loss:159.15899658203125\n",
            "683/3000 train_loss: 92.70930480957031 test_loss:160.834716796875\n",
            "684/3000 train_loss: 83.30241394042969 test_loss:165.02146911621094\n",
            "685/3000 train_loss: 95.3859634399414 test_loss:159.07566833496094\n",
            "686/3000 train_loss: 83.8964614868164 test_loss:155.55999755859375\n",
            "687/3000 train_loss: 89.95645141601562 test_loss:156.2160186767578\n",
            "688/3000 train_loss: 91.02938842773438 test_loss:161.4276885986328\n",
            "689/3000 train_loss: 84.88910675048828 test_loss:155.71865844726562\n",
            "690/3000 train_loss: 81.65782165527344 test_loss:156.0063018798828\n",
            "691/3000 train_loss: 90.79361724853516 test_loss:165.80445861816406\n",
            "692/3000 train_loss: 85.67842102050781 test_loss:156.05758666992188\n",
            "693/3000 train_loss: 82.11689758300781 test_loss:158.2406005859375\n",
            "694/3000 train_loss: 82.87571716308594 test_loss:150.7447052001953\n",
            "695/3000 train_loss: 83.68595123291016 test_loss:157.54598999023438\n",
            "696/3000 train_loss: 81.28334045410156 test_loss:152.28472900390625\n",
            "697/3000 train_loss: 84.42501831054688 test_loss:151.39610290527344\n",
            "698/3000 train_loss: 85.88774871826172 test_loss:150.83590698242188\n",
            "699/3000 train_loss: 92.80859375 test_loss:154.68072509765625\n",
            "700/3000 train_loss: 85.39347076416016 test_loss:155.69052124023438\n",
            "701/3000 train_loss: 84.01976776123047 test_loss:153.28587341308594\n",
            "702/3000 train_loss: 87.72122192382812 test_loss:157.73974609375\n",
            "703/3000 train_loss: 82.43470001220703 test_loss:153.56915283203125\n",
            "704/3000 train_loss: 88.473876953125 test_loss:155.8812713623047\n",
            "705/3000 train_loss: 85.58499908447266 test_loss:157.95323181152344\n",
            "706/3000 train_loss: 84.9465560913086 test_loss:158.1097869873047\n",
            "707/3000 train_loss: 80.52133178710938 test_loss:157.0892791748047\n",
            "708/3000 train_loss: 75.82965850830078 test_loss:156.1131591796875\n",
            "709/3000 train_loss: 89.57650756835938 test_loss:154.41758728027344\n",
            "710/3000 train_loss: 86.64631652832031 test_loss:156.6394805908203\n",
            "711/3000 train_loss: 96.37287139892578 test_loss:153.9346923828125\n",
            "712/3000 train_loss: 88.74312591552734 test_loss:158.35928344726562\n",
            "713/3000 train_loss: 89.64444732666016 test_loss:153.15652465820312\n",
            "714/3000 train_loss: 83.26264953613281 test_loss:151.01214599609375\n",
            "715/3000 train_loss: 82.92272186279297 test_loss:150.44432067871094\n",
            "716/3000 train_loss: 83.48883819580078 test_loss:153.2921905517578\n",
            "717/3000 train_loss: 83.03038787841797 test_loss:155.06552124023438\n",
            "718/3000 train_loss: 84.0462646484375 test_loss:152.51849365234375\n",
            "719/3000 train_loss: 79.1933822631836 test_loss:152.57301330566406\n",
            "720/3000 train_loss: 86.52269744873047 test_loss:149.42694091796875\n",
            "721/3000 train_loss: 81.56521606445312 test_loss:151.78623962402344\n",
            "722/3000 train_loss: 84.60459899902344 test_loss:152.63438415527344\n",
            "723/3000 train_loss: 77.40057373046875 test_loss:155.4503936767578\n",
            "724/3000 train_loss: 82.11612701416016 test_loss:153.49124145507812\n",
            "725/3000 train_loss: 81.79781341552734 test_loss:157.3563690185547\n",
            "726/3000 train_loss: 80.5126953125 test_loss:153.14804077148438\n",
            "727/3000 train_loss: 78.7659683227539 test_loss:154.14036560058594\n",
            "728/3000 train_loss: 80.28679656982422 test_loss:158.0789337158203\n",
            "729/3000 train_loss: 94.18299102783203 test_loss:154.7812042236328\n",
            "730/3000 train_loss: 81.34333801269531 test_loss:155.78135681152344\n",
            "731/3000 train_loss: 85.16629791259766 test_loss:155.3950958251953\n",
            "732/3000 train_loss: 75.99046325683594 test_loss:153.9582977294922\n",
            "733/3000 train_loss: 77.78398132324219 test_loss:157.88890075683594\n",
            "734/3000 train_loss: 78.5739517211914 test_loss:150.52935791015625\n",
            "735/3000 train_loss: 81.12632751464844 test_loss:153.730224609375\n",
            "736/3000 train_loss: 80.21379852294922 test_loss:149.18345642089844\n",
            "737/3000 train_loss: 84.4308853149414 test_loss:156.2434539794922\n",
            "738/3000 train_loss: 82.13539123535156 test_loss:157.1639404296875\n",
            "739/3000 train_loss: 81.32645416259766 test_loss:153.03761291503906\n",
            "740/3000 train_loss: 87.15621185302734 test_loss:148.60398864746094\n",
            "741/3000 train_loss: 76.86930084228516 test_loss:154.24386596679688\n",
            "742/3000 train_loss: 84.86132049560547 test_loss:151.26901245117188\n",
            "743/3000 train_loss: 87.31855773925781 test_loss:149.4040985107422\n",
            "744/3000 train_loss: 80.71959686279297 test_loss:158.77972412109375\n",
            "745/3000 train_loss: 81.61685180664062 test_loss:149.33395385742188\n",
            "746/3000 train_loss: 81.13599395751953 test_loss:151.3467254638672\n",
            "747/3000 train_loss: 89.5581283569336 test_loss:149.2654266357422\n",
            "748/3000 train_loss: 75.32621002197266 test_loss:152.82138061523438\n",
            "749/3000 train_loss: 92.48050689697266 test_loss:153.44947814941406\n",
            "750/3000 train_loss: 80.22417449951172 test_loss:147.71038818359375\n",
            "751/3000 train_loss: 82.61534118652344 test_loss:151.87147521972656\n",
            "752/3000 train_loss: 81.91747283935547 test_loss:148.03411865234375\n",
            "753/3000 train_loss: 74.42472839355469 test_loss:146.47210693359375\n",
            "754/3000 train_loss: 74.82183074951172 test_loss:153.57554626464844\n",
            "755/3000 train_loss: 78.5831069946289 test_loss:147.39166259765625\n",
            "756/3000 train_loss: 81.81551361083984 test_loss:150.04156494140625\n",
            "757/3000 train_loss: 80.91256713867188 test_loss:145.86587524414062\n",
            "758/3000 train_loss: 74.53407287597656 test_loss:148.74005126953125\n",
            "759/3000 train_loss: 76.9238510131836 test_loss:146.35227966308594\n",
            "760/3000 train_loss: 80.09902954101562 test_loss:148.98411560058594\n",
            "761/3000 train_loss: 84.17754364013672 test_loss:150.27532958984375\n",
            "762/3000 train_loss: 80.00942993164062 test_loss:152.33871459960938\n",
            "763/3000 train_loss: 76.49339294433594 test_loss:147.14747619628906\n",
            "764/3000 train_loss: 91.94780731201172 test_loss:152.6065216064453\n",
            "765/3000 train_loss: 79.830810546875 test_loss:150.84999084472656\n",
            "766/3000 train_loss: 73.98817443847656 test_loss:145.7901611328125\n",
            "767/3000 train_loss: 74.16553497314453 test_loss:148.79612731933594\n",
            "768/3000 train_loss: 75.09135437011719 test_loss:145.6904754638672\n",
            "769/3000 train_loss: 73.77845001220703 test_loss:146.7886505126953\n",
            "770/3000 train_loss: 84.54114532470703 test_loss:149.15151977539062\n",
            "771/3000 train_loss: 80.35726928710938 test_loss:151.87094116210938\n",
            "772/3000 train_loss: 70.35001373291016 test_loss:147.5255889892578\n",
            "773/3000 train_loss: 73.41581726074219 test_loss:147.0255126953125\n",
            "774/3000 train_loss: 76.94371795654297 test_loss:148.86395263671875\n",
            "775/3000 train_loss: 79.97093200683594 test_loss:144.5720672607422\n",
            "776/3000 train_loss: 79.32546997070312 test_loss:148.28558349609375\n",
            "777/3000 train_loss: 75.97856140136719 test_loss:145.60353088378906\n",
            "778/3000 train_loss: 68.78629302978516 test_loss:143.56890869140625\n",
            "779/3000 train_loss: 71.6924819946289 test_loss:139.0952606201172\n",
            "780/3000 train_loss: 74.4136734008789 test_loss:144.99908447265625\n",
            "781/3000 train_loss: 74.62860107421875 test_loss:146.1155242919922\n",
            "782/3000 train_loss: 76.13812255859375 test_loss:146.3633575439453\n",
            "783/3000 train_loss: 75.32267761230469 test_loss:148.8302764892578\n",
            "784/3000 train_loss: 76.66160583496094 test_loss:146.14280700683594\n",
            "785/3000 train_loss: 70.95297241210938 test_loss:150.08241271972656\n",
            "786/3000 train_loss: 82.54488372802734 test_loss:150.55923461914062\n",
            "787/3000 train_loss: 72.5572280883789 test_loss:149.2813720703125\n",
            "788/3000 train_loss: 77.05540466308594 test_loss:141.64015197753906\n",
            "789/3000 train_loss: 76.50630187988281 test_loss:148.6925048828125\n",
            "790/3000 train_loss: 71.19547271728516 test_loss:148.26646423339844\n",
            "791/3000 train_loss: 77.28166961669922 test_loss:148.10531616210938\n",
            "792/3000 train_loss: 73.8494644165039 test_loss:141.577392578125\n",
            "793/3000 train_loss: 82.02324676513672 test_loss:144.04945373535156\n",
            "794/3000 train_loss: 71.79791259765625 test_loss:148.1132354736328\n",
            "795/3000 train_loss: 73.32036590576172 test_loss:142.6433563232422\n",
            "796/3000 train_loss: 75.0997543334961 test_loss:148.9859619140625\n",
            "797/3000 train_loss: 71.8287124633789 test_loss:144.32789611816406\n",
            "798/3000 train_loss: 77.58599090576172 test_loss:143.03224182128906\n",
            "799/3000 train_loss: 70.93721008300781 test_loss:140.12937927246094\n",
            "800/3000 train_loss: 73.46701049804688 test_loss:142.86669921875\n",
            "801/3000 train_loss: 70.165283203125 test_loss:141.2518310546875\n",
            "802/3000 train_loss: 71.52140045166016 test_loss:142.28762817382812\n",
            "803/3000 train_loss: 70.52198028564453 test_loss:145.6507568359375\n",
            "804/3000 train_loss: 74.05681610107422 test_loss:144.9012451171875\n",
            "805/3000 train_loss: 69.564697265625 test_loss:146.9685821533203\n",
            "806/3000 train_loss: 73.41560363769531 test_loss:140.44252014160156\n",
            "807/3000 train_loss: 68.05353546142578 test_loss:143.07208251953125\n",
            "808/3000 train_loss: 75.4697036743164 test_loss:144.2923583984375\n",
            "809/3000 train_loss: 75.4015884399414 test_loss:144.58580017089844\n",
            "810/3000 train_loss: 73.13487243652344 test_loss:150.21749877929688\n",
            "811/3000 train_loss: 77.1334457397461 test_loss:145.28237915039062\n",
            "812/3000 train_loss: 71.56221008300781 test_loss:155.12884521484375\n",
            "813/3000 train_loss: 76.92532348632812 test_loss:144.8270263671875\n",
            "814/3000 train_loss: 71.95899963378906 test_loss:149.91061401367188\n",
            "815/3000 train_loss: 67.24919128417969 test_loss:146.25155639648438\n",
            "816/3000 train_loss: 67.2017593383789 test_loss:147.25218200683594\n",
            "817/3000 train_loss: 76.69268798828125 test_loss:150.82974243164062\n",
            "818/3000 train_loss: 69.70372009277344 test_loss:147.19105529785156\n",
            "819/3000 train_loss: 78.04735565185547 test_loss:144.36859130859375\n",
            "820/3000 train_loss: 73.36965942382812 test_loss:141.0953369140625\n",
            "821/3000 train_loss: 73.22853088378906 test_loss:172.5073699951172\n",
            "822/3000 train_loss: 71.6418228149414 test_loss:157.00479125976562\n",
            "823/3000 train_loss: 70.76593780517578 test_loss:147.4086151123047\n",
            "824/3000 train_loss: 76.678466796875 test_loss:147.7889404296875\n",
            "825/3000 train_loss: 72.72406768798828 test_loss:146.21377563476562\n",
            "826/3000 train_loss: 77.83128356933594 test_loss:140.0023956298828\n",
            "827/3000 train_loss: 70.6310806274414 test_loss:137.94154357910156\n",
            "828/3000 train_loss: 77.42454528808594 test_loss:150.0274200439453\n",
            "829/3000 train_loss: 65.67512512207031 test_loss:145.68992614746094\n",
            "830/3000 train_loss: 88.0104751586914 test_loss:145.1151580810547\n",
            "831/3000 train_loss: 71.54724884033203 test_loss:146.77615356445312\n",
            "832/3000 train_loss: 67.46846771240234 test_loss:153.77197265625\n",
            "833/3000 train_loss: 69.24292755126953 test_loss:143.95362854003906\n",
            "834/3000 train_loss: 73.68599700927734 test_loss:145.55921936035156\n",
            "835/3000 train_loss: 70.45197296142578 test_loss:148.8548126220703\n",
            "836/3000 train_loss: 73.31957244873047 test_loss:162.25604248046875\n",
            "837/3000 train_loss: 71.73753356933594 test_loss:162.9307403564453\n",
            "838/3000 train_loss: 74.5201187133789 test_loss:142.77040100097656\n",
            "839/3000 train_loss: 78.65650939941406 test_loss:151.9354705810547\n",
            "840/3000 train_loss: 69.04328155517578 test_loss:143.42291259765625\n",
            "841/3000 train_loss: 61.16435241699219 test_loss:138.16244506835938\n",
            "842/3000 train_loss: 68.39273071289062 test_loss:142.9792938232422\n",
            "843/3000 train_loss: 71.98140716552734 test_loss:143.5414581298828\n",
            "844/3000 train_loss: 72.61431121826172 test_loss:148.88768005371094\n",
            "845/3000 train_loss: 65.50723266601562 test_loss:143.86895751953125\n",
            "846/3000 train_loss: 65.60951232910156 test_loss:142.6129913330078\n",
            "847/3000 train_loss: 68.95731353759766 test_loss:142.728271484375\n",
            "848/3000 train_loss: 71.33676147460938 test_loss:141.39344787597656\n",
            "849/3000 train_loss: 72.69017028808594 test_loss:142.84608459472656\n",
            "850/3000 train_loss: 71.32138061523438 test_loss:142.89031982421875\n",
            "851/3000 train_loss: 65.94169616699219 test_loss:135.86695861816406\n",
            "852/3000 train_loss: 72.10072326660156 test_loss:142.2891845703125\n",
            "853/3000 train_loss: 68.49838256835938 test_loss:138.4307861328125\n",
            "854/3000 train_loss: 67.06212615966797 test_loss:136.128662109375\n",
            "855/3000 train_loss: 64.25425720214844 test_loss:142.06956481933594\n",
            "856/3000 train_loss: 69.28832244873047 test_loss:140.05430603027344\n",
            "857/3000 train_loss: 63.29965591430664 test_loss:147.25819396972656\n",
            "858/3000 train_loss: 67.48798370361328 test_loss:140.1938018798828\n",
            "859/3000 train_loss: 68.21222686767578 test_loss:144.828369140625\n",
            "860/3000 train_loss: 65.73779296875 test_loss:138.78466796875\n",
            "861/3000 train_loss: 67.24729919433594 test_loss:142.38613891601562\n",
            "862/3000 train_loss: 69.24285888671875 test_loss:143.01712036132812\n",
            "863/3000 train_loss: 67.6050796508789 test_loss:153.20657348632812\n",
            "864/3000 train_loss: 76.01285552978516 test_loss:141.78480529785156\n",
            "865/3000 train_loss: 69.66815185546875 test_loss:141.32940673828125\n",
            "866/3000 train_loss: 70.02014923095703 test_loss:140.66322326660156\n",
            "867/3000 train_loss: 69.57374572753906 test_loss:145.6992645263672\n",
            "868/3000 train_loss: 60.45315170288086 test_loss:137.11683654785156\n",
            "869/3000 train_loss: 68.03118133544922 test_loss:140.20663452148438\n",
            "870/3000 train_loss: 70.1444091796875 test_loss:144.7882843017578\n",
            "871/3000 train_loss: 68.975341796875 test_loss:141.31443786621094\n",
            "872/3000 train_loss: 63.06671142578125 test_loss:140.36337280273438\n",
            "873/3000 train_loss: 61.30255126953125 test_loss:144.875732421875\n",
            "874/3000 train_loss: 64.92245483398438 test_loss:137.23875427246094\n",
            "875/3000 train_loss: 62.71599578857422 test_loss:141.88619995117188\n",
            "876/3000 train_loss: 58.97932815551758 test_loss:139.9358673095703\n",
            "877/3000 train_loss: 67.08504486083984 test_loss:138.9275360107422\n",
            "878/3000 train_loss: 64.97296905517578 test_loss:137.84715270996094\n",
            "879/3000 train_loss: 67.26271057128906 test_loss:141.4799041748047\n",
            "880/3000 train_loss: 67.76957702636719 test_loss:139.66818237304688\n",
            "881/3000 train_loss: 61.22568893432617 test_loss:143.4536590576172\n",
            "882/3000 train_loss: 63.18192672729492 test_loss:135.69805908203125\n",
            "883/3000 train_loss: 69.53570556640625 test_loss:142.3543701171875\n",
            "884/3000 train_loss: 63.26709747314453 test_loss:135.72280883789062\n",
            "885/3000 train_loss: 67.78372955322266 test_loss:143.5286865234375\n",
            "886/3000 train_loss: 60.30588150024414 test_loss:142.9512176513672\n",
            "887/3000 train_loss: 73.09102630615234 test_loss:136.28045654296875\n",
            "888/3000 train_loss: 66.41490936279297 test_loss:136.4093780517578\n",
            "889/3000 train_loss: 64.07533264160156 test_loss:140.3666229248047\n",
            "890/3000 train_loss: 61.99279022216797 test_loss:140.6540069580078\n",
            "891/3000 train_loss: 60.938880920410156 test_loss:145.57228088378906\n",
            "892/3000 train_loss: 58.61376953125 test_loss:147.07443237304688\n",
            "893/3000 train_loss: 61.6451530456543 test_loss:140.72677612304688\n",
            "894/3000 train_loss: 65.60900115966797 test_loss:134.12367248535156\n",
            "895/3000 train_loss: 58.42266845703125 test_loss:136.36228942871094\n",
            "896/3000 train_loss: 60.033626556396484 test_loss:136.62586975097656\n",
            "897/3000 train_loss: 63.15024185180664 test_loss:134.4887237548828\n",
            "898/3000 train_loss: 69.77364349365234 test_loss:135.74859619140625\n",
            "899/3000 train_loss: 61.854183197021484 test_loss:144.40057373046875\n",
            "900/3000 train_loss: 63.1660270690918 test_loss:136.0584259033203\n",
            "901/3000 train_loss: 65.41094207763672 test_loss:136.5302276611328\n",
            "902/3000 train_loss: 63.51750946044922 test_loss:136.46937561035156\n",
            "903/3000 train_loss: 60.28617477416992 test_loss:136.20654296875\n",
            "904/3000 train_loss: 65.69273376464844 test_loss:131.34925842285156\n",
            "905/3000 train_loss: 60.865474700927734 test_loss:135.36270141601562\n",
            "906/3000 train_loss: 57.886375427246094 test_loss:138.23033142089844\n",
            "907/3000 train_loss: 66.55337524414062 test_loss:133.9145050048828\n",
            "908/3000 train_loss: 71.5213394165039 test_loss:131.99356079101562\n",
            "909/3000 train_loss: 64.48157501220703 test_loss:138.6998291015625\n",
            "910/3000 train_loss: 65.90296173095703 test_loss:135.22244262695312\n",
            "911/3000 train_loss: 59.1404914855957 test_loss:132.114501953125\n",
            "912/3000 train_loss: 63.73767852783203 test_loss:132.6405487060547\n",
            "913/3000 train_loss: 60.99359893798828 test_loss:129.03016662597656\n",
            "914/3000 train_loss: 67.20072174072266 test_loss:128.88638305664062\n",
            "915/3000 train_loss: 57.20000076293945 test_loss:130.1602020263672\n",
            "916/3000 train_loss: 55.929718017578125 test_loss:133.85336303710938\n",
            "917/3000 train_loss: 60.81529998779297 test_loss:127.0400390625\n",
            "918/3000 train_loss: 59.960426330566406 test_loss:130.3419189453125\n",
            "919/3000 train_loss: 64.214599609375 test_loss:132.0062713623047\n",
            "920/3000 train_loss: 56.97853469848633 test_loss:125.51303100585938\n",
            "921/3000 train_loss: 55.09987258911133 test_loss:126.37492370605469\n",
            "922/3000 train_loss: 60.335731506347656 test_loss:132.22299194335938\n",
            "923/3000 train_loss: 66.2099380493164 test_loss:137.27642822265625\n",
            "924/3000 train_loss: 66.40416717529297 test_loss:129.56298828125\n",
            "925/3000 train_loss: 60.91025161743164 test_loss:136.1568145751953\n",
            "926/3000 train_loss: 61.26313018798828 test_loss:130.5177001953125\n",
            "927/3000 train_loss: 58.22820281982422 test_loss:130.6678466796875\n",
            "928/3000 train_loss: 59.57825469970703 test_loss:129.42010498046875\n",
            "929/3000 train_loss: 55.7227668762207 test_loss:134.72613525390625\n",
            "930/3000 train_loss: 58.41621398925781 test_loss:136.53782653808594\n",
            "931/3000 train_loss: 62.70020294189453 test_loss:125.31140899658203\n",
            "932/3000 train_loss: 60.64308547973633 test_loss:134.9098663330078\n",
            "933/3000 train_loss: 63.50254821777344 test_loss:129.310546875\n",
            "934/3000 train_loss: 59.55982208251953 test_loss:135.85984802246094\n",
            "935/3000 train_loss: 57.80689239501953 test_loss:124.8802490234375\n",
            "936/3000 train_loss: 57.58174514770508 test_loss:135.90185546875\n",
            "937/3000 train_loss: 59.30029296875 test_loss:128.45928955078125\n",
            "938/3000 train_loss: 56.87581253051758 test_loss:128.22393798828125\n",
            "939/3000 train_loss: 62.641822814941406 test_loss:129.3434600830078\n",
            "940/3000 train_loss: 50.3523063659668 test_loss:134.57061767578125\n",
            "941/3000 train_loss: 64.05538940429688 test_loss:129.0570526123047\n",
            "942/3000 train_loss: 57.06210708618164 test_loss:129.48358154296875\n",
            "943/3000 train_loss: 63.08204650878906 test_loss:126.32852172851562\n",
            "944/3000 train_loss: 59.75411605834961 test_loss:127.95711517333984\n",
            "945/3000 train_loss: 52.05600357055664 test_loss:129.5247344970703\n",
            "946/3000 train_loss: 57.62648010253906 test_loss:132.5041046142578\n",
            "947/3000 train_loss: 54.77413558959961 test_loss:126.656982421875\n",
            "948/3000 train_loss: 65.03860473632812 test_loss:132.44573974609375\n",
            "949/3000 train_loss: 60.98862838745117 test_loss:126.66806030273438\n",
            "950/3000 train_loss: 58.49837875366211 test_loss:126.04075622558594\n",
            "951/3000 train_loss: 57.13050842285156 test_loss:131.39849853515625\n",
            "952/3000 train_loss: 59.98518371582031 test_loss:134.5900115966797\n",
            "953/3000 train_loss: 59.375389099121094 test_loss:123.83199310302734\n",
            "954/3000 train_loss: 55.04789733886719 test_loss:123.2663345336914\n",
            "955/3000 train_loss: 52.674293518066406 test_loss:123.20439147949219\n",
            "956/3000 train_loss: 56.61060333251953 test_loss:133.74063110351562\n",
            "957/3000 train_loss: 60.30523681640625 test_loss:131.2161865234375\n",
            "958/3000 train_loss: 54.72245407104492 test_loss:132.23646545410156\n",
            "959/3000 train_loss: 60.844482421875 test_loss:131.20265197753906\n",
            "960/3000 train_loss: 60.41596984863281 test_loss:136.51564025878906\n",
            "961/3000 train_loss: 59.282073974609375 test_loss:130.43911743164062\n",
            "962/3000 train_loss: 54.972694396972656 test_loss:135.8734588623047\n",
            "963/3000 train_loss: 53.572757720947266 test_loss:126.58816528320312\n",
            "964/3000 train_loss: 55.2962760925293 test_loss:125.79424285888672\n",
            "965/3000 train_loss: 58.22489929199219 test_loss:121.58252716064453\n",
            "966/3000 train_loss: 60.875885009765625 test_loss:127.48836517333984\n",
            "967/3000 train_loss: 52.07664108276367 test_loss:130.69192504882812\n",
            "968/3000 train_loss: 55.08523941040039 test_loss:121.31907653808594\n",
            "969/3000 train_loss: 59.05624771118164 test_loss:131.54574584960938\n",
            "970/3000 train_loss: 57.85301208496094 test_loss:125.01618957519531\n",
            "971/3000 train_loss: 54.01124954223633 test_loss:127.95389556884766\n",
            "972/3000 train_loss: 54.72069549560547 test_loss:126.53895568847656\n",
            "973/3000 train_loss: 60.085853576660156 test_loss:128.85794067382812\n",
            "974/3000 train_loss: 55.2153434753418 test_loss:128.15834045410156\n",
            "975/3000 train_loss: 59.50740051269531 test_loss:137.69827270507812\n",
            "976/3000 train_loss: 64.90787506103516 test_loss:135.77987670898438\n",
            "977/3000 train_loss: 58.339317321777344 test_loss:134.28875732421875\n",
            "978/3000 train_loss: 51.79632568359375 test_loss:125.65044403076172\n",
            "979/3000 train_loss: 59.61593246459961 test_loss:136.4781494140625\n",
            "980/3000 train_loss: 58.070045471191406 test_loss:130.29556274414062\n",
            "981/3000 train_loss: 60.86240005493164 test_loss:126.92750549316406\n",
            "982/3000 train_loss: 56.441017150878906 test_loss:139.65496826171875\n",
            "983/3000 train_loss: 55.61746597290039 test_loss:127.38870239257812\n",
            "984/3000 train_loss: 53.001502990722656 test_loss:126.10295104980469\n",
            "985/3000 train_loss: 53.31133270263672 test_loss:125.28132629394531\n",
            "986/3000 train_loss: 52.598026275634766 test_loss:127.11450958251953\n",
            "987/3000 train_loss: 53.70714569091797 test_loss:124.45716094970703\n",
            "988/3000 train_loss: 55.61223602294922 test_loss:137.4524688720703\n",
            "989/3000 train_loss: 47.542964935302734 test_loss:131.1861572265625\n",
            "990/3000 train_loss: 54.37533187866211 test_loss:134.34100341796875\n",
            "991/3000 train_loss: 50.71390914916992 test_loss:128.1851806640625\n",
            "992/3000 train_loss: 48.273460388183594 test_loss:131.42515563964844\n",
            "993/3000 train_loss: 54.39134216308594 test_loss:119.94819641113281\n",
            "994/3000 train_loss: 50.44626235961914 test_loss:132.52285766601562\n",
            "995/3000 train_loss: 55.364776611328125 test_loss:125.47811126708984\n",
            "996/3000 train_loss: 53.94984817504883 test_loss:124.18968963623047\n",
            "997/3000 train_loss: 54.29178237915039 test_loss:129.0013885498047\n",
            "998/3000 train_loss: 56.249122619628906 test_loss:121.54154968261719\n",
            "999/3000 train_loss: 50.256622314453125 test_loss:127.16624450683594\n",
            "1000/3000 train_loss: 52.54682540893555 test_loss:119.32070922851562\n",
            "1001/3000 train_loss: 51.78812789916992 test_loss:123.35134887695312\n",
            "1002/3000 train_loss: 53.01772689819336 test_loss:134.34739685058594\n",
            "1003/3000 train_loss: 53.19978332519531 test_loss:126.67574310302734\n",
            "1004/3000 train_loss: 50.253929138183594 test_loss:122.49868774414062\n",
            "1005/3000 train_loss: 59.60979461669922 test_loss:122.98130798339844\n",
            "1006/3000 train_loss: 51.96459197998047 test_loss:130.9613800048828\n",
            "1007/3000 train_loss: 54.46060562133789 test_loss:124.031005859375\n",
            "1008/3000 train_loss: 54.08155822753906 test_loss:127.21873474121094\n",
            "1009/3000 train_loss: 58.5185546875 test_loss:124.80458068847656\n",
            "1010/3000 train_loss: 55.2731819152832 test_loss:119.90444946289062\n",
            "1011/3000 train_loss: 59.12437438964844 test_loss:126.09968566894531\n",
            "1012/3000 train_loss: 51.86772155761719 test_loss:121.54013061523438\n",
            "1013/3000 train_loss: 51.18113708496094 test_loss:122.90291595458984\n",
            "1014/3000 train_loss: 54.425209045410156 test_loss:127.42408752441406\n",
            "1015/3000 train_loss: 54.202972412109375 test_loss:121.31055450439453\n",
            "1016/3000 train_loss: 52.05137634277344 test_loss:123.41876220703125\n",
            "1017/3000 train_loss: 50.845497131347656 test_loss:124.2917251586914\n",
            "1018/3000 train_loss: 65.40931701660156 test_loss:127.84092712402344\n",
            "1019/3000 train_loss: 49.35552215576172 test_loss:123.0721435546875\n",
            "1020/3000 train_loss: 50.56733322143555 test_loss:125.74710083007812\n",
            "1021/3000 train_loss: 47.548484802246094 test_loss:125.50188446044922\n",
            "1022/3000 train_loss: 47.10227584838867 test_loss:129.56947326660156\n",
            "1023/3000 train_loss: 52.084110260009766 test_loss:123.16876220703125\n",
            "1024/3000 train_loss: 51.02866744995117 test_loss:126.40043640136719\n",
            "1025/3000 train_loss: 50.46211624145508 test_loss:124.01426696777344\n",
            "1026/3000 train_loss: 61.32402801513672 test_loss:118.11495971679688\n",
            "1027/3000 train_loss: 49.58953094482422 test_loss:124.61935424804688\n",
            "1028/3000 train_loss: 47.26533508300781 test_loss:115.17322540283203\n",
            "1029/3000 train_loss: 53.618743896484375 test_loss:123.48481750488281\n",
            "1030/3000 train_loss: 48.685325622558594 test_loss:128.9298553466797\n",
            "1031/3000 train_loss: 49.65737533569336 test_loss:122.31182861328125\n",
            "1032/3000 train_loss: 54.873291015625 test_loss:122.01663208007812\n",
            "1033/3000 train_loss: 52.56352996826172 test_loss:122.18428039550781\n",
            "1034/3000 train_loss: 49.899513244628906 test_loss:118.69535827636719\n",
            "1035/3000 train_loss: 52.047603607177734 test_loss:114.22884368896484\n",
            "1036/3000 train_loss: 53.379112243652344 test_loss:118.83055877685547\n",
            "1037/3000 train_loss: 52.14535903930664 test_loss:119.53166198730469\n",
            "1038/3000 train_loss: 50.916412353515625 test_loss:117.51793670654297\n",
            "1039/3000 train_loss: 51.072410583496094 test_loss:131.05967712402344\n",
            "1040/3000 train_loss: 49.362308502197266 test_loss:116.51770782470703\n",
            "1041/3000 train_loss: 46.5872688293457 test_loss:119.39315032958984\n",
            "1042/3000 train_loss: 45.50168228149414 test_loss:118.80684661865234\n",
            "1043/3000 train_loss: 52.35829544067383 test_loss:116.8038101196289\n",
            "1044/3000 train_loss: 49.835426330566406 test_loss:125.66091918945312\n",
            "1045/3000 train_loss: 52.108543395996094 test_loss:125.59504699707031\n",
            "1046/3000 train_loss: 54.7036018371582 test_loss:116.2387924194336\n",
            "1047/3000 train_loss: 50.91278076171875 test_loss:129.73077392578125\n",
            "1048/3000 train_loss: 52.07011795043945 test_loss:121.01512145996094\n",
            "1049/3000 train_loss: 49.73240280151367 test_loss:121.6159896850586\n",
            "1050/3000 train_loss: 52.937984466552734 test_loss:122.48613739013672\n",
            "1051/3000 train_loss: 50.263389587402344 test_loss:115.74185180664062\n",
            "1052/3000 train_loss: 47.81352996826172 test_loss:116.949462890625\n",
            "1053/3000 train_loss: 56.305023193359375 test_loss:119.85848999023438\n",
            "1054/3000 train_loss: 51.91465759277344 test_loss:125.99219512939453\n",
            "1055/3000 train_loss: 44.35102844238281 test_loss:118.74374389648438\n",
            "1056/3000 train_loss: 46.09772491455078 test_loss:124.08901977539062\n",
            "1057/3000 train_loss: 49.2702522277832 test_loss:119.40950012207031\n",
            "1058/3000 train_loss: 47.25065612792969 test_loss:125.15377807617188\n",
            "1059/3000 train_loss: 50.250091552734375 test_loss:117.21218872070312\n",
            "1060/3000 train_loss: 48.255069732666016 test_loss:120.86934661865234\n",
            "1061/3000 train_loss: 50.718135833740234 test_loss:121.70057678222656\n",
            "1062/3000 train_loss: 49.006065368652344 test_loss:113.4037094116211\n",
            "1063/3000 train_loss: 47.601070404052734 test_loss:122.8046646118164\n",
            "1064/3000 train_loss: 50.416038513183594 test_loss:116.03378295898438\n",
            "1065/3000 train_loss: 55.60149002075195 test_loss:119.00407409667969\n",
            "1066/3000 train_loss: 48.437374114990234 test_loss:118.83777618408203\n",
            "1067/3000 train_loss: 51.35807800292969 test_loss:122.73332977294922\n",
            "1068/3000 train_loss: 49.424644470214844 test_loss:129.0029754638672\n",
            "1069/3000 train_loss: 50.12950897216797 test_loss:115.15487670898438\n",
            "1070/3000 train_loss: 45.51423263549805 test_loss:113.31961822509766\n",
            "1071/3000 train_loss: 50.30925369262695 test_loss:117.02002716064453\n",
            "1072/3000 train_loss: 47.91464614868164 test_loss:117.50315856933594\n",
            "1073/3000 train_loss: 45.7979621887207 test_loss:115.89222717285156\n",
            "1074/3000 train_loss: 45.778045654296875 test_loss:115.35740661621094\n",
            "1075/3000 train_loss: 45.00617218017578 test_loss:118.50921630859375\n",
            "1076/3000 train_loss: 50.86283493041992 test_loss:120.72198486328125\n",
            "1077/3000 train_loss: 49.823524475097656 test_loss:116.93693542480469\n",
            "1078/3000 train_loss: 53.310272216796875 test_loss:118.12886810302734\n",
            "1079/3000 train_loss: 50.22502517700195 test_loss:125.77044677734375\n",
            "1080/3000 train_loss: 46.73030090332031 test_loss:114.06324005126953\n",
            "1081/3000 train_loss: 48.1221923828125 test_loss:119.94580078125\n",
            "1082/3000 train_loss: 45.89728546142578 test_loss:125.9612808227539\n",
            "1083/3000 train_loss: 47.812015533447266 test_loss:122.97869873046875\n",
            "1084/3000 train_loss: 53.56604766845703 test_loss:122.63065338134766\n",
            "1085/3000 train_loss: 49.26399230957031 test_loss:118.87627410888672\n",
            "1086/3000 train_loss: 46.121421813964844 test_loss:118.01253509521484\n",
            "1087/3000 train_loss: 53.05173873901367 test_loss:116.42506408691406\n",
            "1088/3000 train_loss: 46.40033721923828 test_loss:112.88426208496094\n",
            "1089/3000 train_loss: 47.44571304321289 test_loss:119.65225982666016\n",
            "1090/3000 train_loss: 48.32586669921875 test_loss:113.77545166015625\n",
            "1091/3000 train_loss: 46.38539505004883 test_loss:122.85211181640625\n",
            "1092/3000 train_loss: 50.219486236572266 test_loss:117.02412414550781\n",
            "1093/3000 train_loss: 50.01653289794922 test_loss:119.62931060791016\n",
            "1094/3000 train_loss: 54.69851303100586 test_loss:125.16493225097656\n",
            "1095/3000 train_loss: 44.73551559448242 test_loss:120.85517883300781\n",
            "1096/3000 train_loss: 51.817447662353516 test_loss:118.92972564697266\n",
            "1097/3000 train_loss: 49.93307113647461 test_loss:122.49423217773438\n",
            "1098/3000 train_loss: 42.279541015625 test_loss:123.08711242675781\n",
            "1099/3000 train_loss: 47.27216720581055 test_loss:116.32684326171875\n",
            "1100/3000 train_loss: 49.35276794433594 test_loss:125.66043090820312\n",
            "1101/3000 train_loss: 50.33108901977539 test_loss:119.30067443847656\n",
            "1102/3000 train_loss: 46.69493103027344 test_loss:130.97972106933594\n",
            "1103/3000 train_loss: 48.2144889831543 test_loss:121.53053283691406\n",
            "1104/3000 train_loss: 52.57144546508789 test_loss:126.80413055419922\n",
            "1105/3000 train_loss: 56.85333251953125 test_loss:125.5044174194336\n",
            "1106/3000 train_loss: 53.51445770263672 test_loss:122.02137756347656\n",
            "1107/3000 train_loss: 51.457359313964844 test_loss:120.18167114257812\n",
            "1108/3000 train_loss: 47.361507415771484 test_loss:121.54016876220703\n",
            "1109/3000 train_loss: 50.080162048339844 test_loss:124.50537872314453\n",
            "1110/3000 train_loss: 51.515296936035156 test_loss:123.97747039794922\n",
            "1111/3000 train_loss: 54.085636138916016 test_loss:131.40809631347656\n",
            "1112/3000 train_loss: 46.365966796875 test_loss:135.0879364013672\n",
            "1113/3000 train_loss: 48.990936279296875 test_loss:123.1873550415039\n",
            "1114/3000 train_loss: 51.084388732910156 test_loss:124.30270385742188\n",
            "1115/3000 train_loss: 48.19145584106445 test_loss:121.76576232910156\n",
            "1116/3000 train_loss: 51.53153991699219 test_loss:127.8888168334961\n",
            "1117/3000 train_loss: 53.04008102416992 test_loss:122.78005981445312\n",
            "1118/3000 train_loss: 57.27659225463867 test_loss:126.0811538696289\n",
            "1119/3000 train_loss: 51.16531753540039 test_loss:119.18203735351562\n",
            "1120/3000 train_loss: 49.515953063964844 test_loss:126.08999633789062\n",
            "1121/3000 train_loss: 48.79620361328125 test_loss:122.89610290527344\n",
            "1122/3000 train_loss: 47.115848541259766 test_loss:120.37385559082031\n",
            "1123/3000 train_loss: 50.861907958984375 test_loss:110.3258056640625\n",
            "1124/3000 train_loss: 45.58420181274414 test_loss:122.39362335205078\n",
            "1125/3000 train_loss: 41.071388244628906 test_loss:120.0411605834961\n",
            "1126/3000 train_loss: 46.54719543457031 test_loss:121.29170227050781\n",
            "1127/3000 train_loss: 40.99201202392578 test_loss:120.57154846191406\n",
            "1128/3000 train_loss: 43.88834762573242 test_loss:120.88792419433594\n",
            "1129/3000 train_loss: 41.29836654663086 test_loss:122.09660339355469\n",
            "1130/3000 train_loss: 50.6034049987793 test_loss:133.1634063720703\n",
            "1131/3000 train_loss: 51.35809326171875 test_loss:119.50212097167969\n",
            "1132/3000 train_loss: 44.366310119628906 test_loss:123.04285430908203\n",
            "1133/3000 train_loss: 45.146514892578125 test_loss:122.49594116210938\n",
            "1134/3000 train_loss: 48.833030700683594 test_loss:123.24947357177734\n",
            "1135/3000 train_loss: 48.84935760498047 test_loss:119.58737182617188\n",
            "1136/3000 train_loss: 51.225460052490234 test_loss:124.39178466796875\n",
            "1137/3000 train_loss: 44.5074348449707 test_loss:121.09021759033203\n",
            "1138/3000 train_loss: 48.56401824951172 test_loss:118.6535873413086\n",
            "1139/3000 train_loss: 46.9511833190918 test_loss:116.73271942138672\n",
            "1140/3000 train_loss: 49.00001525878906 test_loss:120.7650146484375\n",
            "1141/3000 train_loss: 45.53144836425781 test_loss:112.6504135131836\n",
            "1142/3000 train_loss: 47.6783332824707 test_loss:117.56765747070312\n",
            "1143/3000 train_loss: 43.617271423339844 test_loss:113.66427612304688\n",
            "1144/3000 train_loss: 49.062835693359375 test_loss:124.92158508300781\n",
            "1145/3000 train_loss: 46.370967864990234 test_loss:115.06963348388672\n",
            "1146/3000 train_loss: 44.59229278564453 test_loss:117.43731689453125\n",
            "1147/3000 train_loss: 44.63994598388672 test_loss:114.92761993408203\n",
            "1148/3000 train_loss: 42.996761322021484 test_loss:117.1950912475586\n",
            "1149/3000 train_loss: 50.59734344482422 test_loss:118.57050323486328\n",
            "1150/3000 train_loss: 41.28260040283203 test_loss:120.07981872558594\n",
            "1151/3000 train_loss: 49.67921829223633 test_loss:116.83517456054688\n",
            "1152/3000 train_loss: 50.546730041503906 test_loss:115.17498779296875\n",
            "1153/3000 train_loss: 49.4743537902832 test_loss:117.38115692138672\n",
            "1154/3000 train_loss: 45.37665557861328 test_loss:113.30316162109375\n",
            "1155/3000 train_loss: 48.33875274658203 test_loss:114.61434936523438\n",
            "1156/3000 train_loss: 41.651039123535156 test_loss:123.94107818603516\n",
            "1157/3000 train_loss: 46.73515701293945 test_loss:114.49838256835938\n",
            "1158/3000 train_loss: 43.336727142333984 test_loss:115.82160949707031\n",
            "1159/3000 train_loss: 50.565032958984375 test_loss:115.81147766113281\n",
            "1160/3000 train_loss: 43.877197265625 test_loss:109.1443862915039\n",
            "1161/3000 train_loss: 42.98680877685547 test_loss:115.93048858642578\n",
            "1162/3000 train_loss: 48.94520568847656 test_loss:115.39178466796875\n",
            "1163/3000 train_loss: 48.032737731933594 test_loss:113.93812561035156\n",
            "1164/3000 train_loss: 43.126373291015625 test_loss:114.53987121582031\n",
            "1165/3000 train_loss: 43.58409118652344 test_loss:113.29261016845703\n",
            "1166/3000 train_loss: 45.441566467285156 test_loss:109.25316619873047\n",
            "1167/3000 train_loss: 47.36882019042969 test_loss:112.38003540039062\n",
            "1168/3000 train_loss: 46.49372100830078 test_loss:111.08863830566406\n",
            "1169/3000 train_loss: 43.936561584472656 test_loss:120.1382827758789\n",
            "1170/3000 train_loss: 42.77653884887695 test_loss:116.36847686767578\n",
            "1171/3000 train_loss: 44.60941696166992 test_loss:120.1242904663086\n",
            "1172/3000 train_loss: 43.86014938354492 test_loss:117.16810607910156\n",
            "1173/3000 train_loss: 48.5363655090332 test_loss:121.88797760009766\n",
            "1174/3000 train_loss: 50.23221206665039 test_loss:112.26414489746094\n",
            "1175/3000 train_loss: 43.920406341552734 test_loss:119.26290130615234\n",
            "1176/3000 train_loss: 41.592288970947266 test_loss:113.2825927734375\n",
            "1177/3000 train_loss: 51.230003356933594 test_loss:112.72642517089844\n",
            "1178/3000 train_loss: 45.9411735534668 test_loss:119.1834487915039\n",
            "1179/3000 train_loss: 51.44679641723633 test_loss:120.66067504882812\n",
            "1180/3000 train_loss: 45.32337188720703 test_loss:121.25232696533203\n",
            "1181/3000 train_loss: 41.48106002807617 test_loss:117.9296875\n",
            "1182/3000 train_loss: 40.146018981933594 test_loss:119.91473388671875\n",
            "1183/3000 train_loss: 38.98570251464844 test_loss:115.70420837402344\n",
            "1184/3000 train_loss: 37.09806442260742 test_loss:116.46417236328125\n",
            "1185/3000 train_loss: 48.34202575683594 test_loss:118.93801879882812\n",
            "1186/3000 train_loss: 51.669681549072266 test_loss:116.09346771240234\n",
            "1187/3000 train_loss: 45.79475021362305 test_loss:121.2375259399414\n",
            "1188/3000 train_loss: 44.10167694091797 test_loss:118.05288696289062\n",
            "1189/3000 train_loss: 41.906166076660156 test_loss:114.94712829589844\n",
            "1190/3000 train_loss: 46.615909576416016 test_loss:114.94705963134766\n",
            "1191/3000 train_loss: 38.422515869140625 test_loss:118.25039672851562\n",
            "1192/3000 train_loss: 41.85719299316406 test_loss:114.71499633789062\n",
            "1193/3000 train_loss: 45.46592712402344 test_loss:116.4024658203125\n",
            "1194/3000 train_loss: 44.020145416259766 test_loss:117.77766418457031\n",
            "1195/3000 train_loss: 42.338043212890625 test_loss:124.39714050292969\n",
            "1196/3000 train_loss: 43.10564422607422 test_loss:122.23928833007812\n",
            "1197/3000 train_loss: 38.90713119506836 test_loss:116.91079711914062\n",
            "1198/3000 train_loss: 48.222957611083984 test_loss:111.31614685058594\n",
            "1199/3000 train_loss: 48.165748596191406 test_loss:121.65310668945312\n",
            "1200/3000 train_loss: 52.10539245605469 test_loss:110.3961181640625\n",
            "1201/3000 train_loss: 42.636451721191406 test_loss:116.11942291259766\n",
            "1202/3000 train_loss: 40.2152099609375 test_loss:117.25365447998047\n",
            "1203/3000 train_loss: 39.74876022338867 test_loss:114.3358383178711\n",
            "1204/3000 train_loss: 40.55082321166992 test_loss:108.14292907714844\n",
            "1205/3000 train_loss: 41.42552185058594 test_loss:114.47273254394531\n",
            "1206/3000 train_loss: 41.06731414794922 test_loss:111.33152770996094\n",
            "1207/3000 train_loss: 46.83209228515625 test_loss:114.90477752685547\n",
            "1208/3000 train_loss: 45.07541275024414 test_loss:115.96139526367188\n",
            "1209/3000 train_loss: 39.1209602355957 test_loss:118.53521728515625\n",
            "1210/3000 train_loss: 51.966705322265625 test_loss:110.02398681640625\n",
            "1211/3000 train_loss: 45.29093551635742 test_loss:112.53007507324219\n",
            "1212/3000 train_loss: 43.516822814941406 test_loss:114.25221252441406\n",
            "1213/3000 train_loss: 41.607303619384766 test_loss:112.40422058105469\n",
            "1214/3000 train_loss: 52.57009506225586 test_loss:113.74906158447266\n",
            "1215/3000 train_loss: 43.614295959472656 test_loss:109.27760314941406\n",
            "1216/3000 train_loss: 47.31340789794922 test_loss:117.04525756835938\n",
            "1217/3000 train_loss: 43.7408447265625 test_loss:112.82125854492188\n",
            "1218/3000 train_loss: 44.01014709472656 test_loss:120.11652374267578\n",
            "1219/3000 train_loss: 39.32162094116211 test_loss:108.03301239013672\n",
            "1220/3000 train_loss: 46.53655242919922 test_loss:115.83976745605469\n",
            "1221/3000 train_loss: 45.19432067871094 test_loss:114.40926361083984\n",
            "1222/3000 train_loss: 42.06596755981445 test_loss:114.29060363769531\n",
            "1223/3000 train_loss: 50.47783279418945 test_loss:114.95587158203125\n",
            "1224/3000 train_loss: 41.100650787353516 test_loss:117.17292022705078\n",
            "1225/3000 train_loss: 41.55253601074219 test_loss:116.31914520263672\n",
            "1226/3000 train_loss: 40.95372772216797 test_loss:109.35140991210938\n",
            "1227/3000 train_loss: 41.80958938598633 test_loss:119.30048370361328\n",
            "1228/3000 train_loss: 45.0678596496582 test_loss:113.91378021240234\n",
            "1229/3000 train_loss: 39.566165924072266 test_loss:117.58346557617188\n",
            "1230/3000 train_loss: 39.36962127685547 test_loss:110.25737762451172\n",
            "1231/3000 train_loss: 47.506752014160156 test_loss:113.6899185180664\n",
            "1232/3000 train_loss: 44.23160934448242 test_loss:116.78589630126953\n",
            "1233/3000 train_loss: 41.75053405761719 test_loss:115.45515441894531\n",
            "1234/3000 train_loss: 42.315826416015625 test_loss:112.86560821533203\n",
            "1235/3000 train_loss: 44.435569763183594 test_loss:115.36581420898438\n",
            "1236/3000 train_loss: 43.14964294433594 test_loss:118.82064056396484\n",
            "1237/3000 train_loss: 45.182762145996094 test_loss:112.74139404296875\n",
            "1238/3000 train_loss: 41.63215637207031 test_loss:114.41939544677734\n",
            "1239/3000 train_loss: 39.095890045166016 test_loss:111.69383239746094\n",
            "1240/3000 train_loss: 40.15763854980469 test_loss:114.15327453613281\n",
            "1241/3000 train_loss: 45.717552185058594 test_loss:114.88113403320312\n",
            "1242/3000 train_loss: 42.41642379760742 test_loss:117.99866485595703\n",
            "1243/3000 train_loss: 40.54290771484375 test_loss:114.97223663330078\n",
            "1244/3000 train_loss: 35.67789840698242 test_loss:115.83155822753906\n",
            "1245/3000 train_loss: 49.35371017456055 test_loss:111.01707458496094\n",
            "1246/3000 train_loss: 37.196136474609375 test_loss:112.65679931640625\n",
            "1247/3000 train_loss: 41.36819076538086 test_loss:114.6459732055664\n",
            "1248/3000 train_loss: 36.23297119140625 test_loss:112.19708251953125\n",
            "1249/3000 train_loss: 40.033790588378906 test_loss:116.15298461914062\n",
            "1250/3000 train_loss: 41.33066177368164 test_loss:112.98463439941406\n",
            "1251/3000 train_loss: 40.037113189697266 test_loss:114.01160430908203\n",
            "1252/3000 train_loss: 42.393959045410156 test_loss:113.90191650390625\n",
            "1253/3000 train_loss: 39.44544982910156 test_loss:107.98214721679688\n",
            "1254/3000 train_loss: 42.89976501464844 test_loss:107.32093048095703\n",
            "1255/3000 train_loss: 41.193180084228516 test_loss:115.7891845703125\n",
            "1256/3000 train_loss: 43.68960952758789 test_loss:108.9566879272461\n",
            "1257/3000 train_loss: 39.40183639526367 test_loss:108.63579559326172\n",
            "1258/3000 train_loss: 42.00851058959961 test_loss:106.5439224243164\n",
            "1259/3000 train_loss: 41.04649353027344 test_loss:103.77896881103516\n",
            "1260/3000 train_loss: 43.66328048706055 test_loss:113.2957992553711\n",
            "1261/3000 train_loss: 39.23696517944336 test_loss:116.86418151855469\n",
            "1262/3000 train_loss: 39.810977935791016 test_loss:114.54066467285156\n",
            "1263/3000 train_loss: 42.901248931884766 test_loss:108.31607055664062\n",
            "1264/3000 train_loss: 43.364559173583984 test_loss:113.47637939453125\n",
            "1265/3000 train_loss: 42.335418701171875 test_loss:113.48052978515625\n",
            "1266/3000 train_loss: 50.043235778808594 test_loss:110.2229995727539\n",
            "1267/3000 train_loss: 49.872093200683594 test_loss:105.62596130371094\n",
            "1268/3000 train_loss: 45.36021041870117 test_loss:112.6263656616211\n",
            "1269/3000 train_loss: 41.2076416015625 test_loss:119.65126037597656\n",
            "1270/3000 train_loss: 44.011531829833984 test_loss:109.03067779541016\n",
            "1271/3000 train_loss: 41.502838134765625 test_loss:115.40283203125\n",
            "1272/3000 train_loss: 39.62394714355469 test_loss:110.35124969482422\n",
            "1273/3000 train_loss: 43.57821273803711 test_loss:114.99732971191406\n",
            "1274/3000 train_loss: 45.60630798339844 test_loss:111.3564453125\n",
            "1275/3000 train_loss: 37.047210693359375 test_loss:107.9545669555664\n",
            "1276/3000 train_loss: 35.524600982666016 test_loss:110.36264038085938\n",
            "1277/3000 train_loss: 38.17713928222656 test_loss:112.0533676147461\n",
            "1278/3000 train_loss: 42.007171630859375 test_loss:109.3070068359375\n",
            "1279/3000 train_loss: 36.8690299987793 test_loss:110.3831787109375\n",
            "1280/3000 train_loss: 36.86945724487305 test_loss:119.54714965820312\n",
            "1281/3000 train_loss: 41.56789016723633 test_loss:115.35076904296875\n",
            "1282/3000 train_loss: 37.98166275024414 test_loss:118.55258178710938\n",
            "1283/3000 train_loss: 37.88908386230469 test_loss:116.95380401611328\n",
            "1284/3000 train_loss: 41.91315460205078 test_loss:113.47377014160156\n",
            "1285/3000 train_loss: 45.61069869995117 test_loss:121.21980285644531\n",
            "1286/3000 train_loss: 38.672210693359375 test_loss:111.89578247070312\n",
            "1287/3000 train_loss: 36.5128059387207 test_loss:107.50408935546875\n",
            "1288/3000 train_loss: 36.117828369140625 test_loss:112.12254333496094\n",
            "1289/3000 train_loss: 37.869178771972656 test_loss:118.48210906982422\n",
            "1290/3000 train_loss: 41.40705490112305 test_loss:113.02867889404297\n",
            "1291/3000 train_loss: 47.329097747802734 test_loss:108.47615814208984\n",
            "1292/3000 train_loss: 39.5894775390625 test_loss:119.08946990966797\n",
            "1293/3000 train_loss: 41.65480041503906 test_loss:117.99871063232422\n",
            "1294/3000 train_loss: 37.70195388793945 test_loss:116.73382568359375\n",
            "1295/3000 train_loss: 38.20866775512695 test_loss:111.89784240722656\n",
            "1296/3000 train_loss: 40.87070846557617 test_loss:112.53910064697266\n",
            "1297/3000 train_loss: 44.930519104003906 test_loss:109.20550537109375\n",
            "1298/3000 train_loss: 33.59514236450195 test_loss:115.01712036132812\n",
            "1299/3000 train_loss: 34.864402770996094 test_loss:121.333251953125\n",
            "1300/3000 train_loss: 37.85346221923828 test_loss:114.65966033935547\n",
            "1301/3000 train_loss: 37.407684326171875 test_loss:113.95889282226562\n",
            "1302/3000 train_loss: 38.34050369262695 test_loss:114.18175506591797\n",
            "1303/3000 train_loss: 38.58679962158203 test_loss:113.49170684814453\n",
            "1304/3000 train_loss: 37.06059265136719 test_loss:126.18806457519531\n",
            "1305/3000 train_loss: 48.1339111328125 test_loss:116.26331329345703\n",
            "1306/3000 train_loss: 38.71672821044922 test_loss:118.86064147949219\n",
            "1307/3000 train_loss: 41.60350799560547 test_loss:113.90433502197266\n",
            "1308/3000 train_loss: 41.504554748535156 test_loss:116.54869079589844\n",
            "1309/3000 train_loss: 40.07789611816406 test_loss:116.89871215820312\n",
            "1310/3000 train_loss: 44.713260650634766 test_loss:114.64396667480469\n",
            "1311/3000 train_loss: 43.474483489990234 test_loss:116.3296890258789\n",
            "1312/3000 train_loss: 37.487884521484375 test_loss:113.20699310302734\n",
            "1313/3000 train_loss: 37.90531921386719 test_loss:106.78784942626953\n",
            "1314/3000 train_loss: 41.69009780883789 test_loss:113.4058837890625\n",
            "1315/3000 train_loss: 48.03544235229492 test_loss:114.00214385986328\n",
            "1316/3000 train_loss: 36.70981216430664 test_loss:120.86615753173828\n",
            "1317/3000 train_loss: 39.298831939697266 test_loss:110.70787811279297\n",
            "1318/3000 train_loss: 45.17583084106445 test_loss:120.28994750976562\n",
            "1319/3000 train_loss: 40.28426742553711 test_loss:108.6816635131836\n",
            "1320/3000 train_loss: 35.5509033203125 test_loss:116.22827911376953\n",
            "1321/3000 train_loss: 35.945823669433594 test_loss:108.1112060546875\n",
            "1322/3000 train_loss: 38.07637405395508 test_loss:109.52813720703125\n",
            "1323/3000 train_loss: 34.48680877685547 test_loss:113.38599395751953\n",
            "1324/3000 train_loss: 36.92689514160156 test_loss:110.76886749267578\n",
            "1325/3000 train_loss: 42.47152328491211 test_loss:111.01178741455078\n",
            "1326/3000 train_loss: 38.48120880126953 test_loss:106.18775177001953\n",
            "1327/3000 train_loss: 38.387393951416016 test_loss:105.06267547607422\n",
            "1328/3000 train_loss: 42.971309661865234 test_loss:109.68429565429688\n",
            "1329/3000 train_loss: 44.533756256103516 test_loss:118.98147583007812\n",
            "1330/3000 train_loss: 40.20235824584961 test_loss:102.4751968383789\n",
            "1331/3000 train_loss: 38.9319953918457 test_loss:111.39022827148438\n",
            "1332/3000 train_loss: 37.60087203979492 test_loss:118.00037384033203\n",
            "1333/3000 train_loss: 41.23495864868164 test_loss:108.99409484863281\n",
            "1334/3000 train_loss: 40.64495849609375 test_loss:113.02323150634766\n",
            "1335/3000 train_loss: 39.256649017333984 test_loss:108.9646987915039\n",
            "1336/3000 train_loss: 34.32246398925781 test_loss:110.36006927490234\n",
            "1337/3000 train_loss: 40.05642318725586 test_loss:106.77182006835938\n",
            "1338/3000 train_loss: 43.64836120605469 test_loss:104.82393646240234\n",
            "1339/3000 train_loss: 40.22266387939453 test_loss:108.00593566894531\n",
            "1340/3000 train_loss: 34.15528869628906 test_loss:108.06546020507812\n",
            "1341/3000 train_loss: 47.50509262084961 test_loss:106.10230255126953\n",
            "1342/3000 train_loss: 37.03118896484375 test_loss:110.98782348632812\n",
            "1343/3000 train_loss: 36.87628936767578 test_loss:112.97113037109375\n",
            "1344/3000 train_loss: 40.350830078125 test_loss:110.56636047363281\n",
            "1345/3000 train_loss: 37.32455062866211 test_loss:106.40386199951172\n",
            "1346/3000 train_loss: 40.140289306640625 test_loss:106.24151611328125\n",
            "1347/3000 train_loss: 40.96751022338867 test_loss:110.25077819824219\n",
            "1348/3000 train_loss: 37.463096618652344 test_loss:109.56307983398438\n",
            "1349/3000 train_loss: 38.626953125 test_loss:107.62919616699219\n",
            "1350/3000 train_loss: 38.962432861328125 test_loss:109.00544738769531\n",
            "1351/3000 train_loss: 41.03416442871094 test_loss:113.85639953613281\n",
            "1352/3000 train_loss: 36.92047882080078 test_loss:109.89828491210938\n",
            "1353/3000 train_loss: 45.3062858581543 test_loss:104.21980285644531\n",
            "1354/3000 train_loss: 39.23564529418945 test_loss:108.07501220703125\n",
            "1355/3000 train_loss: 39.78501510620117 test_loss:106.45222473144531\n",
            "1356/3000 train_loss: 39.887882232666016 test_loss:106.9825668334961\n",
            "1357/3000 train_loss: 39.92507553100586 test_loss:107.84527587890625\n",
            "1358/3000 train_loss: 39.70453643798828 test_loss:106.38172149658203\n",
            "1359/3000 train_loss: 35.699974060058594 test_loss:114.89022827148438\n",
            "1360/3000 train_loss: 38.671058654785156 test_loss:117.57481384277344\n",
            "1361/3000 train_loss: 36.04658126831055 test_loss:107.53623962402344\n",
            "1362/3000 train_loss: 35.04267501831055 test_loss:103.8541259765625\n",
            "1363/3000 train_loss: 39.59702682495117 test_loss:111.43235778808594\n",
            "1364/3000 train_loss: 36.056541442871094 test_loss:107.74122619628906\n",
            "1365/3000 train_loss: 36.448299407958984 test_loss:112.88931274414062\n",
            "1366/3000 train_loss: 38.54323959350586 test_loss:103.5481185913086\n",
            "1367/3000 train_loss: 35.51974105834961 test_loss:109.73534393310547\n",
            "1368/3000 train_loss: 38.65745544433594 test_loss:108.22183990478516\n",
            "1369/3000 train_loss: 37.186500549316406 test_loss:109.47151184082031\n",
            "1370/3000 train_loss: 37.08692169189453 test_loss:109.0578842163086\n",
            "1371/3000 train_loss: 37.473968505859375 test_loss:110.34379577636719\n",
            "1372/3000 train_loss: 38.607200622558594 test_loss:100.79487609863281\n",
            "1373/3000 train_loss: 39.203948974609375 test_loss:112.33502960205078\n",
            "1374/3000 train_loss: 37.336944580078125 test_loss:103.39566040039062\n",
            "1375/3000 train_loss: 36.15541076660156 test_loss:109.46785736083984\n",
            "1376/3000 train_loss: 41.44178771972656 test_loss:104.36245727539062\n",
            "1377/3000 train_loss: 37.80793762207031 test_loss:106.82342529296875\n",
            "1378/3000 train_loss: 37.30944061279297 test_loss:108.70402526855469\n",
            "1379/3000 train_loss: 38.72895431518555 test_loss:105.95976257324219\n",
            "1380/3000 train_loss: 34.656925201416016 test_loss:106.80484771728516\n",
            "1381/3000 train_loss: 35.977359771728516 test_loss:105.9912109375\n",
            "1382/3000 train_loss: 40.531700134277344 test_loss:112.34358215332031\n",
            "1383/3000 train_loss: 37.912391662597656 test_loss:109.0926742553711\n",
            "1384/3000 train_loss: 35.495277404785156 test_loss:106.69303894042969\n",
            "1385/3000 train_loss: 35.14883041381836 test_loss:105.65180969238281\n",
            "1386/3000 train_loss: 37.20125961303711 test_loss:105.72452545166016\n",
            "1387/3000 train_loss: 41.62897872924805 test_loss:108.80549621582031\n",
            "1388/3000 train_loss: 39.082393646240234 test_loss:105.92290496826172\n",
            "1389/3000 train_loss: 39.090911865234375 test_loss:109.43431854248047\n",
            "1390/3000 train_loss: 36.76118087768555 test_loss:107.9261474609375\n",
            "1391/3000 train_loss: 34.45414352416992 test_loss:105.68093872070312\n",
            "1392/3000 train_loss: 42.41448974609375 test_loss:113.57603454589844\n",
            "1393/3000 train_loss: 40.46117401123047 test_loss:113.00033569335938\n",
            "1394/3000 train_loss: 38.685401916503906 test_loss:110.60330200195312\n",
            "1395/3000 train_loss: 36.38229751586914 test_loss:104.70690155029297\n",
            "1396/3000 train_loss: 36.34783172607422 test_loss:108.18939971923828\n",
            "1397/3000 train_loss: 38.456031799316406 test_loss:104.39501953125\n",
            "1398/3000 train_loss: 37.634254455566406 test_loss:108.7593765258789\n",
            "1399/3000 train_loss: 41.79997634887695 test_loss:112.14240264892578\n",
            "1400/3000 train_loss: 35.39535903930664 test_loss:101.96611022949219\n",
            "1401/3000 train_loss: 42.1107292175293 test_loss:106.44284057617188\n",
            "1402/3000 train_loss: 36.66210174560547 test_loss:105.17940521240234\n",
            "1403/3000 train_loss: 34.845741271972656 test_loss:113.24673461914062\n",
            "1404/3000 train_loss: 37.06141662597656 test_loss:114.48408508300781\n",
            "1405/3000 train_loss: 34.330116271972656 test_loss:105.7430191040039\n",
            "1406/3000 train_loss: 34.04581069946289 test_loss:104.62136840820312\n",
            "1407/3000 train_loss: 32.72501754760742 test_loss:107.86436462402344\n",
            "1408/3000 train_loss: 33.375022888183594 test_loss:105.01113891601562\n",
            "1409/3000 train_loss: 36.138118743896484 test_loss:106.98590087890625\n",
            "1410/3000 train_loss: 39.676429748535156 test_loss:107.86685943603516\n",
            "1411/3000 train_loss: 33.22126007080078 test_loss:107.9081039428711\n",
            "1412/3000 train_loss: 35.74018096923828 test_loss:103.83009338378906\n",
            "1413/3000 train_loss: 34.619590759277344 test_loss:109.2337417602539\n",
            "1414/3000 train_loss: 34.65291976928711 test_loss:104.56612396240234\n",
            "1415/3000 train_loss: 37.081233978271484 test_loss:105.07408905029297\n",
            "1416/3000 train_loss: 31.39643669128418 test_loss:103.41938781738281\n",
            "1417/3000 train_loss: 32.21643829345703 test_loss:110.8015365600586\n",
            "1418/3000 train_loss: 42.484493255615234 test_loss:103.26615905761719\n",
            "1419/3000 train_loss: 36.814361572265625 test_loss:105.94818878173828\n",
            "1420/3000 train_loss: 37.39326095581055 test_loss:110.16089630126953\n",
            "1421/3000 train_loss: 33.59638595581055 test_loss:105.7373275756836\n",
            "1422/3000 train_loss: 33.8577880859375 test_loss:111.62674713134766\n",
            "1423/3000 train_loss: 34.23133087158203 test_loss:110.63163757324219\n",
            "1424/3000 train_loss: 35.178871154785156 test_loss:109.09233856201172\n",
            "1425/3000 train_loss: 41.22807693481445 test_loss:106.28351593017578\n",
            "1426/3000 train_loss: 32.57622146606445 test_loss:102.35554504394531\n",
            "1427/3000 train_loss: 37.50701141357422 test_loss:107.27442932128906\n",
            "1428/3000 train_loss: 31.06599235534668 test_loss:107.51770782470703\n",
            "1429/3000 train_loss: 41.14493942260742 test_loss:104.22979736328125\n",
            "1430/3000 train_loss: 34.50921630859375 test_loss:114.42936706542969\n",
            "1431/3000 train_loss: 34.302711486816406 test_loss:110.71826171875\n",
            "1432/3000 train_loss: 44.238277435302734 test_loss:108.24188232421875\n",
            "1433/3000 train_loss: 34.705081939697266 test_loss:107.04322814941406\n",
            "1434/3000 train_loss: 39.05010986328125 test_loss:110.59446716308594\n",
            "1435/3000 train_loss: 34.24149703979492 test_loss:102.51001739501953\n",
            "1436/3000 train_loss: 33.50997543334961 test_loss:113.46591186523438\n",
            "1437/3000 train_loss: 34.67552185058594 test_loss:102.33275604248047\n",
            "1438/3000 train_loss: 35.450538635253906 test_loss:112.62252807617188\n",
            "1439/3000 train_loss: 34.4888801574707 test_loss:110.1272964477539\n",
            "1440/3000 train_loss: 34.59477996826172 test_loss:104.34268951416016\n",
            "1441/3000 train_loss: 34.252708435058594 test_loss:105.00696563720703\n",
            "1442/3000 train_loss: 34.67242431640625 test_loss:101.83185577392578\n",
            "1443/3000 train_loss: 34.69560241699219 test_loss:105.33409118652344\n",
            "1444/3000 train_loss: 42.28131103515625 test_loss:114.38375091552734\n",
            "1445/3000 train_loss: 36.65606689453125 test_loss:100.16602325439453\n",
            "1446/3000 train_loss: 32.1223030090332 test_loss:105.75768280029297\n",
            "1447/3000 train_loss: 39.07720184326172 test_loss:105.55023193359375\n",
            "1448/3000 train_loss: 34.202117919921875 test_loss:110.0077133178711\n",
            "1449/3000 train_loss: 32.712581634521484 test_loss:106.81011199951172\n",
            "1450/3000 train_loss: 34.47346115112305 test_loss:111.02046203613281\n",
            "1451/3000 train_loss: 37.45336151123047 test_loss:109.30855560302734\n",
            "1452/3000 train_loss: 37.548099517822266 test_loss:102.46992492675781\n",
            "1453/3000 train_loss: 36.40388870239258 test_loss:110.05606079101562\n",
            "1454/3000 train_loss: 32.665035247802734 test_loss:103.8380126953125\n",
            "1455/3000 train_loss: 39.522666931152344 test_loss:107.85693359375\n",
            "1456/3000 train_loss: 32.06070327758789 test_loss:106.63131713867188\n",
            "1457/3000 train_loss: 37.5048942565918 test_loss:113.6594009399414\n",
            "1458/3000 train_loss: 30.039918899536133 test_loss:99.3416976928711\n",
            "1459/3000 train_loss: 32.420230865478516 test_loss:104.24999237060547\n",
            "1460/3000 train_loss: 37.157249450683594 test_loss:107.10556030273438\n",
            "1461/3000 train_loss: 33.944549560546875 test_loss:104.98005676269531\n",
            "1462/3000 train_loss: 31.926982879638672 test_loss:106.4273452758789\n",
            "1463/3000 train_loss: 32.990753173828125 test_loss:103.15792846679688\n",
            "1464/3000 train_loss: 31.556106567382812 test_loss:106.74765014648438\n",
            "1465/3000 train_loss: 34.85355758666992 test_loss:102.34806060791016\n",
            "1466/3000 train_loss: 45.34046173095703 test_loss:112.56665802001953\n",
            "1467/3000 train_loss: 38.924373626708984 test_loss:103.63650512695312\n",
            "1468/3000 train_loss: 41.106590270996094 test_loss:109.2053451538086\n",
            "1469/3000 train_loss: 30.86553955078125 test_loss:113.39651489257812\n",
            "1470/3000 train_loss: 40.50025939941406 test_loss:99.9505615234375\n",
            "1471/3000 train_loss: 33.17629623413086 test_loss:114.05551147460938\n",
            "1472/3000 train_loss: 40.970252990722656 test_loss:104.69745635986328\n",
            "1473/3000 train_loss: 38.798744201660156 test_loss:109.28295135498047\n",
            "1474/3000 train_loss: 37.503604888916016 test_loss:102.7254638671875\n",
            "1475/3000 train_loss: 38.9992790222168 test_loss:107.97151947021484\n",
            "1476/3000 train_loss: 33.225067138671875 test_loss:114.91098022460938\n",
            "1477/3000 train_loss: 35.81561279296875 test_loss:109.9972915649414\n",
            "1478/3000 train_loss: 33.33548355102539 test_loss:108.58894348144531\n",
            "1479/3000 train_loss: 31.41344451904297 test_loss:104.22605895996094\n",
            "1480/3000 train_loss: 35.29412078857422 test_loss:105.90804290771484\n",
            "1481/3000 train_loss: 42.600154876708984 test_loss:107.50395965576172\n",
            "1482/3000 train_loss: 32.49602127075195 test_loss:109.25521850585938\n",
            "1483/3000 train_loss: 37.83551788330078 test_loss:108.4403305053711\n",
            "1484/3000 train_loss: 33.057918548583984 test_loss:103.5081787109375\n",
            "1485/3000 train_loss: 38.16819763183594 test_loss:100.13838958740234\n",
            "1486/3000 train_loss: 37.73855209350586 test_loss:106.99919128417969\n",
            "1487/3000 train_loss: 36.14020538330078 test_loss:111.39962005615234\n",
            "1488/3000 train_loss: 36.24583435058594 test_loss:105.09200286865234\n",
            "1489/3000 train_loss: 40.45558166503906 test_loss:102.26701354980469\n",
            "1490/3000 train_loss: 31.643625259399414 test_loss:106.20581817626953\n",
            "1491/3000 train_loss: 35.55082321166992 test_loss:97.75814819335938\n",
            "1492/3000 train_loss: 38.25543212890625 test_loss:103.73502349853516\n",
            "1493/3000 train_loss: 35.08580780029297 test_loss:101.29473876953125\n",
            "1494/3000 train_loss: 35.46637725830078 test_loss:103.33432006835938\n",
            "1495/3000 train_loss: 33.4277229309082 test_loss:99.39250946044922\n",
            "1496/3000 train_loss: 34.70564651489258 test_loss:107.86265563964844\n",
            "1497/3000 train_loss: 35.9213981628418 test_loss:100.150634765625\n",
            "1498/3000 train_loss: 36.167442321777344 test_loss:105.89292907714844\n",
            "1499/3000 train_loss: 39.54447555541992 test_loss:111.33006286621094\n",
            "1500/3000 train_loss: 33.734153747558594 test_loss:103.14474487304688\n",
            "1501/3000 train_loss: 33.42909622192383 test_loss:106.34538269042969\n",
            "1502/3000 train_loss: 33.7746696472168 test_loss:109.21708679199219\n",
            "1503/3000 train_loss: 34.477561950683594 test_loss:106.89616394042969\n",
            "1504/3000 train_loss: 31.238378524780273 test_loss:98.61474609375\n",
            "1505/3000 train_loss: 36.03428649902344 test_loss:105.15303039550781\n",
            "1506/3000 train_loss: 36.008338928222656 test_loss:113.05868530273438\n",
            "1507/3000 train_loss: 30.439743041992188 test_loss:97.84871673583984\n",
            "1508/3000 train_loss: 34.444461822509766 test_loss:104.5587387084961\n",
            "1509/3000 train_loss: 32.053897857666016 test_loss:106.61600494384766\n",
            "1510/3000 train_loss: 31.44940185546875 test_loss:103.06659698486328\n",
            "1511/3000 train_loss: 33.502532958984375 test_loss:107.81752014160156\n",
            "1512/3000 train_loss: 36.16807556152344 test_loss:100.03799438476562\n",
            "1513/3000 train_loss: 37.54204177856445 test_loss:101.96691131591797\n",
            "1514/3000 train_loss: 35.337039947509766 test_loss:100.69114685058594\n",
            "1515/3000 train_loss: 40.07801818847656 test_loss:104.8801498413086\n",
            "1516/3000 train_loss: 50.014244079589844 test_loss:104.55699157714844\n",
            "1517/3000 train_loss: 38.40510940551758 test_loss:110.88220977783203\n",
            "1518/3000 train_loss: 35.850162506103516 test_loss:101.83930206298828\n",
            "1519/3000 train_loss: 35.81378173828125 test_loss:110.10858154296875\n",
            "1520/3000 train_loss: 32.107452392578125 test_loss:99.7983627319336\n",
            "1521/3000 train_loss: 34.474266052246094 test_loss:106.5733871459961\n",
            "1522/3000 train_loss: 29.55487823486328 test_loss:108.21067810058594\n",
            "1523/3000 train_loss: 36.52823257446289 test_loss:101.94011688232422\n",
            "1524/3000 train_loss: 40.93369674682617 test_loss:99.55675506591797\n",
            "1525/3000 train_loss: 39.3280143737793 test_loss:107.60581970214844\n",
            "1526/3000 train_loss: 36.95090866088867 test_loss:107.61407470703125\n",
            "1527/3000 train_loss: 36.78881072998047 test_loss:103.0600357055664\n",
            "1528/3000 train_loss: 39.13668441772461 test_loss:107.33277893066406\n",
            "1529/3000 train_loss: 42.1607551574707 test_loss:106.37699890136719\n",
            "1530/3000 train_loss: 33.69704055786133 test_loss:103.14737701416016\n",
            "1531/3000 train_loss: 33.62046432495117 test_loss:96.15092468261719\n",
            "1532/3000 train_loss: 32.266822814941406 test_loss:109.83234405517578\n",
            "1533/3000 train_loss: 30.89027976989746 test_loss:102.00341033935547\n",
            "1534/3000 train_loss: 31.576608657836914 test_loss:105.3084945678711\n",
            "1535/3000 train_loss: 29.46202278137207 test_loss:99.06415557861328\n",
            "1536/3000 train_loss: 32.68553924560547 test_loss:107.21589660644531\n",
            "1537/3000 train_loss: 29.033905029296875 test_loss:99.2729263305664\n",
            "1538/3000 train_loss: 34.038360595703125 test_loss:103.31979370117188\n",
            "1539/3000 train_loss: 32.40994644165039 test_loss:100.51956939697266\n",
            "1540/3000 train_loss: 35.542396545410156 test_loss:102.02556610107422\n",
            "1541/3000 train_loss: 37.901641845703125 test_loss:108.33187103271484\n",
            "1542/3000 train_loss: 34.686561584472656 test_loss:100.86338806152344\n",
            "1543/3000 train_loss: 28.673372268676758 test_loss:100.31985473632812\n",
            "1544/3000 train_loss: 41.263587951660156 test_loss:107.35249328613281\n",
            "1545/3000 train_loss: 31.721393585205078 test_loss:101.2362289428711\n",
            "1546/3000 train_loss: 33.362091064453125 test_loss:106.45173645019531\n",
            "1547/3000 train_loss: 34.87145233154297 test_loss:109.8844223022461\n",
            "1548/3000 train_loss: 39.27372741699219 test_loss:105.64128875732422\n",
            "1549/3000 train_loss: 33.1693229675293 test_loss:104.81692504882812\n",
            "1550/3000 train_loss: 35.054019927978516 test_loss:104.21741485595703\n",
            "1551/3000 train_loss: 29.152584075927734 test_loss:109.93223571777344\n",
            "1552/3000 train_loss: 32.79546356201172 test_loss:101.90988159179688\n",
            "1553/3000 train_loss: 32.975223541259766 test_loss:96.9865493774414\n",
            "1554/3000 train_loss: 33.8396110534668 test_loss:102.51618194580078\n",
            "1555/3000 train_loss: 32.90163040161133 test_loss:104.5179443359375\n",
            "1556/3000 train_loss: 39.357078552246094 test_loss:111.87004089355469\n",
            "1557/3000 train_loss: 32.57970428466797 test_loss:99.640869140625\n",
            "1558/3000 train_loss: 37.212806701660156 test_loss:108.34342193603516\n",
            "1559/3000 train_loss: 32.76852798461914 test_loss:107.2629165649414\n",
            "1560/3000 train_loss: 31.454832077026367 test_loss:107.96931457519531\n",
            "1561/3000 train_loss: 33.04373550415039 test_loss:105.33633422851562\n",
            "1562/3000 train_loss: 33.665870666503906 test_loss:102.46939849853516\n",
            "1563/3000 train_loss: 31.39305305480957 test_loss:101.18036651611328\n",
            "1564/3000 train_loss: 36.04793167114258 test_loss:104.60008239746094\n",
            "1565/3000 train_loss: 34.98589324951172 test_loss:110.53421783447266\n",
            "1566/3000 train_loss: 38.14393615722656 test_loss:104.36235046386719\n",
            "1567/3000 train_loss: 32.21106719970703 test_loss:106.45436096191406\n",
            "1568/3000 train_loss: 35.59133529663086 test_loss:101.47686004638672\n",
            "1569/3000 train_loss: 34.65388488769531 test_loss:99.05612182617188\n",
            "1570/3000 train_loss: 30.569650650024414 test_loss:104.96121978759766\n",
            "1571/3000 train_loss: 41.24945068359375 test_loss:104.83844757080078\n",
            "1572/3000 train_loss: 34.858741760253906 test_loss:102.15039825439453\n",
            "1573/3000 train_loss: 33.33032989501953 test_loss:114.71868896484375\n",
            "1574/3000 train_loss: 38.974853515625 test_loss:98.72840881347656\n",
            "1575/3000 train_loss: 34.535919189453125 test_loss:100.11054229736328\n",
            "1576/3000 train_loss: 33.826210021972656 test_loss:112.24406433105469\n",
            "1577/3000 train_loss: 32.84577178955078 test_loss:102.15869140625\n",
            "1578/3000 train_loss: 36.62364959716797 test_loss:105.02088165283203\n",
            "1579/3000 train_loss: 29.593481063842773 test_loss:110.15426635742188\n",
            "1580/3000 train_loss: 30.96441650390625 test_loss:109.84049224853516\n",
            "1581/3000 train_loss: 37.17780303955078 test_loss:100.9951171875\n",
            "1582/3000 train_loss: 32.115108489990234 test_loss:113.31692504882812\n",
            "1583/3000 train_loss: 32.97984313964844 test_loss:101.75920104980469\n",
            "1584/3000 train_loss: 33.36241912841797 test_loss:109.31820678710938\n",
            "1585/3000 train_loss: 34.22122573852539 test_loss:108.47190856933594\n",
            "1586/3000 train_loss: 34.61296844482422 test_loss:101.68115234375\n",
            "1587/3000 train_loss: 34.913002014160156 test_loss:101.8418960571289\n",
            "1588/3000 train_loss: 37.0943489074707 test_loss:102.48333740234375\n",
            "1589/3000 train_loss: 34.09410858154297 test_loss:104.89068603515625\n",
            "1590/3000 train_loss: 40.6475715637207 test_loss:98.60481262207031\n",
            "1591/3000 train_loss: 37.04423904418945 test_loss:110.69091796875\n",
            "1592/3000 train_loss: 30.501022338867188 test_loss:102.60651397705078\n",
            "1593/3000 train_loss: 31.914215087890625 test_loss:107.04238891601562\n",
            "1594/3000 train_loss: 30.208524703979492 test_loss:95.81072998046875\n",
            "1595/3000 train_loss: 35.643028259277344 test_loss:103.09111785888672\n",
            "1596/3000 train_loss: 39.31785202026367 test_loss:106.2295913696289\n",
            "1597/3000 train_loss: 29.88347053527832 test_loss:103.90608978271484\n",
            "1598/3000 train_loss: 36.964324951171875 test_loss:103.51472473144531\n",
            "1599/3000 train_loss: 34.12645721435547 test_loss:109.39576721191406\n",
            "1600/3000 train_loss: 31.667675018310547 test_loss:109.70519256591797\n",
            "1601/3000 train_loss: 34.654720306396484 test_loss:100.14466094970703\n",
            "1602/3000 train_loss: 36.49538040161133 test_loss:104.90585327148438\n",
            "1603/3000 train_loss: 35.93202209472656 test_loss:104.01400756835938\n",
            "1604/3000 train_loss: 31.059070587158203 test_loss:109.14957427978516\n",
            "1605/3000 train_loss: 36.16289520263672 test_loss:106.72592163085938\n",
            "1606/3000 train_loss: 36.179588317871094 test_loss:97.88086700439453\n",
            "1607/3000 train_loss: 37.61015701293945 test_loss:105.44780731201172\n",
            "1608/3000 train_loss: 30.77978515625 test_loss:110.594970703125\n",
            "1609/3000 train_loss: 31.359905242919922 test_loss:105.94954681396484\n",
            "1610/3000 train_loss: 30.499988555908203 test_loss:100.37596130371094\n",
            "1611/3000 train_loss: 28.901836395263672 test_loss:108.44225311279297\n",
            "1612/3000 train_loss: 34.1954345703125 test_loss:102.70574951171875\n",
            "1613/3000 train_loss: 31.73711585998535 test_loss:102.81670379638672\n",
            "1614/3000 train_loss: 29.835187911987305 test_loss:105.95545959472656\n",
            "1615/3000 train_loss: 31.916255950927734 test_loss:102.79139709472656\n",
            "1616/3000 train_loss: 32.388816833496094 test_loss:102.41905975341797\n",
            "1617/3000 train_loss: 30.910099029541016 test_loss:102.95606994628906\n",
            "1618/3000 train_loss: 32.846763610839844 test_loss:100.20137023925781\n",
            "1619/3000 train_loss: 31.79960823059082 test_loss:104.85873413085938\n",
            "1620/3000 train_loss: 38.23557662963867 test_loss:99.54388427734375\n",
            "1621/3000 train_loss: 33.7109260559082 test_loss:109.87483215332031\n",
            "1622/3000 train_loss: 35.95088577270508 test_loss:104.6501235961914\n",
            "1623/3000 train_loss: 31.818265914916992 test_loss:106.10614013671875\n",
            "1624/3000 train_loss: 32.56855010986328 test_loss:102.32841491699219\n",
            "1625/3000 train_loss: 34.082557678222656 test_loss:105.474365234375\n",
            "1626/3000 train_loss: 30.166797637939453 test_loss:102.35226440429688\n",
            "1627/3000 train_loss: 30.32928466796875 test_loss:105.17333984375\n",
            "1628/3000 train_loss: 34.77853012084961 test_loss:105.3287124633789\n",
            "1629/3000 train_loss: 31.027088165283203 test_loss:104.34357452392578\n",
            "1630/3000 train_loss: 30.763195037841797 test_loss:100.13260650634766\n",
            "1631/3000 train_loss: 34.51892852783203 test_loss:105.42679595947266\n",
            "1632/3000 train_loss: 31.20010757446289 test_loss:101.73959350585938\n",
            "1633/3000 train_loss: 37.76346206665039 test_loss:110.6229248046875\n",
            "1634/3000 train_loss: 34.26888656616211 test_loss:103.6629638671875\n",
            "1635/3000 train_loss: 33.87750244140625 test_loss:96.65597534179688\n",
            "1636/3000 train_loss: 33.54513168334961 test_loss:111.82734680175781\n",
            "1637/3000 train_loss: 33.582210540771484 test_loss:98.31620788574219\n",
            "1638/3000 train_loss: 34.59962844848633 test_loss:110.30644226074219\n",
            "1639/3000 train_loss: 34.44927215576172 test_loss:100.3667221069336\n",
            "1640/3000 train_loss: 32.805564880371094 test_loss:102.4151840209961\n",
            "1641/3000 train_loss: 26.20699691772461 test_loss:102.08008575439453\n",
            "1642/3000 train_loss: 30.35706901550293 test_loss:104.53472137451172\n",
            "1643/3000 train_loss: 29.445301055908203 test_loss:108.70629119873047\n",
            "1644/3000 train_loss: 30.249353408813477 test_loss:105.57781982421875\n",
            "1645/3000 train_loss: 36.67473602294922 test_loss:106.07345581054688\n",
            "1646/3000 train_loss: 33.877113342285156 test_loss:108.74790954589844\n",
            "1647/3000 train_loss: 29.389305114746094 test_loss:107.39913177490234\n",
            "1648/3000 train_loss: 40.77961349487305 test_loss:100.391845703125\n",
            "1649/3000 train_loss: 32.04324722290039 test_loss:116.81275177001953\n",
            "1650/3000 train_loss: 31.428382873535156 test_loss:101.30341339111328\n",
            "1651/3000 train_loss: 33.24883270263672 test_loss:108.85153198242188\n",
            "1652/3000 train_loss: 30.934371948242188 test_loss:103.60263061523438\n",
            "1653/3000 train_loss: 33.112030029296875 test_loss:103.60215759277344\n",
            "1654/3000 train_loss: 31.38347816467285 test_loss:102.80291748046875\n",
            "1655/3000 train_loss: 34.806854248046875 test_loss:107.64398956298828\n",
            "1656/3000 train_loss: 35.7180061340332 test_loss:103.38960266113281\n",
            "1657/3000 train_loss: 36.73332214355469 test_loss:98.18136596679688\n",
            "1658/3000 train_loss: 29.554515838623047 test_loss:105.99049377441406\n",
            "1659/3000 train_loss: 32.10768508911133 test_loss:107.38939666748047\n",
            "1660/3000 train_loss: 32.33974075317383 test_loss:103.73131561279297\n",
            "1661/3000 train_loss: 35.23598861694336 test_loss:102.04370880126953\n",
            "1662/3000 train_loss: 30.310192108154297 test_loss:105.6471939086914\n",
            "1663/3000 train_loss: 33.785003662109375 test_loss:103.28262329101562\n",
            "1664/3000 train_loss: 32.8189697265625 test_loss:108.53033447265625\n",
            "1665/3000 train_loss: 31.104990005493164 test_loss:111.0321273803711\n",
            "1666/3000 train_loss: 32.5146598815918 test_loss:101.77388763427734\n",
            "1667/3000 train_loss: 31.89885711669922 test_loss:101.10653686523438\n",
            "1668/3000 train_loss: 30.64130973815918 test_loss:98.59119415283203\n",
            "1669/3000 train_loss: 31.338987350463867 test_loss:107.34456634521484\n",
            "1670/3000 train_loss: 28.372360229492188 test_loss:105.21256256103516\n",
            "1671/3000 train_loss: 28.525236129760742 test_loss:103.70001220703125\n",
            "1672/3000 train_loss: 34.06462478637695 test_loss:104.96363830566406\n",
            "1673/3000 train_loss: 30.398557662963867 test_loss:101.74311828613281\n",
            "1674/3000 train_loss: 31.481557846069336 test_loss:104.48271942138672\n",
            "1675/3000 train_loss: 34.47431945800781 test_loss:103.1983642578125\n",
            "1676/3000 train_loss: 33.047786712646484 test_loss:108.32594299316406\n",
            "1677/3000 train_loss: 29.704315185546875 test_loss:100.86524200439453\n",
            "1678/3000 train_loss: 29.030540466308594 test_loss:105.78897857666016\n",
            "1679/3000 train_loss: 30.612070083618164 test_loss:102.37450408935547\n",
            "1680/3000 train_loss: 31.36943817138672 test_loss:108.28797912597656\n",
            "1681/3000 train_loss: 29.39065933227539 test_loss:101.33213806152344\n",
            "1682/3000 train_loss: 30.19890022277832 test_loss:101.47608184814453\n",
            "1683/3000 train_loss: 33.322017669677734 test_loss:106.95307922363281\n",
            "1684/3000 train_loss: 38.011444091796875 test_loss:104.56672668457031\n",
            "1685/3000 train_loss: 30.775897979736328 test_loss:108.98281860351562\n",
            "1686/3000 train_loss: 36.86309051513672 test_loss:110.3936767578125\n",
            "1687/3000 train_loss: 36.943851470947266 test_loss:107.19764709472656\n",
            "1688/3000 train_loss: 32.89041519165039 test_loss:107.15615844726562\n",
            "1689/3000 train_loss: 30.63994026184082 test_loss:104.75924682617188\n",
            "1690/3000 train_loss: 33.799583435058594 test_loss:104.04850006103516\n",
            "1691/3000 train_loss: 31.3688907623291 test_loss:107.20244598388672\n",
            "1692/3000 train_loss: 28.629558563232422 test_loss:102.43624877929688\n",
            "1693/3000 train_loss: 31.4123477935791 test_loss:105.67979431152344\n",
            "1694/3000 train_loss: 33.917476654052734 test_loss:101.75843048095703\n",
            "1695/3000 train_loss: 32.794273376464844 test_loss:99.80353546142578\n",
            "1696/3000 train_loss: 31.646785736083984 test_loss:104.5775146484375\n",
            "1697/3000 train_loss: 30.802366256713867 test_loss:106.3011703491211\n",
            "1698/3000 train_loss: 33.204071044921875 test_loss:106.94197082519531\n",
            "1699/3000 train_loss: 31.90198516845703 test_loss:98.90355682373047\n",
            "1700/3000 train_loss: 28.046602249145508 test_loss:107.47770690917969\n",
            "1701/3000 train_loss: 31.99856185913086 test_loss:103.74359893798828\n",
            "1702/3000 train_loss: 40.93807601928711 test_loss:106.07806396484375\n",
            "1703/3000 train_loss: 34.535308837890625 test_loss:96.09109497070312\n",
            "1704/3000 train_loss: 32.76848602294922 test_loss:105.84054565429688\n",
            "1705/3000 train_loss: 28.36101531982422 test_loss:100.07923889160156\n",
            "1706/3000 train_loss: 34.56830978393555 test_loss:97.95323181152344\n",
            "1707/3000 train_loss: 32.7487907409668 test_loss:100.1793212890625\n",
            "1708/3000 train_loss: 31.88642120361328 test_loss:100.81147766113281\n",
            "1709/3000 train_loss: 34.95894241333008 test_loss:103.86563110351562\n",
            "1710/3000 train_loss: 35.97962951660156 test_loss:101.25394439697266\n",
            "1711/3000 train_loss: 31.205869674682617 test_loss:104.23643493652344\n",
            "1712/3000 train_loss: 37.857322692871094 test_loss:97.14888000488281\n",
            "1713/3000 train_loss: 31.60991668701172 test_loss:105.23566436767578\n",
            "1714/3000 train_loss: 35.69105911254883 test_loss:102.1570053100586\n",
            "1715/3000 train_loss: 31.18224334716797 test_loss:102.15081024169922\n",
            "1716/3000 train_loss: 31.037277221679688 test_loss:100.17174530029297\n",
            "1717/3000 train_loss: 31.881250381469727 test_loss:104.58055877685547\n",
            "1718/3000 train_loss: 29.434463500976562 test_loss:96.35631561279297\n",
            "1719/3000 train_loss: 33.45127487182617 test_loss:101.52310943603516\n",
            "1720/3000 train_loss: 32.77649688720703 test_loss:95.04240417480469\n",
            "1721/3000 train_loss: 27.642972946166992 test_loss:106.55707550048828\n",
            "1722/3000 train_loss: 33.24557113647461 test_loss:98.06193542480469\n",
            "1723/3000 train_loss: 30.95258331298828 test_loss:100.26642608642578\n",
            "1724/3000 train_loss: 29.78185272216797 test_loss:108.14674377441406\n",
            "1725/3000 train_loss: 31.902585983276367 test_loss:98.32830810546875\n",
            "1726/3000 train_loss: 30.19158363342285 test_loss:109.47511291503906\n",
            "1727/3000 train_loss: 33.019832611083984 test_loss:106.5850830078125\n",
            "1728/3000 train_loss: 36.84320068359375 test_loss:101.63069152832031\n",
            "1729/3000 train_loss: 33.045021057128906 test_loss:101.03227996826172\n",
            "1730/3000 train_loss: 34.96154022216797 test_loss:101.93364715576172\n",
            "1731/3000 train_loss: 31.02676773071289 test_loss:100.05058288574219\n",
            "1732/3000 train_loss: 26.413305282592773 test_loss:102.79708099365234\n",
            "1733/3000 train_loss: 27.220882415771484 test_loss:108.23243713378906\n",
            "1734/3000 train_loss: 31.804718017578125 test_loss:101.0526351928711\n",
            "1735/3000 train_loss: 30.51885223388672 test_loss:99.09176635742188\n",
            "1736/3000 train_loss: 25.988773345947266 test_loss:100.04549407958984\n",
            "1737/3000 train_loss: 34.95725631713867 test_loss:97.90130615234375\n",
            "1738/3000 train_loss: 31.54731559753418 test_loss:106.31681823730469\n",
            "1739/3000 train_loss: 30.128429412841797 test_loss:97.40997314453125\n",
            "1740/3000 train_loss: 34.0303955078125 test_loss:105.45597076416016\n",
            "1741/3000 train_loss: 34.84530258178711 test_loss:106.73855590820312\n",
            "1742/3000 train_loss: 28.494590759277344 test_loss:103.53474426269531\n",
            "1743/3000 train_loss: 30.685501098632812 test_loss:107.20838165283203\n",
            "1744/3000 train_loss: 29.58621597290039 test_loss:104.05201721191406\n",
            "1745/3000 train_loss: 29.566356658935547 test_loss:105.78109741210938\n",
            "1746/3000 train_loss: 25.120529174804688 test_loss:101.55438232421875\n",
            "1747/3000 train_loss: 28.121524810791016 test_loss:99.99829864501953\n",
            "1748/3000 train_loss: 30.020448684692383 test_loss:96.8773193359375\n",
            "1749/3000 train_loss: 39.20697021484375 test_loss:109.90234375\n",
            "1750/3000 train_loss: 29.754405975341797 test_loss:95.07002258300781\n",
            "1751/3000 train_loss: 29.308895111083984 test_loss:108.99970245361328\n",
            "1752/3000 train_loss: 29.009546279907227 test_loss:100.08221435546875\n",
            "1753/3000 train_loss: 35.77830123901367 test_loss:99.32933807373047\n",
            "1754/3000 train_loss: 29.702560424804688 test_loss:103.04289245605469\n",
            "1755/3000 train_loss: 29.769012451171875 test_loss:101.78589630126953\n",
            "1756/3000 train_loss: 34.839805603027344 test_loss:101.54877471923828\n",
            "1757/3000 train_loss: 32.040611267089844 test_loss:100.49160766601562\n",
            "1758/3000 train_loss: 33.15566635131836 test_loss:103.89537811279297\n",
            "1759/3000 train_loss: 31.147056579589844 test_loss:105.75727844238281\n",
            "1760/3000 train_loss: 28.191707611083984 test_loss:94.35250091552734\n",
            "1761/3000 train_loss: 32.092227935791016 test_loss:108.39303588867188\n",
            "1762/3000 train_loss: 31.336414337158203 test_loss:101.7794418334961\n",
            "1763/3000 train_loss: 26.470394134521484 test_loss:103.41996002197266\n",
            "1764/3000 train_loss: 29.44245147705078 test_loss:105.08535766601562\n",
            "1765/3000 train_loss: 33.50836944580078 test_loss:99.01868438720703\n",
            "1766/3000 train_loss: 30.094036102294922 test_loss:103.07848358154297\n",
            "1767/3000 train_loss: 30.771808624267578 test_loss:105.58061218261719\n",
            "1768/3000 train_loss: 33.3364143371582 test_loss:102.59700012207031\n",
            "1769/3000 train_loss: 26.754972457885742 test_loss:108.66632080078125\n",
            "1770/3000 train_loss: 29.918779373168945 test_loss:98.44609069824219\n",
            "1771/3000 train_loss: 33.58637619018555 test_loss:105.32508850097656\n",
            "1772/3000 train_loss: 28.845935821533203 test_loss:106.17861938476562\n",
            "1773/3000 train_loss: 29.001161575317383 test_loss:96.43455505371094\n",
            "1774/3000 train_loss: 37.24600601196289 test_loss:103.19608306884766\n",
            "1775/3000 train_loss: 27.847660064697266 test_loss:102.22703552246094\n",
            "1776/3000 train_loss: 31.999860763549805 test_loss:98.98727416992188\n",
            "1777/3000 train_loss: 29.977930068969727 test_loss:106.9120864868164\n",
            "1778/3000 train_loss: 29.531295776367188 test_loss:101.6328125\n",
            "1779/3000 train_loss: 29.61473846435547 test_loss:100.84886169433594\n",
            "1780/3000 train_loss: 28.552833557128906 test_loss:107.00557708740234\n",
            "1781/3000 train_loss: 28.77634048461914 test_loss:95.9510269165039\n",
            "1782/3000 train_loss: 30.617088317871094 test_loss:103.13353729248047\n",
            "1783/3000 train_loss: 34.844642639160156 test_loss:105.08452606201172\n",
            "1784/3000 train_loss: 33.313690185546875 test_loss:96.9679183959961\n",
            "1785/3000 train_loss: 28.54132080078125 test_loss:98.029052734375\n",
            "1786/3000 train_loss: 29.452428817749023 test_loss:98.10664367675781\n",
            "1787/3000 train_loss: 31.317506790161133 test_loss:93.96493530273438\n",
            "1788/3000 train_loss: 30.897192001342773 test_loss:105.16094970703125\n",
            "1789/3000 train_loss: 30.06982421875 test_loss:102.688232421875\n",
            "1790/3000 train_loss: 33.89811325073242 test_loss:104.05757141113281\n",
            "1791/3000 train_loss: 30.829599380493164 test_loss:102.02318572998047\n",
            "1792/3000 train_loss: 27.08597755432129 test_loss:98.26556396484375\n",
            "1793/3000 train_loss: 33.259578704833984 test_loss:97.72193908691406\n",
            "1794/3000 train_loss: 28.699209213256836 test_loss:106.06087493896484\n",
            "1795/3000 train_loss: 36.1653938293457 test_loss:99.60367584228516\n",
            "1796/3000 train_loss: 25.375564575195312 test_loss:102.3505859375\n",
            "1797/3000 train_loss: 30.639463424682617 test_loss:99.43073272705078\n",
            "1798/3000 train_loss: 29.023143768310547 test_loss:101.48200225830078\n",
            "1799/3000 train_loss: 27.067096710205078 test_loss:102.84236907958984\n",
            "1800/3000 train_loss: 36.38737487792969 test_loss:97.64997863769531\n",
            "1801/3000 train_loss: 30.063526153564453 test_loss:103.68794250488281\n",
            "1802/3000 train_loss: 31.985919952392578 test_loss:97.9476547241211\n",
            "1803/3000 train_loss: 28.83565902709961 test_loss:102.03092956542969\n",
            "1804/3000 train_loss: 29.35218048095703 test_loss:103.29143524169922\n",
            "1805/3000 train_loss: 32.242897033691406 test_loss:101.01634216308594\n",
            "1806/3000 train_loss: 29.354955673217773 test_loss:103.48570251464844\n",
            "1807/3000 train_loss: 32.19483184814453 test_loss:110.86109924316406\n",
            "1808/3000 train_loss: 30.521669387817383 test_loss:97.000244140625\n",
            "1809/3000 train_loss: 28.7908935546875 test_loss:97.28435516357422\n",
            "1810/3000 train_loss: 25.696218490600586 test_loss:101.89433288574219\n",
            "1811/3000 train_loss: 42.05622100830078 test_loss:97.05282592773438\n",
            "1812/3000 train_loss: 29.622648239135742 test_loss:98.3928451538086\n",
            "1813/3000 train_loss: 29.876419067382812 test_loss:102.6641616821289\n",
            "1814/3000 train_loss: 30.01544189453125 test_loss:102.50056457519531\n",
            "1815/3000 train_loss: 27.37932777404785 test_loss:100.8636474609375\n",
            "1816/3000 train_loss: 30.489126205444336 test_loss:96.93807220458984\n",
            "1817/3000 train_loss: 27.001483917236328 test_loss:102.6519546508789\n",
            "1818/3000 train_loss: 27.901885986328125 test_loss:102.17858123779297\n",
            "1819/3000 train_loss: 29.152713775634766 test_loss:101.21633911132812\n",
            "1820/3000 train_loss: 29.359060287475586 test_loss:104.72294616699219\n",
            "1821/3000 train_loss: 32.71379852294922 test_loss:101.5331802368164\n",
            "1822/3000 train_loss: 29.671180725097656 test_loss:97.98479461669922\n",
            "1823/3000 train_loss: 34.29438781738281 test_loss:108.58160400390625\n",
            "1824/3000 train_loss: 28.626670837402344 test_loss:101.84662628173828\n",
            "1825/3000 train_loss: 31.82636070251465 test_loss:100.09375\n",
            "1826/3000 train_loss: 29.507511138916016 test_loss:105.8363265991211\n",
            "1827/3000 train_loss: 30.28766632080078 test_loss:109.4107666015625\n",
            "1828/3000 train_loss: 29.757797241210938 test_loss:98.8442611694336\n",
            "1829/3000 train_loss: 34.48025131225586 test_loss:106.01981353759766\n",
            "1830/3000 train_loss: 28.092132568359375 test_loss:102.1196517944336\n",
            "1831/3000 train_loss: 32.56871795654297 test_loss:98.04467010498047\n",
            "1832/3000 train_loss: 25.79720115661621 test_loss:103.04578399658203\n",
            "1833/3000 train_loss: 35.379112243652344 test_loss:99.92650604248047\n",
            "1834/3000 train_loss: 29.78121566772461 test_loss:97.81149291992188\n",
            "1835/3000 train_loss: 36.32592010498047 test_loss:98.78034210205078\n",
            "1836/3000 train_loss: 27.806556701660156 test_loss:102.29207611083984\n",
            "1837/3000 train_loss: 29.745973587036133 test_loss:98.79634857177734\n",
            "1838/3000 train_loss: 29.92924690246582 test_loss:102.96243286132812\n",
            "1839/3000 train_loss: 30.74497413635254 test_loss:102.46234130859375\n",
            "1840/3000 train_loss: 32.179344177246094 test_loss:98.54866027832031\n",
            "1841/3000 train_loss: 34.30978012084961 test_loss:103.31194305419922\n",
            "1842/3000 train_loss: 31.77197265625 test_loss:100.0021743774414\n",
            "1843/3000 train_loss: 30.348445892333984 test_loss:99.67804718017578\n",
            "1844/3000 train_loss: 26.485652923583984 test_loss:97.15581512451172\n",
            "1845/3000 train_loss: 37.11140060424805 test_loss:108.50666809082031\n",
            "1846/3000 train_loss: 30.316884994506836 test_loss:103.46067810058594\n",
            "1847/3000 train_loss: 29.101802825927734 test_loss:105.6167221069336\n",
            "1848/3000 train_loss: 28.816936492919922 test_loss:95.00016021728516\n",
            "1849/3000 train_loss: 26.03521156311035 test_loss:98.79005432128906\n",
            "1850/3000 train_loss: 28.306194305419922 test_loss:99.30290985107422\n",
            "1851/3000 train_loss: 30.04498863220215 test_loss:101.39047241210938\n",
            "1852/3000 train_loss: 28.013830184936523 test_loss:101.2486343383789\n",
            "1853/3000 train_loss: 33.29037094116211 test_loss:98.99822235107422\n",
            "1854/3000 train_loss: 28.035375595092773 test_loss:100.05158233642578\n",
            "1855/3000 train_loss: 31.707918167114258 test_loss:102.064208984375\n",
            "1856/3000 train_loss: 25.191078186035156 test_loss:104.68836975097656\n",
            "1857/3000 train_loss: 25.184175491333008 test_loss:101.0792007446289\n",
            "1858/3000 train_loss: 27.013195037841797 test_loss:102.20783233642578\n",
            "1859/3000 train_loss: 26.56541633605957 test_loss:105.96951293945312\n",
            "1860/3000 train_loss: 27.707910537719727 test_loss:104.53981018066406\n",
            "1861/3000 train_loss: 34.058311462402344 test_loss:98.36315155029297\n",
            "1862/3000 train_loss: 31.50718116760254 test_loss:99.11351776123047\n",
            "1863/3000 train_loss: 29.698644638061523 test_loss:106.3039779663086\n",
            "1864/3000 train_loss: 27.93964385986328 test_loss:101.04007720947266\n",
            "1865/3000 train_loss: 33.079185485839844 test_loss:102.62158966064453\n",
            "1866/3000 train_loss: 27.58167266845703 test_loss:102.42591857910156\n",
            "1867/3000 train_loss: 29.237319946289062 test_loss:103.53125\n",
            "1868/3000 train_loss: 31.12582778930664 test_loss:104.5362777709961\n",
            "1869/3000 train_loss: 28.010963439941406 test_loss:98.41175079345703\n",
            "1870/3000 train_loss: 26.827552795410156 test_loss:102.3316650390625\n",
            "1871/3000 train_loss: 26.777563095092773 test_loss:99.31798553466797\n",
            "1872/3000 train_loss: 33.837833404541016 test_loss:106.6173324584961\n",
            "1873/3000 train_loss: 26.556007385253906 test_loss:99.01852416992188\n",
            "1874/3000 train_loss: 26.166221618652344 test_loss:100.91059112548828\n",
            "1875/3000 train_loss: 25.23816680908203 test_loss:101.7259750366211\n",
            "1876/3000 train_loss: 27.41649627685547 test_loss:104.310302734375\n",
            "1877/3000 train_loss: 34.046142578125 test_loss:98.77172088623047\n",
            "1878/3000 train_loss: 25.1595516204834 test_loss:102.13055419921875\n",
            "1879/3000 train_loss: 25.73390769958496 test_loss:102.78372955322266\n",
            "1880/3000 train_loss: 26.761219024658203 test_loss:102.78779602050781\n",
            "1881/3000 train_loss: 29.956201553344727 test_loss:95.7931137084961\n",
            "1882/3000 train_loss: 31.346891403198242 test_loss:92.54418182373047\n",
            "1883/3000 train_loss: 28.217567443847656 test_loss:105.84193420410156\n",
            "1884/3000 train_loss: 31.976802825927734 test_loss:100.44739532470703\n",
            "1885/3000 train_loss: 34.51701736450195 test_loss:100.22630310058594\n",
            "1886/3000 train_loss: 24.84347915649414 test_loss:96.31925201416016\n",
            "1887/3000 train_loss: 27.997289657592773 test_loss:103.27144622802734\n",
            "1888/3000 train_loss: 25.603185653686523 test_loss:93.10803985595703\n",
            "1889/3000 train_loss: 28.649505615234375 test_loss:100.2593002319336\n",
            "1890/3000 train_loss: 35.27140426635742 test_loss:102.14517211914062\n",
            "1891/3000 train_loss: 35.02458572387695 test_loss:99.33619689941406\n",
            "1892/3000 train_loss: 27.607467651367188 test_loss:97.82819366455078\n",
            "1893/3000 train_loss: 25.481586456298828 test_loss:100.298828125\n",
            "1894/3000 train_loss: 31.121253967285156 test_loss:100.75724029541016\n",
            "1895/3000 train_loss: 29.438398361206055 test_loss:98.64466857910156\n",
            "1896/3000 train_loss: 30.786102294921875 test_loss:105.06074523925781\n",
            "1897/3000 train_loss: 28.01116180419922 test_loss:106.7872085571289\n",
            "1898/3000 train_loss: 26.172306060791016 test_loss:102.30117797851562\n",
            "1899/3000 train_loss: 27.877267837524414 test_loss:99.71981048583984\n",
            "1900/3000 train_loss: 28.809059143066406 test_loss:106.97252655029297\n",
            "1901/3000 train_loss: 34.112342834472656 test_loss:98.65348815917969\n",
            "1902/3000 train_loss: 33.111000061035156 test_loss:94.83553314208984\n",
            "1903/3000 train_loss: 26.324146270751953 test_loss:101.02804565429688\n",
            "1904/3000 train_loss: 26.58777618408203 test_loss:105.71903991699219\n",
            "1905/3000 train_loss: 30.212844848632812 test_loss:99.75770568847656\n",
            "1906/3000 train_loss: 35.11713409423828 test_loss:113.13714599609375\n",
            "1907/3000 train_loss: 26.27220916748047 test_loss:93.74305725097656\n",
            "1908/3000 train_loss: 25.336135864257812 test_loss:104.20154571533203\n",
            "1909/3000 train_loss: 25.615123748779297 test_loss:99.27935791015625\n",
            "1910/3000 train_loss: 26.75620460510254 test_loss:102.3788833618164\n",
            "1911/3000 train_loss: 24.635692596435547 test_loss:110.53582763671875\n",
            "1912/3000 train_loss: 32.17122268676758 test_loss:96.43822479248047\n",
            "1913/3000 train_loss: 26.002655029296875 test_loss:108.63299560546875\n",
            "1914/3000 train_loss: 33.72401428222656 test_loss:96.34081268310547\n",
            "1915/3000 train_loss: 26.40057373046875 test_loss:97.16815948486328\n",
            "1916/3000 train_loss: 29.013093948364258 test_loss:104.75962829589844\n",
            "1917/3000 train_loss: 30.927587509155273 test_loss:94.08171844482422\n",
            "1918/3000 train_loss: 27.835098266601562 test_loss:106.11627960205078\n",
            "1919/3000 train_loss: 28.917388916015625 test_loss:98.78005981445312\n",
            "1920/3000 train_loss: 28.9632568359375 test_loss:101.4875717163086\n",
            "1921/3000 train_loss: 31.026023864746094 test_loss:106.07823181152344\n",
            "1922/3000 train_loss: 34.683475494384766 test_loss:95.57828521728516\n",
            "1923/3000 train_loss: 28.630361557006836 test_loss:101.28935241699219\n",
            "1924/3000 train_loss: 31.565914154052734 test_loss:98.93238830566406\n",
            "1925/3000 train_loss: 29.60076141357422 test_loss:98.81951904296875\n",
            "1926/3000 train_loss: 26.733930587768555 test_loss:105.1466293334961\n",
            "1927/3000 train_loss: 32.763580322265625 test_loss:96.55076599121094\n",
            "1928/3000 train_loss: 23.61618995666504 test_loss:98.84097290039062\n",
            "1929/3000 train_loss: 25.178768157958984 test_loss:99.56839752197266\n",
            "1930/3000 train_loss: 24.959861755371094 test_loss:101.39590454101562\n",
            "1931/3000 train_loss: 29.552352905273438 test_loss:104.27806854248047\n",
            "1932/3000 train_loss: 30.85516357421875 test_loss:102.10529327392578\n",
            "1933/3000 train_loss: 29.596446990966797 test_loss:97.0523910522461\n",
            "1934/3000 train_loss: 28.52280044555664 test_loss:105.83641052246094\n",
            "1935/3000 train_loss: 25.900463104248047 test_loss:102.47669982910156\n",
            "1936/3000 train_loss: 25.516141891479492 test_loss:97.07704162597656\n",
            "1937/3000 train_loss: 29.166736602783203 test_loss:99.43231201171875\n",
            "1938/3000 train_loss: 32.99538040161133 test_loss:102.04179382324219\n",
            "1939/3000 train_loss: 29.032136917114258 test_loss:96.57842254638672\n",
            "1940/3000 train_loss: 36.33903884887695 test_loss:105.73634338378906\n",
            "1941/3000 train_loss: 35.34773635864258 test_loss:101.08018493652344\n",
            "1942/3000 train_loss: 27.709692001342773 test_loss:95.95938873291016\n",
            "1943/3000 train_loss: 31.212255477905273 test_loss:107.99567413330078\n",
            "1944/3000 train_loss: 27.866775512695312 test_loss:96.99298095703125\n",
            "1945/3000 train_loss: 32.5684928894043 test_loss:98.02531433105469\n",
            "1946/3000 train_loss: 28.339265823364258 test_loss:111.26927185058594\n",
            "1947/3000 train_loss: 26.93088722229004 test_loss:94.60315704345703\n",
            "1948/3000 train_loss: 26.17253875732422 test_loss:108.52415466308594\n",
            "1949/3000 train_loss: 25.727853775024414 test_loss:97.17218780517578\n",
            "1950/3000 train_loss: 27.913162231445312 test_loss:101.28762817382812\n",
            "1951/3000 train_loss: 25.722023010253906 test_loss:99.95345306396484\n",
            "1952/3000 train_loss: 27.83799934387207 test_loss:102.09028625488281\n",
            "1953/3000 train_loss: 26.447113037109375 test_loss:103.68900299072266\n",
            "1954/3000 train_loss: 27.98416519165039 test_loss:100.6764907836914\n",
            "1955/3000 train_loss: 26.732423782348633 test_loss:102.45549774169922\n",
            "1956/3000 train_loss: 24.9874210357666 test_loss:99.26568603515625\n",
            "1957/3000 train_loss: 25.54353904724121 test_loss:103.96725463867188\n",
            "1958/3000 train_loss: 27.926067352294922 test_loss:109.88204193115234\n",
            "1959/3000 train_loss: 27.087024688720703 test_loss:97.39447784423828\n",
            "1960/3000 train_loss: 26.385456085205078 test_loss:98.77003479003906\n",
            "1961/3000 train_loss: 23.742033004760742 test_loss:99.9490966796875\n",
            "1962/3000 train_loss: 27.145645141601562 test_loss:93.5522689819336\n",
            "1963/3000 train_loss: 27.805084228515625 test_loss:94.01937866210938\n",
            "1964/3000 train_loss: 30.16922378540039 test_loss:95.83819580078125\n",
            "1965/3000 train_loss: 25.803279876708984 test_loss:96.19054412841797\n",
            "1966/3000 train_loss: 24.106069564819336 test_loss:100.98322296142578\n",
            "1967/3000 train_loss: 30.668289184570312 test_loss:90.664306640625\n",
            "1968/3000 train_loss: 27.444185256958008 test_loss:99.47608184814453\n",
            "1969/3000 train_loss: 26.401700973510742 test_loss:99.61801147460938\n",
            "1970/3000 train_loss: 31.238698959350586 test_loss:95.68643188476562\n",
            "1971/3000 train_loss: 29.592517852783203 test_loss:99.81204223632812\n",
            "1972/3000 train_loss: 23.64397430419922 test_loss:98.05364227294922\n",
            "1973/3000 train_loss: 26.210304260253906 test_loss:93.62405395507812\n",
            "1974/3000 train_loss: 25.414981842041016 test_loss:98.82520294189453\n",
            "1975/3000 train_loss: 32.073246002197266 test_loss:100.67668914794922\n",
            "1976/3000 train_loss: 28.5345516204834 test_loss:94.95906066894531\n",
            "1977/3000 train_loss: 27.948448181152344 test_loss:101.54972839355469\n",
            "1978/3000 train_loss: 33.258079528808594 test_loss:95.9048843383789\n",
            "1979/3000 train_loss: 28.138853073120117 test_loss:103.22917938232422\n",
            "1980/3000 train_loss: 29.847625732421875 test_loss:96.53872680664062\n",
            "1981/3000 train_loss: 26.408016204833984 test_loss:104.45035552978516\n",
            "1982/3000 train_loss: 26.235698699951172 test_loss:101.02318572998047\n",
            "1983/3000 train_loss: 27.232959747314453 test_loss:103.3631362915039\n",
            "1984/3000 train_loss: 27.909955978393555 test_loss:103.137939453125\n",
            "1985/3000 train_loss: 26.399553298950195 test_loss:98.65380859375\n",
            "1986/3000 train_loss: 25.83883285522461 test_loss:97.55535888671875\n",
            "1987/3000 train_loss: 26.384838104248047 test_loss:95.45002746582031\n",
            "1988/3000 train_loss: 29.09023666381836 test_loss:105.63163757324219\n",
            "1989/3000 train_loss: 25.785608291625977 test_loss:96.35708618164062\n",
            "1990/3000 train_loss: 25.04781150817871 test_loss:99.76848602294922\n",
            "1991/3000 train_loss: 29.31442642211914 test_loss:105.39583587646484\n",
            "1992/3000 train_loss: 26.500835418701172 test_loss:100.14213562011719\n",
            "1993/3000 train_loss: 28.275867462158203 test_loss:98.81234741210938\n",
            "1994/3000 train_loss: 30.520999908447266 test_loss:99.0918197631836\n",
            "1995/3000 train_loss: 29.837730407714844 test_loss:101.83042907714844\n",
            "1996/3000 train_loss: 25.189697265625 test_loss:103.21458435058594\n",
            "1997/3000 train_loss: 27.511043548583984 test_loss:95.44857025146484\n",
            "1998/3000 train_loss: 26.991127014160156 test_loss:113.25894165039062\n",
            "1999/3000 train_loss: 31.712860107421875 test_loss:92.24514770507812\n",
            "2000/3000 train_loss: 34.58161163330078 test_loss:101.624755859375\n",
            "2001/3000 train_loss: 27.62734031677246 test_loss:96.33928680419922\n",
            "2002/3000 train_loss: 28.240636825561523 test_loss:95.52477264404297\n",
            "2003/3000 train_loss: 25.284894943237305 test_loss:97.85128021240234\n",
            "2004/3000 train_loss: 25.40043830871582 test_loss:94.64604949951172\n",
            "2005/3000 train_loss: 29.114994049072266 test_loss:99.10616302490234\n",
            "2006/3000 train_loss: 31.214221954345703 test_loss:98.37395477294922\n",
            "2007/3000 train_loss: 27.081459045410156 test_loss:107.02323913574219\n",
            "2008/3000 train_loss: 27.386070251464844 test_loss:95.01048278808594\n",
            "2009/3000 train_loss: 27.155635833740234 test_loss:95.20822143554688\n",
            "2010/3000 train_loss: 30.759000778198242 test_loss:107.643798828125\n",
            "2011/3000 train_loss: 31.59048080444336 test_loss:104.59751892089844\n",
            "2012/3000 train_loss: 23.721900939941406 test_loss:96.55886840820312\n",
            "2013/3000 train_loss: 25.77136993408203 test_loss:95.78137969970703\n",
            "2014/3000 train_loss: 25.87807846069336 test_loss:96.70783996582031\n",
            "2015/3000 train_loss: 33.98276901245117 test_loss:102.1767807006836\n",
            "2016/3000 train_loss: 26.07495880126953 test_loss:92.68030548095703\n",
            "2017/3000 train_loss: 29.022804260253906 test_loss:95.43341064453125\n",
            "2018/3000 train_loss: 30.435956954956055 test_loss:99.435546875\n",
            "2019/3000 train_loss: 27.021337509155273 test_loss:100.5357437133789\n",
            "2020/3000 train_loss: 24.173656463623047 test_loss:90.69585418701172\n",
            "2021/3000 train_loss: 26.29757308959961 test_loss:101.30717468261719\n",
            "2022/3000 train_loss: 29.1795711517334 test_loss:98.15450286865234\n",
            "2023/3000 train_loss: 28.995941162109375 test_loss:96.28177642822266\n",
            "2024/3000 train_loss: 26.13865852355957 test_loss:100.46585845947266\n",
            "2025/3000 train_loss: 26.35143280029297 test_loss:98.42662048339844\n",
            "2026/3000 train_loss: 25.355403900146484 test_loss:94.36868286132812\n",
            "2027/3000 train_loss: 27.595075607299805 test_loss:96.22557067871094\n",
            "2028/3000 train_loss: 26.831422805786133 test_loss:96.71570587158203\n",
            "2029/3000 train_loss: 30.51789665222168 test_loss:102.48346710205078\n",
            "2030/3000 train_loss: 22.8065185546875 test_loss:91.21501922607422\n",
            "2031/3000 train_loss: 23.606178283691406 test_loss:97.2623519897461\n",
            "2032/3000 train_loss: 24.486587524414062 test_loss:97.06708526611328\n",
            "2033/3000 train_loss: 26.387189865112305 test_loss:97.93795013427734\n",
            "2034/3000 train_loss: 29.7445125579834 test_loss:96.5126724243164\n",
            "2035/3000 train_loss: 30.965734481811523 test_loss:105.73260498046875\n",
            "2036/3000 train_loss: 24.29963493347168 test_loss:91.445068359375\n",
            "2037/3000 train_loss: 29.364587783813477 test_loss:99.36487579345703\n",
            "2038/3000 train_loss: 29.067340850830078 test_loss:92.05577087402344\n",
            "2039/3000 train_loss: 31.40619468688965 test_loss:98.06185913085938\n",
            "2040/3000 train_loss: 27.549297332763672 test_loss:95.75983428955078\n",
            "2041/3000 train_loss: 25.914045333862305 test_loss:94.5260009765625\n",
            "2042/3000 train_loss: 26.56342315673828 test_loss:96.99504852294922\n",
            "2043/3000 train_loss: 26.56874656677246 test_loss:93.8777847290039\n",
            "2044/3000 train_loss: 24.882354736328125 test_loss:96.99201202392578\n",
            "2045/3000 train_loss: 24.648937225341797 test_loss:104.86061096191406\n",
            "2046/3000 train_loss: 24.510807037353516 test_loss:100.3850326538086\n",
            "2047/3000 train_loss: 25.08176612854004 test_loss:93.12828826904297\n",
            "2048/3000 train_loss: 24.299596786499023 test_loss:96.76754760742188\n",
            "2049/3000 train_loss: 27.225914001464844 test_loss:99.39878845214844\n",
            "2050/3000 train_loss: 25.265092849731445 test_loss:98.14646911621094\n",
            "2051/3000 train_loss: 23.46411895751953 test_loss:99.69194030761719\n",
            "2052/3000 train_loss: 24.134061813354492 test_loss:94.5801010131836\n",
            "2053/3000 train_loss: 24.84862518310547 test_loss:95.73194122314453\n",
            "2054/3000 train_loss: 26.910097122192383 test_loss:96.57379150390625\n",
            "2055/3000 train_loss: 25.440576553344727 test_loss:96.07621002197266\n",
            "2056/3000 train_loss: 29.179351806640625 test_loss:101.90961456298828\n",
            "2057/3000 train_loss: 28.55710220336914 test_loss:99.30120849609375\n",
            "2058/3000 train_loss: 29.157333374023438 test_loss:98.68448638916016\n",
            "2059/3000 train_loss: 27.721149444580078 test_loss:95.44103240966797\n",
            "2060/3000 train_loss: 24.61669158935547 test_loss:102.47469329833984\n",
            "2061/3000 train_loss: 27.740169525146484 test_loss:97.59153747558594\n",
            "2062/3000 train_loss: 25.58451271057129 test_loss:100.82142639160156\n",
            "2063/3000 train_loss: 24.555652618408203 test_loss:98.92567443847656\n",
            "2064/3000 train_loss: 26.72714614868164 test_loss:101.74578857421875\n",
            "2065/3000 train_loss: 24.424230575561523 test_loss:96.37569427490234\n",
            "2066/3000 train_loss: 29.14438247680664 test_loss:112.35069274902344\n",
            "2067/3000 train_loss: 30.463150024414062 test_loss:94.47643280029297\n",
            "2068/3000 train_loss: 28.296314239501953 test_loss:96.9796371459961\n",
            "2069/3000 train_loss: 25.957603454589844 test_loss:95.0455551147461\n",
            "2070/3000 train_loss: 24.815990447998047 test_loss:95.52802276611328\n",
            "2071/3000 train_loss: 25.564727783203125 test_loss:99.53070068359375\n",
            "2072/3000 train_loss: 27.87851333618164 test_loss:102.5576171875\n",
            "2073/3000 train_loss: 26.43260383605957 test_loss:96.961669921875\n",
            "2074/3000 train_loss: 26.37574577331543 test_loss:101.9444808959961\n",
            "2075/3000 train_loss: 29.98311996459961 test_loss:103.98054504394531\n",
            "2076/3000 train_loss: 25.929874420166016 test_loss:104.56065368652344\n",
            "2077/3000 train_loss: 35.35108947753906 test_loss:100.11996459960938\n",
            "2078/3000 train_loss: 26.58241844177246 test_loss:92.81903076171875\n",
            "2079/3000 train_loss: 24.022886276245117 test_loss:106.97587585449219\n",
            "2080/3000 train_loss: 26.410125732421875 test_loss:99.81201934814453\n",
            "2081/3000 train_loss: 27.37359619140625 test_loss:97.82606506347656\n",
            "2082/3000 train_loss: 27.00114631652832 test_loss:98.11432647705078\n",
            "2083/3000 train_loss: 29.39244270324707 test_loss:100.24420166015625\n",
            "2084/3000 train_loss: 23.98175621032715 test_loss:96.35598754882812\n",
            "2085/3000 train_loss: 26.20771026611328 test_loss:101.63159942626953\n",
            "2086/3000 train_loss: 29.930307388305664 test_loss:96.15306854248047\n",
            "2087/3000 train_loss: 22.626514434814453 test_loss:103.925048828125\n",
            "2088/3000 train_loss: 26.03978157043457 test_loss:99.19715118408203\n",
            "2089/3000 train_loss: 26.527223587036133 test_loss:103.16856384277344\n",
            "2090/3000 train_loss: 26.44313621520996 test_loss:94.30494689941406\n",
            "2091/3000 train_loss: 27.95443344116211 test_loss:99.14606475830078\n",
            "2092/3000 train_loss: 23.45651626586914 test_loss:98.27184295654297\n",
            "2093/3000 train_loss: 31.03522491455078 test_loss:98.91143798828125\n",
            "2094/3000 train_loss: 25.0900936126709 test_loss:98.498046875\n",
            "2095/3000 train_loss: 21.528244018554688 test_loss:95.35552215576172\n",
            "2096/3000 train_loss: 26.911327362060547 test_loss:87.67018127441406\n",
            "2097/3000 train_loss: 25.978256225585938 test_loss:98.726318359375\n",
            "2098/3000 train_loss: 25.493677139282227 test_loss:93.44061279296875\n",
            "2099/3000 train_loss: 26.624351501464844 test_loss:90.42427062988281\n",
            "2100/3000 train_loss: 29.29836082458496 test_loss:95.41366577148438\n",
            "2101/3000 train_loss: 36.11907958984375 test_loss:93.78401184082031\n",
            "2102/3000 train_loss: 25.999696731567383 test_loss:110.9144287109375\n",
            "2103/3000 train_loss: 26.29947280883789 test_loss:93.7413101196289\n",
            "2104/3000 train_loss: 30.683269500732422 test_loss:102.1771469116211\n",
            "2105/3000 train_loss: 25.72439193725586 test_loss:97.19977569580078\n",
            "2106/3000 train_loss: 26.168195724487305 test_loss:99.43040466308594\n",
            "2107/3000 train_loss: 28.851667404174805 test_loss:95.72587585449219\n",
            "2108/3000 train_loss: 29.960433959960938 test_loss:104.5605239868164\n",
            "2109/3000 train_loss: 26.773839950561523 test_loss:93.26961517333984\n",
            "2110/3000 train_loss: 28.34963607788086 test_loss:103.72185516357422\n",
            "2111/3000 train_loss: 28.235143661499023 test_loss:97.8425064086914\n",
            "2112/3000 train_loss: 28.58392906188965 test_loss:95.04361724853516\n",
            "2113/3000 train_loss: 26.732723236083984 test_loss:92.25948333740234\n",
            "2114/3000 train_loss: 28.210525512695312 test_loss:102.16776275634766\n",
            "2115/3000 train_loss: 24.191179275512695 test_loss:98.78182220458984\n",
            "2116/3000 train_loss: 25.677213668823242 test_loss:97.2408218383789\n",
            "2117/3000 train_loss: 22.16284942626953 test_loss:98.63335418701172\n",
            "2118/3000 train_loss: 26.59270668029785 test_loss:96.83423614501953\n",
            "2119/3000 train_loss: 29.59046173095703 test_loss:102.53581237792969\n",
            "2120/3000 train_loss: 26.489532470703125 test_loss:98.87014770507812\n",
            "2121/3000 train_loss: 20.85993003845215 test_loss:93.74580383300781\n",
            "2122/3000 train_loss: 29.679931640625 test_loss:98.93722534179688\n",
            "2123/3000 train_loss: 27.740182876586914 test_loss:97.33500671386719\n",
            "2124/3000 train_loss: 22.490612030029297 test_loss:92.14634704589844\n",
            "2125/3000 train_loss: 28.890178680419922 test_loss:100.22733306884766\n",
            "2126/3000 train_loss: 26.812442779541016 test_loss:92.18354034423828\n",
            "2127/3000 train_loss: 25.2147274017334 test_loss:104.46566009521484\n",
            "2128/3000 train_loss: 29.263702392578125 test_loss:95.9360580444336\n",
            "2129/3000 train_loss: 35.895721435546875 test_loss:98.72955322265625\n",
            "2130/3000 train_loss: 27.558496475219727 test_loss:101.45890808105469\n",
            "2131/3000 train_loss: 24.727258682250977 test_loss:98.18408966064453\n",
            "2132/3000 train_loss: 27.507234573364258 test_loss:99.82240295410156\n",
            "2133/3000 train_loss: 27.367061614990234 test_loss:102.07731628417969\n",
            "2134/3000 train_loss: 26.350704193115234 test_loss:95.3121109008789\n",
            "2135/3000 train_loss: 23.25701904296875 test_loss:100.31914520263672\n",
            "2136/3000 train_loss: 21.69658660888672 test_loss:97.67999267578125\n",
            "2137/3000 train_loss: 25.21237564086914 test_loss:100.03998565673828\n",
            "2138/3000 train_loss: 27.69560432434082 test_loss:95.52297973632812\n",
            "2139/3000 train_loss: 25.016393661499023 test_loss:101.51275634765625\n",
            "2140/3000 train_loss: 26.842884063720703 test_loss:106.28732299804688\n",
            "2141/3000 train_loss: 26.187105178833008 test_loss:103.01551818847656\n",
            "2142/3000 train_loss: 25.72433853149414 test_loss:100.17529296875\n",
            "2143/3000 train_loss: 28.479541778564453 test_loss:107.10018920898438\n",
            "2144/3000 train_loss: 25.69208335876465 test_loss:94.65064239501953\n",
            "2145/3000 train_loss: 27.895401000976562 test_loss:102.4327621459961\n",
            "2146/3000 train_loss: 28.117202758789062 test_loss:106.1654052734375\n",
            "2147/3000 train_loss: 28.45877456665039 test_loss:93.91862487792969\n",
            "2148/3000 train_loss: 25.792757034301758 test_loss:104.57540130615234\n",
            "2149/3000 train_loss: 21.416831970214844 test_loss:93.54489135742188\n",
            "2150/3000 train_loss: 26.356473922729492 test_loss:100.73747253417969\n",
            "2151/3000 train_loss: 28.179927825927734 test_loss:98.92802429199219\n",
            "2152/3000 train_loss: 24.359651565551758 test_loss:93.96300506591797\n",
            "2153/3000 train_loss: 24.22170639038086 test_loss:99.74143981933594\n",
            "2154/3000 train_loss: 22.81162452697754 test_loss:96.8160629272461\n",
            "2155/3000 train_loss: 25.61558723449707 test_loss:99.76434326171875\n",
            "2156/3000 train_loss: 34.78312301635742 test_loss:98.71681213378906\n",
            "2157/3000 train_loss: 23.772199630737305 test_loss:102.58724212646484\n",
            "2158/3000 train_loss: 21.94306182861328 test_loss:98.41142272949219\n",
            "2159/3000 train_loss: 22.918062210083008 test_loss:104.73274993896484\n",
            "2160/3000 train_loss: 26.39280891418457 test_loss:95.82018280029297\n",
            "2161/3000 train_loss: 24.602384567260742 test_loss:98.00180053710938\n",
            "2162/3000 train_loss: 24.291507720947266 test_loss:98.13980102539062\n",
            "2163/3000 train_loss: 25.650367736816406 test_loss:101.21968841552734\n",
            "2164/3000 train_loss: 29.40009307861328 test_loss:96.36499786376953\n",
            "2165/3000 train_loss: 27.78075408935547 test_loss:102.08755493164062\n",
            "2166/3000 train_loss: 22.478408813476562 test_loss:94.97982025146484\n",
            "2167/3000 train_loss: 22.94490623474121 test_loss:100.89957427978516\n",
            "2168/3000 train_loss: 25.809633255004883 test_loss:101.50406646728516\n",
            "2169/3000 train_loss: 25.385337829589844 test_loss:94.97908020019531\n",
            "2170/3000 train_loss: 24.17913055419922 test_loss:95.35586547851562\n",
            "2171/3000 train_loss: 26.59609031677246 test_loss:98.21102142333984\n",
            "2172/3000 train_loss: 21.809873580932617 test_loss:100.30750274658203\n",
            "2173/3000 train_loss: 24.767793655395508 test_loss:98.45372772216797\n",
            "2174/3000 train_loss: 25.204267501831055 test_loss:102.24208068847656\n",
            "2175/3000 train_loss: 26.705211639404297 test_loss:100.94263458251953\n",
            "2176/3000 train_loss: 23.137432098388672 test_loss:100.14251708984375\n",
            "2177/3000 train_loss: 24.1660213470459 test_loss:101.38142395019531\n",
            "2178/3000 train_loss: 24.427654266357422 test_loss:102.17899322509766\n",
            "2179/3000 train_loss: 27.637008666992188 test_loss:97.86998748779297\n",
            "2180/3000 train_loss: 30.754220962524414 test_loss:91.0719223022461\n",
            "2181/3000 train_loss: 27.037843704223633 test_loss:106.68255615234375\n",
            "2182/3000 train_loss: 24.63437271118164 test_loss:100.8469467163086\n",
            "2183/3000 train_loss: 26.27555274963379 test_loss:99.1390380859375\n",
            "2184/3000 train_loss: 24.80261993408203 test_loss:95.85803985595703\n",
            "2185/3000 train_loss: 24.355030059814453 test_loss:96.95667266845703\n",
            "2186/3000 train_loss: 26.68389129638672 test_loss:97.64434051513672\n",
            "2187/3000 train_loss: 29.45576286315918 test_loss:98.55924987792969\n",
            "2188/3000 train_loss: 24.15123748779297 test_loss:103.8873291015625\n",
            "2189/3000 train_loss: 27.870651245117188 test_loss:96.44935607910156\n",
            "2190/3000 train_loss: 28.847230911254883 test_loss:95.83822631835938\n",
            "2191/3000 train_loss: 26.36290740966797 test_loss:106.97950744628906\n",
            "2192/3000 train_loss: 27.491744995117188 test_loss:102.73009490966797\n",
            "2193/3000 train_loss: 31.175546646118164 test_loss:99.92893981933594\n",
            "2194/3000 train_loss: 28.578174591064453 test_loss:99.43348693847656\n",
            "2195/3000 train_loss: 22.956209182739258 test_loss:99.6177978515625\n",
            "2196/3000 train_loss: 23.978925704956055 test_loss:98.2601089477539\n",
            "2197/3000 train_loss: 26.137615203857422 test_loss:94.21862030029297\n",
            "2198/3000 train_loss: 21.290019989013672 test_loss:99.98307037353516\n",
            "2199/3000 train_loss: 25.57513427734375 test_loss:99.26007080078125\n",
            "2200/3000 train_loss: 26.439496994018555 test_loss:102.60578155517578\n",
            "2201/3000 train_loss: 24.085756301879883 test_loss:94.69976806640625\n",
            "2202/3000 train_loss: 21.071077346801758 test_loss:99.82170104980469\n",
            "2203/3000 train_loss: 20.94919204711914 test_loss:100.35452270507812\n",
            "2204/3000 train_loss: 19.467487335205078 test_loss:98.64796447753906\n",
            "2205/3000 train_loss: 24.485992431640625 test_loss:97.77538299560547\n",
            "2206/3000 train_loss: 27.102174758911133 test_loss:94.33840942382812\n",
            "2207/3000 train_loss: 26.521488189697266 test_loss:100.81855010986328\n",
            "2208/3000 train_loss: 28.98923110961914 test_loss:98.80852508544922\n",
            "2209/3000 train_loss: 27.921642303466797 test_loss:103.85533142089844\n",
            "2210/3000 train_loss: 27.00155258178711 test_loss:101.2769775390625\n",
            "2211/3000 train_loss: 26.155906677246094 test_loss:99.7166519165039\n",
            "2212/3000 train_loss: 29.144113540649414 test_loss:97.50919342041016\n",
            "2213/3000 train_loss: 27.01495361328125 test_loss:103.55970764160156\n",
            "2214/3000 train_loss: 29.811595916748047 test_loss:88.54032897949219\n",
            "2215/3000 train_loss: 26.129995346069336 test_loss:101.91734313964844\n",
            "2216/3000 train_loss: 25.840932846069336 test_loss:95.51431274414062\n",
            "2217/3000 train_loss: 23.045249938964844 test_loss:100.47968292236328\n",
            "2218/3000 train_loss: 26.58539390563965 test_loss:98.41244506835938\n",
            "2219/3000 train_loss: 24.405559539794922 test_loss:96.13821411132812\n",
            "2220/3000 train_loss: 27.249168395996094 test_loss:97.55838012695312\n",
            "2221/3000 train_loss: 24.633901596069336 test_loss:100.97638702392578\n",
            "2222/3000 train_loss: 26.244096755981445 test_loss:100.29986572265625\n",
            "2223/3000 train_loss: 27.80807113647461 test_loss:98.87628936767578\n",
            "2224/3000 train_loss: 31.135215759277344 test_loss:104.77693939208984\n",
            "2225/3000 train_loss: 22.088760375976562 test_loss:95.14594268798828\n",
            "2226/3000 train_loss: 28.642316818237305 test_loss:92.10408020019531\n",
            "2227/3000 train_loss: 26.95802116394043 test_loss:99.48013305664062\n",
            "2228/3000 train_loss: 24.876609802246094 test_loss:98.16374969482422\n",
            "2229/3000 train_loss: 26.908910751342773 test_loss:97.58097839355469\n",
            "2230/3000 train_loss: 27.699663162231445 test_loss:100.09827423095703\n",
            "2231/3000 train_loss: 23.70922088623047 test_loss:93.37969970703125\n",
            "2232/3000 train_loss: 24.32256507873535 test_loss:92.291748046875\n",
            "2233/3000 train_loss: 24.720182418823242 test_loss:101.67561340332031\n",
            "2234/3000 train_loss: 27.152742385864258 test_loss:94.83675384521484\n",
            "2235/3000 train_loss: 27.330827713012695 test_loss:99.33960723876953\n",
            "2236/3000 train_loss: 21.25191307067871 test_loss:98.39010620117188\n",
            "2237/3000 train_loss: 27.382457733154297 test_loss:95.74504852294922\n",
            "2238/3000 train_loss: 26.37193489074707 test_loss:95.9636001586914\n",
            "2239/3000 train_loss: 25.778501510620117 test_loss:99.71465301513672\n",
            "2240/3000 train_loss: 21.67966651916504 test_loss:93.02928924560547\n",
            "2241/3000 train_loss: 32.9004020690918 test_loss:98.67472076416016\n",
            "2242/3000 train_loss: 28.332677841186523 test_loss:99.23670196533203\n",
            "2243/3000 train_loss: 23.028484344482422 test_loss:97.27410125732422\n",
            "2244/3000 train_loss: 23.795738220214844 test_loss:104.58220672607422\n",
            "2245/3000 train_loss: 24.542709350585938 test_loss:89.13970947265625\n",
            "2246/3000 train_loss: 23.335344314575195 test_loss:98.184814453125\n",
            "2247/3000 train_loss: 24.18885612487793 test_loss:96.54183197021484\n",
            "2248/3000 train_loss: 24.371488571166992 test_loss:98.02723693847656\n",
            "2249/3000 train_loss: 27.710678100585938 test_loss:96.27686309814453\n",
            "2250/3000 train_loss: 25.304914474487305 test_loss:92.69416809082031\n",
            "2251/3000 train_loss: 22.15857696533203 test_loss:98.12885284423828\n",
            "2252/3000 train_loss: 25.463306427001953 test_loss:94.23656463623047\n",
            "2253/3000 train_loss: 22.53184700012207 test_loss:102.00469970703125\n",
            "2254/3000 train_loss: 24.73025131225586 test_loss:94.25972747802734\n",
            "2255/3000 train_loss: 25.96564292907715 test_loss:99.05216217041016\n",
            "2256/3000 train_loss: 22.907209396362305 test_loss:94.45111083984375\n",
            "2257/3000 train_loss: 25.3826904296875 test_loss:93.62466430664062\n",
            "2258/3000 train_loss: 22.2974853515625 test_loss:95.58277893066406\n",
            "2259/3000 train_loss: 24.979206085205078 test_loss:95.30415344238281\n",
            "2260/3000 train_loss: 21.9578857421875 test_loss:101.8230972290039\n",
            "2261/3000 train_loss: 27.739612579345703 test_loss:91.77974700927734\n",
            "2262/3000 train_loss: 27.572214126586914 test_loss:99.470947265625\n",
            "2263/3000 train_loss: 27.88323974609375 test_loss:97.60123443603516\n",
            "2264/3000 train_loss: 21.971200942993164 test_loss:97.86823272705078\n",
            "2265/3000 train_loss: 24.336393356323242 test_loss:91.81359100341797\n",
            "2266/3000 train_loss: 26.45924186706543 test_loss:103.22235107421875\n",
            "2267/3000 train_loss: 23.97056007385254 test_loss:91.67884826660156\n",
            "2268/3000 train_loss: 26.81304168701172 test_loss:97.89396667480469\n",
            "2269/3000 train_loss: 23.72428321838379 test_loss:94.84053039550781\n",
            "2270/3000 train_loss: 22.961406707763672 test_loss:95.54624938964844\n",
            "2271/3000 train_loss: 20.724550247192383 test_loss:96.74388885498047\n",
            "2272/3000 train_loss: 27.08670997619629 test_loss:95.87039947509766\n",
            "2273/3000 train_loss: 23.78843879699707 test_loss:92.15762329101562\n",
            "2274/3000 train_loss: 24.135059356689453 test_loss:98.70414733886719\n",
            "2275/3000 train_loss: 21.719139099121094 test_loss:102.07433319091797\n",
            "2276/3000 train_loss: 24.983993530273438 test_loss:88.89838409423828\n",
            "2277/3000 train_loss: 22.691606521606445 test_loss:96.62730407714844\n",
            "2278/3000 train_loss: 21.87217903137207 test_loss:96.22248077392578\n",
            "2279/3000 train_loss: 22.427486419677734 test_loss:95.71006774902344\n",
            "2280/3000 train_loss: 27.773618698120117 test_loss:96.54356384277344\n",
            "2281/3000 train_loss: 23.400907516479492 test_loss:91.78653717041016\n",
            "2282/3000 train_loss: 23.381126403808594 test_loss:95.50288391113281\n",
            "2283/3000 train_loss: 26.32162094116211 test_loss:98.72930908203125\n",
            "2284/3000 train_loss: 23.790454864501953 test_loss:98.727783203125\n",
            "2285/3000 train_loss: 23.618776321411133 test_loss:93.49665832519531\n",
            "2286/3000 train_loss: 26.627094268798828 test_loss:98.40764617919922\n",
            "2287/3000 train_loss: 22.836305618286133 test_loss:91.45511627197266\n",
            "2288/3000 train_loss: 23.126026153564453 test_loss:98.06980895996094\n",
            "2289/3000 train_loss: 20.696617126464844 test_loss:93.6157455444336\n",
            "2290/3000 train_loss: 31.101089477539062 test_loss:110.69427490234375\n",
            "2291/3000 train_loss: 23.672500610351562 test_loss:97.18008422851562\n",
            "2292/3000 train_loss: 29.1754150390625 test_loss:90.50345611572266\n",
            "2293/3000 train_loss: 21.740882873535156 test_loss:96.32796478271484\n",
            "2294/3000 train_loss: 22.37942886352539 test_loss:98.34503173828125\n",
            "2295/3000 train_loss: 27.52678680419922 test_loss:98.66500854492188\n",
            "2296/3000 train_loss: 23.557594299316406 test_loss:93.51075744628906\n",
            "2297/3000 train_loss: 24.928930282592773 test_loss:94.7945785522461\n",
            "2298/3000 train_loss: 23.637409210205078 test_loss:89.42432403564453\n",
            "2299/3000 train_loss: 28.718496322631836 test_loss:98.0628662109375\n",
            "2300/3000 train_loss: 19.59823226928711 test_loss:98.0771484375\n",
            "2301/3000 train_loss: 24.047073364257812 test_loss:94.53638458251953\n",
            "2302/3000 train_loss: 23.33626365661621 test_loss:93.36591339111328\n",
            "2303/3000 train_loss: 23.867321014404297 test_loss:92.55048370361328\n",
            "2304/3000 train_loss: 25.54669189453125 test_loss:103.15528869628906\n",
            "2305/3000 train_loss: 22.997173309326172 test_loss:96.49980926513672\n",
            "2306/3000 train_loss: 24.400102615356445 test_loss:98.93592834472656\n",
            "2307/3000 train_loss: 25.746089935302734 test_loss:95.52679443359375\n",
            "2308/3000 train_loss: 28.48756980895996 test_loss:100.01063537597656\n",
            "2309/3000 train_loss: 22.49954605102539 test_loss:100.07235717773438\n",
            "2310/3000 train_loss: 26.231029510498047 test_loss:93.39038848876953\n",
            "2311/3000 train_loss: 29.53350830078125 test_loss:101.87950897216797\n",
            "2312/3000 train_loss: 28.873186111450195 test_loss:95.59934997558594\n",
            "2313/3000 train_loss: 25.51099967956543 test_loss:93.08938598632812\n",
            "2314/3000 train_loss: 25.22536849975586 test_loss:97.5032958984375\n",
            "2315/3000 train_loss: 24.848045349121094 test_loss:100.76289367675781\n",
            "2316/3000 train_loss: 23.066707611083984 test_loss:89.81368255615234\n",
            "2317/3000 train_loss: 28.99847984313965 test_loss:99.73072052001953\n",
            "2318/3000 train_loss: 25.59111213684082 test_loss:93.47449493408203\n",
            "2319/3000 train_loss: 23.74109649658203 test_loss:93.24650573730469\n",
            "2320/3000 train_loss: 23.69544792175293 test_loss:94.4371337890625\n",
            "2321/3000 train_loss: 26.50067901611328 test_loss:101.87982940673828\n",
            "2322/3000 train_loss: 29.378034591674805 test_loss:91.54647827148438\n",
            "2323/3000 train_loss: 24.90272331237793 test_loss:96.74144744873047\n",
            "2324/3000 train_loss: 20.005014419555664 test_loss:97.63888549804688\n",
            "2325/3000 train_loss: 21.220155715942383 test_loss:97.81536865234375\n",
            "2326/3000 train_loss: 25.15598487854004 test_loss:98.24162292480469\n",
            "2327/3000 train_loss: 26.5048770904541 test_loss:96.59200286865234\n",
            "2328/3000 train_loss: 22.209651947021484 test_loss:94.5216293334961\n",
            "2329/3000 train_loss: 25.92465591430664 test_loss:95.45710754394531\n",
            "2330/3000 train_loss: 26.580936431884766 test_loss:98.80684661865234\n",
            "2331/3000 train_loss: 25.175582885742188 test_loss:92.45065307617188\n",
            "2332/3000 train_loss: 22.125408172607422 test_loss:91.20931243896484\n",
            "2333/3000 train_loss: 28.256752014160156 test_loss:89.5204849243164\n",
            "2334/3000 train_loss: 26.299570083618164 test_loss:98.59539031982422\n",
            "2335/3000 train_loss: 19.733091354370117 test_loss:91.4792709350586\n",
            "2336/3000 train_loss: 21.03264617919922 test_loss:96.08899688720703\n",
            "2337/3000 train_loss: 24.713151931762695 test_loss:99.46189880371094\n",
            "2338/3000 train_loss: 22.752477645874023 test_loss:94.7530517578125\n",
            "2339/3000 train_loss: 27.95309066772461 test_loss:94.4150619506836\n",
            "2340/3000 train_loss: 27.099084854125977 test_loss:98.01918029785156\n",
            "2341/3000 train_loss: 23.093482971191406 test_loss:99.32100677490234\n",
            "2342/3000 train_loss: 20.880754470825195 test_loss:96.82008361816406\n",
            "2343/3000 train_loss: 26.388221740722656 test_loss:91.25586700439453\n",
            "2344/3000 train_loss: 23.797786712646484 test_loss:101.710205078125\n",
            "2345/3000 train_loss: 22.867555618286133 test_loss:95.62801361083984\n",
            "2346/3000 train_loss: 24.109325408935547 test_loss:102.50799560546875\n",
            "2347/3000 train_loss: 25.295143127441406 test_loss:100.31486511230469\n",
            "2348/3000 train_loss: 22.807371139526367 test_loss:99.23006439208984\n",
            "2349/3000 train_loss: 24.960559844970703 test_loss:99.1191635131836\n",
            "2350/3000 train_loss: 21.577043533325195 test_loss:97.26547241210938\n",
            "2351/3000 train_loss: 24.11101722717285 test_loss:96.48875427246094\n",
            "2352/3000 train_loss: 26.016021728515625 test_loss:95.3165512084961\n",
            "2353/3000 train_loss: 27.441673278808594 test_loss:102.48371124267578\n",
            "2354/3000 train_loss: 21.423158645629883 test_loss:100.79261779785156\n",
            "2355/3000 train_loss: 24.078824996948242 test_loss:92.07313537597656\n",
            "2356/3000 train_loss: 21.078155517578125 test_loss:95.0560531616211\n",
            "2357/3000 train_loss: 25.908910751342773 test_loss:98.95519256591797\n",
            "2358/3000 train_loss: 25.39023208618164 test_loss:96.66976928710938\n",
            "2359/3000 train_loss: 25.52591323852539 test_loss:97.9554672241211\n",
            "2360/3000 train_loss: 20.840595245361328 test_loss:95.91242218017578\n",
            "2361/3000 train_loss: 22.161518096923828 test_loss:95.15031433105469\n",
            "2362/3000 train_loss: 25.569368362426758 test_loss:99.36087799072266\n",
            "2363/3000 train_loss: 22.7117977142334 test_loss:94.06735229492188\n",
            "2364/3000 train_loss: 17.999378204345703 test_loss:100.3149185180664\n",
            "2365/3000 train_loss: 23.46078109741211 test_loss:101.59410095214844\n",
            "2366/3000 train_loss: 25.367332458496094 test_loss:92.14541625976562\n",
            "2367/3000 train_loss: 21.259380340576172 test_loss:96.2309341430664\n",
            "2368/3000 train_loss: 23.508386611938477 test_loss:94.51615905761719\n",
            "2369/3000 train_loss: 25.237192153930664 test_loss:95.4843978881836\n",
            "2370/3000 train_loss: 23.822647094726562 test_loss:96.51936340332031\n",
            "2371/3000 train_loss: 24.74569320678711 test_loss:90.6507568359375\n",
            "2372/3000 train_loss: 23.6390438079834 test_loss:99.60566711425781\n",
            "2373/3000 train_loss: 25.399320602416992 test_loss:89.6882095336914\n",
            "2374/3000 train_loss: 27.800920486450195 test_loss:97.45695495605469\n",
            "2375/3000 train_loss: 26.346195220947266 test_loss:89.55182647705078\n",
            "2376/3000 train_loss: 19.710533142089844 test_loss:93.93672180175781\n",
            "2377/3000 train_loss: 20.641515731811523 test_loss:96.03472137451172\n",
            "2378/3000 train_loss: 21.405500411987305 test_loss:96.67174530029297\n",
            "2379/3000 train_loss: 23.7623233795166 test_loss:96.32978820800781\n",
            "2380/3000 train_loss: 28.435836791992188 test_loss:94.29963684082031\n",
            "2381/3000 train_loss: 22.895648956298828 test_loss:97.55335998535156\n",
            "2382/3000 train_loss: 25.389240264892578 test_loss:101.75177764892578\n",
            "2383/3000 train_loss: 24.79014015197754 test_loss:93.54812622070312\n",
            "2384/3000 train_loss: 24.40236473083496 test_loss:94.10689544677734\n",
            "2385/3000 train_loss: 20.83544158935547 test_loss:100.29354858398438\n",
            "2386/3000 train_loss: 23.389820098876953 test_loss:91.33394622802734\n",
            "2387/3000 train_loss: 21.642555236816406 test_loss:97.22843170166016\n",
            "2388/3000 train_loss: 26.735567092895508 test_loss:97.264404296875\n",
            "2389/3000 train_loss: 28.110755920410156 test_loss:101.37161254882812\n",
            "2390/3000 train_loss: 23.370033264160156 test_loss:94.71363830566406\n",
            "2391/3000 train_loss: 24.12253761291504 test_loss:96.19454956054688\n",
            "2392/3000 train_loss: 25.653745651245117 test_loss:98.54094696044922\n",
            "2393/3000 train_loss: 24.573923110961914 test_loss:93.24861907958984\n",
            "2394/3000 train_loss: 21.88016128540039 test_loss:96.91494750976562\n",
            "2395/3000 train_loss: 26.740253448486328 test_loss:98.3852310180664\n",
            "2396/3000 train_loss: 23.63988494873047 test_loss:93.69459533691406\n",
            "2397/3000 train_loss: 22.694726943969727 test_loss:93.83757781982422\n",
            "2398/3000 train_loss: 26.119464874267578 test_loss:92.37165832519531\n",
            "2399/3000 train_loss: 26.525707244873047 test_loss:99.26123809814453\n",
            "2400/3000 train_loss: 22.132179260253906 test_loss:91.25579833984375\n",
            "2401/3000 train_loss: 22.721471786499023 test_loss:92.8539810180664\n",
            "2402/3000 train_loss: 21.72525405883789 test_loss:95.25910186767578\n",
            "2403/3000 train_loss: 23.09245491027832 test_loss:92.12606811523438\n",
            "2404/3000 train_loss: 24.31208038330078 test_loss:102.03496551513672\n",
            "2405/3000 train_loss: 23.57558250427246 test_loss:89.07869720458984\n",
            "2406/3000 train_loss: 28.355575561523438 test_loss:98.5025863647461\n",
            "2407/3000 train_loss: 21.85064125061035 test_loss:92.32120513916016\n",
            "2408/3000 train_loss: 26.101123809814453 test_loss:95.65233612060547\n",
            "2409/3000 train_loss: 22.735570907592773 test_loss:91.75568389892578\n",
            "2410/3000 train_loss: 23.137409210205078 test_loss:99.05192565917969\n",
            "2411/3000 train_loss: 22.84710121154785 test_loss:89.21089172363281\n",
            "2412/3000 train_loss: 23.38365936279297 test_loss:94.75072479248047\n",
            "2413/3000 train_loss: 20.945648193359375 test_loss:90.57166290283203\n",
            "2414/3000 train_loss: 24.730012893676758 test_loss:97.13920593261719\n",
            "2415/3000 train_loss: 20.44229507446289 test_loss:90.5971450805664\n",
            "2416/3000 train_loss: 25.948949813842773 test_loss:92.59183502197266\n",
            "2417/3000 train_loss: 24.646324157714844 test_loss:100.629150390625\n",
            "2418/3000 train_loss: 26.18067169189453 test_loss:92.92534637451172\n",
            "2419/3000 train_loss: 25.982519149780273 test_loss:91.13327026367188\n",
            "2420/3000 train_loss: 24.323074340820312 test_loss:95.63431549072266\n",
            "2421/3000 train_loss: 19.854705810546875 test_loss:94.21627807617188\n",
            "2422/3000 train_loss: 22.997581481933594 test_loss:91.53057098388672\n",
            "2423/3000 train_loss: 25.635108947753906 test_loss:101.70767211914062\n",
            "2424/3000 train_loss: 20.56868553161621 test_loss:93.6274185180664\n",
            "2425/3000 train_loss: 20.235355377197266 test_loss:94.32698822021484\n",
            "2426/3000 train_loss: 19.600980758666992 test_loss:96.03678131103516\n",
            "2427/3000 train_loss: 26.695783615112305 test_loss:93.16837310791016\n",
            "2428/3000 train_loss: 23.086181640625 test_loss:93.34061431884766\n",
            "2429/3000 train_loss: 31.653947830200195 test_loss:102.67913818359375\n",
            "2430/3000 train_loss: 23.874738693237305 test_loss:93.53356170654297\n",
            "2431/3000 train_loss: 25.97071075439453 test_loss:100.19013977050781\n",
            "2432/3000 train_loss: 23.30913734436035 test_loss:91.50422668457031\n",
            "2433/3000 train_loss: 25.328733444213867 test_loss:93.65621185302734\n",
            "2434/3000 train_loss: 29.198427200317383 test_loss:102.20354461669922\n",
            "2435/3000 train_loss: 23.717260360717773 test_loss:98.35860443115234\n",
            "2436/3000 train_loss: 26.480737686157227 test_loss:95.53480529785156\n",
            "2437/3000 train_loss: 22.753511428833008 test_loss:101.0624771118164\n",
            "2438/3000 train_loss: 24.369543075561523 test_loss:91.92317199707031\n",
            "2439/3000 train_loss: 27.28209114074707 test_loss:95.89636993408203\n",
            "2440/3000 train_loss: 23.737285614013672 test_loss:92.21316528320312\n",
            "2441/3000 train_loss: 23.457332611083984 test_loss:90.9030532836914\n",
            "2442/3000 train_loss: 23.21474266052246 test_loss:99.88667297363281\n",
            "2443/3000 train_loss: 25.996789932250977 test_loss:105.91036987304688\n",
            "2444/3000 train_loss: 23.633039474487305 test_loss:87.83061981201172\n",
            "2445/3000 train_loss: 25.505455017089844 test_loss:90.8250961303711\n",
            "2446/3000 train_loss: 21.887195587158203 test_loss:90.79786682128906\n",
            "2447/3000 train_loss: 25.12042236328125 test_loss:96.65757751464844\n",
            "2448/3000 train_loss: 22.079429626464844 test_loss:89.27883911132812\n",
            "2449/3000 train_loss: 24.87810516357422 test_loss:90.08280181884766\n",
            "2450/3000 train_loss: 26.254959106445312 test_loss:95.01824951171875\n",
            "2451/3000 train_loss: 24.625579833984375 test_loss:93.81047058105469\n",
            "2452/3000 train_loss: 24.145015716552734 test_loss:89.72980499267578\n",
            "2453/3000 train_loss: 21.077045440673828 test_loss:99.85960388183594\n",
            "2454/3000 train_loss: 21.604595184326172 test_loss:90.74491882324219\n",
            "2455/3000 train_loss: 23.392663955688477 test_loss:95.65644836425781\n",
            "2456/3000 train_loss: 25.32921028137207 test_loss:96.93435668945312\n",
            "2457/3000 train_loss: 20.849489212036133 test_loss:94.50047302246094\n",
            "2458/3000 train_loss: 22.63018226623535 test_loss:88.59489440917969\n",
            "2459/3000 train_loss: 27.476444244384766 test_loss:94.31293487548828\n",
            "2460/3000 train_loss: 25.985225677490234 test_loss:100.79507446289062\n",
            "2461/3000 train_loss: 27.50257682800293 test_loss:95.1083755493164\n",
            "2462/3000 train_loss: 23.46315574645996 test_loss:94.99678039550781\n",
            "2463/3000 train_loss: 22.063039779663086 test_loss:96.89324188232422\n",
            "2464/3000 train_loss: 23.998266220092773 test_loss:91.20853424072266\n",
            "2465/3000 train_loss: 20.547718048095703 test_loss:95.18802642822266\n",
            "2466/3000 train_loss: 22.17355728149414 test_loss:91.88530731201172\n",
            "2467/3000 train_loss: 27.416013717651367 test_loss:96.54692840576172\n",
            "2468/3000 train_loss: 22.805927276611328 test_loss:95.54539489746094\n",
            "2469/3000 train_loss: 26.995725631713867 test_loss:99.1806411743164\n",
            "2470/3000 train_loss: 20.76250648498535 test_loss:89.79891204833984\n",
            "2471/3000 train_loss: 22.928123474121094 test_loss:87.884521484375\n",
            "2472/3000 train_loss: 23.337474822998047 test_loss:94.45144653320312\n",
            "2473/3000 train_loss: 22.73933219909668 test_loss:88.71561431884766\n",
            "2474/3000 train_loss: 28.01139259338379 test_loss:95.51589965820312\n",
            "2475/3000 train_loss: 27.244373321533203 test_loss:96.08649444580078\n",
            "2476/3000 train_loss: 23.576431274414062 test_loss:87.90837097167969\n",
            "2477/3000 train_loss: 25.997217178344727 test_loss:99.9144515991211\n",
            "2478/3000 train_loss: 20.1924991607666 test_loss:98.1445083618164\n",
            "2479/3000 train_loss: 22.156192779541016 test_loss:96.29955291748047\n",
            "2480/3000 train_loss: 24.393413543701172 test_loss:90.59488677978516\n",
            "2481/3000 train_loss: 21.218055725097656 test_loss:107.31838989257812\n",
            "2482/3000 train_loss: 21.84044075012207 test_loss:92.66571044921875\n",
            "2483/3000 train_loss: 23.811241149902344 test_loss:95.03375244140625\n",
            "2484/3000 train_loss: 29.094017028808594 test_loss:93.11398315429688\n",
            "2485/3000 train_loss: 24.350648880004883 test_loss:94.20585632324219\n",
            "2486/3000 train_loss: 22.065101623535156 test_loss:94.64871215820312\n",
            "2487/3000 train_loss: 20.356969833374023 test_loss:99.50180053710938\n",
            "2488/3000 train_loss: 25.148574829101562 test_loss:91.17021179199219\n",
            "2489/3000 train_loss: 23.197351455688477 test_loss:103.50927734375\n",
            "2490/3000 train_loss: 24.367877960205078 test_loss:89.80162048339844\n",
            "2491/3000 train_loss: 27.10179901123047 test_loss:93.67742156982422\n",
            "2492/3000 train_loss: 28.052120208740234 test_loss:101.39876556396484\n",
            "2493/3000 train_loss: 29.25745964050293 test_loss:90.21134185791016\n",
            "2494/3000 train_loss: 21.737590789794922 test_loss:100.82596588134766\n",
            "2495/3000 train_loss: 24.864421844482422 test_loss:87.24908447265625\n",
            "2496/3000 train_loss: 24.18301773071289 test_loss:100.78233337402344\n",
            "2497/3000 train_loss: 23.35947608947754 test_loss:93.38304901123047\n",
            "2498/3000 train_loss: 22.84738540649414 test_loss:91.02912139892578\n",
            "2499/3000 train_loss: 23.656126022338867 test_loss:96.94483947753906\n",
            "2500/3000 train_loss: 27.968259811401367 test_loss:90.46490478515625\n",
            "2501/3000 train_loss: 22.870853424072266 test_loss:97.70098876953125\n",
            "2502/3000 train_loss: 23.643375396728516 test_loss:95.076171875\n",
            "2503/3000 train_loss: 18.845500946044922 test_loss:92.41846466064453\n",
            "2504/3000 train_loss: 23.20118522644043 test_loss:92.1655502319336\n",
            "2505/3000 train_loss: 22.752689361572266 test_loss:91.59675598144531\n",
            "2506/3000 train_loss: 24.893142700195312 test_loss:90.01506805419922\n",
            "2507/3000 train_loss: 28.210975646972656 test_loss:94.50846099853516\n",
            "2508/3000 train_loss: 23.618684768676758 test_loss:92.34798431396484\n",
            "2509/3000 train_loss: 29.409168243408203 test_loss:91.1353530883789\n",
            "2510/3000 train_loss: 21.164960861206055 test_loss:99.25630187988281\n",
            "2511/3000 train_loss: 23.90155029296875 test_loss:93.35041809082031\n",
            "2512/3000 train_loss: 21.81467056274414 test_loss:93.1287612915039\n",
            "2513/3000 train_loss: 20.405466079711914 test_loss:89.5677719116211\n",
            "2514/3000 train_loss: 25.665376663208008 test_loss:105.0002670288086\n",
            "2515/3000 train_loss: 31.50364875793457 test_loss:94.30402374267578\n",
            "2516/3000 train_loss: 26.716449737548828 test_loss:95.78363800048828\n",
            "2517/3000 train_loss: 24.905202865600586 test_loss:97.30099487304688\n",
            "2518/3000 train_loss: 25.0516300201416 test_loss:91.07926177978516\n",
            "2519/3000 train_loss: 27.261577606201172 test_loss:93.595458984375\n",
            "2520/3000 train_loss: 25.023582458496094 test_loss:93.2738265991211\n",
            "2521/3000 train_loss: 21.015573501586914 test_loss:91.29423522949219\n",
            "2522/3000 train_loss: 23.040809631347656 test_loss:95.09603118896484\n",
            "2523/3000 train_loss: 20.528440475463867 test_loss:97.42081451416016\n",
            "2524/3000 train_loss: 24.259674072265625 test_loss:91.17640686035156\n",
            "2525/3000 train_loss: 20.661407470703125 test_loss:97.77836608886719\n",
            "2526/3000 train_loss: 31.783092498779297 test_loss:88.09158325195312\n",
            "2527/3000 train_loss: 26.699533462524414 test_loss:99.08141326904297\n",
            "2528/3000 train_loss: 24.542743682861328 test_loss:96.33402252197266\n",
            "2529/3000 train_loss: 23.047544479370117 test_loss:98.68460083007812\n",
            "2530/3000 train_loss: 22.495027542114258 test_loss:91.27926635742188\n",
            "2531/3000 train_loss: 24.174394607543945 test_loss:94.11605834960938\n",
            "2532/3000 train_loss: 21.275497436523438 test_loss:94.80363464355469\n",
            "2533/3000 train_loss: 18.89306640625 test_loss:95.6217041015625\n",
            "2534/3000 train_loss: 19.595169067382812 test_loss:99.62300109863281\n",
            "2535/3000 train_loss: 21.858295440673828 test_loss:90.8308334350586\n",
            "2536/3000 train_loss: 22.988487243652344 test_loss:99.9398193359375\n",
            "2537/3000 train_loss: 24.750133514404297 test_loss:94.19053649902344\n",
            "2538/3000 train_loss: 19.18697738647461 test_loss:95.29912567138672\n",
            "2539/3000 train_loss: 21.5340633392334 test_loss:88.62954711914062\n",
            "2540/3000 train_loss: 21.334762573242188 test_loss:92.092529296875\n",
            "2541/3000 train_loss: 24.63258171081543 test_loss:89.12149047851562\n",
            "2542/3000 train_loss: 31.762746810913086 test_loss:93.98513793945312\n",
            "2543/3000 train_loss: 23.454050064086914 test_loss:98.5710220336914\n",
            "2544/3000 train_loss: 24.876502990722656 test_loss:90.70109558105469\n",
            "2545/3000 train_loss: 21.038801193237305 test_loss:99.6759262084961\n",
            "2546/3000 train_loss: 18.487897872924805 test_loss:89.55607604980469\n",
            "2547/3000 train_loss: 20.8906192779541 test_loss:94.94937896728516\n",
            "2548/3000 train_loss: 23.564617156982422 test_loss:91.9960708618164\n",
            "2549/3000 train_loss: 24.104938507080078 test_loss:91.39068603515625\n",
            "2550/3000 train_loss: 24.235000610351562 test_loss:97.64867401123047\n",
            "2551/3000 train_loss: 26.439931869506836 test_loss:96.22449493408203\n",
            "2552/3000 train_loss: 25.54254913330078 test_loss:91.92965698242188\n",
            "2553/3000 train_loss: 20.128183364868164 test_loss:92.11809539794922\n",
            "2554/3000 train_loss: 24.365463256835938 test_loss:94.70255279541016\n",
            "2555/3000 train_loss: 29.08066177368164 test_loss:96.05302429199219\n",
            "2556/3000 train_loss: 22.121307373046875 test_loss:91.92269134521484\n",
            "2557/3000 train_loss: 24.996990203857422 test_loss:89.01599884033203\n",
            "2558/3000 train_loss: 22.12132453918457 test_loss:91.17837524414062\n",
            "2559/3000 train_loss: 21.589859008789062 test_loss:98.33802795410156\n",
            "2560/3000 train_loss: 21.843095779418945 test_loss:90.16899108886719\n",
            "2561/3000 train_loss: 27.348058700561523 test_loss:91.43306732177734\n",
            "2562/3000 train_loss: 22.67346954345703 test_loss:95.01618957519531\n",
            "2563/3000 train_loss: 22.208370208740234 test_loss:91.36051940917969\n",
            "2564/3000 train_loss: 22.384695053100586 test_loss:93.27037048339844\n",
            "2565/3000 train_loss: 26.274681091308594 test_loss:97.28319549560547\n",
            "2566/3000 train_loss: 25.07094955444336 test_loss:90.28128051757812\n",
            "2567/3000 train_loss: 22.2882137298584 test_loss:88.63216400146484\n",
            "2568/3000 train_loss: 26.590646743774414 test_loss:93.34578704833984\n",
            "2569/3000 train_loss: 18.669551849365234 test_loss:91.66504669189453\n",
            "2570/3000 train_loss: 21.280101776123047 test_loss:100.24115753173828\n",
            "2571/3000 train_loss: 21.932804107666016 test_loss:86.92601776123047\n",
            "2572/3000 train_loss: 24.650888442993164 test_loss:96.50858306884766\n",
            "2573/3000 train_loss: 24.192155838012695 test_loss:95.19440460205078\n",
            "2574/3000 train_loss: 22.064729690551758 test_loss:94.35171508789062\n",
            "2575/3000 train_loss: 19.383153915405273 test_loss:95.19921875\n",
            "2576/3000 train_loss: 19.074419021606445 test_loss:97.15933990478516\n",
            "2577/3000 train_loss: 22.85053825378418 test_loss:90.16212463378906\n",
            "2578/3000 train_loss: 17.447053909301758 test_loss:88.5455322265625\n",
            "2579/3000 train_loss: 19.81476402282715 test_loss:91.4056396484375\n",
            "2580/3000 train_loss: 27.094938278198242 test_loss:91.97836303710938\n",
            "2581/3000 train_loss: 21.545249938964844 test_loss:89.64281463623047\n",
            "2582/3000 train_loss: 24.104286193847656 test_loss:96.69673919677734\n",
            "2583/3000 train_loss: 28.20625877380371 test_loss:89.12818908691406\n",
            "2584/3000 train_loss: 21.750009536743164 test_loss:96.6176986694336\n",
            "2585/3000 train_loss: 20.35433578491211 test_loss:94.23453521728516\n",
            "2586/3000 train_loss: 21.867538452148438 test_loss:87.09981536865234\n",
            "2587/3000 train_loss: 22.46988868713379 test_loss:97.75303649902344\n",
            "2588/3000 train_loss: 24.924924850463867 test_loss:89.6676254272461\n",
            "2589/3000 train_loss: 24.306068420410156 test_loss:100.45263671875\n",
            "2590/3000 train_loss: 28.209007263183594 test_loss:90.5584716796875\n",
            "2591/3000 train_loss: 19.678823471069336 test_loss:92.62797546386719\n",
            "2592/3000 train_loss: 20.86676025390625 test_loss:89.13329315185547\n",
            "2593/3000 train_loss: 25.697038650512695 test_loss:91.29573822021484\n",
            "2594/3000 train_loss: 20.985803604125977 test_loss:91.76526641845703\n",
            "2595/3000 train_loss: 21.089786529541016 test_loss:93.81497192382812\n",
            "2596/3000 train_loss: 21.513973236083984 test_loss:94.46710968017578\n",
            "2597/3000 train_loss: 20.00971031188965 test_loss:97.10272216796875\n",
            "2598/3000 train_loss: 26.883953094482422 test_loss:88.39263916015625\n",
            "2599/3000 train_loss: 21.283660888671875 test_loss:94.35222625732422\n",
            "2600/3000 train_loss: 22.45224952697754 test_loss:92.32999420166016\n",
            "2601/3000 train_loss: 25.71207046508789 test_loss:97.4280014038086\n",
            "2602/3000 train_loss: 33.10865020751953 test_loss:93.5961685180664\n",
            "2603/3000 train_loss: 23.1263427734375 test_loss:84.50712585449219\n",
            "2604/3000 train_loss: 24.32071304321289 test_loss:94.16632080078125\n",
            "2605/3000 train_loss: 24.36330223083496 test_loss:95.51829528808594\n",
            "2606/3000 train_loss: 27.741065979003906 test_loss:96.6460952758789\n",
            "2607/3000 train_loss: 24.943937301635742 test_loss:98.1904067993164\n",
            "2608/3000 train_loss: 22.774389266967773 test_loss:94.5893325805664\n",
            "2609/3000 train_loss: 24.826723098754883 test_loss:93.60734558105469\n",
            "2610/3000 train_loss: 20.606061935424805 test_loss:98.56654357910156\n",
            "2611/3000 train_loss: 24.353740692138672 test_loss:95.4403305053711\n",
            "2612/3000 train_loss: 21.82257652282715 test_loss:91.14655303955078\n",
            "2613/3000 train_loss: 18.393455505371094 test_loss:90.05375671386719\n",
            "2614/3000 train_loss: 21.662397384643555 test_loss:97.31902313232422\n",
            "2615/3000 train_loss: 23.9416561126709 test_loss:89.06620025634766\n",
            "2616/3000 train_loss: 21.58500862121582 test_loss:94.20643615722656\n",
            "2617/3000 train_loss: 22.492450714111328 test_loss:92.067138671875\n",
            "2618/3000 train_loss: 23.38265609741211 test_loss:93.85723876953125\n",
            "2619/3000 train_loss: 21.9088191986084 test_loss:97.15483856201172\n",
            "2620/3000 train_loss: 24.458269119262695 test_loss:91.91364288330078\n",
            "2621/3000 train_loss: 20.536300659179688 test_loss:94.17496490478516\n",
            "2622/3000 train_loss: 24.299476623535156 test_loss:91.05519104003906\n",
            "2623/3000 train_loss: 24.84218978881836 test_loss:92.1395034790039\n",
            "2624/3000 train_loss: 22.302173614501953 test_loss:94.52716827392578\n",
            "2625/3000 train_loss: 23.708650588989258 test_loss:86.93496704101562\n",
            "2626/3000 train_loss: 20.406003952026367 test_loss:92.84675598144531\n",
            "2627/3000 train_loss: 23.985933303833008 test_loss:93.39395904541016\n",
            "2628/3000 train_loss: 22.683452606201172 test_loss:91.33924102783203\n",
            "2629/3000 train_loss: 25.46597671508789 test_loss:95.52698516845703\n",
            "2630/3000 train_loss: 24.18169593811035 test_loss:98.98213958740234\n",
            "2631/3000 train_loss: 18.450538635253906 test_loss:92.33926391601562\n",
            "2632/3000 train_loss: 32.5262336730957 test_loss:95.29124450683594\n",
            "2633/3000 train_loss: 28.951187133789062 test_loss:101.22962951660156\n",
            "2634/3000 train_loss: 24.30120086669922 test_loss:99.34394836425781\n",
            "2635/3000 train_loss: 21.4808349609375 test_loss:92.75861358642578\n",
            "2636/3000 train_loss: 18.318143844604492 test_loss:91.146484375\n",
            "2637/3000 train_loss: 22.757511138916016 test_loss:91.734130859375\n",
            "2638/3000 train_loss: 23.985023498535156 test_loss:90.47044372558594\n",
            "2639/3000 train_loss: 22.884822845458984 test_loss:91.5496826171875\n",
            "2640/3000 train_loss: 20.904502868652344 test_loss:92.06361389160156\n",
            "2641/3000 train_loss: 21.58601951599121 test_loss:88.62301635742188\n",
            "2642/3000 train_loss: 23.117473602294922 test_loss:97.08385467529297\n",
            "2643/3000 train_loss: 25.337921142578125 test_loss:89.64673614501953\n",
            "2644/3000 train_loss: 21.719762802124023 test_loss:92.2540512084961\n",
            "2645/3000 train_loss: 23.28998565673828 test_loss:90.13863372802734\n",
            "2646/3000 train_loss: 24.276081085205078 test_loss:92.35370635986328\n",
            "2647/3000 train_loss: 24.552343368530273 test_loss:95.23807525634766\n",
            "2648/3000 train_loss: 21.609487533569336 test_loss:87.41368103027344\n",
            "2649/3000 train_loss: 23.42033576965332 test_loss:94.3639144897461\n",
            "2650/3000 train_loss: 18.276506423950195 test_loss:94.45368957519531\n",
            "2651/3000 train_loss: 26.648189544677734 test_loss:93.0947036743164\n",
            "2652/3000 train_loss: 22.0166072845459 test_loss:98.00570678710938\n",
            "2653/3000 train_loss: 19.177745819091797 test_loss:95.74922180175781\n",
            "2654/3000 train_loss: 22.961931228637695 test_loss:92.104736328125\n",
            "2655/3000 train_loss: 21.96788215637207 test_loss:100.33594512939453\n",
            "2656/3000 train_loss: 21.614500045776367 test_loss:90.22444152832031\n",
            "2657/3000 train_loss: 22.744760513305664 test_loss:94.02580261230469\n",
            "2658/3000 train_loss: 20.879369735717773 test_loss:92.74115753173828\n",
            "2659/3000 train_loss: 24.65536880493164 test_loss:94.3335952758789\n",
            "2660/3000 train_loss: 22.40843963623047 test_loss:94.14173889160156\n",
            "2661/3000 train_loss: 20.43480110168457 test_loss:93.87378692626953\n",
            "2662/3000 train_loss: 22.33816909790039 test_loss:91.52481079101562\n",
            "2663/3000 train_loss: 20.616859436035156 test_loss:89.88665008544922\n",
            "2664/3000 train_loss: 24.239219665527344 test_loss:93.1910629272461\n",
            "2665/3000 train_loss: 26.086345672607422 test_loss:92.14508056640625\n",
            "2666/3000 train_loss: 27.713855743408203 test_loss:86.46942138671875\n",
            "2667/3000 train_loss: 22.80034637451172 test_loss:98.27684020996094\n",
            "2668/3000 train_loss: 21.963518142700195 test_loss:86.22806549072266\n",
            "2669/3000 train_loss: 24.455480575561523 test_loss:89.26820373535156\n",
            "2670/3000 train_loss: 24.272178649902344 test_loss:89.69229888916016\n",
            "2671/3000 train_loss: 20.669185638427734 test_loss:92.50576782226562\n",
            "2672/3000 train_loss: 21.399471282958984 test_loss:89.45264434814453\n",
            "2673/3000 train_loss: 18.055267333984375 test_loss:94.08832550048828\n",
            "2674/3000 train_loss: 27.338157653808594 test_loss:98.70504760742188\n",
            "2675/3000 train_loss: 23.767181396484375 test_loss:90.27404022216797\n",
            "2676/3000 train_loss: 25.86309242248535 test_loss:87.78276062011719\n",
            "2677/3000 train_loss: 22.51983642578125 test_loss:94.38005828857422\n",
            "2678/3000 train_loss: 23.09357261657715 test_loss:87.27225494384766\n",
            "2679/3000 train_loss: 19.75253677368164 test_loss:93.84347534179688\n",
            "2680/3000 train_loss: 19.614076614379883 test_loss:94.34864807128906\n",
            "2681/3000 train_loss: 22.293535232543945 test_loss:95.41267395019531\n",
            "2682/3000 train_loss: 21.513566970825195 test_loss:91.82437896728516\n",
            "2683/3000 train_loss: 21.595537185668945 test_loss:92.08140563964844\n",
            "2684/3000 train_loss: 19.179506301879883 test_loss:91.23546600341797\n",
            "2685/3000 train_loss: 18.84812355041504 test_loss:90.39846801757812\n",
            "2686/3000 train_loss: 18.366308212280273 test_loss:93.39019775390625\n",
            "2687/3000 train_loss: 21.17531967163086 test_loss:94.1676025390625\n",
            "2688/3000 train_loss: 18.985069274902344 test_loss:91.19515991210938\n",
            "2689/3000 train_loss: 20.2327880859375 test_loss:95.27588653564453\n",
            "2690/3000 train_loss: 22.199451446533203 test_loss:87.93968963623047\n",
            "2691/3000 train_loss: 22.86781120300293 test_loss:97.16659545898438\n",
            "2692/3000 train_loss: 20.652246475219727 test_loss:90.02056884765625\n",
            "2693/3000 train_loss: 19.471698760986328 test_loss:91.8214340209961\n",
            "2694/3000 train_loss: 18.603010177612305 test_loss:91.6519546508789\n",
            "2695/3000 train_loss: 24.684675216674805 test_loss:95.81087493896484\n",
            "2696/3000 train_loss: 21.916223526000977 test_loss:93.14411163330078\n",
            "2697/3000 train_loss: 20.28118133544922 test_loss:93.7220230102539\n",
            "2698/3000 train_loss: 25.058855056762695 test_loss:92.46306610107422\n",
            "2699/3000 train_loss: 21.73110580444336 test_loss:91.24518585205078\n",
            "2700/3000 train_loss: 24.80479621887207 test_loss:98.1263656616211\n",
            "2701/3000 train_loss: 22.111907958984375 test_loss:93.28746795654297\n",
            "2702/3000 train_loss: 23.90671730041504 test_loss:93.4852294921875\n",
            "2703/3000 train_loss: 22.301292419433594 test_loss:91.32823181152344\n",
            "2704/3000 train_loss: 22.446338653564453 test_loss:91.84490203857422\n",
            "2705/3000 train_loss: 25.938892364501953 test_loss:91.0816650390625\n",
            "2706/3000 train_loss: 21.799320220947266 test_loss:93.627197265625\n",
            "2707/3000 train_loss: 18.568098068237305 test_loss:91.52776336669922\n",
            "2708/3000 train_loss: 22.75107192993164 test_loss:91.20389556884766\n",
            "2709/3000 train_loss: 25.52206039428711 test_loss:94.9143295288086\n",
            "2710/3000 train_loss: 29.741165161132812 test_loss:87.14387512207031\n",
            "2711/3000 train_loss: 19.971982955932617 test_loss:93.18221282958984\n",
            "2712/3000 train_loss: 20.72468376159668 test_loss:89.8232192993164\n",
            "2713/3000 train_loss: 22.128332138061523 test_loss:91.48348999023438\n",
            "2714/3000 train_loss: 18.5848331451416 test_loss:90.31713104248047\n",
            "2715/3000 train_loss: 23.176597595214844 test_loss:88.74911499023438\n",
            "2716/3000 train_loss: 21.436975479125977 test_loss:94.79325103759766\n",
            "2717/3000 train_loss: 21.041667938232422 test_loss:92.0810775756836\n",
            "2718/3000 train_loss: 21.577980041503906 test_loss:93.3345947265625\n",
            "2719/3000 train_loss: 22.20248031616211 test_loss:92.78609466552734\n",
            "2720/3000 train_loss: 23.190710067749023 test_loss:93.11408996582031\n",
            "2721/3000 train_loss: 19.9560604095459 test_loss:92.54520416259766\n",
            "2722/3000 train_loss: 22.568281173706055 test_loss:95.74675750732422\n",
            "2723/3000 train_loss: 19.142587661743164 test_loss:94.29593658447266\n",
            "2724/3000 train_loss: 20.65380096435547 test_loss:92.59566497802734\n",
            "2725/3000 train_loss: 24.14019012451172 test_loss:89.33696746826172\n",
            "2726/3000 train_loss: 23.51354217529297 test_loss:88.98497772216797\n",
            "2727/3000 train_loss: 20.49599838256836 test_loss:91.08032989501953\n",
            "2728/3000 train_loss: 23.687808990478516 test_loss:96.45590209960938\n",
            "2729/3000 train_loss: 24.396188735961914 test_loss:86.26518249511719\n",
            "2730/3000 train_loss: 24.259525299072266 test_loss:92.43984985351562\n",
            "2731/3000 train_loss: 19.6923770904541 test_loss:92.3542251586914\n",
            "2732/3000 train_loss: 24.420639038085938 test_loss:88.88917541503906\n",
            "2733/3000 train_loss: 20.062450408935547 test_loss:90.72927856445312\n",
            "2734/3000 train_loss: 21.486080169677734 test_loss:89.55780792236328\n",
            "2735/3000 train_loss: 22.730775833129883 test_loss:93.28070831298828\n",
            "2736/3000 train_loss: 21.53740692138672 test_loss:99.92391967773438\n",
            "2737/3000 train_loss: 22.65414047241211 test_loss:91.59321594238281\n",
            "2738/3000 train_loss: 18.806564331054688 test_loss:94.6305923461914\n",
            "2739/3000 train_loss: 27.407106399536133 test_loss:100.73098754882812\n",
            "2740/3000 train_loss: 27.270021438598633 test_loss:86.869384765625\n",
            "2741/3000 train_loss: 26.096088409423828 test_loss:87.540283203125\n",
            "2742/3000 train_loss: 27.127216339111328 test_loss:96.61941528320312\n",
            "2743/3000 train_loss: 19.99757194519043 test_loss:89.79737854003906\n",
            "2744/3000 train_loss: 24.492666244506836 test_loss:95.31075286865234\n",
            "2745/3000 train_loss: 22.5946102142334 test_loss:92.31537628173828\n",
            "2746/3000 train_loss: 22.22694206237793 test_loss:91.3467788696289\n",
            "2747/3000 train_loss: 18.94568634033203 test_loss:92.618896484375\n",
            "2748/3000 train_loss: 21.45233917236328 test_loss:101.97681427001953\n",
            "2749/3000 train_loss: 21.558815002441406 test_loss:88.2203140258789\n",
            "2750/3000 train_loss: 22.878602981567383 test_loss:90.05439758300781\n",
            "2751/3000 train_loss: 21.209186553955078 test_loss:98.7745361328125\n",
            "2752/3000 train_loss: 19.61431312561035 test_loss:91.58541107177734\n",
            "2753/3000 train_loss: 21.589126586914062 test_loss:97.9841537475586\n",
            "2754/3000 train_loss: 20.832489013671875 test_loss:90.61222839355469\n",
            "2755/3000 train_loss: 22.839031219482422 test_loss:90.25759887695312\n",
            "2756/3000 train_loss: 19.601070404052734 test_loss:96.40122985839844\n",
            "2757/3000 train_loss: 24.731184005737305 test_loss:93.55233764648438\n",
            "2758/3000 train_loss: 21.31385612487793 test_loss:95.74191284179688\n",
            "2759/3000 train_loss: 23.878297805786133 test_loss:91.93436431884766\n",
            "2760/3000 train_loss: 24.35493278503418 test_loss:85.80363464355469\n",
            "2761/3000 train_loss: 23.354406356811523 test_loss:93.0625\n",
            "2762/3000 train_loss: 21.450214385986328 test_loss:100.80184173583984\n",
            "2763/3000 train_loss: 23.37717628479004 test_loss:95.4993896484375\n",
            "2764/3000 train_loss: 26.155202865600586 test_loss:96.3912582397461\n",
            "2765/3000 train_loss: 16.662900924682617 test_loss:87.28397369384766\n",
            "2766/3000 train_loss: 20.748123168945312 test_loss:92.44889068603516\n",
            "2767/3000 train_loss: 20.52586555480957 test_loss:89.89537048339844\n",
            "2768/3000 train_loss: 22.506086349487305 test_loss:91.45880126953125\n",
            "2769/3000 train_loss: 20.066650390625 test_loss:92.89144134521484\n",
            "2770/3000 train_loss: 20.30583381652832 test_loss:91.8922348022461\n",
            "2771/3000 train_loss: 20.028337478637695 test_loss:89.92310333251953\n",
            "2772/3000 train_loss: 30.302658081054688 test_loss:94.35506439208984\n",
            "2773/3000 train_loss: 25.71067237854004 test_loss:96.18109130859375\n",
            "2774/3000 train_loss: 21.902027130126953 test_loss:93.88427734375\n",
            "2775/3000 train_loss: 22.8972225189209 test_loss:89.69645690917969\n",
            "2776/3000 train_loss: 25.521772384643555 test_loss:95.3122329711914\n",
            "2777/3000 train_loss: 22.302066802978516 test_loss:94.20177459716797\n",
            "2778/3000 train_loss: 19.448837280273438 test_loss:87.15882110595703\n",
            "2779/3000 train_loss: 21.630090713500977 test_loss:101.72640991210938\n",
            "2780/3000 train_loss: 20.968454360961914 test_loss:88.66363525390625\n",
            "2781/3000 train_loss: 20.970237731933594 test_loss:97.75899505615234\n",
            "2782/3000 train_loss: 24.306652069091797 test_loss:87.72288513183594\n",
            "2783/3000 train_loss: 29.792137145996094 test_loss:100.33050537109375\n",
            "2784/3000 train_loss: 25.61803436279297 test_loss:93.76114654541016\n",
            "2785/3000 train_loss: 21.84695816040039 test_loss:90.24113464355469\n",
            "2786/3000 train_loss: 22.390947341918945 test_loss:94.73776245117188\n",
            "2787/3000 train_loss: 18.823333740234375 test_loss:91.5934066772461\n",
            "2788/3000 train_loss: 20.533397674560547 test_loss:90.45343017578125\n",
            "2789/3000 train_loss: 21.370994567871094 test_loss:97.20032501220703\n",
            "2790/3000 train_loss: 20.709775924682617 test_loss:88.5206069946289\n",
            "2791/3000 train_loss: 21.324935913085938 test_loss:87.9608383178711\n",
            "2792/3000 train_loss: 19.314151763916016 test_loss:93.5060806274414\n",
            "2793/3000 train_loss: 21.799148559570312 test_loss:91.67179870605469\n",
            "2794/3000 train_loss: 18.204811096191406 test_loss:90.01102447509766\n",
            "2795/3000 train_loss: 21.4655704498291 test_loss:93.50922393798828\n",
            "2796/3000 train_loss: 21.21380043029785 test_loss:88.65716552734375\n",
            "2797/3000 train_loss: 19.620134353637695 test_loss:91.70462036132812\n",
            "2798/3000 train_loss: 28.491764068603516 test_loss:96.1637954711914\n",
            "2799/3000 train_loss: 20.883115768432617 test_loss:92.14031982421875\n",
            "2800/3000 train_loss: 19.885787963867188 test_loss:89.26094055175781\n",
            "2801/3000 train_loss: 19.1385440826416 test_loss:100.21625518798828\n",
            "2802/3000 train_loss: 17.028430938720703 test_loss:92.78630065917969\n",
            "2803/3000 train_loss: 23.388713836669922 test_loss:94.66251373291016\n",
            "2804/3000 train_loss: 22.550703048706055 test_loss:98.91069030761719\n",
            "2805/3000 train_loss: 31.33050537109375 test_loss:90.87203979492188\n",
            "2806/3000 train_loss: 25.411848068237305 test_loss:87.5416488647461\n",
            "2807/3000 train_loss: 24.213029861450195 test_loss:89.37036895751953\n",
            "2808/3000 train_loss: 22.762271881103516 test_loss:91.29598236083984\n",
            "2809/3000 train_loss: 19.838459014892578 test_loss:90.1196060180664\n",
            "2810/3000 train_loss: 19.271278381347656 test_loss:87.11923217773438\n",
            "2811/3000 train_loss: 21.611860275268555 test_loss:85.86035919189453\n",
            "2812/3000 train_loss: 22.51907730102539 test_loss:91.43167877197266\n",
            "2813/3000 train_loss: 18.965055465698242 test_loss:96.08184814453125\n",
            "2814/3000 train_loss: 24.160457611083984 test_loss:93.60377502441406\n",
            "2815/3000 train_loss: 30.970455169677734 test_loss:88.14208984375\n",
            "2816/3000 train_loss: 21.591381072998047 test_loss:97.6259994506836\n",
            "2817/3000 train_loss: 17.210176467895508 test_loss:93.36003112792969\n",
            "2818/3000 train_loss: 20.49825668334961 test_loss:92.77769470214844\n",
            "2819/3000 train_loss: 22.200824737548828 test_loss:89.3496322631836\n",
            "2820/3000 train_loss: 26.193492889404297 test_loss:89.3778305053711\n",
            "2821/3000 train_loss: 24.897350311279297 test_loss:95.25090789794922\n",
            "2822/3000 train_loss: 22.023157119750977 test_loss:98.29316711425781\n",
            "2823/3000 train_loss: 21.864179611206055 test_loss:97.12496185302734\n",
            "2824/3000 train_loss: 24.08694076538086 test_loss:95.60018920898438\n",
            "2825/3000 train_loss: 19.702865600585938 test_loss:93.53555297851562\n",
            "2826/3000 train_loss: 24.61835289001465 test_loss:93.77494049072266\n",
            "2827/3000 train_loss: 19.80516242980957 test_loss:85.82032775878906\n",
            "2828/3000 train_loss: 22.841291427612305 test_loss:90.36599731445312\n",
            "2829/3000 train_loss: 23.143817901611328 test_loss:83.78468322753906\n",
            "2830/3000 train_loss: 25.47377586364746 test_loss:100.7080307006836\n",
            "2831/3000 train_loss: 19.039785385131836 test_loss:90.40159606933594\n",
            "2832/3000 train_loss: 19.87956428527832 test_loss:94.3470230102539\n",
            "2833/3000 train_loss: 22.281455993652344 test_loss:92.50389862060547\n",
            "2834/3000 train_loss: 17.77535057067871 test_loss:89.14334869384766\n",
            "2835/3000 train_loss: 23.52283477783203 test_loss:88.33027648925781\n",
            "2836/3000 train_loss: 19.45754051208496 test_loss:90.21575164794922\n",
            "2837/3000 train_loss: 18.216489791870117 test_loss:92.88270568847656\n",
            "2838/3000 train_loss: 20.4993953704834 test_loss:92.38041687011719\n",
            "2839/3000 train_loss: 21.571510314941406 test_loss:93.19124603271484\n",
            "2840/3000 train_loss: 22.434696197509766 test_loss:93.91422271728516\n",
            "2841/3000 train_loss: 20.459426879882812 test_loss:87.47611236572266\n",
            "2842/3000 train_loss: 20.477724075317383 test_loss:94.98800659179688\n",
            "2843/3000 train_loss: 16.480215072631836 test_loss:92.19091796875\n",
            "2844/3000 train_loss: 22.687969207763672 test_loss:88.5211181640625\n",
            "2845/3000 train_loss: 27.159387588500977 test_loss:94.83529663085938\n",
            "2846/3000 train_loss: 26.447980880737305 test_loss:91.46660614013672\n",
            "2847/3000 train_loss: 23.103261947631836 test_loss:90.1167984008789\n",
            "2848/3000 train_loss: 23.465740203857422 test_loss:88.61804962158203\n",
            "2849/3000 train_loss: 21.254724502563477 test_loss:99.0815200805664\n",
            "2850/3000 train_loss: 25.961374282836914 test_loss:86.29011535644531\n",
            "2851/3000 train_loss: 23.108102798461914 test_loss:99.009033203125\n",
            "2852/3000 train_loss: 23.71693229675293 test_loss:88.08002471923828\n",
            "2853/3000 train_loss: 20.160091400146484 test_loss:90.95734405517578\n",
            "2854/3000 train_loss: 22.461252212524414 test_loss:93.42768096923828\n",
            "2855/3000 train_loss: 24.103628158569336 test_loss:89.73866271972656\n",
            "2856/3000 train_loss: 20.787521362304688 test_loss:92.2584228515625\n",
            "2857/3000 train_loss: 21.171606063842773 test_loss:88.56206512451172\n",
            "2858/3000 train_loss: 29.0098934173584 test_loss:93.92552947998047\n",
            "2859/3000 train_loss: 17.8023738861084 test_loss:94.18064880371094\n",
            "2860/3000 train_loss: 20.60578155517578 test_loss:86.70988464355469\n",
            "2861/3000 train_loss: 21.617679595947266 test_loss:92.81344604492188\n",
            "2862/3000 train_loss: 17.958698272705078 test_loss:88.5249252319336\n",
            "2863/3000 train_loss: 19.556081771850586 test_loss:88.4283218383789\n",
            "2864/3000 train_loss: 21.312376022338867 test_loss:95.8586196899414\n",
            "2865/3000 train_loss: 20.913877487182617 test_loss:90.91959381103516\n",
            "2866/3000 train_loss: 22.659879684448242 test_loss:88.86299896240234\n",
            "2867/3000 train_loss: 20.61886215209961 test_loss:90.25884246826172\n",
            "2868/3000 train_loss: 25.231781005859375 test_loss:95.0053939819336\n",
            "2869/3000 train_loss: 22.38323402404785 test_loss:91.3550033569336\n",
            "2870/3000 train_loss: 21.424488067626953 test_loss:90.06581115722656\n",
            "2871/3000 train_loss: 22.95692253112793 test_loss:98.1064224243164\n",
            "2872/3000 train_loss: 19.99945831298828 test_loss:89.60848999023438\n",
            "2873/3000 train_loss: 21.99530029296875 test_loss:93.6882095336914\n",
            "2874/3000 train_loss: 18.782302856445312 test_loss:88.44607543945312\n",
            "2875/3000 train_loss: 21.21238136291504 test_loss:94.02680206298828\n",
            "2876/3000 train_loss: 21.43126106262207 test_loss:87.52689361572266\n",
            "2877/3000 train_loss: 19.3743953704834 test_loss:94.86173248291016\n",
            "2878/3000 train_loss: 23.63880157470703 test_loss:92.17555236816406\n",
            "2879/3000 train_loss: 25.11933708190918 test_loss:90.45341491699219\n",
            "2880/3000 train_loss: 19.700210571289062 test_loss:89.93641662597656\n",
            "2881/3000 train_loss: 20.730329513549805 test_loss:90.1355972290039\n",
            "2882/3000 train_loss: 19.693138122558594 test_loss:93.52426147460938\n",
            "2883/3000 train_loss: 24.132984161376953 test_loss:85.83586883544922\n",
            "2884/3000 train_loss: 22.76791763305664 test_loss:86.43609619140625\n",
            "2885/3000 train_loss: 22.293643951416016 test_loss:90.72618865966797\n",
            "2886/3000 train_loss: 20.984209060668945 test_loss:90.16136169433594\n",
            "2887/3000 train_loss: 21.18840217590332 test_loss:94.15792846679688\n",
            "2888/3000 train_loss: 23.928373336791992 test_loss:82.28498077392578\n",
            "2889/3000 train_loss: 24.96337890625 test_loss:91.2444076538086\n",
            "2890/3000 train_loss: 19.252702713012695 test_loss:90.42572784423828\n",
            "2891/3000 train_loss: 19.542814254760742 test_loss:92.31971740722656\n",
            "2892/3000 train_loss: 24.182340621948242 test_loss:88.5698471069336\n",
            "2893/3000 train_loss: 23.599403381347656 test_loss:89.97035217285156\n",
            "2894/3000 train_loss: 22.765274047851562 test_loss:90.67323303222656\n",
            "2895/3000 train_loss: 20.53396987915039 test_loss:89.23125457763672\n",
            "2896/3000 train_loss: 22.57435417175293 test_loss:98.71524810791016\n",
            "2897/3000 train_loss: 20.533050537109375 test_loss:85.55724334716797\n",
            "2898/3000 train_loss: 22.904170989990234 test_loss:96.8572998046875\n",
            "2899/3000 train_loss: 21.603548049926758 test_loss:90.00386047363281\n",
            "2900/3000 train_loss: 21.552976608276367 test_loss:85.14044952392578\n",
            "2901/3000 train_loss: 21.695919036865234 test_loss:96.2159652709961\n",
            "2902/3000 train_loss: 20.407033920288086 test_loss:86.69393157958984\n",
            "2903/3000 train_loss: 17.343441009521484 test_loss:89.3830795288086\n",
            "2904/3000 train_loss: 17.955766677856445 test_loss:86.83930206298828\n",
            "2905/3000 train_loss: 17.51906967163086 test_loss:92.86560821533203\n",
            "2906/3000 train_loss: 22.494770050048828 test_loss:86.46824645996094\n",
            "2907/3000 train_loss: 24.318967819213867 test_loss:84.69800567626953\n",
            "2908/3000 train_loss: 21.728591918945312 test_loss:98.16165161132812\n",
            "2909/3000 train_loss: 22.25810432434082 test_loss:89.40460968017578\n",
            "2910/3000 train_loss: 19.213539123535156 test_loss:90.50093078613281\n",
            "2911/3000 train_loss: 22.457599639892578 test_loss:88.36133575439453\n",
            "2912/3000 train_loss: 19.203838348388672 test_loss:90.33696746826172\n",
            "2913/3000 train_loss: 19.165184020996094 test_loss:94.26305389404297\n",
            "2914/3000 train_loss: 21.421646118164062 test_loss:85.88993835449219\n",
            "2915/3000 train_loss: 23.000377655029297 test_loss:90.25713348388672\n",
            "2916/3000 train_loss: 18.57978057861328 test_loss:89.9183578491211\n",
            "2917/3000 train_loss: 21.40034294128418 test_loss:88.21456909179688\n",
            "2918/3000 train_loss: 22.27849578857422 test_loss:91.7216796875\n",
            "2919/3000 train_loss: 19.759620666503906 test_loss:88.81863403320312\n",
            "2920/3000 train_loss: 20.939228057861328 test_loss:87.67835998535156\n",
            "2921/3000 train_loss: 19.358333587646484 test_loss:91.32109832763672\n",
            "2922/3000 train_loss: 23.083250045776367 test_loss:84.03227996826172\n",
            "2923/3000 train_loss: 20.54473304748535 test_loss:98.69164276123047\n",
            "2924/3000 train_loss: 18.64427947998047 test_loss:89.7327880859375\n",
            "2925/3000 train_loss: 19.225772857666016 test_loss:94.09772491455078\n",
            "2926/3000 train_loss: 21.56655502319336 test_loss:86.82803344726562\n",
            "2927/3000 train_loss: 18.85888671875 test_loss:88.46404266357422\n",
            "2928/3000 train_loss: 20.44721221923828 test_loss:89.24043273925781\n",
            "2929/3000 train_loss: 25.187183380126953 test_loss:89.29134368896484\n",
            "2930/3000 train_loss: 23.610366821289062 test_loss:82.50711059570312\n",
            "2931/3000 train_loss: 20.31789207458496 test_loss:89.99240112304688\n",
            "2932/3000 train_loss: 19.852779388427734 test_loss:87.09736633300781\n",
            "2933/3000 train_loss: 19.88608741760254 test_loss:89.99214935302734\n",
            "2934/3000 train_loss: 19.397592544555664 test_loss:94.1497573852539\n",
            "2935/3000 train_loss: 24.678874969482422 test_loss:87.07049560546875\n",
            "2936/3000 train_loss: 16.92978858947754 test_loss:87.98037719726562\n",
            "2937/3000 train_loss: 25.057035446166992 test_loss:91.00296783447266\n",
            "2938/3000 train_loss: 22.280092239379883 test_loss:90.42801666259766\n",
            "2939/3000 train_loss: 20.160554885864258 test_loss:92.24537658691406\n",
            "2940/3000 train_loss: 21.62682342529297 test_loss:95.80196380615234\n",
            "2941/3000 train_loss: 19.02851104736328 test_loss:88.2240219116211\n",
            "2942/3000 train_loss: 21.45868682861328 test_loss:91.14810180664062\n",
            "2943/3000 train_loss: 20.542903900146484 test_loss:94.71549987792969\n",
            "2944/3000 train_loss: 23.968111038208008 test_loss:89.44286346435547\n",
            "2945/3000 train_loss: 20.464927673339844 test_loss:97.59416961669922\n",
            "2946/3000 train_loss: 21.93303680419922 test_loss:90.36878204345703\n",
            "2947/3000 train_loss: 23.88764190673828 test_loss:92.23719024658203\n",
            "2948/3000 train_loss: 22.982940673828125 test_loss:92.98502349853516\n",
            "2949/3000 train_loss: 18.333999633789062 test_loss:84.90033721923828\n",
            "2950/3000 train_loss: 20.0328311920166 test_loss:87.48095703125\n",
            "2951/3000 train_loss: 22.143047332763672 test_loss:95.53339385986328\n",
            "2952/3000 train_loss: 19.729515075683594 test_loss:90.76132202148438\n",
            "2953/3000 train_loss: 21.404653549194336 test_loss:91.01231384277344\n",
            "2954/3000 train_loss: 23.144672393798828 test_loss:92.17292785644531\n",
            "2955/3000 train_loss: 22.527713775634766 test_loss:88.3638687133789\n",
            "2956/3000 train_loss: 17.665103912353516 test_loss:92.08457946777344\n",
            "2957/3000 train_loss: 21.454421997070312 test_loss:87.16468811035156\n",
            "2958/3000 train_loss: 17.95338249206543 test_loss:94.26117706298828\n",
            "2959/3000 train_loss: 21.42717933654785 test_loss:88.38838958740234\n",
            "2960/3000 train_loss: 20.22722816467285 test_loss:86.71916961669922\n",
            "2961/3000 train_loss: 24.054285049438477 test_loss:88.99088287353516\n",
            "2962/3000 train_loss: 23.438283920288086 test_loss:95.13652801513672\n",
            "2963/3000 train_loss: 19.160860061645508 test_loss:89.15419006347656\n",
            "2964/3000 train_loss: 28.60470199584961 test_loss:98.49665832519531\n",
            "2965/3000 train_loss: 26.361221313476562 test_loss:99.35404968261719\n",
            "2966/3000 train_loss: 24.374753952026367 test_loss:89.46277618408203\n",
            "2967/3000 train_loss: 22.451732635498047 test_loss:93.55738830566406\n",
            "2968/3000 train_loss: 22.077863693237305 test_loss:91.26197814941406\n",
            "2969/3000 train_loss: 20.971559524536133 test_loss:92.77593231201172\n",
            "2970/3000 train_loss: 25.211631774902344 test_loss:96.16018676757812\n",
            "2971/3000 train_loss: 20.760055541992188 test_loss:89.37687683105469\n",
            "2972/3000 train_loss: 23.880006790161133 test_loss:87.5731201171875\n",
            "2973/3000 train_loss: 21.53114128112793 test_loss:87.7314453125\n",
            "2974/3000 train_loss: 19.197587966918945 test_loss:94.30553436279297\n",
            "2975/3000 train_loss: 22.4229736328125 test_loss:92.1281967163086\n",
            "2976/3000 train_loss: 19.36309814453125 test_loss:90.2994613647461\n",
            "2977/3000 train_loss: 20.617021560668945 test_loss:92.72509002685547\n",
            "2978/3000 train_loss: 17.835647583007812 test_loss:88.39934539794922\n",
            "2979/3000 train_loss: 27.83303451538086 test_loss:95.6255111694336\n",
            "2980/3000 train_loss: 22.37779998779297 test_loss:94.85796356201172\n",
            "2981/3000 train_loss: 22.57517433166504 test_loss:90.39119720458984\n",
            "2982/3000 train_loss: 22.056446075439453 test_loss:92.04194641113281\n",
            "2983/3000 train_loss: 23.148868560791016 test_loss:89.88490295410156\n",
            "2984/3000 train_loss: 22.07466697692871 test_loss:93.2433853149414\n",
            "2985/3000 train_loss: 19.451839447021484 test_loss:91.66588592529297\n",
            "2986/3000 train_loss: 19.550823211669922 test_loss:89.237060546875\n",
            "2987/3000 train_loss: 21.103679656982422 test_loss:96.55107116699219\n",
            "2988/3000 train_loss: 19.908193588256836 test_loss:87.93484497070312\n",
            "2989/3000 train_loss: 16.861112594604492 test_loss:88.87073516845703\n",
            "2990/3000 train_loss: 24.935171127319336 test_loss:93.39659118652344\n",
            "2991/3000 train_loss: 21.840051651000977 test_loss:88.34899139404297\n",
            "2992/3000 train_loss: 22.695955276489258 test_loss:87.16004943847656\n",
            "2993/3000 train_loss: 20.90416145324707 test_loss:92.58676147460938\n",
            "2994/3000 train_loss: 17.048105239868164 test_loss:87.87406921386719\n",
            "2995/3000 train_loss: 22.30185890197754 test_loss:91.13495635986328\n",
            "2996/3000 train_loss: 17.782522201538086 test_loss:94.8275146484375\n",
            "2997/3000 train_loss: 19.699892044067383 test_loss:93.05436706542969\n",
            "2998/3000 train_loss: 21.25349235534668 test_loss:89.00201416015625\n",
            "2999/3000 train_loss: 22.789779663085938 test_loss:93.01313781738281\n",
            "3000/3000 train_loss: 20.2514705657959 test_loss:87.97216033935547\n"
          ]
        }
      ],
      "source": [
        "losses = train(model = unet, optimizer = optimizer, criterion=criterion, data_tr=train_data,\n",
        "               data_val = test_data, scheduler = scheduler,device = device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "6Ew7_F0-q7aL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f554a42-f5f4-46fc-a66b-33f42ba320a1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(87.9722)"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ],
      "source": [
        "avg_loss = 0\n",
        "# unet.eval()\n",
        "# unet.train()\n",
        "preds = []\n",
        "i = 0\n",
        "test_anomaly_losses = []\n",
        "test_normal_losses = []\n",
        "test_losses = []\n",
        "test_real = y_test.tolist()\n",
        "for batch in test_data:\n",
        "  with torch.no_grad():\n",
        "    # unet.train()\n",
        "    predictions = unet(batch.to(device)).cpu()\n",
        "    preds.append(predictions)\n",
        "  loss = criterion(predictions, batch.cpu())\n",
        "  for j in range(len(predictions)):\n",
        "    if int(y_test[i]) == 1:\n",
        "      test_anomaly_losses.append(float(criterion(predictions[j], batch[j])))\n",
        "    else:\n",
        "      test_normal_losses.append(float(criterion(predictions[j], batch[j])))\n",
        "    i += 1\n",
        "    test_losses.append(criterion(predictions[j], batch[j]))\n",
        "  # print(loss)\n",
        "  # print(loss)\n",
        "  avg_loss += loss / len(test_data)\n",
        "# avg_loss\n",
        "\n",
        "avg_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "VpDKorrRso9o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9091de8f-64f4-461f-b1ed-36a317060352"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(104.29707790791304, 31.161448097229005)"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ],
      "source": [
        "sum(test_anomaly_losses)/len(test_anomaly_losses) , sum(test_normal_losses)/len(test_normal_losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "cJE0-57Qts3E"
      },
      "outputs": [],
      "source": [
        "# torch.save(unet, \"unet_fan2_2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LEvbZKYuh7J",
        "outputId": "b8e9aff2-c763-43a4-b52d-92d8d09818b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9310919540229885\n"
          ]
        }
      ],
      "source": [
        "results = []\n",
        "vals = np.arange(30, 130, 0.1).tolist()\n",
        "for threshold in vals:\n",
        "  preds = []\n",
        "  for j in range(len(test_losses)):\n",
        "    if test_losses[j] > threshold:\n",
        "      preds.append(1)\n",
        "    else:\n",
        "      preds.append(0)\n",
        "  \n",
        "  results.append(roc_auc_score(test_real,preds))\n",
        "\n",
        "print(max(results))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "W4H4vpFX35yK"
      },
      "outputs": [],
      "source": [
        "def get_logmelspectrogram(waveform):\n",
        "    melspec = librosa.feature.melspectrogram(y=waveform.numpy(), hop_length=250, n_mels = 304)\n",
        "\n",
        "    logmelspec = librosa.power_to_db(melspec)\n",
        "\n",
        "    return logmelspec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vo1-S_qcuUZR"
      },
      "outputs": [],
      "source": [
        "# train_logmelspecs, test_logmelspecs = mean_logmelspecs(df_train), mean_logmelspecs(df_test)\n",
        "train_data1 = []\n",
        "for wave in df_train:\n",
        "  train_data1.append(get_logmelspectrogram(wave)[0])\n",
        "\n",
        "test_data1 = []\n",
        "for wave in df_test:\n",
        "  test_data1.append(get_logmelspectrogram(wave)[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGd9oI5IEVMx",
        "outputId": "73a79d4e-9e50-4b29-e7ea-55ee60a08c22"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-30-68ec04120629>:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  train_data1 = torch.FloatTensor(train_data1)\n"
          ]
        }
      ],
      "source": [
        "train_data1 = torch.FloatTensor(train_data1)\n",
        "test_data1 = torch.FloatTensor(test_data1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMOi9331OVb4"
      },
      "outputs": [],
      "source": [
        "train_logs = DataLoader(train_data1.reshape(916*304,641),batch_size = 304)\n",
        "test_logs = DataLoader(test_data1.reshape(459*304,641),batch_size = 304)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9rD6tuI1rfe"
      },
      "outputs": [],
      "source": [
        "unet1 = UNet_FC(in_features=641).to(device)\n",
        "optimizer1 = Adam(params = unet1.parameters(), lr = 10e-3)\n",
        "# optimizer = Adam(params = unet.parameters())\n",
        "criterion1 = nn.MSELoss()\n",
        "scheduler1 = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer1, factor=0.5, \n",
        "                                                       min_lr=10e-4, mode = 'min',\n",
        "                                                       patience = 30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qr6k85ma3ftD",
        "outputId": "67174f45-79f9-41f1-8959-a0c75a2b17fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 train_loss: 21.703369140625 test_loss:17.446470260620117\n"
          ]
        }
      ],
      "source": [
        "losses = train(model = unet1, optimizer = optimizer1, criterion=criterion1, data_tr=train_logs,\n",
        "               data_val = test_logs, device = device, epochs = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrPbpKgSPx7v",
        "outputId": "a7bd10e8-e2bd-4579-8211-d6eaaa879711"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(17.4465)"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "avg_loss = 0\n",
        "# unet.eval()\n",
        "# unet.train()\n",
        "preds = []\n",
        "i = 0\n",
        "test_anomaly_losses = []\n",
        "test_normal_losses = []\n",
        "test_losses = []\n",
        "test_real = y_test.tolist()\n",
        "for batch in test_logs:\n",
        "  with torch.no_grad():\n",
        "    # unet.train()\n",
        "    predictions = unet1(batch.to(device)).cpu()\n",
        "    preds.append(predictions)\n",
        "    loss = criterion(predictions, batch.cpu())\n",
        "    test_losses.append(loss)\n",
        "    if int(y_test[i]) == 1:\n",
        "      test_anomaly_losses.append(loss)\n",
        "    else:\n",
        "      test_normal_losses.append(loss)\n",
        "    i += 1\n",
        "  # print(loss)\n",
        "  # print(loss)\n",
        "  avg_loss += loss / len(test_logs)\n",
        "# avg_loss\n",
        "\n",
        "avg_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Z5Z1XYFN_x2",
        "outputId": "9d767817-5525-443f-db72-2588edb4cfbc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor(18.0466), tensor(15.2920))"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sum(test_anomaly_losses)/len(test_anomaly_losses) , sum(test_normal_losses)/len(test_normal_losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "er74WfG7P_B1",
        "outputId": "884ef8f3-8bfe-4d71-bc01-534f5d5a0a03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.5875626740947075\n"
          ]
        }
      ],
      "source": [
        "results = []\n",
        "vals = np.arange(10, 21, 0.1).tolist()\n",
        "for threshold in vals:\n",
        "  preds = []\n",
        "  for j in range(len(test_losses)):\n",
        "    if test_losses[j] > threshold:\n",
        "      preds.append(1)\n",
        "    else:\n",
        "      preds.append(0)\n",
        "  \n",
        "  results.append(roc_auc_score(test_real,preds))\n",
        "\n",
        "print(max(results))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QaSSqG8SbAw2"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}