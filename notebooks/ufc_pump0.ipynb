{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "9SStKf4G0V5H"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "import torchaudio\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from skimage.util import img_as_ubyte\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import io\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_XtxbKLZq5KX",
    "outputId": "37023990-2b40-4e1f-e63e-1dba18c3acdb"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CYxHegIM0Z4i",
    "outputId": "f68f6fab-6d0f-4c1f-9d0a-527e166123f6"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive/\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "h9DATQwS0ivD"
   },
   "outputs": [],
   "source": [
    "class MimiiDataset(Dataset):\n",
    "    def __init__(self,audio_dir, n_fft = 1024, win_length = 1024,\n",
    "                 hop_length = 512,power = 2,n_mels = 128,pad_mode = 'reflect',\n",
    "                 sr = 16000,center = True,norm = None):\n",
    "      \n",
    "        super(MimiiDataset, self).__init__()\n",
    "        self.audio_dir = audio_dir\n",
    "        self.n_mels = n_mels\n",
    "        self.n_fft = n_fft\n",
    "        self.win_length = win_length\n",
    "        self.hop_length = hop_length\n",
    "        self.power = power\n",
    "        self.pad_mode = pad_mode\n",
    "        self.sr = sr\n",
    "        self.center = center\n",
    "        self.norm = norm\n",
    "\n",
    "    def get_files(self):\n",
    "       return self.train_files, self.test_files\n",
    "    \n",
    "    def get_data(self,device, id):\n",
    "        \n",
    "        self.train_files, self.train_labels = self._train_file_list(device, id)\n",
    "        self.test_files, self.test_labels = self._test_file_list(device, id)\n",
    "        \n",
    "        self.train_data = self.get_audios(self.train_files)\n",
    "        self.test_data = self.get_audios(self.test_files)\n",
    "        \n",
    "        return self.train_data, self.test_data, self.train_labels, self.test_labels\n",
    "    \n",
    "    def _train_file_list(self, device, id):\n",
    "        query = os.path.abspath(\n",
    "            f\"{self.audio_dir}/{device}/train/normal_id_0{id}*.wav\"\n",
    "        )\n",
    "        train_normal_files = sorted(glob.glob(query))\n",
    "        train_normal_labels = np.zeros(len(train_normal_files))\n",
    "        \n",
    "        query = os.path.abspath(\n",
    "                f\"{self.audio_dir}/{device}/train/anomaly_id_0{id}*.wav\"\n",
    "            )\n",
    "        train_anomaly_files = sorted(glob.glob(query))\n",
    "        train_anomaly_labels = np.ones(len(train_anomaly_files))\n",
    "        \n",
    "        train_file_list = np.concatenate((train_normal_files, train_anomaly_files), axis=0)\n",
    "        train_labels = np.concatenate((train_normal_labels, train_anomaly_labels), axis=0)\n",
    "        \n",
    "        return train_file_list, train_labels\n",
    "    \n",
    "    def _test_file_list(self, device, id):     \n",
    "        query = os.path.abspath(\n",
    "            f\"{self.audio_dir}/{device}/test/normal_id_0{id}*.wav\"\n",
    "            )\n",
    "        test_normal_files = sorted(glob.glob(query))\n",
    "        test_normal_labels = np.zeros(len(test_normal_files))\n",
    "        \n",
    "        query = os.path.abspath(\n",
    "            f\"{self.audio_dir}/{device}/test/anomaly_id_0{id}*.wav\"\n",
    "            )\n",
    "        test_anomaly_files = sorted(glob.glob(query))\n",
    "        test_anomaly_labels = np.ones(len(test_anomaly_files))\n",
    "        \n",
    "        test_file_list = np.concatenate((test_normal_files, \n",
    "                                          test_anomaly_files), axis=0)\n",
    "        test_labels = np.concatenate((test_normal_labels,\n",
    "                                      test_anomaly_labels), axis=0)\n",
    "          \n",
    "        return test_file_list, test_labels\n",
    "\n",
    "    def normalize(self,tensor):\n",
    "        tensor_minusmean = tensor - tensor.mean()\n",
    "        return tensor_minusmean/np.absolute(tensor_minusmean).max()\n",
    "\n",
    "    def make0min(self,tensornd):\n",
    "        tensor = tensornd.numpy()\n",
    "        res = np.where(tensor == 0, 1E-19 , tensor)\n",
    "        return torch.from_numpy(res)\n",
    "\n",
    "    def spectrogrameToImage(self,specgram):\n",
    "        # specgram = torchaudio.transforms.MelSpectrogram(n_fft=1024, win_length=1024, \n",
    "        #                                                 hop_length=512, power=2, \n",
    "        #                                                 normalized=True, n_mels=128)(waveform )\n",
    "        specgram= self.make0min(specgram)\n",
    "        specgram = specgram.log2()[0,:,:].numpy()\n",
    "        \n",
    "        tr2image = transforms.Compose([transforms.ToPILImage()])\n",
    "\n",
    "        specgram= self.normalize(specgram)\n",
    "        # specgram = img_as_ubyte(specgram)\n",
    "        specgramImage = tr2image(specgram)\n",
    "        return specgramImage\n",
    "\n",
    "    def get_logmelspectrogram(self, waveform):\n",
    "        melspec = librosa.feature.melspectrogram(\n",
    "          n_fft=self.n_fft, win_length=self.win_length, hop_length=self.hop_length,\n",
    "          power=self.power,n_mels=self.n_mels,pad_mode=self.pad_mode,sr=self.sr,\n",
    "          center=self.center,norm=self.norm,htk=True,\n",
    "          y = waveform.numpy()\n",
    "        )\n",
    "\n",
    "        logmelspec = librosa.power_to_db(melspec)\n",
    "\n",
    "        return logmelspec\n",
    "\n",
    "    def get_melspectrogram(self,waveform):\n",
    "        melspec = librosa.feature.melspectrogram(\n",
    "            n_fft=self.n_fft, win_length=self.win_length, hop_length=self.hop_length,\n",
    "            power=self.power,n_mels=self.n_mels,pad_mode=self.pad_mode,sr=self.sr,\n",
    "            center=self.center,norm=self.norm,htk=True,\n",
    "            y = waveform.numpy()\n",
    "        )\n",
    "\n",
    "        return melspec\n",
    "    \n",
    "    def get_mfcc(self,waveform):\n",
    "        mfcc = librosa.feature.mfcc(    \n",
    "            n_fft=self.n_fft, win_length=self.win_length, \n",
    "            hop_length=self.hop_length,pad_mode=self.pad_mode,sr=self.sr,\n",
    "            center=self.center,norm=self.norm,n_mfcc=40,\n",
    "            y = waveform.numpy()\n",
    "        )\n",
    "\n",
    "        return mfcc\n",
    "\n",
    "    def get_chroma_stft(self,waveform):\n",
    "        stft = librosa.feature.chroma_stft(\n",
    "            n_fft=self.n_fft, win_length=self.win_length, \n",
    "            hop_length=self.hop_length,pad_mode=self.pad_mode,sr=self.sr,\n",
    "            center=self.center,norm=self.norm,n_chroma=12,\n",
    "            y=waveform.numpy()\n",
    "        )\n",
    "\n",
    "        return stft\n",
    "\n",
    "    def get_spectral_contrast(self,waveform):\n",
    "        spec_contrast = librosa.feature.spectral_contrast(    \n",
    "            n_fft=self.n_fft, win_length=self.win_length,center=self.center,\n",
    "            hop_length=self.hop_length,pad_mode=self.pad_mode,sr=self.sr,\n",
    "            y = waveform.numpy()\n",
    "        )\n",
    "\n",
    "        return spec_contrast\n",
    "    \n",
    "    def get_tonnetz(self,waveform):\n",
    "        harmonic = librosa.effects.harmonic(waveform.numpy())\n",
    "        tonnetz = librosa.feature.tonnetz(y=harmonic,sr=self.sr)\n",
    "\n",
    "        return tonnetz\n",
    "\n",
    "    def get_audios(self, file_list):\n",
    "        data = []\n",
    "        for i in range(len(file_list)):\n",
    "          y, sr = torchaudio.load(file_list[i])  \n",
    "          data.append(y)\n",
    "\n",
    "        return data\n",
    "    def _derive_data(self, file_list):\n",
    "        train_data = []\n",
    "        test_data = []\n",
    "        train_mode = True\n",
    "        for file_list in [self.train_files, self.test_files]:\n",
    "          tr2tensor = transforms.Compose([transforms.PILToTensor()])\n",
    "          data = []\n",
    "          for j in range(len(file_list)):\n",
    "            y, sr = torchaudio.load(file_list[j])  \n",
    "            spec = self.get_melspectrogram(y)\n",
    "            spec = self.spectrogrameToImage(spec)\n",
    "            spec = spec.convert('RGB')\n",
    "            vectors = tr2tensor(spec)\n",
    "            if train_mode:     \n",
    "              train_data.append(vectors)\n",
    "            else:\n",
    "              test_data.append(vectors)\n",
    "            \n",
    "          train_mode = False\n",
    "                \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "S96soeIc0o13"
   },
   "outputs": [],
   "source": [
    "dataset = MimiiDataset('/content/drive/MyDrive/mimii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "Gn2zdn92doi1"
   },
   "outputs": [],
   "source": [
    "_, _, y_train, y_test = dataset.get_data('pump', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "SgjpeWy_RV1C"
   },
   "outputs": [],
   "source": [
    "train_mixed_f = torch.load('/content/drive/MyDrive/mixed_features/train_mf_pump0.pt')\n",
    "test_mixed_f = torch.load('/content/drive/MyDrive/mixed_features/test_mf_pump0.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "jWMPVGu1qiEq"
   },
   "outputs": [],
   "source": [
    "train_data = DataLoader(train_mixed_f, batch_size=32, shuffle = True)\n",
    "test_data = DataLoader(test_mixed_f, batch_size = 32, shuffle= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "vNTBTRe6qnBq"
   },
   "outputs": [],
   "source": [
    "class UNet_FC(nn.Module):\n",
    "\n",
    "  def __init__(self, in_features):\n",
    "    super().__init__()\n",
    "    \n",
    "    self.bn = nn.BatchNorm1d(128)\n",
    "    self.relu = nn.ReLU()\n",
    "\n",
    "    self.fc0 = nn.Linear(in_features=in_features,out_features=in_features)\n",
    "\n",
    "    # encoder\n",
    "    self.fc1 = nn.Linear(in_features=in_features, out_features=128)\n",
    "    self.fc2 = nn.Linear(in_features=128,out_features=128)\n",
    "    self.fc3 = nn.Linear(in_features=128, out_features=128)\n",
    "    self.fc4 = nn.Linear(in_features=128, out_features=128)\n",
    "    self.fc5 = nn.Linear(in_features=128, out_features=8)\n",
    "\n",
    "    # decoder\n",
    "    self.fc6 = nn.Linear(in_features=8, out_features=128)\n",
    "    self.fc7 = nn.Linear(in_features=128*2, out_features=128)\n",
    "    self.fc8 = nn.Linear(in_features=128*2, out_features=128)\n",
    "    self.fc9 = nn.Linear(in_features=128*2, out_features=128)\n",
    "\n",
    "    self.out = nn.Linear(in_features=128*2, out_features=in_features)\n",
    "\n",
    "  def forward(self, x):\n",
    "    input = self.fc0(x)\n",
    "\n",
    "    x1 = self.relu(self.bn(self.fc1(input)))\n",
    "    x2 = self.relu(self.bn(self.fc2(x1)))\n",
    "    x3 = self.relu(self.bn(self.fc3(x2)))\n",
    "    x4 = self.relu(self.bn(self.fc4(x3)))\n",
    "    x5 = self.relu(self.fc5(x4))\n",
    "\n",
    "    xy = [x5, x4, x3, x2, x1]\n",
    "\n",
    "    x6 = self.relu(self.fc6(xy[0]))\n",
    "    con1 = torch.cat((x6,xy[1]), 1) \n",
    "    x7 = self.relu(self.bn(self.fc7(con1)))\n",
    "    con2 = torch.cat((x7,xy[2]), 1)\n",
    "    x8 = self.relu(self.bn(self.fc8(con2)))\n",
    "    con3 = torch.cat((x8,xy[3]), 1)\n",
    "    x9 = self.relu(self.bn(self.fc9(con3)))\n",
    "    con4 = torch.cat((x9,xy[4]), 1)\n",
    "\n",
    "    x10 = self.out(con4)\n",
    "\n",
    "    return x10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "ZfgcBtQ3qn5l"
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, data_tr, data_val, scheduler = None,\n",
    "          epochs = 3000, device = 'cpu'):\n",
    "    # X_val, Y_val = next(iter(data_val))\n",
    "    losses = []\n",
    "    prev_avg_loss = 100000\n",
    "    for epoch in range(epochs):\n",
    "        train_avg_loss = 0\n",
    "        test_avg_loss = 0\n",
    "        # model.train()  # train mode\n",
    "        for batch in data_tr:\n",
    "          # data to device\n",
    "          batch = batch.to(device)\n",
    "          # set parameter gradients to zero\n",
    "          optimizer.zero_grad()\n",
    "          # forward\n",
    "          # print(Y_batch.shape)\n",
    "          predictions = model(batch)\n",
    "          loss = criterion(predictions, batch)\n",
    "          loss.backward() # backward-pass\n",
    "          optimizer.step()  # update weights\n",
    "          # calculate loss to show the user\n",
    "          if scheduler:\n",
    "            scheduler.step(loss)\n",
    "          train_avg_loss += loss / len(data_tr)\n",
    "\n",
    "        # model.eval()\n",
    "        for batch in data_val:\n",
    "          with torch.no_grad():\n",
    "            preds = model(batch.to(device)).cpu()\n",
    "            loss = criterion(preds,batch)\n",
    "            test_avg_loss += loss / len(data_val)\n",
    "                    \n",
    "        losses.append(train_avg_loss.item())\n",
    "        # if (epoch+1)%50 == 0:\n",
    "        print(\"{}/{} train_loss: {} test_loss:{}\".format(epoch+1, epochs, train_avg_loss, test_avg_loss))\n",
    "        # if test_avg_loss < 70:\n",
    "        #   break\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "ptkVTF55quOL"
   },
   "outputs": [],
   "source": [
    "unet = UNet_FC(in_features=193).to(device)\n",
    "optimizer = Adam(params = unet.parameters(), lr = 10e-3)\n",
    "# optimizer = Adam(params = unet.parameters())\n",
    "criterion = nn.MSELoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, \n",
    "                                                       min_lr=10e-4, mode = 'min',\n",
    "                                                       patience = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QkfmYl9oXhcB",
    "outputId": "57c3f734-a808-4981-a769-e6e5f5011e24"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1/3000 train_loss: 305430.09375 test_loss:279746.71875\n",
      "2/3000 train_loss: 297537.15625 test_loss:271467.6875\n",
      "3/3000 train_loss: 289552.1875 test_loss:263689.25\n",
      "4/3000 train_loss: 281042.96875 test_loss:254197.921875\n",
      "5/3000 train_loss: 269057.5 test_loss:242412.078125\n",
      "6/3000 train_loss: 256500.25 test_loss:229535.28125\n",
      "7/3000 train_loss: 242359.015625 test_loss:215343.921875\n",
      "8/3000 train_loss: 227065.953125 test_loss:200630.59375\n",
      "9/3000 train_loss: 211167.578125 test_loss:185352.21875\n",
      "10/3000 train_loss: 194197.84375 test_loss:168830.328125\n",
      "11/3000 train_loss: 177082.453125 test_loss:152964.828125\n",
      "12/3000 train_loss: 161046.0 test_loss:136776.296875\n",
      "13/3000 train_loss: 143095.96875 test_loss:120466.21875\n",
      "14/3000 train_loss: 125945.9921875 test_loss:105066.59375\n",
      "15/3000 train_loss: 109513.171875 test_loss:90024.390625\n",
      "16/3000 train_loss: 94264.84375 test_loss:76492.515625\n",
      "17/3000 train_loss: 80632.1875 test_loss:66121.765625\n",
      "18/3000 train_loss: 68433.0078125 test_loss:53162.4296875\n",
      "19/3000 train_loss: 55645.5078125 test_loss:42405.4296875\n",
      "20/3000 train_loss: 44398.859375 test_loss:33822.95703125\n",
      "21/3000 train_loss: 35153.671875 test_loss:26293.22265625\n",
      "22/3000 train_loss: 27683.0859375 test_loss:19798.595703125\n",
      "23/3000 train_loss: 21305.51171875 test_loss:14945.212890625\n",
      "24/3000 train_loss: 15959.0966796875 test_loss:10818.4013671875\n",
      "25/3000 train_loss: 11866.5810546875 test_loss:7949.9755859375\n",
      "26/3000 train_loss: 9085.083984375 test_loss:5751.640625\n",
      "27/3000 train_loss: 6488.55078125 test_loss:4090.651611328125\n",
      "28/3000 train_loss: 4976.96142578125 test_loss:3314.268310546875\n",
      "29/3000 train_loss: 3668.932373046875 test_loss:2330.966552734375\n",
      "30/3000 train_loss: 2497.128662109375 test_loss:1893.39013671875\n",
      "31/3000 train_loss: 1876.304443359375 test_loss:1603.3671875\n",
      "32/3000 train_loss: 1424.216552734375 test_loss:1353.462646484375\n",
      "33/3000 train_loss: 1215.60009765625 test_loss:1426.72998046875\n",
      "34/3000 train_loss: 1034.7197265625 test_loss:1379.26513671875\n",
      "35/3000 train_loss: 928.3317260742188 test_loss:1295.6568603515625\n",
      "36/3000 train_loss: 825.7022094726562 test_loss:1373.291015625\n",
      "37/3000 train_loss: 782.64990234375 test_loss:1378.327392578125\n",
      "38/3000 train_loss: 679.4880981445312 test_loss:1298.89501953125\n",
      "39/3000 train_loss: 655.3500366210938 test_loss:1304.6409912109375\n",
      "40/3000 train_loss: 631.2150268554688 test_loss:1272.6976318359375\n",
      "41/3000 train_loss: 608.0216674804688 test_loss:1220.8658447265625\n",
      "42/3000 train_loss: 590.4157104492188 test_loss:1253.69482421875\n",
      "43/3000 train_loss: 591.0643920898438 test_loss:1229.835693359375\n",
      "44/3000 train_loss: 594.9765014648438 test_loss:1244.7845458984375\n",
      "45/3000 train_loss: 643.7886962890625 test_loss:1230.46923828125\n",
      "46/3000 train_loss: 597.0331420898438 test_loss:1302.0364990234375\n",
      "47/3000 train_loss: 587.2654418945312 test_loss:1227.6287841796875\n",
      "48/3000 train_loss: 550.8104248046875 test_loss:1254.326416015625\n",
      "49/3000 train_loss: 565.6424560546875 test_loss:1265.4404296875\n",
      "50/3000 train_loss: 565.8414306640625 test_loss:1234.96826171875\n",
      "51/3000 train_loss: 567.5269775390625 test_loss:1245.2247314453125\n",
      "52/3000 train_loss: 558.445068359375 test_loss:1259.00634765625\n",
      "53/3000 train_loss: 556.4013671875 test_loss:1261.9566650390625\n",
      "54/3000 train_loss: 534.9331665039062 test_loss:1286.6195068359375\n",
      "55/3000 train_loss: 531.9832763671875 test_loss:1273.1231689453125\n",
      "56/3000 train_loss: 539.577880859375 test_loss:1281.476318359375\n",
      "57/3000 train_loss: 518.5732421875 test_loss:1238.2225341796875\n",
      "58/3000 train_loss: 532.5589599609375 test_loss:1237.9783935546875\n",
      "59/3000 train_loss: 547.6060791015625 test_loss:1259.60595703125\n",
      "60/3000 train_loss: 501.9512634277344 test_loss:1232.463623046875\n",
      "61/3000 train_loss: 556.4749755859375 test_loss:1201.578857421875\n",
      "62/3000 train_loss: 537.3672485351562 test_loss:1206.2142333984375\n",
      "63/3000 train_loss: 543.7078857421875 test_loss:1219.908935546875\n",
      "64/3000 train_loss: 517.854736328125 test_loss:1214.2447509765625\n",
      "65/3000 train_loss: 515.3838500976562 test_loss:1206.26123046875\n",
      "66/3000 train_loss: 496.40283203125 test_loss:1203.084228515625\n",
      "67/3000 train_loss: 567.0953369140625 test_loss:1216.42822265625\n",
      "68/3000 train_loss: 700.5946044921875 test_loss:1320.897705078125\n",
      "69/3000 train_loss: 589.5597534179688 test_loss:1269.1483154296875\n",
      "70/3000 train_loss: 583.972900390625 test_loss:1239.072998046875\n",
      "71/3000 train_loss: 519.3572998046875 test_loss:1223.7001953125\n",
      "72/3000 train_loss: 493.93194580078125 test_loss:1229.79931640625\n",
      "73/3000 train_loss: 487.4259033203125 test_loss:1235.311279296875\n",
      "74/3000 train_loss: 484.7890625 test_loss:1215.8487548828125\n",
      "75/3000 train_loss: 496.7557373046875 test_loss:1217.6064453125\n",
      "76/3000 train_loss: 491.664794921875 test_loss:1232.8077392578125\n",
      "77/3000 train_loss: 497.4429626464844 test_loss:1218.569091796875\n",
      "78/3000 train_loss: 474.0112609863281 test_loss:1217.918212890625\n",
      "79/3000 train_loss: 496.3790283203125 test_loss:1230.3143310546875\n",
      "80/3000 train_loss: 497.6427917480469 test_loss:1242.066162109375\n",
      "81/3000 train_loss: 475.23870849609375 test_loss:1237.25634765625\n",
      "82/3000 train_loss: 472.84527587890625 test_loss:1243.3275146484375\n",
      "83/3000 train_loss: 465.3961181640625 test_loss:1296.0919189453125\n",
      "84/3000 train_loss: 449.9131164550781 test_loss:1256.0157470703125\n",
      "85/3000 train_loss: 454.56097412109375 test_loss:1228.98046875\n",
      "86/3000 train_loss: 465.5351867675781 test_loss:1216.942626953125\n",
      "87/3000 train_loss: 454.273193359375 test_loss:1191.0657958984375\n",
      "88/3000 train_loss: 455.7209167480469 test_loss:1215.9383544921875\n",
      "89/3000 train_loss: 490.70037841796875 test_loss:1204.9276123046875\n",
      "90/3000 train_loss: 462.9345397949219 test_loss:1223.0340576171875\n",
      "91/3000 train_loss: 458.9579772949219 test_loss:1204.31689453125\n",
      "92/3000 train_loss: 445.55401611328125 test_loss:1202.765380859375\n",
      "93/3000 train_loss: 474.08514404296875 test_loss:1219.0098876953125\n",
      "94/3000 train_loss: 457.4022216796875 test_loss:1243.0428466796875\n",
      "95/3000 train_loss: 445.0908508300781 test_loss:1214.9346923828125\n",
      "96/3000 train_loss: 429.6572570800781 test_loss:1234.14794921875\n",
      "97/3000 train_loss: 438.7718811035156 test_loss:1205.6715087890625\n",
      "98/3000 train_loss: 445.3878173828125 test_loss:1204.33447265625\n",
      "99/3000 train_loss: 453.34136962890625 test_loss:1180.0028076171875\n",
      "100/3000 train_loss: 456.2547912597656 test_loss:1176.5374755859375\n",
      "101/3000 train_loss: 431.75372314453125 test_loss:1188.1724853515625\n",
      "102/3000 train_loss: 427.04925537109375 test_loss:1188.57958984375\n",
      "103/3000 train_loss: 426.49517822265625 test_loss:1198.950439453125\n",
      "104/3000 train_loss: 444.85552978515625 test_loss:1182.2578125\n",
      "105/3000 train_loss: 435.69024658203125 test_loss:1204.4853515625\n",
      "106/3000 train_loss: 440.35968017578125 test_loss:1198.172607421875\n",
      "107/3000 train_loss: 423.62127685546875 test_loss:1200.139892578125\n",
      "108/3000 train_loss: 425.204833984375 test_loss:1190.8203125\n",
      "109/3000 train_loss: 404.56951904296875 test_loss:1187.5078125\n",
      "110/3000 train_loss: 420.4141845703125 test_loss:1197.892578125\n",
      "111/3000 train_loss: 414.28643798828125 test_loss:1206.2799072265625\n",
      "112/3000 train_loss: 442.40667724609375 test_loss:1173.104736328125\n",
      "113/3000 train_loss: 411.3375549316406 test_loss:1163.3731689453125\n",
      "114/3000 train_loss: 414.8447265625 test_loss:1163.664794921875\n",
      "115/3000 train_loss: 393.98638916015625 test_loss:1158.6702880859375\n",
      "116/3000 train_loss: 478.85882568359375 test_loss:1166.6839599609375\n",
      "117/3000 train_loss: 418.27215576171875 test_loss:1184.3380126953125\n",
      "118/3000 train_loss: 428.0489501953125 test_loss:1161.54931640625\n",
      "119/3000 train_loss: 445.18768310546875 test_loss:1196.367919921875\n",
      "120/3000 train_loss: 412.3548889160156 test_loss:1189.9525146484375\n",
      "121/3000 train_loss: 409.2719421386719 test_loss:1207.555419921875\n",
      "122/3000 train_loss: 392.7727355957031 test_loss:1163.3306884765625\n",
      "123/3000 train_loss: 376.2384338378906 test_loss:1142.79833984375\n",
      "124/3000 train_loss: 421.56109619140625 test_loss:1175.4798583984375\n",
      "125/3000 train_loss: 387.0068359375 test_loss:1162.253173828125\n",
      "126/3000 train_loss: 397.0936279296875 test_loss:1151.8775634765625\n",
      "127/3000 train_loss: 397.1950378417969 test_loss:1152.952392578125\n",
      "128/3000 train_loss: 418.3197021484375 test_loss:1182.8663330078125\n",
      "129/3000 train_loss: 379.3618469238281 test_loss:1143.59765625\n",
      "130/3000 train_loss: 380.9056701660156 test_loss:1147.7464599609375\n",
      "131/3000 train_loss: 391.5036926269531 test_loss:1131.7047119140625\n",
      "132/3000 train_loss: 378.6278381347656 test_loss:1146.2230224609375\n",
      "133/3000 train_loss: 377.5609436035156 test_loss:1161.270751953125\n",
      "134/3000 train_loss: 366.9429626464844 test_loss:1177.4783935546875\n",
      "135/3000 train_loss: 385.81488037109375 test_loss:1141.6851806640625\n",
      "136/3000 train_loss: 380.51373291015625 test_loss:1146.580322265625\n",
      "137/3000 train_loss: 354.1438293457031 test_loss:1156.55029296875\n",
      "138/3000 train_loss: 357.5341796875 test_loss:1143.897705078125\n",
      "139/3000 train_loss: 345.1253967285156 test_loss:1148.0128173828125\n",
      "140/3000 train_loss: 355.10406494140625 test_loss:1132.9150390625\n",
      "141/3000 train_loss: 376.673095703125 test_loss:1148.625732421875\n",
      "142/3000 train_loss: 350.6865539550781 test_loss:1135.14501953125\n",
      "143/3000 train_loss: 339.0910339355469 test_loss:1128.413330078125\n",
      "144/3000 train_loss: 340.39715576171875 test_loss:1137.982666015625\n",
      "145/3000 train_loss: 363.9791564941406 test_loss:1122.0560302734375\n",
      "146/3000 train_loss: 373.3387756347656 test_loss:1119.033447265625\n",
      "147/3000 train_loss: 333.0455017089844 test_loss:1105.0601806640625\n",
      "148/3000 train_loss: 336.19830322265625 test_loss:1117.119140625\n",
      "149/3000 train_loss: 343.80712890625 test_loss:1097.155029296875\n",
      "150/3000 train_loss: 342.0863952636719 test_loss:1117.97265625\n",
      "151/3000 train_loss: 338.9166564941406 test_loss:1120.1932373046875\n",
      "152/3000 train_loss: 337.5896301269531 test_loss:1146.02392578125\n",
      "153/3000 train_loss: 329.0010070800781 test_loss:1111.6824951171875\n",
      "154/3000 train_loss: 325.1849365234375 test_loss:1097.5745849609375\n",
      "155/3000 train_loss: 329.0820617675781 test_loss:1107.483154296875\n",
      "156/3000 train_loss: 355.89984130859375 test_loss:1144.3411865234375\n",
      "157/3000 train_loss: 347.31298828125 test_loss:1114.1832275390625\n",
      "158/3000 train_loss: 308.559326171875 test_loss:1102.3193359375\n",
      "159/3000 train_loss: 321.86871337890625 test_loss:1111.66455078125\n",
      "160/3000 train_loss: 315.4156799316406 test_loss:1107.8353271484375\n",
      "161/3000 train_loss: 313.1590270996094 test_loss:1079.0679931640625\n",
      "162/3000 train_loss: 313.8963928222656 test_loss:1086.46484375\n",
      "163/3000 train_loss: 311.4745178222656 test_loss:1074.9561767578125\n",
      "164/3000 train_loss: 321.63525390625 test_loss:1093.015625\n",
      "165/3000 train_loss: 310.89434814453125 test_loss:1063.084228515625\n",
      "166/3000 train_loss: 298.63037109375 test_loss:1048.2623291015625\n",
      "167/3000 train_loss: 299.9534912109375 test_loss:1068.5062255859375\n",
      "168/3000 train_loss: 296.29852294921875 test_loss:1052.6568603515625\n",
      "169/3000 train_loss: 296.8974304199219 test_loss:1063.817626953125\n",
      "170/3000 train_loss: 296.9542236328125 test_loss:1042.0712890625\n",
      "171/3000 train_loss: 290.2185363769531 test_loss:1039.6002197265625\n",
      "172/3000 train_loss: 301.6224365234375 test_loss:1044.417236328125\n",
      "173/3000 train_loss: 275.5600280761719 test_loss:1043.744384765625\n",
      "174/3000 train_loss: 284.1627502441406 test_loss:1056.5340576171875\n",
      "175/3000 train_loss: 279.70233154296875 test_loss:1053.159912109375\n",
      "176/3000 train_loss: 296.3187255859375 test_loss:1054.1951904296875\n",
      "177/3000 train_loss: 297.13702392578125 test_loss:1083.23779296875\n",
      "178/3000 train_loss: 280.2805480957031 test_loss:1045.1614990234375\n",
      "179/3000 train_loss: 297.83294677734375 test_loss:1053.8134765625\n",
      "180/3000 train_loss: 283.7988586425781 test_loss:1041.312255859375\n",
      "181/3000 train_loss: 292.3311767578125 test_loss:1032.336669921875\n",
      "182/3000 train_loss: 329.07830810546875 test_loss:1027.426513671875\n",
      "183/3000 train_loss: 303.55657958984375 test_loss:1035.4716796875\n",
      "184/3000 train_loss: 286.8456726074219 test_loss:1021.968017578125\n",
      "185/3000 train_loss: 292.22357177734375 test_loss:1042.61376953125\n",
      "186/3000 train_loss: 276.7845764160156 test_loss:1038.312255859375\n",
      "187/3000 train_loss: 259.2998962402344 test_loss:1013.9677734375\n",
      "188/3000 train_loss: 281.23626708984375 test_loss:1005.7770385742188\n",
      "189/3000 train_loss: 275.9963073730469 test_loss:1076.35791015625\n",
      "190/3000 train_loss: 311.3434753417969 test_loss:1030.12060546875\n",
      "191/3000 train_loss: 282.3762512207031 test_loss:1102.6973876953125\n",
      "192/3000 train_loss: 275.7269592285156 test_loss:1129.70703125\n",
      "193/3000 train_loss: 261.36767578125 test_loss:1089.031982421875\n",
      "194/3000 train_loss: 256.2201843261719 test_loss:1073.0968017578125\n",
      "195/3000 train_loss: 261.4799499511719 test_loss:1094.580078125\n",
      "196/3000 train_loss: 264.6617736816406 test_loss:1052.67724609375\n",
      "197/3000 train_loss: 267.55450439453125 test_loss:1046.163818359375\n",
      "198/3000 train_loss: 252.21380615234375 test_loss:1028.4879150390625\n",
      "199/3000 train_loss: 260.51666259765625 test_loss:1014.609375\n",
      "200/3000 train_loss: 254.41307067871094 test_loss:1012.705322265625\n",
      "201/3000 train_loss: 280.4500732421875 test_loss:1001.412353515625\n",
      "202/3000 train_loss: 262.22808837890625 test_loss:1057.4278564453125\n",
      "203/3000 train_loss: 250.33303833007812 test_loss:979.53759765625\n",
      "204/3000 train_loss: 235.41465759277344 test_loss:990.4971923828125\n",
      "205/3000 train_loss: 257.33306884765625 test_loss:980.1383056640625\n",
      "206/3000 train_loss: 251.24215698242188 test_loss:1002.2454833984375\n",
      "207/3000 train_loss: 245.23634338378906 test_loss:1026.921630859375\n",
      "208/3000 train_loss: 245.36004638671875 test_loss:1052.9273681640625\n",
      "209/3000 train_loss: 241.05105590820312 test_loss:1006.5740966796875\n",
      "210/3000 train_loss: 257.1544189453125 test_loss:1027.46630859375\n",
      "211/3000 train_loss: 244.50428771972656 test_loss:1004.6466064453125\n",
      "212/3000 train_loss: 234.27928161621094 test_loss:1002.8030395507812\n",
      "213/3000 train_loss: 236.52035522460938 test_loss:1016.0115966796875\n",
      "214/3000 train_loss: 223.09117126464844 test_loss:1056.72412109375\n",
      "215/3000 train_loss: 231.25738525390625 test_loss:1046.7537841796875\n",
      "216/3000 train_loss: 247.5352783203125 test_loss:1028.00927734375\n",
      "217/3000 train_loss: 244.66880798339844 test_loss:1104.0274658203125\n",
      "218/3000 train_loss: 245.71719360351562 test_loss:1118.8699951171875\n",
      "219/3000 train_loss: 237.33740234375 test_loss:1141.871826171875\n",
      "220/3000 train_loss: 227.38754272460938 test_loss:1119.985107421875\n",
      "221/3000 train_loss: 224.5852508544922 test_loss:1149.65673828125\n",
      "222/3000 train_loss: 225.4370574951172 test_loss:1145.604736328125\n",
      "223/3000 train_loss: 214.48435974121094 test_loss:1128.3902587890625\n",
      "224/3000 train_loss: 226.351806640625 test_loss:1113.14111328125\n",
      "225/3000 train_loss: 230.66246032714844 test_loss:1173.682861328125\n",
      "226/3000 train_loss: 233.24392700195312 test_loss:1149.8994140625\n",
      "227/3000 train_loss: 213.56689453125 test_loss:1143.3779296875\n",
      "228/3000 train_loss: 323.0577697753906 test_loss:1166.234375\n",
      "229/3000 train_loss: 238.9560546875 test_loss:1166.7777099609375\n",
      "230/3000 train_loss: 246.76930236816406 test_loss:1183.788818359375\n",
      "231/3000 train_loss: 251.77178955078125 test_loss:1189.367431640625\n",
      "232/3000 train_loss: 223.4097900390625 test_loss:1169.0264892578125\n",
      "233/3000 train_loss: 212.89935302734375 test_loss:1166.4957275390625\n",
      "234/3000 train_loss: 204.7822265625 test_loss:1162.197509765625\n",
      "235/3000 train_loss: 199.1790313720703 test_loss:1200.3253173828125\n",
      "236/3000 train_loss: 224.5521697998047 test_loss:1151.012939453125\n",
      "237/3000 train_loss: 214.55145263671875 test_loss:1159.051513671875\n",
      "238/3000 train_loss: 211.41859436035156 test_loss:1147.744384765625\n",
      "239/3000 train_loss: 206.68980407714844 test_loss:1197.21533203125\n",
      "240/3000 train_loss: 200.19509887695312 test_loss:1181.06201171875\n",
      "241/3000 train_loss: 212.5450439453125 test_loss:1147.9405517578125\n",
      "242/3000 train_loss: 210.67019653320312 test_loss:1135.94384765625\n",
      "243/3000 train_loss: 198.065673828125 test_loss:1162.58544921875\n",
      "244/3000 train_loss: 206.9691619873047 test_loss:1163.65771484375\n",
      "245/3000 train_loss: 200.19541931152344 test_loss:1152.340576171875\n",
      "246/3000 train_loss: 196.283447265625 test_loss:1102.32568359375\n",
      "247/3000 train_loss: 191.96168518066406 test_loss:1153.71435546875\n",
      "248/3000 train_loss: 195.90513610839844 test_loss:1162.6240234375\n",
      "249/3000 train_loss: 192.69146728515625 test_loss:1123.7982177734375\n",
      "250/3000 train_loss: 201.77835083007812 test_loss:1114.5052490234375\n",
      "251/3000 train_loss: 182.78224182128906 test_loss:1147.932861328125\n",
      "252/3000 train_loss: 178.7692413330078 test_loss:1144.4483642578125\n",
      "253/3000 train_loss: 190.9955596923828 test_loss:1156.915283203125\n",
      "254/3000 train_loss: 193.64273071289062 test_loss:1170.64208984375\n",
      "255/3000 train_loss: 199.3563690185547 test_loss:1144.292724609375\n",
      "256/3000 train_loss: 186.21763610839844 test_loss:1120.632080078125\n",
      "257/3000 train_loss: 177.1521759033203 test_loss:1134.0946044921875\n",
      "258/3000 train_loss: 186.22897338867188 test_loss:1122.4130859375\n",
      "259/3000 train_loss: 208.6741485595703 test_loss:1066.5426025390625\n",
      "260/3000 train_loss: 191.7117919921875 test_loss:1130.030517578125\n",
      "261/3000 train_loss: 176.10781860351562 test_loss:1084.008544921875\n",
      "262/3000 train_loss: 179.94461059570312 test_loss:1100.385498046875\n",
      "263/3000 train_loss: 178.2439422607422 test_loss:1094.912841796875\n",
      "264/3000 train_loss: 185.21005249023438 test_loss:1123.6636962890625\n",
      "265/3000 train_loss: 184.8528594970703 test_loss:1114.3243408203125\n",
      "266/3000 train_loss: 168.42808532714844 test_loss:1077.739501953125\n",
      "267/3000 train_loss: 173.5896759033203 test_loss:1106.52294921875\n",
      "268/3000 train_loss: 183.20555114746094 test_loss:1115.7550048828125\n",
      "269/3000 train_loss: 180.79266357421875 test_loss:1074.98681640625\n",
      "270/3000 train_loss: 178.55364990234375 test_loss:1100.9134521484375\n",
      "271/3000 train_loss: 168.20445251464844 test_loss:1088.653076171875\n",
      "272/3000 train_loss: 175.86749267578125 test_loss:1094.6685791015625\n",
      "273/3000 train_loss: 185.1942138671875 test_loss:1098.4918212890625\n",
      "274/3000 train_loss: 187.91909790039062 test_loss:1122.8717041015625\n",
      "275/3000 train_loss: 180.54550170898438 test_loss:1082.631103515625\n",
      "276/3000 train_loss: 172.18182373046875 test_loss:1080.64599609375\n",
      "277/3000 train_loss: 176.75181579589844 test_loss:1042.3377685546875\n",
      "278/3000 train_loss: 164.94200134277344 test_loss:1072.755615234375\n",
      "279/3000 train_loss: 176.09848022460938 test_loss:1085.1795654296875\n",
      "280/3000 train_loss: 172.34072875976562 test_loss:1049.17529296875\n",
      "281/3000 train_loss: 176.59701538085938 test_loss:1105.8409423828125\n",
      "282/3000 train_loss: 165.11094665527344 test_loss:1084.2877197265625\n",
      "283/3000 train_loss: 163.67259216308594 test_loss:1047.4969482421875\n",
      "284/3000 train_loss: 165.25205993652344 test_loss:1081.40234375\n",
      "285/3000 train_loss: 180.2885284423828 test_loss:1059.663818359375\n",
      "286/3000 train_loss: 173.17617797851562 test_loss:1045.107421875\n",
      "287/3000 train_loss: 172.41030883789062 test_loss:1058.4884033203125\n",
      "288/3000 train_loss: 193.1880645751953 test_loss:1121.9951171875\n",
      "289/3000 train_loss: 187.8260955810547 test_loss:1126.9185791015625\n",
      "290/3000 train_loss: 161.78236389160156 test_loss:1084.717529296875\n",
      "291/3000 train_loss: 173.0994110107422 test_loss:1117.177001953125\n",
      "292/3000 train_loss: 163.8669891357422 test_loss:1121.55859375\n",
      "293/3000 train_loss: 163.57492065429688 test_loss:1097.69775390625\n",
      "294/3000 train_loss: 147.03363037109375 test_loss:1114.5504150390625\n",
      "295/3000 train_loss: 159.4938201904297 test_loss:1106.6575927734375\n",
      "296/3000 train_loss: 149.1435089111328 test_loss:1060.079345703125\n",
      "297/3000 train_loss: 165.76280212402344 test_loss:1079.624755859375\n",
      "298/3000 train_loss: 176.77899169921875 test_loss:1086.08349609375\n",
      "299/3000 train_loss: 178.19419860839844 test_loss:1097.9638671875\n",
      "300/3000 train_loss: 174.57272338867188 test_loss:1047.0340576171875\n",
      "301/3000 train_loss: 175.4529571533203 test_loss:1120.594482421875\n",
      "302/3000 train_loss: 166.57666015625 test_loss:1097.83056640625\n",
      "303/3000 train_loss: 179.2798309326172 test_loss:1083.819091796875\n",
      "304/3000 train_loss: 174.84149169921875 test_loss:1079.9603271484375\n",
      "305/3000 train_loss: 159.2718048095703 test_loss:1076.2237548828125\n",
      "306/3000 train_loss: 158.72549438476562 test_loss:1055.350830078125\n",
      "307/3000 train_loss: 165.85073852539062 test_loss:1052.444580078125\n",
      "308/3000 train_loss: 165.84361267089844 test_loss:1060.393310546875\n",
      "309/3000 train_loss: 152.13409423828125 test_loss:1049.59033203125\n",
      "310/3000 train_loss: 154.95111083984375 test_loss:1064.01513671875\n",
      "311/3000 train_loss: 151.172607421875 test_loss:1061.300048828125\n",
      "312/3000 train_loss: 155.0558319091797 test_loss:1056.4981689453125\n",
      "313/3000 train_loss: 164.74176025390625 test_loss:1066.8096923828125\n",
      "314/3000 train_loss: 164.3836212158203 test_loss:1009.2932739257812\n",
      "315/3000 train_loss: 149.98345947265625 test_loss:1032.407470703125\n",
      "316/3000 train_loss: 175.2059783935547 test_loss:1088.072021484375\n",
      "317/3000 train_loss: 177.26329040527344 test_loss:996.906982421875\n",
      "318/3000 train_loss: 152.52915954589844 test_loss:1045.66357421875\n",
      "319/3000 train_loss: 160.60113525390625 test_loss:1086.871826171875\n",
      "320/3000 train_loss: 159.9553680419922 test_loss:1036.42578125\n",
      "321/3000 train_loss: 152.6683807373047 test_loss:1052.05859375\n",
      "322/3000 train_loss: 158.20452880859375 test_loss:1052.3050537109375\n",
      "323/3000 train_loss: 152.33995056152344 test_loss:1006.5454711914062\n",
      "324/3000 train_loss: 171.42955017089844 test_loss:1115.927734375\n",
      "325/3000 train_loss: 160.5362548828125 test_loss:1071.55419921875\n",
      "326/3000 train_loss: 165.9447784423828 test_loss:1061.1966552734375\n",
      "327/3000 train_loss: 152.24501037597656 test_loss:1049.50048828125\n",
      "328/3000 train_loss: 153.3499298095703 test_loss:1045.4141845703125\n",
      "329/3000 train_loss: 147.65013122558594 test_loss:1067.0272216796875\n",
      "330/3000 train_loss: 146.40869140625 test_loss:1040.3895263671875\n",
      "331/3000 train_loss: 155.5995330810547 test_loss:1067.44091796875\n",
      "332/3000 train_loss: 159.1923828125 test_loss:1054.9705810546875\n",
      "333/3000 train_loss: 138.26922607421875 test_loss:1081.109619140625\n",
      "334/3000 train_loss: 137.6074981689453 test_loss:1064.157470703125\n",
      "335/3000 train_loss: 161.82359313964844 test_loss:1042.5584716796875\n",
      "336/3000 train_loss: 140.05125427246094 test_loss:1044.24267578125\n",
      "337/3000 train_loss: 143.17189025878906 test_loss:1043.7158203125\n",
      "338/3000 train_loss: 142.9312744140625 test_loss:1054.466796875\n",
      "339/3000 train_loss: 154.35484313964844 test_loss:1055.092041015625\n",
      "340/3000 train_loss: 141.74200439453125 test_loss:1055.925048828125\n",
      "341/3000 train_loss: 147.97219848632812 test_loss:1056.2965087890625\n",
      "342/3000 train_loss: 161.65914916992188 test_loss:1035.800537109375\n",
      "343/3000 train_loss: 147.55255126953125 test_loss:1055.3438720703125\n",
      "344/3000 train_loss: 144.92910766601562 test_loss:1036.4373779296875\n",
      "345/3000 train_loss: 155.1291961669922 test_loss:1031.6895751953125\n",
      "346/3000 train_loss: 141.34002685546875 test_loss:1014.60302734375\n",
      "347/3000 train_loss: 137.0004425048828 test_loss:1030.52001953125\n",
      "348/3000 train_loss: 157.12547302246094 test_loss:1040.462158203125\n",
      "349/3000 train_loss: 140.72540283203125 test_loss:1052.148193359375\n",
      "350/3000 train_loss: 141.29510498046875 test_loss:1054.7427978515625\n",
      "351/3000 train_loss: 138.77413940429688 test_loss:1027.320068359375\n",
      "352/3000 train_loss: 148.50619506835938 test_loss:1049.1429443359375\n",
      "353/3000 train_loss: 145.09010314941406 test_loss:1043.923828125\n",
      "354/3000 train_loss: 135.9557342529297 test_loss:1045.3916015625\n",
      "355/3000 train_loss: 154.7587890625 test_loss:1018.683837890625\n",
      "356/3000 train_loss: 144.4634552001953 test_loss:1033.2635498046875\n",
      "357/3000 train_loss: 136.3795623779297 test_loss:1046.978271484375\n",
      "358/3000 train_loss: 143.68655395507812 test_loss:1035.389892578125\n",
      "359/3000 train_loss: 141.727783203125 test_loss:1068.980224609375\n",
      "360/3000 train_loss: 141.1157989501953 test_loss:1043.525390625\n",
      "361/3000 train_loss: 135.23672485351562 test_loss:1028.5499267578125\n",
      "362/3000 train_loss: 130.74273681640625 test_loss:1018.9627075195312\n",
      "363/3000 train_loss: 136.20172119140625 test_loss:1019.7672729492188\n",
      "364/3000 train_loss: 144.82713317871094 test_loss:1057.21630859375\n",
      "365/3000 train_loss: 148.2140655517578 test_loss:1019.2811279296875\n",
      "366/3000 train_loss: 141.21868896484375 test_loss:1038.2105712890625\n",
      "367/3000 train_loss: 148.86366271972656 test_loss:1033.07666015625\n",
      "368/3000 train_loss: 143.7152557373047 test_loss:1038.25048828125\n",
      "369/3000 train_loss: 155.32501220703125 test_loss:1070.590576171875\n",
      "370/3000 train_loss: 142.90045166015625 test_loss:1053.3857421875\n",
      "371/3000 train_loss: 140.60194396972656 test_loss:1021.79638671875\n",
      "372/3000 train_loss: 167.5278778076172 test_loss:1102.581787109375\n",
      "373/3000 train_loss: 153.1800994873047 test_loss:1066.84326171875\n",
      "374/3000 train_loss: 134.822021484375 test_loss:1052.6826171875\n",
      "375/3000 train_loss: 137.2415313720703 test_loss:1029.513671875\n",
      "376/3000 train_loss: 134.27496337890625 test_loss:1047.478271484375\n",
      "377/3000 train_loss: 129.8591766357422 test_loss:1035.0284423828125\n",
      "378/3000 train_loss: 128.1492919921875 test_loss:1018.093505859375\n",
      "379/3000 train_loss: 133.01068115234375 test_loss:1043.7481689453125\n",
      "380/3000 train_loss: 139.23504638671875 test_loss:1029.7095947265625\n",
      "381/3000 train_loss: 132.283935546875 test_loss:1065.5556640625\n",
      "382/3000 train_loss: 144.61636352539062 test_loss:1069.827880859375\n",
      "383/3000 train_loss: 136.36737060546875 test_loss:1061.373291015625\n",
      "384/3000 train_loss: 135.24432373046875 test_loss:1063.4444580078125\n",
      "385/3000 train_loss: 131.7125701904297 test_loss:1059.05810546875\n",
      "386/3000 train_loss: 135.71780395507812 test_loss:1075.9044189453125\n",
      "387/3000 train_loss: 164.60894775390625 test_loss:1237.60888671875\n",
      "388/3000 train_loss: 183.22915649414062 test_loss:1300.576416015625\n",
      "389/3000 train_loss: 140.79278564453125 test_loss:1193.169189453125\n",
      "390/3000 train_loss: 147.4073028564453 test_loss:1117.7933349609375\n",
      "391/3000 train_loss: 132.1863250732422 test_loss:1122.0323486328125\n",
      "392/3000 train_loss: 132.5292205810547 test_loss:1130.2958984375\n",
      "393/3000 train_loss: 125.92201232910156 test_loss:1093.0477294921875\n",
      "394/3000 train_loss: 140.47349548339844 test_loss:1093.2652587890625\n",
      "395/3000 train_loss: 126.6497573852539 test_loss:1079.70947265625\n",
      "396/3000 train_loss: 132.2874298095703 test_loss:1070.4818115234375\n",
      "397/3000 train_loss: 129.08224487304688 test_loss:1113.685546875\n",
      "398/3000 train_loss: 125.66163635253906 test_loss:1066.9599609375\n",
      "399/3000 train_loss: 130.88204956054688 test_loss:1061.0947265625\n",
      "400/3000 train_loss: 137.2041473388672 test_loss:1052.570068359375\n",
      "401/3000 train_loss: 131.1060791015625 test_loss:1026.4649658203125\n",
      "402/3000 train_loss: 126.79002380371094 test_loss:1066.0521240234375\n",
      "403/3000 train_loss: 120.8750991821289 test_loss:1054.788330078125\n",
      "404/3000 train_loss: 125.05486297607422 test_loss:1063.56005859375\n",
      "405/3000 train_loss: 135.14022827148438 test_loss:1065.4697265625\n",
      "406/3000 train_loss: 125.56012725830078 test_loss:1091.736572265625\n",
      "407/3000 train_loss: 123.3411865234375 test_loss:1071.576416015625\n",
      "408/3000 train_loss: 126.83642578125 test_loss:1080.862060546875\n",
      "409/3000 train_loss: 114.71317291259766 test_loss:1037.1846923828125\n",
      "410/3000 train_loss: 126.9004898071289 test_loss:1063.135986328125\n",
      "411/3000 train_loss: 130.41363525390625 test_loss:1101.730712890625\n",
      "412/3000 train_loss: 142.5113067626953 test_loss:1062.796875\n",
      "413/3000 train_loss: 126.18282318115234 test_loss:1100.30322265625\n",
      "414/3000 train_loss: 129.99411010742188 test_loss:1072.6885986328125\n",
      "415/3000 train_loss: 134.35235595703125 test_loss:1064.304931640625\n",
      "416/3000 train_loss: 138.31431579589844 test_loss:1075.705322265625\n",
      "417/3000 train_loss: 123.04649353027344 test_loss:1047.822509765625\n",
      "418/3000 train_loss: 126.93513488769531 test_loss:1064.2213134765625\n",
      "419/3000 train_loss: 123.23224639892578 test_loss:1052.2572021484375\n",
      "420/3000 train_loss: 120.84871673583984 test_loss:1076.7310791015625\n",
      "421/3000 train_loss: 120.09663391113281 test_loss:1060.0712890625\n",
      "422/3000 train_loss: 119.6150894165039 test_loss:1036.824951171875\n",
      "423/3000 train_loss: 120.96331024169922 test_loss:1051.0380859375\n",
      "424/3000 train_loss: 115.33422088623047 test_loss:1084.85205078125\n",
      "425/3000 train_loss: 129.43417358398438 test_loss:1071.6297607421875\n",
      "426/3000 train_loss: 114.1561279296875 test_loss:1060.398681640625\n",
      "427/3000 train_loss: 139.32821655273438 test_loss:1059.65771484375\n",
      "428/3000 train_loss: 122.09309387207031 test_loss:1089.53662109375\n",
      "429/3000 train_loss: 127.08467102050781 test_loss:1080.264892578125\n",
      "430/3000 train_loss: 116.19602966308594 test_loss:1064.995361328125\n",
      "431/3000 train_loss: 111.57796478271484 test_loss:1057.2147216796875\n",
      "432/3000 train_loss: 118.92610931396484 test_loss:1063.469970703125\n",
      "433/3000 train_loss: 123.48128509521484 test_loss:1092.96533203125\n",
      "434/3000 train_loss: 116.43447875976562 test_loss:1057.208251953125\n",
      "435/3000 train_loss: 125.29965209960938 test_loss:1087.708740234375\n",
      "436/3000 train_loss: 131.61111450195312 test_loss:1061.5760498046875\n",
      "437/3000 train_loss: 118.44857788085938 test_loss:1051.68115234375\n",
      "438/3000 train_loss: 110.21135711669922 test_loss:1063.4698486328125\n",
      "439/3000 train_loss: 111.1677474975586 test_loss:1050.768310546875\n",
      "440/3000 train_loss: 123.64349365234375 test_loss:1022.4691162109375\n",
      "441/3000 train_loss: 118.4993896484375 test_loss:1074.995849609375\n",
      "442/3000 train_loss: 106.698974609375 test_loss:1049.91650390625\n",
      "443/3000 train_loss: 120.0129165649414 test_loss:1042.0054931640625\n",
      "444/3000 train_loss: 105.74092864990234 test_loss:1091.9154052734375\n",
      "445/3000 train_loss: 121.20796966552734 test_loss:1076.624267578125\n",
      "446/3000 train_loss: 122.86502838134766 test_loss:1116.5489501953125\n",
      "447/3000 train_loss: 121.25797271728516 test_loss:1088.6666259765625\n",
      "448/3000 train_loss: 125.08406066894531 test_loss:1050.9210205078125\n",
      "449/3000 train_loss: 112.28843688964844 test_loss:1054.11279296875\n",
      "450/3000 train_loss: 113.69451141357422 test_loss:1072.1732177734375\n",
      "451/3000 train_loss: 106.94631958007812 test_loss:1035.5966796875\n",
      "452/3000 train_loss: 130.18563842773438 test_loss:1080.9061279296875\n",
      "453/3000 train_loss: 112.81983184814453 test_loss:1099.873291015625\n",
      "454/3000 train_loss: 130.6595916748047 test_loss:1064.058837890625\n",
      "455/3000 train_loss: 119.62960815429688 test_loss:1075.149658203125\n",
      "456/3000 train_loss: 123.83634948730469 test_loss:1069.3505859375\n",
      "457/3000 train_loss: 113.06461334228516 test_loss:1069.970458984375\n",
      "458/3000 train_loss: 126.164306640625 test_loss:1074.6273193359375\n",
      "459/3000 train_loss: 112.0755386352539 test_loss:1023.7109375\n",
      "460/3000 train_loss: 114.56288146972656 test_loss:1075.1671142578125\n",
      "461/3000 train_loss: 109.16594696044922 test_loss:1031.6043701171875\n",
      "462/3000 train_loss: 116.73258209228516 test_loss:1093.64111328125\n",
      "463/3000 train_loss: 114.53821563720703 test_loss:1039.616943359375\n",
      "464/3000 train_loss: 112.11724853515625 test_loss:1058.834228515625\n",
      "465/3000 train_loss: 117.10615539550781 test_loss:1037.50341796875\n",
      "466/3000 train_loss: 121.64742279052734 test_loss:1055.1585693359375\n",
      "467/3000 train_loss: 108.3428955078125 test_loss:1074.1324462890625\n",
      "468/3000 train_loss: 110.93338012695312 test_loss:1065.1092529296875\n",
      "469/3000 train_loss: 107.86332702636719 test_loss:1056.415283203125\n",
      "470/3000 train_loss: 120.67471313476562 test_loss:1078.724853515625\n",
      "471/3000 train_loss: 113.65081787109375 test_loss:1059.886962890625\n",
      "472/3000 train_loss: 118.1106948852539 test_loss:1075.259521484375\n",
      "473/3000 train_loss: 127.22476959228516 test_loss:1080.260498046875\n",
      "474/3000 train_loss: 121.17375946044922 test_loss:1052.911865234375\n",
      "475/3000 train_loss: 116.18296813964844 test_loss:1081.125\n",
      "476/3000 train_loss: 112.76726531982422 test_loss:1065.472412109375\n",
      "477/3000 train_loss: 108.68612670898438 test_loss:1100.859130859375\n",
      "478/3000 train_loss: 114.86231994628906 test_loss:1046.8763427734375\n",
      "479/3000 train_loss: 101.63086700439453 test_loss:1070.906005859375\n",
      "480/3000 train_loss: 111.76085662841797 test_loss:1105.44287109375\n",
      "481/3000 train_loss: 117.03666687011719 test_loss:1039.7489013671875\n",
      "482/3000 train_loss: 114.5566635131836 test_loss:1062.3746337890625\n",
      "483/3000 train_loss: 109.88053894042969 test_loss:1077.7431640625\n",
      "484/3000 train_loss: 104.96217346191406 test_loss:1096.244873046875\n",
      "485/3000 train_loss: 110.07791900634766 test_loss:1094.5289306640625\n",
      "486/3000 train_loss: 111.78303527832031 test_loss:1062.315185546875\n",
      "487/3000 train_loss: 112.14749145507812 test_loss:1096.17626953125\n",
      "488/3000 train_loss: 113.86026000976562 test_loss:1111.4185791015625\n",
      "489/3000 train_loss: 124.10342407226562 test_loss:1070.409912109375\n",
      "490/3000 train_loss: 111.71368408203125 test_loss:1092.5751953125\n",
      "491/3000 train_loss: 113.20919799804688 test_loss:1064.96533203125\n",
      "492/3000 train_loss: 115.50050354003906 test_loss:1113.236572265625\n",
      "493/3000 train_loss: 112.26361846923828 test_loss:1079.62451171875\n",
      "494/3000 train_loss: 101.60798645019531 test_loss:1088.1083984375\n",
      "495/3000 train_loss: 102.79359436035156 test_loss:1065.7841796875\n",
      "496/3000 train_loss: 99.54228973388672 test_loss:1060.06884765625\n",
      "497/3000 train_loss: 111.2770767211914 test_loss:1087.1947021484375\n",
      "498/3000 train_loss: 105.208251953125 test_loss:1058.30029296875\n",
      "499/3000 train_loss: 104.90152740478516 test_loss:1050.6221923828125\n",
      "500/3000 train_loss: 115.66297912597656 test_loss:1110.7818603515625\n",
      "501/3000 train_loss: 107.77470397949219 test_loss:1078.1385498046875\n",
      "502/3000 train_loss: 97.93797302246094 test_loss:1075.8175048828125\n",
      "503/3000 train_loss: 105.6684799194336 test_loss:1068.5146484375\n",
      "504/3000 train_loss: 102.26654052734375 test_loss:1059.276611328125\n",
      "505/3000 train_loss: 107.38496398925781 test_loss:1082.791259765625\n",
      "506/3000 train_loss: 111.74066925048828 test_loss:1079.770263671875\n",
      "507/3000 train_loss: 111.71430969238281 test_loss:1083.968994140625\n",
      "508/3000 train_loss: 103.32356262207031 test_loss:1069.046630859375\n",
      "509/3000 train_loss: 103.61918640136719 test_loss:1100.418701171875\n",
      "510/3000 train_loss: 104.43402099609375 test_loss:1078.338134765625\n",
      "511/3000 train_loss: 119.27017211914062 test_loss:1052.6429443359375\n",
      "512/3000 train_loss: 105.03153991699219 test_loss:1089.8162841796875\n",
      "513/3000 train_loss: 98.21566009521484 test_loss:1069.4677734375\n",
      "514/3000 train_loss: 98.88906860351562 test_loss:1085.1024169921875\n",
      "515/3000 train_loss: 101.112548828125 test_loss:1117.161376953125\n",
      "516/3000 train_loss: 101.19915008544922 test_loss:1109.694091796875\n",
      "517/3000 train_loss: 97.90369415283203 test_loss:1127.86669921875\n",
      "518/3000 train_loss: 96.21593475341797 test_loss:1129.3087158203125\n",
      "519/3000 train_loss: 102.35711669921875 test_loss:1111.1075439453125\n",
      "520/3000 train_loss: 94.549072265625 test_loss:1127.992431640625\n",
      "521/3000 train_loss: 106.40441131591797 test_loss:1117.414794921875\n",
      "522/3000 train_loss: 103.78993225097656 test_loss:1102.9456787109375\n",
      "523/3000 train_loss: 98.55516052246094 test_loss:1092.5428466796875\n",
      "524/3000 train_loss: 106.740478515625 test_loss:1110.137451171875\n",
      "525/3000 train_loss: 102.04129028320312 test_loss:1103.7823486328125\n",
      "526/3000 train_loss: 104.3248062133789 test_loss:1084.558349609375\n",
      "527/3000 train_loss: 98.55976867675781 test_loss:1101.9205322265625\n",
      "528/3000 train_loss: 116.1461181640625 test_loss:1093.1236572265625\n",
      "529/3000 train_loss: 100.2445297241211 test_loss:1075.147705078125\n",
      "530/3000 train_loss: 96.74224853515625 test_loss:1057.90673828125\n",
      "531/3000 train_loss: 109.9135971069336 test_loss:1140.2335205078125\n",
      "532/3000 train_loss: 100.59673309326172 test_loss:1135.27392578125\n",
      "533/3000 train_loss: 102.32718658447266 test_loss:1136.3310546875\n",
      "534/3000 train_loss: 96.67101287841797 test_loss:1094.7109375\n",
      "535/3000 train_loss: 98.23151397705078 test_loss:1141.1416015625\n",
      "536/3000 train_loss: 101.8038101196289 test_loss:1129.1439208984375\n",
      "537/3000 train_loss: 97.48505401611328 test_loss:1075.0872802734375\n",
      "538/3000 train_loss: 92.93478393554688 test_loss:1100.7723388671875\n",
      "539/3000 train_loss: 88.66845703125 test_loss:1132.45361328125\n",
      "540/3000 train_loss: 107.72420501708984 test_loss:1106.2215576171875\n",
      "541/3000 train_loss: 88.23274993896484 test_loss:1088.0164794921875\n",
      "542/3000 train_loss: 94.20172882080078 test_loss:1116.3050537109375\n",
      "543/3000 train_loss: 94.73898315429688 test_loss:1060.519287109375\n",
      "544/3000 train_loss: 95.01322937011719 test_loss:1072.4071044921875\n",
      "545/3000 train_loss: 104.29640197753906 test_loss:1119.963623046875\n",
      "546/3000 train_loss: 102.62317657470703 test_loss:1112.17236328125\n",
      "547/3000 train_loss: 98.01060485839844 test_loss:1161.4573974609375\n",
      "548/3000 train_loss: 106.18354034423828 test_loss:1160.91162109375\n",
      "549/3000 train_loss: 126.77652740478516 test_loss:1154.0787353515625\n",
      "550/3000 train_loss: 99.83560180664062 test_loss:1158.7421875\n",
      "551/3000 train_loss: 93.4144515991211 test_loss:1160.22021484375\n",
      "552/3000 train_loss: 114.67337036132812 test_loss:1189.7969970703125\n",
      "553/3000 train_loss: 93.24195861816406 test_loss:1146.7451171875\n",
      "554/3000 train_loss: 106.3353500366211 test_loss:1171.6673583984375\n",
      "555/3000 train_loss: 102.25393676757812 test_loss:1143.482177734375\n",
      "556/3000 train_loss: 88.88697814941406 test_loss:1181.90625\n",
      "557/3000 train_loss: 90.44801330566406 test_loss:1125.4449462890625\n",
      "558/3000 train_loss: 99.28451538085938 test_loss:1196.4791259765625\n",
      "559/3000 train_loss: 86.83565521240234 test_loss:1181.8671875\n",
      "560/3000 train_loss: 96.59364318847656 test_loss:1135.8651123046875\n",
      "561/3000 train_loss: 89.48049926757812 test_loss:1113.230224609375\n",
      "562/3000 train_loss: 89.79164123535156 test_loss:1129.218017578125\n",
      "563/3000 train_loss: 89.75269317626953 test_loss:1162.699951171875\n",
      "564/3000 train_loss: 94.83185577392578 test_loss:1118.510498046875\n",
      "565/3000 train_loss: 88.9263687133789 test_loss:1169.7451171875\n",
      "566/3000 train_loss: 97.12369537353516 test_loss:1121.47998046875\n",
      "567/3000 train_loss: 87.80928039550781 test_loss:1148.2642822265625\n",
      "568/3000 train_loss: 107.46468353271484 test_loss:1168.9998779296875\n",
      "569/3000 train_loss: 89.73094940185547 test_loss:1148.21142578125\n",
      "570/3000 train_loss: 89.97879791259766 test_loss:1143.73583984375\n",
      "571/3000 train_loss: 95.22929382324219 test_loss:1185.17822265625\n",
      "572/3000 train_loss: 92.33563232421875 test_loss:1169.5770263671875\n",
      "573/3000 train_loss: 88.1460952758789 test_loss:1196.033203125\n",
      "574/3000 train_loss: 87.00894165039062 test_loss:1173.6624755859375\n",
      "575/3000 train_loss: 110.56329345703125 test_loss:1235.91650390625\n",
      "576/3000 train_loss: 91.02307891845703 test_loss:1143.8385009765625\n",
      "577/3000 train_loss: 98.89608764648438 test_loss:1165.22265625\n",
      "578/3000 train_loss: 84.65839385986328 test_loss:1176.00341796875\n",
      "579/3000 train_loss: 91.15497589111328 test_loss:1177.999755859375\n",
      "580/3000 train_loss: 87.9355697631836 test_loss:1132.1778564453125\n",
      "581/3000 train_loss: 97.02742004394531 test_loss:1178.7252197265625\n",
      "582/3000 train_loss: 86.20365142822266 test_loss:1153.0103759765625\n",
      "583/3000 train_loss: 87.08798217773438 test_loss:1143.650634765625\n",
      "584/3000 train_loss: 110.01547241210938 test_loss:1162.4893798828125\n",
      "585/3000 train_loss: 93.33390045166016 test_loss:1130.8262939453125\n",
      "586/3000 train_loss: 92.23218536376953 test_loss:1141.716796875\n",
      "587/3000 train_loss: 85.2447738647461 test_loss:1116.458740234375\n",
      "588/3000 train_loss: 98.59475708007812 test_loss:1137.591552734375\n",
      "589/3000 train_loss: 75.95127868652344 test_loss:1109.2659912109375\n",
      "590/3000 train_loss: 90.45806884765625 test_loss:1144.736328125\n",
      "591/3000 train_loss: 83.40499877929688 test_loss:1109.9769287109375\n",
      "592/3000 train_loss: 92.03453826904297 test_loss:1148.15380859375\n",
      "593/3000 train_loss: 90.60240936279297 test_loss:1145.6541748046875\n",
      "594/3000 train_loss: 84.91612243652344 test_loss:1133.543701171875\n",
      "595/3000 train_loss: 87.28318786621094 test_loss:1143.4188232421875\n",
      "596/3000 train_loss: 83.52499389648438 test_loss:1142.31884765625\n",
      "597/3000 train_loss: 81.1358413696289 test_loss:1156.119140625\n",
      "598/3000 train_loss: 94.31678009033203 test_loss:1134.7899169921875\n",
      "599/3000 train_loss: 94.28205108642578 test_loss:1182.585693359375\n",
      "600/3000 train_loss: 93.3317642211914 test_loss:1183.02783203125\n",
      "601/3000 train_loss: 82.96998596191406 test_loss:1153.765380859375\n",
      "602/3000 train_loss: 83.24508666992188 test_loss:1166.6468505859375\n",
      "603/3000 train_loss: 89.0943603515625 test_loss:1180.205322265625\n",
      "604/3000 train_loss: 85.86688232421875 test_loss:1166.1575927734375\n",
      "605/3000 train_loss: 95.73603057861328 test_loss:1154.81005859375\n",
      "606/3000 train_loss: 87.35737609863281 test_loss:1177.589599609375\n",
      "607/3000 train_loss: 82.5994644165039 test_loss:1171.083740234375\n",
      "608/3000 train_loss: 87.7084732055664 test_loss:1180.974609375\n",
      "609/3000 train_loss: 95.37398529052734 test_loss:1133.2144775390625\n",
      "610/3000 train_loss: 80.0213851928711 test_loss:1155.7652587890625\n",
      "611/3000 train_loss: 84.56484985351562 test_loss:1178.3294677734375\n",
      "612/3000 train_loss: 84.46292114257812 test_loss:1168.832763671875\n",
      "613/3000 train_loss: 80.48308563232422 test_loss:1182.8345947265625\n",
      "614/3000 train_loss: 87.40489196777344 test_loss:1178.50390625\n",
      "615/3000 train_loss: 84.25379943847656 test_loss:1160.977783203125\n",
      "616/3000 train_loss: 76.35828399658203 test_loss:1200.5982666015625\n",
      "617/3000 train_loss: 77.34717559814453 test_loss:1159.7740478515625\n",
      "618/3000 train_loss: 93.79122161865234 test_loss:1256.2303466796875\n",
      "619/3000 train_loss: 96.43929290771484 test_loss:1257.3116455078125\n",
      "620/3000 train_loss: 84.12371063232422 test_loss:1213.598388671875\n",
      "621/3000 train_loss: 84.2910385131836 test_loss:1253.757568359375\n",
      "622/3000 train_loss: 84.5650405883789 test_loss:1213.2265625\n",
      "623/3000 train_loss: 93.94296264648438 test_loss:1219.826904296875\n",
      "624/3000 train_loss: 77.55992889404297 test_loss:1221.985595703125\n",
      "625/3000 train_loss: 86.17932891845703 test_loss:1229.4130859375\n",
      "626/3000 train_loss: 86.01919555664062 test_loss:1187.5677490234375\n",
      "627/3000 train_loss: 89.1414794921875 test_loss:1196.47705078125\n",
      "628/3000 train_loss: 76.2803955078125 test_loss:1174.31298828125\n",
      "629/3000 train_loss: 88.57496643066406 test_loss:1229.4248046875\n",
      "630/3000 train_loss: 76.74690246582031 test_loss:1217.164306640625\n",
      "631/3000 train_loss: 84.08834075927734 test_loss:1194.41357421875\n",
      "632/3000 train_loss: 79.71389770507812 test_loss:1143.763671875\n",
      "633/3000 train_loss: 90.85900115966797 test_loss:1128.496826171875\n",
      "634/3000 train_loss: 92.82147979736328 test_loss:1199.2955322265625\n",
      "635/3000 train_loss: 88.99813079833984 test_loss:1177.614501953125\n",
      "636/3000 train_loss: 81.19559478759766 test_loss:1180.51513671875\n",
      "637/3000 train_loss: 77.44318389892578 test_loss:1170.1898193359375\n",
      "638/3000 train_loss: 111.33589935302734 test_loss:1252.6417236328125\n",
      "639/3000 train_loss: 91.5772705078125 test_loss:1177.87353515625\n",
      "640/3000 train_loss: 88.64542388916016 test_loss:1195.5517578125\n",
      "641/3000 train_loss: 78.97328186035156 test_loss:1210.305419921875\n",
      "642/3000 train_loss: 83.95065307617188 test_loss:1227.95947265625\n",
      "643/3000 train_loss: 73.0326156616211 test_loss:1193.9560546875\n",
      "644/3000 train_loss: 89.6841812133789 test_loss:1208.11572265625\n",
      "645/3000 train_loss: 78.3022232055664 test_loss:1182.2408447265625\n",
      "646/3000 train_loss: 82.44385528564453 test_loss:1179.1177978515625\n",
      "647/3000 train_loss: 76.95625305175781 test_loss:1159.6273193359375\n",
      "648/3000 train_loss: 81.35505676269531 test_loss:1209.680908203125\n",
      "649/3000 train_loss: 87.56807708740234 test_loss:1172.898193359375\n",
      "650/3000 train_loss: 87.4227294921875 test_loss:1237.7064208984375\n",
      "651/3000 train_loss: 87.57669067382812 test_loss:1162.365234375\n",
      "652/3000 train_loss: 95.14575958251953 test_loss:1144.3189697265625\n",
      "653/3000 train_loss: 83.67710876464844 test_loss:1167.5771484375\n",
      "654/3000 train_loss: 92.96220397949219 test_loss:1200.1845703125\n",
      "655/3000 train_loss: 90.57915496826172 test_loss:1230.4537353515625\n",
      "656/3000 train_loss: 78.99799346923828 test_loss:1195.4781494140625\n",
      "657/3000 train_loss: 75.31229400634766 test_loss:1200.23193359375\n",
      "658/3000 train_loss: 70.91130065917969 test_loss:1158.7354736328125\n",
      "659/3000 train_loss: 73.45409393310547 test_loss:1154.06494140625\n",
      "660/3000 train_loss: 90.25202178955078 test_loss:1166.08740234375\n",
      "661/3000 train_loss: 78.17674255371094 test_loss:1204.5010986328125\n",
      "662/3000 train_loss: 79.6495132446289 test_loss:1231.2003173828125\n",
      "663/3000 train_loss: 78.0401382446289 test_loss:1200.9215087890625\n",
      "664/3000 train_loss: 76.99717712402344 test_loss:1199.18505859375\n",
      "665/3000 train_loss: 87.69453430175781 test_loss:1179.7364501953125\n",
      "666/3000 train_loss: 89.88446807861328 test_loss:1190.853271484375\n",
      "667/3000 train_loss: 73.01517486572266 test_loss:1163.3331298828125\n",
      "668/3000 train_loss: 76.95469665527344 test_loss:1171.171875\n",
      "669/3000 train_loss: 74.87451934814453 test_loss:1184.15283203125\n",
      "670/3000 train_loss: 78.37959289550781 test_loss:1183.55712890625\n",
      "671/3000 train_loss: 78.79222106933594 test_loss:1146.5528564453125\n",
      "672/3000 train_loss: 75.64549255371094 test_loss:1198.865234375\n",
      "673/3000 train_loss: 81.84571075439453 test_loss:1140.914306640625\n",
      "674/3000 train_loss: 87.74179077148438 test_loss:1170.937744140625\n",
      "675/3000 train_loss: 77.78380584716797 test_loss:1166.6429443359375\n",
      "676/3000 train_loss: 72.64845275878906 test_loss:1150.210205078125\n",
      "677/3000 train_loss: 75.85379028320312 test_loss:1200.955078125\n",
      "678/3000 train_loss: 70.66328430175781 test_loss:1158.7655029296875\n",
      "679/3000 train_loss: 76.44373321533203 test_loss:1172.6494140625\n",
      "680/3000 train_loss: 85.59331512451172 test_loss:1154.3740234375\n",
      "681/3000 train_loss: 77.9754867553711 test_loss:1123.4107666015625\n",
      "682/3000 train_loss: 92.02186584472656 test_loss:1156.95703125\n",
      "683/3000 train_loss: 85.44276428222656 test_loss:1136.9903564453125\n",
      "684/3000 train_loss: 78.44476318359375 test_loss:1135.116455078125\n",
      "685/3000 train_loss: 78.13639831542969 test_loss:1156.451171875\n",
      "686/3000 train_loss: 89.07170104980469 test_loss:1169.900390625\n",
      "687/3000 train_loss: 82.50741577148438 test_loss:1142.1856689453125\n",
      "688/3000 train_loss: 72.05300903320312 test_loss:1135.615966796875\n",
      "689/3000 train_loss: 78.59469604492188 test_loss:1149.3919677734375\n",
      "690/3000 train_loss: 83.57782745361328 test_loss:1110.558349609375\n",
      "691/3000 train_loss: 76.48654174804688 test_loss:1147.520263671875\n",
      "692/3000 train_loss: 86.05101013183594 test_loss:1135.0234375\n",
      "693/3000 train_loss: 75.60342407226562 test_loss:1178.163330078125\n",
      "694/3000 train_loss: 81.48776245117188 test_loss:1210.4354248046875\n",
      "695/3000 train_loss: 92.73096466064453 test_loss:1242.354248046875\n",
      "696/3000 train_loss: 84.3158950805664 test_loss:1190.8621826171875\n",
      "697/3000 train_loss: 76.34046936035156 test_loss:1177.6190185546875\n",
      "698/3000 train_loss: 76.05079650878906 test_loss:1183.811279296875\n",
      "699/3000 train_loss: 86.73097229003906 test_loss:1195.5439453125\n",
      "700/3000 train_loss: 70.59062194824219 test_loss:1185.116455078125\n",
      "701/3000 train_loss: 73.70266723632812 test_loss:1195.3463134765625\n",
      "702/3000 train_loss: 75.2866439819336 test_loss:1154.155029296875\n",
      "703/3000 train_loss: 75.21669006347656 test_loss:1189.889404296875\n",
      "704/3000 train_loss: 80.04071044921875 test_loss:1153.56494140625\n",
      "705/3000 train_loss: 73.10292053222656 test_loss:1170.06591796875\n",
      "706/3000 train_loss: 78.91991424560547 test_loss:1206.891357421875\n",
      "707/3000 train_loss: 81.4312515258789 test_loss:1174.6485595703125\n",
      "708/3000 train_loss: 72.53043365478516 test_loss:1154.423828125\n",
      "709/3000 train_loss: 82.178955078125 test_loss:1157.2830810546875\n",
      "710/3000 train_loss: 71.12543487548828 test_loss:1149.794677734375\n",
      "711/3000 train_loss: 75.53547668457031 test_loss:1149.61572265625\n",
      "712/3000 train_loss: 74.19090270996094 test_loss:1174.7286376953125\n",
      "713/3000 train_loss: 75.63463592529297 test_loss:1133.407470703125\n",
      "714/3000 train_loss: 75.68826293945312 test_loss:1185.594970703125\n",
      "715/3000 train_loss: 74.84918975830078 test_loss:1166.4698486328125\n",
      "716/3000 train_loss: 69.01087951660156 test_loss:1167.4202880859375\n",
      "717/3000 train_loss: 83.50446319580078 test_loss:1150.9454345703125\n",
      "718/3000 train_loss: 80.43759155273438 test_loss:1154.0823974609375\n",
      "719/3000 train_loss: 86.32424926757812 test_loss:1205.4464111328125\n",
      "720/3000 train_loss: 66.71958923339844 test_loss:1206.922119140625\n",
      "721/3000 train_loss: 73.70216369628906 test_loss:1191.9683837890625\n",
      "722/3000 train_loss: 74.5326156616211 test_loss:1149.27490234375\n",
      "723/3000 train_loss: 78.90766143798828 test_loss:1177.910888671875\n",
      "724/3000 train_loss: 76.94923400878906 test_loss:1176.67236328125\n",
      "725/3000 train_loss: 79.73208618164062 test_loss:1168.027587890625\n",
      "726/3000 train_loss: 78.18289947509766 test_loss:1133.36474609375\n",
      "727/3000 train_loss: 73.02450561523438 test_loss:1151.629638671875\n",
      "728/3000 train_loss: 71.16700744628906 test_loss:1144.653564453125\n",
      "729/3000 train_loss: 78.65895080566406 test_loss:1158.901123046875\n",
      "730/3000 train_loss: 68.39903259277344 test_loss:1177.732421875\n",
      "731/3000 train_loss: 75.4444580078125 test_loss:1204.940673828125\n",
      "732/3000 train_loss: 76.13594818115234 test_loss:1142.58447265625\n",
      "733/3000 train_loss: 74.34100341796875 test_loss:1168.904296875\n",
      "734/3000 train_loss: 73.59099578857422 test_loss:1169.915771484375\n",
      "735/3000 train_loss: 68.77147674560547 test_loss:1167.6026611328125\n",
      "736/3000 train_loss: 72.77857971191406 test_loss:1179.2879638671875\n",
      "737/3000 train_loss: 75.30931091308594 test_loss:1155.400634765625\n",
      "738/3000 train_loss: 71.77330017089844 test_loss:1152.635498046875\n",
      "739/3000 train_loss: 78.28736877441406 test_loss:1132.2442626953125\n",
      "740/3000 train_loss: 71.11797332763672 test_loss:1195.0355224609375\n",
      "741/3000 train_loss: 72.80003356933594 test_loss:1173.806884765625\n",
      "742/3000 train_loss: 71.75335693359375 test_loss:1142.48388671875\n",
      "743/3000 train_loss: 68.10737609863281 test_loss:1153.7548828125\n",
      "744/3000 train_loss: 64.71736907958984 test_loss:1184.090576171875\n",
      "745/3000 train_loss: 77.03318786621094 test_loss:1132.519775390625\n",
      "746/3000 train_loss: 69.09872436523438 test_loss:1147.8525390625\n",
      "747/3000 train_loss: 74.34690856933594 test_loss:1116.09765625\n",
      "748/3000 train_loss: 76.32451629638672 test_loss:1161.0772705078125\n",
      "749/3000 train_loss: 73.0635986328125 test_loss:1108.9208984375\n",
      "750/3000 train_loss: 69.49700164794922 test_loss:1160.377197265625\n",
      "751/3000 train_loss: 67.38304901123047 test_loss:1153.735107421875\n",
      "752/3000 train_loss: 66.22529602050781 test_loss:1154.6932373046875\n",
      "753/3000 train_loss: 62.34731674194336 test_loss:1194.269287109375\n",
      "754/3000 train_loss: 71.00859069824219 test_loss:1162.735107421875\n",
      "755/3000 train_loss: 94.24687194824219 test_loss:1173.03369140625\n",
      "756/3000 train_loss: 72.11254119873047 test_loss:1168.6474609375\n",
      "757/3000 train_loss: 67.13606262207031 test_loss:1142.666015625\n",
      "758/3000 train_loss: 76.59949493408203 test_loss:1132.2523193359375\n",
      "759/3000 train_loss: 76.73917388916016 test_loss:1111.25341796875\n",
      "760/3000 train_loss: 72.52495574951172 test_loss:1156.97412109375\n",
      "761/3000 train_loss: 72.4738998413086 test_loss:1106.16357421875\n",
      "762/3000 train_loss: 72.09335327148438 test_loss:1123.9144287109375\n",
      "763/3000 train_loss: 70.15652465820312 test_loss:1117.4676513671875\n",
      "764/3000 train_loss: 68.72457122802734 test_loss:1141.9833984375\n",
      "765/3000 train_loss: 81.25655364990234 test_loss:1130.974365234375\n",
      "766/3000 train_loss: 67.81913757324219 test_loss:1180.49365234375\n",
      "767/3000 train_loss: 72.49915313720703 test_loss:1128.9581298828125\n",
      "768/3000 train_loss: 67.51290893554688 test_loss:1115.6644287109375\n",
      "769/3000 train_loss: 72.98056030273438 test_loss:1129.4599609375\n",
      "770/3000 train_loss: 73.2520523071289 test_loss:1117.650146484375\n",
      "771/3000 train_loss: 69.00474548339844 test_loss:1095.8363037109375\n",
      "772/3000 train_loss: 65.05696105957031 test_loss:1130.151611328125\n",
      "773/3000 train_loss: 70.4694595336914 test_loss:1164.4715576171875\n",
      "774/3000 train_loss: 73.45680236816406 test_loss:1131.692138671875\n",
      "775/3000 train_loss: 69.51909637451172 test_loss:1117.914794921875\n",
      "776/3000 train_loss: 74.90803527832031 test_loss:1205.3106689453125\n",
      "777/3000 train_loss: 78.4433822631836 test_loss:1181.177490234375\n",
      "778/3000 train_loss: 71.29027557373047 test_loss:1179.1151123046875\n",
      "779/3000 train_loss: 73.40315246582031 test_loss:1178.7696533203125\n",
      "780/3000 train_loss: 72.9280014038086 test_loss:1207.4844970703125\n",
      "781/3000 train_loss: 81.17829895019531 test_loss:1209.000732421875\n",
      "782/3000 train_loss: 79.26910400390625 test_loss:1232.0765380859375\n",
      "783/3000 train_loss: 84.59656524658203 test_loss:1167.674072265625\n",
      "784/3000 train_loss: 79.67138671875 test_loss:1188.7589111328125\n",
      "785/3000 train_loss: 70.45535278320312 test_loss:1200.7198486328125\n",
      "786/3000 train_loss: 77.72783660888672 test_loss:1106.729248046875\n",
      "787/3000 train_loss: 67.22516632080078 test_loss:1161.450439453125\n",
      "788/3000 train_loss: 74.38973999023438 test_loss:1136.170654296875\n",
      "789/3000 train_loss: 73.08528137207031 test_loss:1120.4639892578125\n",
      "790/3000 train_loss: 67.47180938720703 test_loss:1159.9638671875\n",
      "791/3000 train_loss: 74.53534698486328 test_loss:1152.718505859375\n",
      "792/3000 train_loss: 75.5637435913086 test_loss:1168.41650390625\n",
      "793/3000 train_loss: 74.51512908935547 test_loss:1126.3681640625\n",
      "794/3000 train_loss: 76.37401580810547 test_loss:1107.8310546875\n",
      "795/3000 train_loss: 72.54615783691406 test_loss:1169.25390625\n",
      "796/3000 train_loss: 69.76423645019531 test_loss:1194.4200439453125\n",
      "797/3000 train_loss: 70.72357177734375 test_loss:1152.0693359375\n",
      "798/3000 train_loss: 67.49883270263672 test_loss:1132.477294921875\n",
      "799/3000 train_loss: 72.08050537109375 test_loss:1140.27001953125\n",
      "800/3000 train_loss: 66.61451721191406 test_loss:1177.5174560546875\n",
      "801/3000 train_loss: 70.96363067626953 test_loss:1137.1285400390625\n",
      "802/3000 train_loss: 61.49390411376953 test_loss:1128.7784423828125\n",
      "803/3000 train_loss: 65.52843475341797 test_loss:1148.320068359375\n",
      "804/3000 train_loss: 67.13825988769531 test_loss:1099.9451904296875\n",
      "805/3000 train_loss: 70.43244171142578 test_loss:1105.50244140625\n",
      "806/3000 train_loss: 68.49079895019531 test_loss:1115.607421875\n",
      "807/3000 train_loss: 76.74208068847656 test_loss:1098.769775390625\n",
      "808/3000 train_loss: 69.42201232910156 test_loss:1120.56591796875\n",
      "809/3000 train_loss: 76.51492309570312 test_loss:1100.355712890625\n",
      "810/3000 train_loss: 74.20852661132812 test_loss:1110.6859130859375\n",
      "811/3000 train_loss: 80.62049865722656 test_loss:1163.93505859375\n",
      "812/3000 train_loss: 84.53803253173828 test_loss:1117.466064453125\n",
      "813/3000 train_loss: 66.26165008544922 test_loss:1125.06982421875\n",
      "814/3000 train_loss: 67.4510269165039 test_loss:1128.2952880859375\n",
      "815/3000 train_loss: 63.89043045043945 test_loss:1119.3013916015625\n",
      "816/3000 train_loss: 77.07816314697266 test_loss:1121.708740234375\n",
      "817/3000 train_loss: 65.26275634765625 test_loss:1126.110595703125\n",
      "818/3000 train_loss: 81.4661636352539 test_loss:1134.0550537109375\n",
      "819/3000 train_loss: 73.13569641113281 test_loss:1128.04296875\n",
      "820/3000 train_loss: 63.10396194458008 test_loss:1139.0252685546875\n",
      "821/3000 train_loss: 69.09642791748047 test_loss:1087.2470703125\n",
      "822/3000 train_loss: 61.715171813964844 test_loss:1120.5806884765625\n",
      "823/3000 train_loss: 73.17071533203125 test_loss:1105.9610595703125\n",
      "824/3000 train_loss: 67.76593780517578 test_loss:1070.228271484375\n",
      "825/3000 train_loss: 70.07208251953125 test_loss:1136.927734375\n",
      "826/3000 train_loss: 68.76719665527344 test_loss:1105.5234375\n",
      "827/3000 train_loss: 66.97830963134766 test_loss:1099.665771484375\n",
      "828/3000 train_loss: 74.41707611083984 test_loss:1110.2044677734375\n",
      "829/3000 train_loss: 65.98261260986328 test_loss:1111.882568359375\n",
      "830/3000 train_loss: 67.03160095214844 test_loss:1099.7646484375\n",
      "831/3000 train_loss: 71.66532135009766 test_loss:1125.3924560546875\n",
      "832/3000 train_loss: 68.8217544555664 test_loss:1138.7294921875\n",
      "833/3000 train_loss: 66.06956481933594 test_loss:1109.8460693359375\n",
      "834/3000 train_loss: 62.89352035522461 test_loss:1141.0531005859375\n",
      "835/3000 train_loss: 68.68445587158203 test_loss:1154.59814453125\n",
      "836/3000 train_loss: 65.188720703125 test_loss:1160.4390869140625\n",
      "837/3000 train_loss: 65.68290710449219 test_loss:1151.326904296875\n",
      "838/3000 train_loss: 65.06312561035156 test_loss:1114.482421875\n",
      "839/3000 train_loss: 68.03522491455078 test_loss:1133.936767578125\n",
      "840/3000 train_loss: 65.48304748535156 test_loss:1142.66943359375\n",
      "841/3000 train_loss: 69.06716918945312 test_loss:1142.969482421875\n",
      "842/3000 train_loss: 66.74536895751953 test_loss:1152.2208251953125\n",
      "843/3000 train_loss: 74.4656753540039 test_loss:1116.851318359375\n",
      "844/3000 train_loss: 66.00399017333984 test_loss:1150.86572265625\n",
      "845/3000 train_loss: 60.53252410888672 test_loss:1134.769287109375\n",
      "846/3000 train_loss: 64.81439208984375 test_loss:1133.5849609375\n",
      "847/3000 train_loss: 78.76546478271484 test_loss:1154.717529296875\n",
      "848/3000 train_loss: 74.7052230834961 test_loss:1073.926025390625\n",
      "849/3000 train_loss: 73.17633056640625 test_loss:1132.97509765625\n",
      "850/3000 train_loss: 71.74475860595703 test_loss:1161.721435546875\n",
      "851/3000 train_loss: 73.52964782714844 test_loss:1120.03466796875\n",
      "852/3000 train_loss: 62.99052047729492 test_loss:1118.06982421875\n",
      "853/3000 train_loss: 62.291778564453125 test_loss:1116.595458984375\n",
      "854/3000 train_loss: 65.47393798828125 test_loss:1126.275146484375\n",
      "855/3000 train_loss: 71.40314483642578 test_loss:1096.9501953125\n",
      "856/3000 train_loss: 69.21308898925781 test_loss:1111.9248046875\n",
      "857/3000 train_loss: 67.42728424072266 test_loss:1069.2633056640625\n",
      "858/3000 train_loss: 70.99028015136719 test_loss:1118.454345703125\n",
      "859/3000 train_loss: 70.20940399169922 test_loss:1115.2470703125\n",
      "860/3000 train_loss: 60.921913146972656 test_loss:1133.683349609375\n",
      "861/3000 train_loss: 70.84274291992188 test_loss:1103.5924072265625\n",
      "862/3000 train_loss: 72.6700439453125 test_loss:1117.6583251953125\n",
      "863/3000 train_loss: 67.2548599243164 test_loss:1145.8355712890625\n",
      "864/3000 train_loss: 66.48187255859375 test_loss:1072.8375244140625\n",
      "865/3000 train_loss: 63.22074508666992 test_loss:1095.0572509765625\n",
      "866/3000 train_loss: 65.38944244384766 test_loss:1107.623291015625\n",
      "867/3000 train_loss: 63.94987869262695 test_loss:1111.670166015625\n",
      "868/3000 train_loss: 68.05753326416016 test_loss:1105.906982421875\n",
      "869/3000 train_loss: 64.37511444091797 test_loss:1183.49267578125\n",
      "870/3000 train_loss: 69.7972640991211 test_loss:1121.19189453125\n",
      "871/3000 train_loss: 54.906768798828125 test_loss:1129.744873046875\n",
      "872/3000 train_loss: 68.07527160644531 test_loss:1134.1925048828125\n",
      "873/3000 train_loss: 63.86818313598633 test_loss:1107.2930908203125\n",
      "874/3000 train_loss: 74.48747253417969 test_loss:1115.580810546875\n",
      "875/3000 train_loss: 72.38385009765625 test_loss:1103.145263671875\n",
      "876/3000 train_loss: 59.72739791870117 test_loss:1143.4951171875\n",
      "877/3000 train_loss: 70.5145034790039 test_loss:1129.041015625\n",
      "878/3000 train_loss: 64.79154968261719 test_loss:1065.03271484375\n",
      "879/3000 train_loss: 63.47359848022461 test_loss:1098.503662109375\n",
      "880/3000 train_loss: 76.32898712158203 test_loss:1084.014404296875\n",
      "881/3000 train_loss: 62.375465393066406 test_loss:1088.35595703125\n",
      "882/3000 train_loss: 74.82440948486328 test_loss:1125.3837890625\n",
      "883/3000 train_loss: 77.5572509765625 test_loss:1109.2850341796875\n",
      "884/3000 train_loss: 88.34286499023438 test_loss:1078.8284912109375\n",
      "885/3000 train_loss: 79.88272857666016 test_loss:1147.5654296875\n",
      "886/3000 train_loss: 77.33926391601562 test_loss:1081.12744140625\n",
      "887/3000 train_loss: 71.39724731445312 test_loss:1101.76171875\n",
      "888/3000 train_loss: 69.46568298339844 test_loss:1090.51220703125\n",
      "889/3000 train_loss: 72.65767669677734 test_loss:1069.5615234375\n",
      "890/3000 train_loss: 56.417694091796875 test_loss:1094.4581298828125\n",
      "891/3000 train_loss: 73.9610824584961 test_loss:1106.0809326171875\n",
      "892/3000 train_loss: 64.77194213867188 test_loss:1067.82470703125\n",
      "893/3000 train_loss: 64.5691909790039 test_loss:1060.0098876953125\n",
      "894/3000 train_loss: 58.09040069580078 test_loss:1087.66455078125\n",
      "895/3000 train_loss: 76.16648864746094 test_loss:1098.771240234375\n",
      "896/3000 train_loss: 67.18923950195312 test_loss:1042.0413818359375\n",
      "897/3000 train_loss: 59.84900665283203 test_loss:1112.4935302734375\n",
      "898/3000 train_loss: 60.78858184814453 test_loss:1039.6820068359375\n",
      "899/3000 train_loss: 60.97565460205078 test_loss:1084.5162353515625\n",
      "900/3000 train_loss: 61.766788482666016 test_loss:1088.4560546875\n",
      "901/3000 train_loss: 65.37071990966797 test_loss:1063.6856689453125\n",
      "902/3000 train_loss: 67.5875244140625 test_loss:1090.515869140625\n",
      "903/3000 train_loss: 66.84291076660156 test_loss:1081.524658203125\n",
      "904/3000 train_loss: 63.568115234375 test_loss:1073.893310546875\n",
      "905/3000 train_loss: 62.61810302734375 test_loss:1095.2926025390625\n",
      "906/3000 train_loss: 65.83899688720703 test_loss:1060.95947265625\n",
      "907/3000 train_loss: 63.521820068359375 test_loss:1094.654052734375\n",
      "908/3000 train_loss: 55.60129165649414 test_loss:1050.563232421875\n",
      "909/3000 train_loss: 68.24022674560547 test_loss:1107.167236328125\n",
      "910/3000 train_loss: 65.5718765258789 test_loss:1077.4248046875\n",
      "911/3000 train_loss: 63.231285095214844 test_loss:1103.619140625\n",
      "912/3000 train_loss: 67.03132629394531 test_loss:1093.5067138671875\n",
      "913/3000 train_loss: 58.756832122802734 test_loss:1111.061767578125\n",
      "914/3000 train_loss: 67.38562774658203 test_loss:1119.533447265625\n",
      "915/3000 train_loss: 58.63577651977539 test_loss:1079.392578125\n",
      "916/3000 train_loss: 58.9085807800293 test_loss:1094.3946533203125\n",
      "917/3000 train_loss: 63.12451934814453 test_loss:1078.5069580078125\n",
      "918/3000 train_loss: 65.15132141113281 test_loss:1119.199462890625\n",
      "919/3000 train_loss: 59.59735870361328 test_loss:1108.86181640625\n",
      "920/3000 train_loss: 54.77397155761719 test_loss:1109.826171875\n",
      "921/3000 train_loss: 59.36193084716797 test_loss:1064.303466796875\n",
      "922/3000 train_loss: 62.4488410949707 test_loss:1086.068359375\n",
      "923/3000 train_loss: 66.72662353515625 test_loss:1111.6396484375\n",
      "924/3000 train_loss: 67.53984069824219 test_loss:1087.7451171875\n",
      "925/3000 train_loss: 54.105201721191406 test_loss:1076.4478759765625\n",
      "926/3000 train_loss: 61.51372528076172 test_loss:1071.15966796875\n",
      "927/3000 train_loss: 67.5940170288086 test_loss:1114.9736328125\n",
      "928/3000 train_loss: 55.10079574584961 test_loss:1086.917724609375\n",
      "929/3000 train_loss: 59.78491973876953 test_loss:1157.301513671875\n",
      "930/3000 train_loss: 57.79889678955078 test_loss:1114.82470703125\n",
      "931/3000 train_loss: 60.68343734741211 test_loss:1120.5025634765625\n",
      "932/3000 train_loss: 73.84474182128906 test_loss:1115.828125\n",
      "933/3000 train_loss: 63.109642028808594 test_loss:1134.201171875\n",
      "934/3000 train_loss: 57.381919860839844 test_loss:1112.826904296875\n",
      "935/3000 train_loss: 59.263641357421875 test_loss:1127.76318359375\n",
      "936/3000 train_loss: 64.41170501708984 test_loss:1141.8590087890625\n",
      "937/3000 train_loss: 68.0318374633789 test_loss:1110.905517578125\n",
      "938/3000 train_loss: 56.82741928100586 test_loss:1087.8658447265625\n",
      "939/3000 train_loss: 57.20635223388672 test_loss:1103.952392578125\n",
      "940/3000 train_loss: 57.41390609741211 test_loss:1118.26123046875\n",
      "941/3000 train_loss: 59.29273986816406 test_loss:1112.9876708984375\n",
      "942/3000 train_loss: 54.975528717041016 test_loss:1084.771240234375\n",
      "943/3000 train_loss: 75.20967864990234 test_loss:1172.8643798828125\n",
      "944/3000 train_loss: 67.9126205444336 test_loss:1138.7742919921875\n",
      "945/3000 train_loss: 62.35173034667969 test_loss:1095.82763671875\n",
      "946/3000 train_loss: 68.34689331054688 test_loss:1062.5233154296875\n",
      "947/3000 train_loss: 62.5022087097168 test_loss:1100.25048828125\n",
      "948/3000 train_loss: 63.89976501464844 test_loss:1097.2664794921875\n",
      "949/3000 train_loss: 64.64511108398438 test_loss:1079.9197998046875\n",
      "950/3000 train_loss: 55.56053161621094 test_loss:1093.122802734375\n",
      "951/3000 train_loss: 64.89317321777344 test_loss:1086.158935546875\n",
      "952/3000 train_loss: 59.32289123535156 test_loss:1114.661376953125\n",
      "953/3000 train_loss: 58.12490463256836 test_loss:1134.513671875\n",
      "954/3000 train_loss: 63.1784782409668 test_loss:1075.9024658203125\n",
      "955/3000 train_loss: 57.31398010253906 test_loss:1122.144775390625\n",
      "956/3000 train_loss: 66.95490264892578 test_loss:1098.21533203125\n",
      "957/3000 train_loss: 62.99054718017578 test_loss:1103.8876953125\n",
      "958/3000 train_loss: 72.09026336669922 test_loss:1132.457763671875\n",
      "959/3000 train_loss: 58.97674560546875 test_loss:1110.6995849609375\n",
      "960/3000 train_loss: 57.75556182861328 test_loss:1100.0975341796875\n",
      "961/3000 train_loss: 66.72700500488281 test_loss:1094.5074462890625\n",
      "962/3000 train_loss: 61.19029998779297 test_loss:1063.5048828125\n",
      "963/3000 train_loss: 65.44547271728516 test_loss:1111.0020751953125\n",
      "964/3000 train_loss: 63.56080627441406 test_loss:1082.6895751953125\n",
      "965/3000 train_loss: 55.3510856628418 test_loss:1121.7469482421875\n",
      "966/3000 train_loss: 63.45697784423828 test_loss:1089.733154296875\n",
      "967/3000 train_loss: 64.15218353271484 test_loss:1141.2001953125\n",
      "968/3000 train_loss: 52.26521301269531 test_loss:1124.267333984375\n",
      "969/3000 train_loss: 57.96309280395508 test_loss:1109.9013671875\n",
      "970/3000 train_loss: 61.09941101074219 test_loss:1102.734619140625\n",
      "971/3000 train_loss: 59.50456237792969 test_loss:1071.47705078125\n",
      "972/3000 train_loss: 56.231571197509766 test_loss:1098.091796875\n",
      "973/3000 train_loss: 57.4248161315918 test_loss:1081.13427734375\n",
      "974/3000 train_loss: 56.97431182861328 test_loss:1090.97119140625\n",
      "975/3000 train_loss: 65.33758544921875 test_loss:1111.32177734375\n",
      "976/3000 train_loss: 71.01448059082031 test_loss:1090.3408203125\n",
      "977/3000 train_loss: 58.15326690673828 test_loss:1098.5458984375\n",
      "978/3000 train_loss: 59.96541976928711 test_loss:1119.6917724609375\n",
      "979/3000 train_loss: 58.709312438964844 test_loss:1153.748046875\n",
      "980/3000 train_loss: 62.33525848388672 test_loss:1124.6728515625\n",
      "981/3000 train_loss: 63.576637268066406 test_loss:1116.413330078125\n",
      "982/3000 train_loss: 64.36592102050781 test_loss:1093.1875\n",
      "983/3000 train_loss: 59.82440948486328 test_loss:1115.480712890625\n",
      "984/3000 train_loss: 67.46973419189453 test_loss:1079.7835693359375\n",
      "985/3000 train_loss: 65.14083862304688 test_loss:1111.3160400390625\n",
      "986/3000 train_loss: 60.50167465209961 test_loss:1087.49853515625\n",
      "987/3000 train_loss: 54.413490295410156 test_loss:1120.138916015625\n",
      "988/3000 train_loss: 56.389381408691406 test_loss:1088.5648193359375\n",
      "989/3000 train_loss: 55.52193832397461 test_loss:1149.8779296875\n",
      "990/3000 train_loss: 54.87562561035156 test_loss:1130.048828125\n",
      "991/3000 train_loss: 49.47833251953125 test_loss:1129.3404541015625\n",
      "992/3000 train_loss: 56.77805709838867 test_loss:1138.2127685546875\n",
      "993/3000 train_loss: 64.94300079345703 test_loss:1097.897216796875\n",
      "994/3000 train_loss: 64.79581451416016 test_loss:1085.66748046875\n",
      "995/3000 train_loss: 60.70823669433594 test_loss:1096.51513671875\n",
      "996/3000 train_loss: 64.66845703125 test_loss:1114.828369140625\n",
      "997/3000 train_loss: 54.566837310791016 test_loss:1049.0943603515625\n",
      "998/3000 train_loss: 52.91901779174805 test_loss:1081.85107421875\n",
      "999/3000 train_loss: 57.784873962402344 test_loss:1108.11962890625\n",
      "1000/3000 train_loss: 51.39673614501953 test_loss:1089.315673828125\n",
      "1001/3000 train_loss: 65.99262237548828 test_loss:1113.546630859375\n",
      "1002/3000 train_loss: 57.32383346557617 test_loss:1078.214111328125\n",
      "1003/3000 train_loss: 57.12002944946289 test_loss:1094.5262451171875\n",
      "1004/3000 train_loss: 55.00752258300781 test_loss:1100.179931640625\n",
      "1005/3000 train_loss: 62.545406341552734 test_loss:1087.2008056640625\n",
      "1006/3000 train_loss: 63.57431411743164 test_loss:1145.1649169921875\n",
      "1007/3000 train_loss: 72.65137481689453 test_loss:1087.7486572265625\n",
      "1008/3000 train_loss: 65.94938659667969 test_loss:1097.736083984375\n",
      "1009/3000 train_loss: 61.5508918762207 test_loss:1121.829833984375\n",
      "1010/3000 train_loss: 60.497528076171875 test_loss:1071.1119384765625\n",
      "1011/3000 train_loss: 60.27058792114258 test_loss:1155.8519287109375\n",
      "1012/3000 train_loss: 56.842041015625 test_loss:1114.0574951171875\n",
      "1013/3000 train_loss: 59.58302688598633 test_loss:1121.82568359375\n",
      "1014/3000 train_loss: 54.62175369262695 test_loss:1109.926025390625\n",
      "1015/3000 train_loss: 58.07735824584961 test_loss:1111.2542724609375\n",
      "1016/3000 train_loss: 59.75999069213867 test_loss:1131.957763671875\n",
      "1017/3000 train_loss: 53.42912292480469 test_loss:1108.536376953125\n",
      "1018/3000 train_loss: 56.02467727661133 test_loss:1089.393310546875\n",
      "1019/3000 train_loss: 52.65687561035156 test_loss:1116.44677734375\n",
      "1020/3000 train_loss: 55.90729904174805 test_loss:1143.19873046875\n",
      "1021/3000 train_loss: 61.38893508911133 test_loss:1102.97802734375\n",
      "1022/3000 train_loss: 58.96690368652344 test_loss:1078.4002685546875\n",
      "1023/3000 train_loss: 55.195220947265625 test_loss:1092.0809326171875\n",
      "1024/3000 train_loss: 58.76734924316406 test_loss:1088.526123046875\n",
      "1025/3000 train_loss: 64.22137451171875 test_loss:1114.7857666015625\n",
      "1026/3000 train_loss: 61.31753921508789 test_loss:1143.42822265625\n",
      "1027/3000 train_loss: 67.31400299072266 test_loss:1145.421630859375\n",
      "1028/3000 train_loss: 57.656429290771484 test_loss:1123.55517578125\n",
      "1029/3000 train_loss: 59.36186599731445 test_loss:1111.497314453125\n",
      "1030/3000 train_loss: 58.627384185791016 test_loss:1103.18359375\n",
      "1031/3000 train_loss: 64.223388671875 test_loss:1115.3472900390625\n",
      "1032/3000 train_loss: 55.26433563232422 test_loss:1074.5548095703125\n",
      "1033/3000 train_loss: 68.6423568725586 test_loss:1142.9510498046875\n",
      "1034/3000 train_loss: 55.984718322753906 test_loss:1150.1690673828125\n",
      "1035/3000 train_loss: 54.960777282714844 test_loss:1128.575439453125\n",
      "1036/3000 train_loss: 53.15131759643555 test_loss:1119.619873046875\n",
      "1037/3000 train_loss: 55.736000061035156 test_loss:1147.39599609375\n",
      "1038/3000 train_loss: 54.879425048828125 test_loss:1094.936767578125\n",
      "1039/3000 train_loss: 55.684669494628906 test_loss:1119.529296875\n",
      "1040/3000 train_loss: 56.10096740722656 test_loss:1109.02880859375\n",
      "1041/3000 train_loss: 51.529354095458984 test_loss:1112.2288818359375\n",
      "1042/3000 train_loss: 56.72535705566406 test_loss:1117.635986328125\n",
      "1043/3000 train_loss: 67.10710906982422 test_loss:1099.821044921875\n",
      "1044/3000 train_loss: 61.653587341308594 test_loss:1119.488037109375\n",
      "1045/3000 train_loss: 54.236236572265625 test_loss:1097.69775390625\n",
      "1046/3000 train_loss: 52.68570327758789 test_loss:1101.021240234375\n",
      "1047/3000 train_loss: 60.440303802490234 test_loss:1137.4664306640625\n",
      "1048/3000 train_loss: 52.43122863769531 test_loss:1093.09228515625\n",
      "1049/3000 train_loss: 59.53738784790039 test_loss:1079.802001953125\n",
      "1050/3000 train_loss: 61.98143768310547 test_loss:1102.7830810546875\n",
      "1051/3000 train_loss: 61.93998718261719 test_loss:1102.34912109375\n",
      "1052/3000 train_loss: 61.5801887512207 test_loss:1098.72412109375\n",
      "1053/3000 train_loss: 59.37478256225586 test_loss:1088.58447265625\n",
      "1054/3000 train_loss: 62.31223678588867 test_loss:1113.1278076171875\n",
      "1055/3000 train_loss: 54.06604766845703 test_loss:1138.265625\n",
      "1056/3000 train_loss: 63.72255325317383 test_loss:1107.1231689453125\n",
      "1057/3000 train_loss: 52.83419418334961 test_loss:1140.8941650390625\n",
      "1058/3000 train_loss: 60.84055709838867 test_loss:1113.9830322265625\n",
      "1059/3000 train_loss: 50.84307861328125 test_loss:1129.916015625\n",
      "1060/3000 train_loss: 53.54697799682617 test_loss:1136.8951416015625\n",
      "1061/3000 train_loss: 54.457916259765625 test_loss:1118.5137939453125\n",
      "1062/3000 train_loss: 53.63910675048828 test_loss:1151.689208984375\n",
      "1063/3000 train_loss: 57.15522003173828 test_loss:1104.8433837890625\n",
      "1064/3000 train_loss: 52.972965240478516 test_loss:1129.8140869140625\n",
      "1065/3000 train_loss: 50.542510986328125 test_loss:1118.6397705078125\n",
      "1066/3000 train_loss: 59.06871032714844 test_loss:1100.9976806640625\n",
      "1067/3000 train_loss: 57.840904235839844 test_loss:1101.1953125\n",
      "1068/3000 train_loss: 54.34534454345703 test_loss:1091.995361328125\n",
      "1069/3000 train_loss: 54.75275802612305 test_loss:1122.931640625\n",
      "1070/3000 train_loss: 57.2679328918457 test_loss:1124.32275390625\n",
      "1071/3000 train_loss: 60.49917221069336 test_loss:1110.525146484375\n",
      "1072/3000 train_loss: 55.09131622314453 test_loss:1104.573974609375\n",
      "1073/3000 train_loss: 52.93939971923828 test_loss:1090.5037841796875\n",
      "1074/3000 train_loss: 53.20487976074219 test_loss:1065.5721435546875\n",
      "1075/3000 train_loss: 51.95762634277344 test_loss:1039.8892822265625\n",
      "1076/3000 train_loss: 54.77452850341797 test_loss:1042.81689453125\n",
      "1077/3000 train_loss: 60.334808349609375 test_loss:1051.04248046875\n",
      "1078/3000 train_loss: 60.62504577636719 test_loss:1036.705322265625\n",
      "1079/3000 train_loss: 57.740787506103516 test_loss:1087.271484375\n",
      "1080/3000 train_loss: 47.16960906982422 test_loss:1062.46728515625\n",
      "1081/3000 train_loss: 63.530277252197266 test_loss:1057.42578125\n",
      "1082/3000 train_loss: 52.55184555053711 test_loss:1088.5809326171875\n",
      "1083/3000 train_loss: 63.547767639160156 test_loss:1068.816162109375\n",
      "1084/3000 train_loss: 60.11151885986328 test_loss:1068.5218505859375\n",
      "1085/3000 train_loss: 61.762840270996094 test_loss:1054.767578125\n",
      "1086/3000 train_loss: 64.55747985839844 test_loss:1168.817138671875\n",
      "1087/3000 train_loss: 63.4130973815918 test_loss:1057.58984375\n",
      "1088/3000 train_loss: 56.427146911621094 test_loss:1100.213623046875\n",
      "1089/3000 train_loss: 69.76661682128906 test_loss:1133.034912109375\n",
      "1090/3000 train_loss: 65.14342498779297 test_loss:1124.7620849609375\n",
      "1091/3000 train_loss: 56.23969268798828 test_loss:1094.884765625\n",
      "1092/3000 train_loss: 55.98200607299805 test_loss:1115.4598388671875\n",
      "1093/3000 train_loss: 54.311893463134766 test_loss:1108.79443359375\n",
      "1094/3000 train_loss: 51.57715606689453 test_loss:1079.314697265625\n",
      "1095/3000 train_loss: 52.9085807800293 test_loss:1070.668212890625\n",
      "1096/3000 train_loss: 62.12907409667969 test_loss:1122.718994140625\n",
      "1097/3000 train_loss: 49.828033447265625 test_loss:1114.0047607421875\n",
      "1098/3000 train_loss: 53.82689666748047 test_loss:1052.8662109375\n",
      "1099/3000 train_loss: 48.54598617553711 test_loss:1079.568115234375\n",
      "1100/3000 train_loss: 49.176063537597656 test_loss:1049.1519775390625\n",
      "1101/3000 train_loss: 68.15032196044922 test_loss:1065.2996826171875\n",
      "1102/3000 train_loss: 54.76075744628906 test_loss:1091.9561767578125\n",
      "1103/3000 train_loss: 53.55375289916992 test_loss:1087.837646484375\n",
      "1104/3000 train_loss: 48.87060546875 test_loss:1093.5869140625\n",
      "1105/3000 train_loss: 51.894874572753906 test_loss:1153.88427734375\n",
      "1106/3000 train_loss: 58.997398376464844 test_loss:1127.380859375\n",
      "1107/3000 train_loss: 56.82502365112305 test_loss:1111.2589111328125\n",
      "1108/3000 train_loss: 46.16876220703125 test_loss:1061.607177734375\n",
      "1109/3000 train_loss: 50.271087646484375 test_loss:1091.343017578125\n",
      "1110/3000 train_loss: 56.8764533996582 test_loss:1043.3447265625\n",
      "1111/3000 train_loss: 50.97223663330078 test_loss:1044.6455078125\n",
      "1112/3000 train_loss: 53.6458625793457 test_loss:1065.7998046875\n",
      "1113/3000 train_loss: 50.35063934326172 test_loss:1110.6953125\n",
      "1114/3000 train_loss: 48.54816436767578 test_loss:1066.72412109375\n",
      "1115/3000 train_loss: 54.22998046875 test_loss:1019.7446899414062\n",
      "1116/3000 train_loss: 54.40194320678711 test_loss:1053.566162109375\n",
      "1117/3000 train_loss: 47.57477569580078 test_loss:1082.03125\n",
      "1118/3000 train_loss: 53.13178634643555 test_loss:1075.268798828125\n",
      "1119/3000 train_loss: 54.538482666015625 test_loss:1037.2137451171875\n",
      "1120/3000 train_loss: 58.635276794433594 test_loss:1101.735595703125\n",
      "1121/3000 train_loss: 55.636993408203125 test_loss:1063.82275390625\n",
      "1122/3000 train_loss: 47.786128997802734 test_loss:1072.0445556640625\n",
      "1123/3000 train_loss: 57.14101791381836 test_loss:1068.104248046875\n",
      "1124/3000 train_loss: 48.261688232421875 test_loss:1051.2764892578125\n",
      "1125/3000 train_loss: 50.0494499206543 test_loss:1039.2777099609375\n",
      "1126/3000 train_loss: 53.22320556640625 test_loss:1064.201904296875\n",
      "1127/3000 train_loss: 54.09776306152344 test_loss:1061.091796875\n",
      "1128/3000 train_loss: 58.665000915527344 test_loss:1069.16943359375\n",
      "1129/3000 train_loss: 55.74663543701172 test_loss:1062.5245361328125\n",
      "1130/3000 train_loss: 54.98110580444336 test_loss:1070.44189453125\n",
      "1131/3000 train_loss: 57.872310638427734 test_loss:1056.548095703125\n",
      "1132/3000 train_loss: 52.01945877075195 test_loss:1002.9927368164062\n",
      "1133/3000 train_loss: 61.48512649536133 test_loss:1051.415771484375\n",
      "1134/3000 train_loss: 49.58339309692383 test_loss:1046.7269287109375\n",
      "1135/3000 train_loss: 64.02351379394531 test_loss:1044.1895751953125\n",
      "1136/3000 train_loss: 50.60331726074219 test_loss:1086.4930419921875\n",
      "1137/3000 train_loss: 51.97029495239258 test_loss:1098.341064453125\n",
      "1138/3000 train_loss: 47.38646697998047 test_loss:1115.1929931640625\n",
      "1139/3000 train_loss: 53.53646469116211 test_loss:1119.8779296875\n",
      "1140/3000 train_loss: 58.14699935913086 test_loss:1137.869873046875\n",
      "1141/3000 train_loss: 55.162105560302734 test_loss:1170.728515625\n",
      "1142/3000 train_loss: 52.467308044433594 test_loss:1186.92822265625\n",
      "1143/3000 train_loss: 54.13908386230469 test_loss:1135.1817626953125\n",
      "1144/3000 train_loss: 56.960689544677734 test_loss:1197.684326171875\n",
      "1145/3000 train_loss: 46.10532760620117 test_loss:1164.75439453125\n",
      "1146/3000 train_loss: 55.06953430175781 test_loss:1137.1663818359375\n",
      "1147/3000 train_loss: 46.82595443725586 test_loss:1118.85205078125\n",
      "1148/3000 train_loss: 80.12857055664062 test_loss:1088.322265625\n",
      "1149/3000 train_loss: 57.78337860107422 test_loss:1028.068603515625\n",
      "1150/3000 train_loss: 55.0805778503418 test_loss:1067.132080078125\n",
      "1151/3000 train_loss: 51.24495315551758 test_loss:1066.97705078125\n",
      "1152/3000 train_loss: 60.95957565307617 test_loss:1068.366455078125\n",
      "1153/3000 train_loss: 66.30763244628906 test_loss:1059.696533203125\n",
      "1154/3000 train_loss: 53.057865142822266 test_loss:1121.7303466796875\n",
      "1155/3000 train_loss: 55.50545120239258 test_loss:1117.8458251953125\n",
      "1156/3000 train_loss: 49.781982421875 test_loss:1104.760986328125\n",
      "1157/3000 train_loss: 47.933380126953125 test_loss:1091.609130859375\n",
      "1158/3000 train_loss: 55.16351318359375 test_loss:1143.37451171875\n",
      "1159/3000 train_loss: 52.13801956176758 test_loss:1121.63818359375\n",
      "1160/3000 train_loss: 54.800437927246094 test_loss:1124.347900390625\n",
      "1161/3000 train_loss: 52.973846435546875 test_loss:1093.2940673828125\n",
      "1162/3000 train_loss: 51.13444519042969 test_loss:1120.0599365234375\n",
      "1163/3000 train_loss: 50.19583511352539 test_loss:1113.09130859375\n",
      "1164/3000 train_loss: 49.53913879394531 test_loss:1141.6845703125\n",
      "1165/3000 train_loss: 54.074241638183594 test_loss:1123.775390625\n",
      "1166/3000 train_loss: 49.77760314941406 test_loss:1105.770751953125\n",
      "1167/3000 train_loss: 52.31224822998047 test_loss:1095.6260986328125\n",
      "1168/3000 train_loss: 45.715415954589844 test_loss:1051.388427734375\n",
      "1169/3000 train_loss: 52.689247131347656 test_loss:1072.167724609375\n",
      "1170/3000 train_loss: 53.499961853027344 test_loss:1075.7584228515625\n",
      "1171/3000 train_loss: 55.901084899902344 test_loss:1081.7166748046875\n",
      "1172/3000 train_loss: 49.66341781616211 test_loss:1119.09033203125\n",
      "1173/3000 train_loss: 49.600006103515625 test_loss:1071.28564453125\n",
      "1174/3000 train_loss: 48.28839111328125 test_loss:1091.6644287109375\n",
      "1175/3000 train_loss: 48.22739791870117 test_loss:1108.1212158203125\n",
      "1176/3000 train_loss: 59.73261260986328 test_loss:1087.1549072265625\n",
      "1177/3000 train_loss: 50.132564544677734 test_loss:1091.373046875\n",
      "1178/3000 train_loss: 48.35753631591797 test_loss:1078.000244140625\n",
      "1179/3000 train_loss: 47.55592346191406 test_loss:1095.516845703125\n",
      "1180/3000 train_loss: 49.125579833984375 test_loss:1118.3590087890625\n",
      "1181/3000 train_loss: 50.77650451660156 test_loss:1120.693603515625\n",
      "1182/3000 train_loss: 51.25848388671875 test_loss:1096.630859375\n",
      "1183/3000 train_loss: 50.477882385253906 test_loss:1053.653076171875\n",
      "1184/3000 train_loss: 51.191856384277344 test_loss:1071.19140625\n",
      "1185/3000 train_loss: 50.45780563354492 test_loss:1117.19921875\n",
      "1186/3000 train_loss: 54.35717010498047 test_loss:1107.43310546875\n",
      "1187/3000 train_loss: 49.09748077392578 test_loss:1093.461181640625\n",
      "1188/3000 train_loss: 48.42516326904297 test_loss:1077.6588134765625\n",
      "1189/3000 train_loss: 52.05401611328125 test_loss:1106.922607421875\n",
      "1190/3000 train_loss: 45.85398864746094 test_loss:1120.5533447265625\n",
      "1191/3000 train_loss: 47.699913024902344 test_loss:1138.22265625\n",
      "1192/3000 train_loss: 51.14963150024414 test_loss:1117.302978515625\n",
      "1193/3000 train_loss: 46.40591049194336 test_loss:1106.3829345703125\n",
      "1194/3000 train_loss: 49.44393539428711 test_loss:1146.6566162109375\n",
      "1195/3000 train_loss: 50.58705139160156 test_loss:1112.397216796875\n",
      "1196/3000 train_loss: 46.8647575378418 test_loss:1125.138916015625\n",
      "1197/3000 train_loss: 54.296287536621094 test_loss:1160.880615234375\n",
      "1198/3000 train_loss: 46.152523040771484 test_loss:1133.553466796875\n",
      "1199/3000 train_loss: 52.489288330078125 test_loss:1142.132080078125\n",
      "1200/3000 train_loss: 49.77294158935547 test_loss:1164.66259765625\n",
      "1201/3000 train_loss: 50.269073486328125 test_loss:1153.4342041015625\n",
      "1202/3000 train_loss: 48.93828201293945 test_loss:1129.1806640625\n",
      "1203/3000 train_loss: 56.119354248046875 test_loss:1132.8204345703125\n",
      "1204/3000 train_loss: 53.0231819152832 test_loss:1105.77392578125\n",
      "1205/3000 train_loss: 46.47990798950195 test_loss:1091.5009765625\n",
      "1206/3000 train_loss: 54.5660400390625 test_loss:1107.176513671875\n",
      "1207/3000 train_loss: 56.259559631347656 test_loss:1121.0928955078125\n",
      "1208/3000 train_loss: 54.22022247314453 test_loss:1071.010986328125\n",
      "1209/3000 train_loss: 54.42382049560547 test_loss:1104.38818359375\n",
      "1210/3000 train_loss: 52.1765022277832 test_loss:1091.300048828125\n",
      "1211/3000 train_loss: 59.8069953918457 test_loss:1115.64013671875\n",
      "1212/3000 train_loss: 54.055519104003906 test_loss:1144.7637939453125\n",
      "1213/3000 train_loss: 53.91500473022461 test_loss:1111.74267578125\n",
      "1214/3000 train_loss: 50.67705535888672 test_loss:1113.90576171875\n",
      "1215/3000 train_loss: 60.04706954956055 test_loss:1080.6383056640625\n",
      "1216/3000 train_loss: 47.79196548461914 test_loss:1102.9456787109375\n",
      "1217/3000 train_loss: 51.826805114746094 test_loss:1101.6910400390625\n",
      "1218/3000 train_loss: 56.390708923339844 test_loss:1072.8846435546875\n",
      "1219/3000 train_loss: 54.20893859863281 test_loss:1102.30908203125\n",
      "1220/3000 train_loss: 46.5430793762207 test_loss:1026.1795654296875\n",
      "1221/3000 train_loss: 47.17286682128906 test_loss:1062.8267822265625\n",
      "1222/3000 train_loss: 49.24577331542969 test_loss:1050.4234619140625\n",
      "1223/3000 train_loss: 46.03110122680664 test_loss:1027.5916748046875\n",
      "1224/3000 train_loss: 48.88156509399414 test_loss:1041.1728515625\n",
      "1225/3000 train_loss: 49.48283767700195 test_loss:1068.6099853515625\n",
      "1226/3000 train_loss: 51.35457992553711 test_loss:1028.122314453125\n",
      "1227/3000 train_loss: 45.38668441772461 test_loss:1067.412353515625\n",
      "1228/3000 train_loss: 46.682220458984375 test_loss:1078.9222412109375\n",
      "1229/3000 train_loss: 40.534996032714844 test_loss:1068.0849609375\n",
      "1230/3000 train_loss: 48.81557083129883 test_loss:1082.724609375\n",
      "1231/3000 train_loss: 58.597164154052734 test_loss:1056.74755859375\n",
      "1232/3000 train_loss: 62.33091354370117 test_loss:1071.970947265625\n",
      "1233/3000 train_loss: 52.46886444091797 test_loss:1065.36328125\n",
      "1234/3000 train_loss: 50.40249252319336 test_loss:1058.46533203125\n",
      "1235/3000 train_loss: 52.6030158996582 test_loss:1093.5533447265625\n",
      "1236/3000 train_loss: 49.22503662109375 test_loss:1072.3984375\n",
      "1237/3000 train_loss: 47.71470642089844 test_loss:1080.9569091796875\n",
      "1238/3000 train_loss: 50.14394760131836 test_loss:1104.9383544921875\n",
      "1239/3000 train_loss: 42.56721115112305 test_loss:1106.689697265625\n",
      "1240/3000 train_loss: 60.36136245727539 test_loss:1034.394775390625\n",
      "1241/3000 train_loss: 55.12493896484375 test_loss:1055.8035888671875\n",
      "1242/3000 train_loss: 45.69750213623047 test_loss:1040.9453125\n",
      "1243/3000 train_loss: 50.6541862487793 test_loss:1040.610595703125\n",
      "1244/3000 train_loss: 46.38435363769531 test_loss:1073.598876953125\n",
      "1245/3000 train_loss: 48.946041107177734 test_loss:1082.7476806640625\n",
      "1246/3000 train_loss: 55.190093994140625 test_loss:1071.2474365234375\n",
      "1247/3000 train_loss: 50.827396392822266 test_loss:1026.58349609375\n",
      "1248/3000 train_loss: 52.35087585449219 test_loss:1095.2593994140625\n",
      "1249/3000 train_loss: 50.16077423095703 test_loss:1090.2933349609375\n",
      "1250/3000 train_loss: 49.71357727050781 test_loss:1021.229736328125\n",
      "1251/3000 train_loss: 49.05197525024414 test_loss:1048.2044677734375\n",
      "1252/3000 train_loss: 46.74020767211914 test_loss:1022.8892822265625\n",
      "1253/3000 train_loss: 47.99309158325195 test_loss:1044.50439453125\n",
      "1254/3000 train_loss: 50.58116912841797 test_loss:1058.57568359375\n",
      "1255/3000 train_loss: 55.157081604003906 test_loss:1070.355224609375\n",
      "1256/3000 train_loss: 48.811161041259766 test_loss:1031.1591796875\n",
      "1257/3000 train_loss: 52.32189178466797 test_loss:1028.142822265625\n",
      "1258/3000 train_loss: 46.43359375 test_loss:1044.7987060546875\n",
      "1259/3000 train_loss: 54.83176040649414 test_loss:1041.10986328125\n",
      "1260/3000 train_loss: 55.77677536010742 test_loss:1044.746337890625\n",
      "1261/3000 train_loss: 50.129940032958984 test_loss:1038.9903564453125\n",
      "1262/3000 train_loss: 47.93167495727539 test_loss:1069.28955078125\n",
      "1263/3000 train_loss: 44.886192321777344 test_loss:1038.1591796875\n",
      "1264/3000 train_loss: 45.22371292114258 test_loss:1053.94580078125\n",
      "1265/3000 train_loss: 41.69458770751953 test_loss:1037.1885986328125\n",
      "1266/3000 train_loss: 48.03688430786133 test_loss:1086.25537109375\n",
      "1267/3000 train_loss: 46.605125427246094 test_loss:1063.785888671875\n",
      "1268/3000 train_loss: 52.9759521484375 test_loss:1071.627685546875\n",
      "1269/3000 train_loss: 51.60193634033203 test_loss:1007.3364868164062\n",
      "1270/3000 train_loss: 51.880855560302734 test_loss:983.7125854492188\n",
      "1271/3000 train_loss: 48.66347885131836 test_loss:1021.901611328125\n",
      "1272/3000 train_loss: 54.30250549316406 test_loss:1008.3323364257812\n",
      "1273/3000 train_loss: 61.6640510559082 test_loss:1099.356201171875\n",
      "1274/3000 train_loss: 41.21607971191406 test_loss:1036.066650390625\n",
      "1275/3000 train_loss: 52.34732437133789 test_loss:1012.5467529296875\n",
      "1276/3000 train_loss: 42.16541290283203 test_loss:1022.942138671875\n",
      "1277/3000 train_loss: 61.87994384765625 test_loss:1057.51416015625\n",
      "1278/3000 train_loss: 47.33446502685547 test_loss:1039.66259765625\n",
      "1279/3000 train_loss: 47.45818328857422 test_loss:1069.24169921875\n",
      "1280/3000 train_loss: 49.9522705078125 test_loss:1032.5634765625\n",
      "1281/3000 train_loss: 43.65641403198242 test_loss:1055.9771728515625\n",
      "1282/3000 train_loss: 46.98117446899414 test_loss:1065.9420166015625\n",
      "1283/3000 train_loss: 52.69948959350586 test_loss:1023.4708862304688\n",
      "1284/3000 train_loss: 48.92018508911133 test_loss:1000.799560546875\n",
      "1285/3000 train_loss: 47.3128547668457 test_loss:1048.9794921875\n",
      "1286/3000 train_loss: 52.583641052246094 test_loss:1048.8331298828125\n",
      "1287/3000 train_loss: 43.32843017578125 test_loss:1024.4454345703125\n",
      "1288/3000 train_loss: 47.4423713684082 test_loss:1044.3978271484375\n",
      "1289/3000 train_loss: 48.29217529296875 test_loss:1021.9998779296875\n",
      "1290/3000 train_loss: 62.4112663269043 test_loss:1018.24853515625\n",
      "1291/3000 train_loss: 45.59107208251953 test_loss:1052.707763671875\n",
      "1292/3000 train_loss: 47.3009147644043 test_loss:1016.2600708007812\n",
      "1293/3000 train_loss: 48.70836639404297 test_loss:1015.8277587890625\n",
      "1294/3000 train_loss: 52.350337982177734 test_loss:1055.513427734375\n",
      "1295/3000 train_loss: 42.17318344116211 test_loss:1037.611328125\n",
      "1296/3000 train_loss: 47.379268646240234 test_loss:1079.4666748046875\n",
      "1297/3000 train_loss: 53.3980598449707 test_loss:1074.099853515625\n",
      "1298/3000 train_loss: 59.6929931640625 test_loss:1094.1973876953125\n",
      "1299/3000 train_loss: 52.2769775390625 test_loss:1112.90625\n",
      "1300/3000 train_loss: 47.2718505859375 test_loss:1039.63818359375\n",
      "1301/3000 train_loss: 45.25062942504883 test_loss:1032.8873291015625\n",
      "1302/3000 train_loss: 44.793251037597656 test_loss:1027.098876953125\n",
      "1303/3000 train_loss: 49.128929138183594 test_loss:1048.470947265625\n",
      "1304/3000 train_loss: 46.1150016784668 test_loss:1026.3590087890625\n",
      "1305/3000 train_loss: 52.409637451171875 test_loss:1044.4813232421875\n",
      "1306/3000 train_loss: 46.68495178222656 test_loss:1036.1015625\n",
      "1307/3000 train_loss: 49.314208984375 test_loss:1081.2696533203125\n",
      "1308/3000 train_loss: 56.44178771972656 test_loss:1076.67578125\n",
      "1309/3000 train_loss: 46.655433654785156 test_loss:1021.8233642578125\n",
      "1310/3000 train_loss: 47.822181701660156 test_loss:1053.3033447265625\n",
      "1311/3000 train_loss: 49.342533111572266 test_loss:1026.012451171875\n",
      "1312/3000 train_loss: 60.10575485229492 test_loss:998.625\n",
      "1313/3000 train_loss: 61.73158645629883 test_loss:1032.313232421875\n",
      "1314/3000 train_loss: 56.30405044555664 test_loss:1006.8284912109375\n",
      "1315/3000 train_loss: 53.67646789550781 test_loss:1085.8076171875\n",
      "1316/3000 train_loss: 55.1361198425293 test_loss:1074.677978515625\n",
      "1317/3000 train_loss: 51.05253982543945 test_loss:1000.6075439453125\n",
      "1318/3000 train_loss: 45.39649963378906 test_loss:1057.923095703125\n",
      "1319/3000 train_loss: 52.16324996948242 test_loss:1021.6386108398438\n",
      "1320/3000 train_loss: 52.98245620727539 test_loss:1055.598876953125\n",
      "1321/3000 train_loss: 47.76069641113281 test_loss:1036.9547119140625\n",
      "1322/3000 train_loss: 52.424293518066406 test_loss:1082.60595703125\n",
      "1323/3000 train_loss: 56.41819763183594 test_loss:1072.35888671875\n",
      "1324/3000 train_loss: 60.62892532348633 test_loss:1042.819091796875\n",
      "1325/3000 train_loss: 42.382171630859375 test_loss:1067.1038818359375\n",
      "1326/3000 train_loss: 43.648433685302734 test_loss:1049.622802734375\n",
      "1327/3000 train_loss: 42.64695739746094 test_loss:1051.618408203125\n",
      "1328/3000 train_loss: 45.606468200683594 test_loss:1021.01025390625\n",
      "1329/3000 train_loss: 50.346214294433594 test_loss:1044.853759765625\n",
      "1330/3000 train_loss: 50.7773551940918 test_loss:1050.230224609375\n",
      "1331/3000 train_loss: 42.12163162231445 test_loss:1023.5339965820312\n",
      "1332/3000 train_loss: 46.638763427734375 test_loss:1030.5887451171875\n",
      "1333/3000 train_loss: 46.79838180541992 test_loss:961.3524169921875\n",
      "1334/3000 train_loss: 56.93416976928711 test_loss:970.8642578125\n",
      "1335/3000 train_loss: 49.720420837402344 test_loss:961.99267578125\n",
      "1336/3000 train_loss: 43.981407165527344 test_loss:990.5367431640625\n",
      "1337/3000 train_loss: 46.126441955566406 test_loss:999.18505859375\n",
      "1338/3000 train_loss: 48.73808288574219 test_loss:1063.2940673828125\n",
      "1339/3000 train_loss: 40.36429214477539 test_loss:1037.337646484375\n",
      "1340/3000 train_loss: 43.02947998046875 test_loss:1042.398681640625\n",
      "1341/3000 train_loss: 49.708763122558594 test_loss:1034.2215576171875\n",
      "1342/3000 train_loss: 43.98664855957031 test_loss:1002.718017578125\n",
      "1343/3000 train_loss: 46.65602111816406 test_loss:1044.57568359375\n",
      "1344/3000 train_loss: 48.133792877197266 test_loss:1024.3856201171875\n",
      "1345/3000 train_loss: 43.647850036621094 test_loss:1104.1949462890625\n",
      "1346/3000 train_loss: 46.19657516479492 test_loss:1048.7764892578125\n",
      "1347/3000 train_loss: 49.317222595214844 test_loss:1036.3157958984375\n",
      "1348/3000 train_loss: 44.85545349121094 test_loss:1041.788818359375\n",
      "1349/3000 train_loss: 49.97303771972656 test_loss:1036.740966796875\n",
      "1350/3000 train_loss: 46.07486343383789 test_loss:1006.56640625\n",
      "1351/3000 train_loss: 50.242218017578125 test_loss:1040.2066650390625\n",
      "1352/3000 train_loss: 43.60161209106445 test_loss:1061.106689453125\n",
      "1353/3000 train_loss: 52.292724609375 test_loss:1050.7293701171875\n",
      "1354/3000 train_loss: 46.95539474487305 test_loss:1067.43359375\n",
      "1355/3000 train_loss: 45.11159896850586 test_loss:1065.555419921875\n",
      "1356/3000 train_loss: 54.841461181640625 test_loss:1135.21728515625\n",
      "1357/3000 train_loss: 43.7281494140625 test_loss:1083.8720703125\n",
      "1358/3000 train_loss: 44.6343994140625 test_loss:1084.712890625\n",
      "1359/3000 train_loss: 47.050601959228516 test_loss:1059.5167236328125\n",
      "1360/3000 train_loss: 44.41303253173828 test_loss:1063.771484375\n",
      "1361/3000 train_loss: 50.52500915527344 test_loss:1100.152099609375\n",
      "1362/3000 train_loss: 47.68846893310547 test_loss:1090.61767578125\n",
      "1363/3000 train_loss: 44.93610382080078 test_loss:1065.053955078125\n",
      "1364/3000 train_loss: 44.451904296875 test_loss:1035.277587890625\n",
      "1365/3000 train_loss: 48.902042388916016 test_loss:1026.807373046875\n",
      "1366/3000 train_loss: 47.0670051574707 test_loss:1034.1700439453125\n",
      "1367/3000 train_loss: 42.54936218261719 test_loss:1034.499267578125\n",
      "1368/3000 train_loss: 39.251991271972656 test_loss:1030.5997314453125\n",
      "1369/3000 train_loss: 49.63543701171875 test_loss:1032.842529296875\n",
      "1370/3000 train_loss: 41.47101974487305 test_loss:1047.7911376953125\n",
      "1371/3000 train_loss: 46.53667068481445 test_loss:1053.3089599609375\n",
      "1372/3000 train_loss: 45.34354019165039 test_loss:1069.5423583984375\n",
      "1373/3000 train_loss: 45.544677734375 test_loss:1015.085693359375\n",
      "1374/3000 train_loss: 48.914730072021484 test_loss:1040.06396484375\n",
      "1375/3000 train_loss: 43.63136672973633 test_loss:1075.131591796875\n",
      "1376/3000 train_loss: 41.20774459838867 test_loss:1026.39453125\n",
      "1377/3000 train_loss: 50.473548889160156 test_loss:1070.0567626953125\n",
      "1378/3000 train_loss: 54.70524215698242 test_loss:1029.3115234375\n",
      "1379/3000 train_loss: 48.81740951538086 test_loss:1047.6495361328125\n",
      "1380/3000 train_loss: 51.96107482910156 test_loss:1008.5842895507812\n",
      "1381/3000 train_loss: 44.1862907409668 test_loss:1023.914306640625\n",
      "1382/3000 train_loss: 50.66330337524414 test_loss:1025.9376220703125\n",
      "1383/3000 train_loss: 49.2557373046875 test_loss:1051.580322265625\n",
      "1384/3000 train_loss: 46.28521728515625 test_loss:1026.9105224609375\n",
      "1385/3000 train_loss: 56.86308288574219 test_loss:1012.5537719726562\n",
      "1386/3000 train_loss: 54.45964050292969 test_loss:1058.4451904296875\n",
      "1387/3000 train_loss: 46.58452606201172 test_loss:1032.0931396484375\n",
      "1388/3000 train_loss: 47.23630905151367 test_loss:1052.840576171875\n",
      "1389/3000 train_loss: 47.669002532958984 test_loss:1047.3251953125\n",
      "1390/3000 train_loss: 46.71955871582031 test_loss:1007.25390625\n",
      "1391/3000 train_loss: 41.07085037231445 test_loss:1015.3323974609375\n",
      "1392/3000 train_loss: 49.52595138549805 test_loss:1045.904541015625\n",
      "1393/3000 train_loss: 46.041419982910156 test_loss:1061.8131103515625\n",
      "1394/3000 train_loss: 41.8922119140625 test_loss:1040.395263671875\n",
      "1395/3000 train_loss: 45.48305892944336 test_loss:1063.0604248046875\n",
      "1396/3000 train_loss: 50.56975555419922 test_loss:1013.125\n",
      "1397/3000 train_loss: 47.24483871459961 test_loss:1047.34228515625\n",
      "1398/3000 train_loss: 47.78227615356445 test_loss:1019.8248901367188\n",
      "1399/3000 train_loss: 49.47898864746094 test_loss:1016.887451171875\n",
      "1400/3000 train_loss: 55.44176483154297 test_loss:1038.681640625\n",
      "1401/3000 train_loss: 44.10143280029297 test_loss:968.9508056640625\n",
      "1402/3000 train_loss: 44.859214782714844 test_loss:950.6263427734375\n",
      "1403/3000 train_loss: 45.166629791259766 test_loss:958.0931396484375\n",
      "1404/3000 train_loss: 45.01047897338867 test_loss:906.5115966796875\n",
      "1405/3000 train_loss: 44.52951431274414 test_loss:985.4228515625\n",
      "1406/3000 train_loss: 49.967437744140625 test_loss:1027.9757080078125\n",
      "1407/3000 train_loss: 50.150821685791016 test_loss:964.069091796875\n",
      "1408/3000 train_loss: 49.65485763549805 test_loss:1012.4938354492188\n",
      "1409/3000 train_loss: 56.20245361328125 test_loss:1036.12109375\n",
      "1410/3000 train_loss: 41.72218322753906 test_loss:1061.642822265625\n",
      "1411/3000 train_loss: 43.386817932128906 test_loss:1076.435791015625\n",
      "1412/3000 train_loss: 46.393497467041016 test_loss:1031.035888671875\n",
      "1413/3000 train_loss: 62.966190338134766 test_loss:1085.26904296875\n",
      "1414/3000 train_loss: 60.26372146606445 test_loss:1159.1099853515625\n",
      "1415/3000 train_loss: 44.69929122924805 test_loss:1107.484375\n",
      "1416/3000 train_loss: 49.9896125793457 test_loss:1100.9666748046875\n",
      "1417/3000 train_loss: 51.98689270019531 test_loss:1129.35107421875\n",
      "1418/3000 train_loss: 53.83818054199219 test_loss:1085.9051513671875\n",
      "1419/3000 train_loss: 49.88539123535156 test_loss:1063.06787109375\n",
      "1420/3000 train_loss: 44.42129898071289 test_loss:1062.0364990234375\n",
      "1421/3000 train_loss: 46.53309631347656 test_loss:1062.2470703125\n",
      "1422/3000 train_loss: 41.70479202270508 test_loss:1049.326904296875\n",
      "1423/3000 train_loss: 42.20102310180664 test_loss:1018.8851318359375\n",
      "1424/3000 train_loss: 44.854854583740234 test_loss:1010.0736694335938\n",
      "1425/3000 train_loss: 65.13137817382812 test_loss:1036.5498046875\n",
      "1426/3000 train_loss: 45.25895309448242 test_loss:995.1017456054688\n",
      "1427/3000 train_loss: 44.943885803222656 test_loss:982.7747802734375\n",
      "1428/3000 train_loss: 48.90570831298828 test_loss:954.3050537109375\n",
      "1429/3000 train_loss: 37.86546325683594 test_loss:1025.018310546875\n",
      "1430/3000 train_loss: 48.62367248535156 test_loss:977.3831787109375\n",
      "1431/3000 train_loss: 48.55411148071289 test_loss:969.5984497070312\n",
      "1432/3000 train_loss: 47.3343391418457 test_loss:1011.2196044921875\n",
      "1433/3000 train_loss: 45.717472076416016 test_loss:1016.04296875\n",
      "1434/3000 train_loss: 45.52151870727539 test_loss:993.9595947265625\n",
      "1435/3000 train_loss: 43.77184295654297 test_loss:989.726806640625\n",
      "1436/3000 train_loss: 57.80895233154297 test_loss:994.5131225585938\n",
      "1437/3000 train_loss: 46.98369598388672 test_loss:981.3331298828125\n",
      "1438/3000 train_loss: 46.99088668823242 test_loss:1001.1673583984375\n",
      "1439/3000 train_loss: 46.2952766418457 test_loss:1011.0371704101562\n",
      "1440/3000 train_loss: 42.781639099121094 test_loss:1065.1243896484375\n",
      "1441/3000 train_loss: 45.53329086303711 test_loss:1051.93798828125\n",
      "1442/3000 train_loss: 58.439048767089844 test_loss:1069.1971435546875\n",
      "1443/3000 train_loss: 53.62671661376953 test_loss:942.9144897460938\n",
      "1444/3000 train_loss: 57.24991226196289 test_loss:1008.9212646484375\n",
      "1445/3000 train_loss: 43.28157424926758 test_loss:993.8621826171875\n",
      "1446/3000 train_loss: 40.8287353515625 test_loss:1008.74365234375\n",
      "1447/3000 train_loss: 43.28523635864258 test_loss:1021.59912109375\n",
      "1448/3000 train_loss: 43.78878402709961 test_loss:976.9998168945312\n",
      "1449/3000 train_loss: 42.16917419433594 test_loss:1010.098876953125\n",
      "1450/3000 train_loss: 44.47897720336914 test_loss:1045.3839111328125\n",
      "1451/3000 train_loss: 44.80240249633789 test_loss:1044.83203125\n",
      "1452/3000 train_loss: 48.0955810546875 test_loss:1008.1572265625\n",
      "1453/3000 train_loss: 43.29715347290039 test_loss:1023.3262939453125\n",
      "1454/3000 train_loss: 51.67086410522461 test_loss:1027.2542724609375\n",
      "1455/3000 train_loss: 45.31743621826172 test_loss:1026.490234375\n",
      "1456/3000 train_loss: 43.32514953613281 test_loss:1004.7607421875\n",
      "1457/3000 train_loss: 41.41071701049805 test_loss:1002.3326416015625\n",
      "1458/3000 train_loss: 46.09347915649414 test_loss:960.1376953125\n",
      "1459/3000 train_loss: 45.982994079589844 test_loss:995.1356201171875\n",
      "1460/3000 train_loss: 41.6011848449707 test_loss:1039.5703125\n",
      "1461/3000 train_loss: 54.89663314819336 test_loss:1019.8974609375\n",
      "1462/3000 train_loss: 48.541481018066406 test_loss:1042.94873046875\n",
      "1463/3000 train_loss: 46.39784240722656 test_loss:1046.524658203125\n",
      "1464/3000 train_loss: 43.659156799316406 test_loss:1054.5240478515625\n",
      "1465/3000 train_loss: 45.92897033691406 test_loss:1031.6051025390625\n",
      "1466/3000 train_loss: 43.93766403198242 test_loss:1031.8919677734375\n",
      "1467/3000 train_loss: 44.30474090576172 test_loss:985.4501342773438\n",
      "1468/3000 train_loss: 46.23564147949219 test_loss:998.1299438476562\n",
      "1469/3000 train_loss: 46.02031326293945 test_loss:1002.9779052734375\n",
      "1470/3000 train_loss: 42.19395065307617 test_loss:998.7159423828125\n",
      "1471/3000 train_loss: 41.379398345947266 test_loss:1017.2171020507812\n",
      "1472/3000 train_loss: 44.43886184692383 test_loss:978.8217163085938\n",
      "1473/3000 train_loss: 45.291263580322266 test_loss:960.0126953125\n",
      "1474/3000 train_loss: 40.3381462097168 test_loss:976.8775634765625\n",
      "1475/3000 train_loss: 47.43158721923828 test_loss:974.8115234375\n",
      "1476/3000 train_loss: 42.03697204589844 test_loss:1005.9945068359375\n",
      "1477/3000 train_loss: 39.91194152832031 test_loss:951.5054931640625\n",
      "1478/3000 train_loss: 47.460453033447266 test_loss:988.674560546875\n",
      "1479/3000 train_loss: 55.1500358581543 test_loss:995.4552001953125\n",
      "1480/3000 train_loss: 50.62265396118164 test_loss:1035.888671875\n",
      "1481/3000 train_loss: 56.897850036621094 test_loss:1067.0863037109375\n",
      "1482/3000 train_loss: 55.001163482666016 test_loss:996.03857421875\n",
      "1483/3000 train_loss: 47.651973724365234 test_loss:1025.49365234375\n",
      "1484/3000 train_loss: 46.950164794921875 test_loss:1009.7937622070312\n",
      "1485/3000 train_loss: 48.98546600341797 test_loss:1001.755615234375\n",
      "1486/3000 train_loss: 51.986236572265625 test_loss:983.5595703125\n",
      "1487/3000 train_loss: 42.755577087402344 test_loss:994.4949340820312\n",
      "1488/3000 train_loss: 48.016212463378906 test_loss:1029.57177734375\n",
      "1489/3000 train_loss: 45.448638916015625 test_loss:1014.1285400390625\n",
      "1490/3000 train_loss: 51.14000701904297 test_loss:1029.009521484375\n",
      "1491/3000 train_loss: 50.11064147949219 test_loss:1080.72705078125\n",
      "1492/3000 train_loss: 46.48784637451172 test_loss:1017.5402221679688\n",
      "1493/3000 train_loss: 55.05571746826172 test_loss:1007.798828125\n",
      "1494/3000 train_loss: 39.408599853515625 test_loss:998.0787353515625\n",
      "1495/3000 train_loss: 45.067710876464844 test_loss:1006.6107788085938\n",
      "1496/3000 train_loss: 45.545745849609375 test_loss:1009.2188110351562\n",
      "1497/3000 train_loss: 43.999427795410156 test_loss:1004.298828125\n",
      "1498/3000 train_loss: 43.584129333496094 test_loss:1041.2125244140625\n",
      "1499/3000 train_loss: 59.1072998046875 test_loss:1073.42333984375\n",
      "1500/3000 train_loss: 50.95380401611328 test_loss:1044.34619140625\n",
      "1501/3000 train_loss: 52.24623107910156 test_loss:1088.579833984375\n",
      "1502/3000 train_loss: 45.5096435546875 test_loss:1054.96533203125\n",
      "1503/3000 train_loss: 43.87657928466797 test_loss:1057.612548828125\n",
      "1504/3000 train_loss: 37.96900939941406 test_loss:1049.9659423828125\n",
      "1505/3000 train_loss: 48.411720275878906 test_loss:1064.3424072265625\n",
      "1506/3000 train_loss: 42.940120697021484 test_loss:1024.2862548828125\n",
      "1507/3000 train_loss: 43.20132064819336 test_loss:1065.4681396484375\n",
      "1508/3000 train_loss: 46.899757385253906 test_loss:1059.9412841796875\n",
      "1509/3000 train_loss: 43.99565887451172 test_loss:998.0003051757812\n",
      "1510/3000 train_loss: 47.77397155761719 test_loss:1039.88818359375\n",
      "1511/3000 train_loss: 44.321510314941406 test_loss:1035.892822265625\n",
      "1512/3000 train_loss: 44.85078811645508 test_loss:1032.0479736328125\n",
      "1513/3000 train_loss: 43.296207427978516 test_loss:1054.0579833984375\n",
      "1514/3000 train_loss: 42.335487365722656 test_loss:1067.814453125\n",
      "1515/3000 train_loss: 44.53065872192383 test_loss:1053.5711669921875\n",
      "1516/3000 train_loss: 45.96776580810547 test_loss:1019.3255004882812\n",
      "1517/3000 train_loss: 49.261940002441406 test_loss:1037.306640625\n",
      "1518/3000 train_loss: 43.49571228027344 test_loss:1011.2261962890625\n",
      "1519/3000 train_loss: 44.15235900878906 test_loss:976.9093627929688\n",
      "1520/3000 train_loss: 40.79634475708008 test_loss:1010.12255859375\n",
      "1521/3000 train_loss: 48.490203857421875 test_loss:957.79736328125\n",
      "1522/3000 train_loss: 44.85491180419922 test_loss:1019.9385986328125\n",
      "1523/3000 train_loss: 42.02705764770508 test_loss:1004.7698974609375\n",
      "1524/3000 train_loss: 46.41773986816406 test_loss:1026.5367431640625\n",
      "1525/3000 train_loss: 43.39045333862305 test_loss:1071.548828125\n",
      "1526/3000 train_loss: 42.31199645996094 test_loss:1132.33642578125\n",
      "1527/3000 train_loss: 49.34204864501953 test_loss:1051.1334228515625\n",
      "1528/3000 train_loss: 50.04427719116211 test_loss:1033.1444091796875\n",
      "1529/3000 train_loss: 50.02808380126953 test_loss:1041.65576171875\n",
      "1530/3000 train_loss: 50.26012420654297 test_loss:1049.1292724609375\n",
      "1531/3000 train_loss: 55.22077941894531 test_loss:1041.415283203125\n",
      "1532/3000 train_loss: 43.877689361572266 test_loss:995.13232421875\n",
      "1533/3000 train_loss: 48.34513854980469 test_loss:1015.994384765625\n",
      "1534/3000 train_loss: 45.229652404785156 test_loss:1029.81884765625\n",
      "1535/3000 train_loss: 40.35855484008789 test_loss:1040.435302734375\n",
      "1536/3000 train_loss: 39.085350036621094 test_loss:1004.86865234375\n",
      "1537/3000 train_loss: 45.138893127441406 test_loss:1029.1531982421875\n",
      "1538/3000 train_loss: 39.68516159057617 test_loss:1089.4837646484375\n",
      "1539/3000 train_loss: 48.35736846923828 test_loss:1057.112060546875\n",
      "1540/3000 train_loss: 49.93482208251953 test_loss:1062.2115478515625\n",
      "1541/3000 train_loss: 49.718780517578125 test_loss:1031.300537109375\n",
      "1542/3000 train_loss: 41.418033599853516 test_loss:1027.9093017578125\n",
      "1543/3000 train_loss: 43.647377014160156 test_loss:1008.7285766601562\n",
      "1544/3000 train_loss: 44.17131805419922 test_loss:1015.810791015625\n",
      "1545/3000 train_loss: 46.123008728027344 test_loss:979.230224609375\n",
      "1546/3000 train_loss: 42.20233917236328 test_loss:996.6922607421875\n",
      "1547/3000 train_loss: 53.84556579589844 test_loss:1056.604736328125\n",
      "1548/3000 train_loss: 51.26482391357422 test_loss:1086.43798828125\n",
      "1549/3000 train_loss: 41.801551818847656 test_loss:1051.962158203125\n",
      "1550/3000 train_loss: 38.80466079711914 test_loss:1049.9520263671875\n",
      "1551/3000 train_loss: 38.812286376953125 test_loss:1053.6654052734375\n",
      "1552/3000 train_loss: 39.360206604003906 test_loss:1011.2381591796875\n",
      "1553/3000 train_loss: 36.794700622558594 test_loss:1020.462646484375\n",
      "1554/3000 train_loss: 48.80006408691406 test_loss:1022.8758544921875\n",
      "1555/3000 train_loss: 53.63969039916992 test_loss:1081.437255859375\n",
      "1556/3000 train_loss: 61.61914825439453 test_loss:1091.7420654296875\n",
      "1557/3000 train_loss: 39.98108673095703 test_loss:1056.2249755859375\n",
      "1558/3000 train_loss: 46.21830368041992 test_loss:1049.271728515625\n",
      "1559/3000 train_loss: 48.70820617675781 test_loss:1090.518798828125\n",
      "1560/3000 train_loss: 47.44115447998047 test_loss:1076.051025390625\n",
      "1561/3000 train_loss: 49.678245544433594 test_loss:1071.343505859375\n",
      "1562/3000 train_loss: 42.07229232788086 test_loss:1016.6805419921875\n",
      "1563/3000 train_loss: 42.568912506103516 test_loss:1073.9091796875\n",
      "1564/3000 train_loss: 41.721656799316406 test_loss:1070.841796875\n",
      "1565/3000 train_loss: 38.08835983276367 test_loss:1115.939208984375\n",
      "1566/3000 train_loss: 40.01904296875 test_loss:1063.448486328125\n",
      "1567/3000 train_loss: 44.20697784423828 test_loss:1055.9884033203125\n",
      "1568/3000 train_loss: 46.89995193481445 test_loss:1049.0595703125\n",
      "1569/3000 train_loss: 47.26823425292969 test_loss:1075.5338134765625\n",
      "1570/3000 train_loss: 46.92110824584961 test_loss:1072.4461669921875\n",
      "1571/3000 train_loss: 49.225624084472656 test_loss:1092.794921875\n",
      "1572/3000 train_loss: 42.828041076660156 test_loss:1045.1138916015625\n",
      "1573/3000 train_loss: 43.60488510131836 test_loss:1033.5394287109375\n",
      "1574/3000 train_loss: 41.728538513183594 test_loss:1053.7430419921875\n",
      "1575/3000 train_loss: 52.27909851074219 test_loss:1049.21337890625\n",
      "1576/3000 train_loss: 44.23788833618164 test_loss:1040.829833984375\n",
      "1577/3000 train_loss: 44.35044479370117 test_loss:1051.433837890625\n",
      "1578/3000 train_loss: 40.346561431884766 test_loss:1022.3818359375\n",
      "1579/3000 train_loss: 42.58274459838867 test_loss:1046.707275390625\n",
      "1580/3000 train_loss: 45.72517395019531 test_loss:1090.026123046875\n",
      "1581/3000 train_loss: 49.605682373046875 test_loss:1095.919677734375\n",
      "1582/3000 train_loss: 51.08573913574219 test_loss:1031.5760498046875\n",
      "1583/3000 train_loss: 42.72486877441406 test_loss:1008.9279174804688\n",
      "1584/3000 train_loss: 43.392478942871094 test_loss:1015.4664306640625\n",
      "1585/3000 train_loss: 41.96146774291992 test_loss:1055.96142578125\n",
      "1586/3000 train_loss: 41.067230224609375 test_loss:1035.46533203125\n",
      "1587/3000 train_loss: 38.62513732910156 test_loss:1034.090576171875\n",
      "1588/3000 train_loss: 46.80086898803711 test_loss:1012.1378784179688\n",
      "1589/3000 train_loss: 49.83433532714844 test_loss:1006.8255615234375\n",
      "1590/3000 train_loss: 48.237770080566406 test_loss:976.5535278320312\n",
      "1591/3000 train_loss: 39.3567008972168 test_loss:1023.4859619140625\n",
      "1592/3000 train_loss: 40.47730255126953 test_loss:1029.9384765625\n",
      "1593/3000 train_loss: 49.20164489746094 test_loss:1017.7904663085938\n",
      "1594/3000 train_loss: 50.814762115478516 test_loss:1015.7699584960938\n",
      "1595/3000 train_loss: 42.425987243652344 test_loss:1050.2685546875\n",
      "1596/3000 train_loss: 39.333831787109375 test_loss:1030.94140625\n",
      "1597/3000 train_loss: 43.893836975097656 test_loss:996.5784912109375\n",
      "1598/3000 train_loss: 53.741355895996094 test_loss:1015.5355224609375\n",
      "1599/3000 train_loss: 46.672508239746094 test_loss:1038.677001953125\n",
      "1600/3000 train_loss: 43.55307388305664 test_loss:1034.3228759765625\n",
      "1601/3000 train_loss: 42.98763656616211 test_loss:1005.4873657226562\n",
      "1602/3000 train_loss: 43.78071594238281 test_loss:975.8741455078125\n",
      "1603/3000 train_loss: 39.115692138671875 test_loss:996.8955078125\n",
      "1604/3000 train_loss: 43.14785385131836 test_loss:996.0283813476562\n",
      "1605/3000 train_loss: 42.17938995361328 test_loss:961.3721923828125\n",
      "1606/3000 train_loss: 42.845741271972656 test_loss:956.2679443359375\n",
      "1607/3000 train_loss: 43.49483108520508 test_loss:926.1971435546875\n",
      "1608/3000 train_loss: 45.64501190185547 test_loss:956.949951171875\n",
      "1609/3000 train_loss: 48.1839714050293 test_loss:998.4161376953125\n",
      "1610/3000 train_loss: 42.78339385986328 test_loss:892.5070190429688\n",
      "1611/3000 train_loss: 42.317169189453125 test_loss:968.732177734375\n",
      "1612/3000 train_loss: 39.807960510253906 test_loss:957.8720092773438\n",
      "1613/3000 train_loss: 46.06036376953125 test_loss:984.5308837890625\n",
      "1614/3000 train_loss: 42.93006896972656 test_loss:1001.1099853515625\n",
      "1615/3000 train_loss: 45.17900466918945 test_loss:1002.4197998046875\n",
      "1616/3000 train_loss: 42.77311325073242 test_loss:1008.1754760742188\n",
      "1617/3000 train_loss: 49.631141662597656 test_loss:1012.6445922851562\n",
      "1618/3000 train_loss: 43.73387145996094 test_loss:980.3935546875\n",
      "1619/3000 train_loss: 41.23958969116211 test_loss:985.9801635742188\n",
      "1620/3000 train_loss: 50.96903991699219 test_loss:988.246826171875\n",
      "1621/3000 train_loss: 47.1837043762207 test_loss:1031.55029296875\n",
      "1622/3000 train_loss: 41.20089340209961 test_loss:926.078125\n",
      "1623/3000 train_loss: 44.56257629394531 test_loss:946.1514282226562\n",
      "1624/3000 train_loss: 42.209964752197266 test_loss:921.2222900390625\n",
      "1625/3000 train_loss: 45.41973876953125 test_loss:917.278076171875\n",
      "1626/3000 train_loss: 48.6390266418457 test_loss:1016.088134765625\n",
      "1627/3000 train_loss: 46.5426139831543 test_loss:1049.0535888671875\n",
      "1628/3000 train_loss: 40.17679977416992 test_loss:1014.1343383789062\n",
      "1629/3000 train_loss: 42.269344329833984 test_loss:1014.300048828125\n",
      "1630/3000 train_loss: 38.30252456665039 test_loss:1028.659423828125\n",
      "1631/3000 train_loss: 41.6148796081543 test_loss:1025.17578125\n",
      "1632/3000 train_loss: 40.7952766418457 test_loss:1026.205322265625\n",
      "1633/3000 train_loss: 39.82795715332031 test_loss:1021.0992431640625\n",
      "1634/3000 train_loss: 48.12086868286133 test_loss:1005.886474609375\n",
      "1635/3000 train_loss: 45.122859954833984 test_loss:1037.1480712890625\n",
      "1636/3000 train_loss: 43.39359664916992 test_loss:1056.2335205078125\n",
      "1637/3000 train_loss: 40.59571838378906 test_loss:1062.973876953125\n",
      "1638/3000 train_loss: 39.84321975708008 test_loss:1097.9803466796875\n",
      "1639/3000 train_loss: 44.690574645996094 test_loss:1067.2862548828125\n",
      "1640/3000 train_loss: 47.95967102050781 test_loss:1045.6475830078125\n",
      "1641/3000 train_loss: 38.744136810302734 test_loss:1085.4752197265625\n",
      "1642/3000 train_loss: 38.87239074707031 test_loss:1056.482177734375\n",
      "1643/3000 train_loss: 44.79157638549805 test_loss:1038.40087890625\n",
      "1644/3000 train_loss: 40.96481704711914 test_loss:1031.894775390625\n",
      "1645/3000 train_loss: 44.20484924316406 test_loss:1053.22607421875\n",
      "1646/3000 train_loss: 41.48588562011719 test_loss:1079.905029296875\n",
      "1647/3000 train_loss: 40.52927017211914 test_loss:1061.877197265625\n",
      "1648/3000 train_loss: 42.1164436340332 test_loss:1073.17431640625\n",
      "1649/3000 train_loss: 38.18013000488281 test_loss:1040.9312744140625\n",
      "1650/3000 train_loss: 38.1306266784668 test_loss:1117.088623046875\n",
      "1651/3000 train_loss: 44.6642951965332 test_loss:1107.110595703125\n",
      "1652/3000 train_loss: 42.99388122558594 test_loss:1086.01806640625\n",
      "1653/3000 train_loss: 38.63996887207031 test_loss:1073.7818603515625\n",
      "1654/3000 train_loss: 39.91017150878906 test_loss:1046.3079833984375\n",
      "1655/3000 train_loss: 47.994712829589844 test_loss:1104.526611328125\n",
      "1656/3000 train_loss: 43.54710388183594 test_loss:1137.028564453125\n",
      "1657/3000 train_loss: 47.07191467285156 test_loss:1098.972412109375\n",
      "1658/3000 train_loss: 41.6405029296875 test_loss:1078.356689453125\n",
      "1659/3000 train_loss: 45.774078369140625 test_loss:1139.466552734375\n",
      "1660/3000 train_loss: 40.72888946533203 test_loss:1135.22314453125\n",
      "1661/3000 train_loss: 37.526432037353516 test_loss:1109.1646728515625\n",
      "1662/3000 train_loss: 43.98126220703125 test_loss:1112.8148193359375\n",
      "1663/3000 train_loss: 42.53801727294922 test_loss:1065.046142578125\n",
      "1664/3000 train_loss: 40.45204162597656 test_loss:1044.9681396484375\n",
      "1665/3000 train_loss: 40.13017272949219 test_loss:1027.3721923828125\n",
      "1666/3000 train_loss: 41.75870895385742 test_loss:1013.8429565429688\n",
      "1667/3000 train_loss: 46.8699951171875 test_loss:1012.6448974609375\n",
      "1668/3000 train_loss: 47.841712951660156 test_loss:1057.2733154296875\n",
      "1669/3000 train_loss: 43.34801483154297 test_loss:998.630126953125\n",
      "1670/3000 train_loss: 38.695899963378906 test_loss:1037.185791015625\n",
      "1671/3000 train_loss: 40.39234924316406 test_loss:1023.875\n",
      "1672/3000 train_loss: 39.742103576660156 test_loss:1033.7413330078125\n",
      "1673/3000 train_loss: 39.80451965332031 test_loss:1085.0208740234375\n",
      "1674/3000 train_loss: 43.518314361572266 test_loss:1088.5213623046875\n",
      "1675/3000 train_loss: 48.12637710571289 test_loss:1103.02392578125\n",
      "1676/3000 train_loss: 40.87890625 test_loss:1141.455810546875\n",
      "1677/3000 train_loss: 43.301902770996094 test_loss:1044.94970703125\n",
      "1678/3000 train_loss: 39.57453918457031 test_loss:1126.796875\n",
      "1679/3000 train_loss: 45.79474639892578 test_loss:1070.4588623046875\n",
      "1680/3000 train_loss: 42.76817321777344 test_loss:1079.232177734375\n",
      "1681/3000 train_loss: 50.686641693115234 test_loss:1049.3516845703125\n",
      "1682/3000 train_loss: 40.3535270690918 test_loss:1065.064697265625\n",
      "1683/3000 train_loss: 42.78028869628906 test_loss:1115.3916015625\n",
      "1684/3000 train_loss: 40.66526794433594 test_loss:1108.27783203125\n",
      "1685/3000 train_loss: 40.98405456542969 test_loss:1047.8409423828125\n",
      "1686/3000 train_loss: 42.43131637573242 test_loss:1046.8447265625\n",
      "1687/3000 train_loss: 44.591365814208984 test_loss:1028.877685546875\n",
      "1688/3000 train_loss: 40.754825592041016 test_loss:1022.7821044921875\n",
      "1689/3000 train_loss: 40.70842742919922 test_loss:1034.197998046875\n",
      "1690/3000 train_loss: 49.629608154296875 test_loss:1094.854736328125\n",
      "1691/3000 train_loss: 47.04819869995117 test_loss:1051.4825439453125\n",
      "1692/3000 train_loss: 39.343360900878906 test_loss:1082.595703125\n",
      "1693/3000 train_loss: 42.08926773071289 test_loss:1029.6787109375\n",
      "1694/3000 train_loss: 39.632450103759766 test_loss:1038.2550048828125\n",
      "1695/3000 train_loss: 39.12083435058594 test_loss:1056.387451171875\n",
      "1696/3000 train_loss: 37.388671875 test_loss:1084.550048828125\n",
      "1697/3000 train_loss: 38.17430114746094 test_loss:1072.07666015625\n",
      "1698/3000 train_loss: 48.59987258911133 test_loss:1052.45556640625\n",
      "1699/3000 train_loss: 41.19236373901367 test_loss:994.6583862304688\n",
      "1700/3000 train_loss: 44.820987701416016 test_loss:988.343505859375\n",
      "1701/3000 train_loss: 44.12973403930664 test_loss:1016.267822265625\n",
      "1702/3000 train_loss: 43.73136520385742 test_loss:1014.122802734375\n",
      "1703/3000 train_loss: 38.0663948059082 test_loss:1009.659423828125\n",
      "1704/3000 train_loss: 43.29827117919922 test_loss:1041.438232421875\n",
      "1705/3000 train_loss: 38.18912887573242 test_loss:1073.353759765625\n",
      "1706/3000 train_loss: 40.80096435546875 test_loss:1043.6824951171875\n",
      "1707/3000 train_loss: 38.39558029174805 test_loss:1064.9219970703125\n",
      "1708/3000 train_loss: 43.9820556640625 test_loss:1077.577880859375\n",
      "1709/3000 train_loss: 37.38798904418945 test_loss:1046.8585205078125\n",
      "1710/3000 train_loss: 41.480369567871094 test_loss:1000.50048828125\n",
      "1711/3000 train_loss: 39.639774322509766 test_loss:1017.8133544921875\n",
      "1712/3000 train_loss: 38.68313980102539 test_loss:1048.125244140625\n",
      "1713/3000 train_loss: 42.68417739868164 test_loss:1034.78173828125\n",
      "1714/3000 train_loss: 43.6516227722168 test_loss:1071.724365234375\n",
      "1715/3000 train_loss: 39.379878997802734 test_loss:1040.7642822265625\n",
      "1716/3000 train_loss: 39.96366882324219 test_loss:1031.625244140625\n",
      "1717/3000 train_loss: 43.4649543762207 test_loss:1055.231201171875\n",
      "1718/3000 train_loss: 44.70513916015625 test_loss:1047.10546875\n",
      "1719/3000 train_loss: 35.632606506347656 test_loss:989.8233642578125\n",
      "1720/3000 train_loss: 34.96018981933594 test_loss:1002.3441162109375\n",
      "1721/3000 train_loss: 43.81722640991211 test_loss:1010.02392578125\n",
      "1722/3000 train_loss: 45.83799743652344 test_loss:1002.2255859375\n",
      "1723/3000 train_loss: 40.324851989746094 test_loss:987.826416015625\n",
      "1724/3000 train_loss: 44.024925231933594 test_loss:1042.7236328125\n",
      "1725/3000 train_loss: 42.713314056396484 test_loss:1058.58984375\n",
      "1726/3000 train_loss: 44.4762077331543 test_loss:1004.8313598632812\n",
      "1727/3000 train_loss: 46.51837158203125 test_loss:1032.4273681640625\n",
      "1728/3000 train_loss: 37.46401596069336 test_loss:1049.294189453125\n",
      "1729/3000 train_loss: 38.7827033996582 test_loss:1021.837890625\n",
      "1730/3000 train_loss: 43.92750549316406 test_loss:1033.244873046875\n",
      "1731/3000 train_loss: 41.80782699584961 test_loss:1025.6820068359375\n",
      "1732/3000 train_loss: 42.48786544799805 test_loss:1040.1019287109375\n",
      "1733/3000 train_loss: 43.456298828125 test_loss:1038.332275390625\n",
      "1734/3000 train_loss: 39.01532745361328 test_loss:1041.458251953125\n",
      "1735/3000 train_loss: 41.63130187988281 test_loss:1023.2550048828125\n",
      "1736/3000 train_loss: 43.47822189331055 test_loss:1038.38134765625\n",
      "1737/3000 train_loss: 42.79302215576172 test_loss:1032.048828125\n",
      "1738/3000 train_loss: 38.12748718261719 test_loss:1055.7314453125\n",
      "1739/3000 train_loss: 44.498043060302734 test_loss:1078.0833740234375\n",
      "1740/3000 train_loss: 36.612998962402344 test_loss:1075.2706298828125\n",
      "1741/3000 train_loss: 37.90481948852539 test_loss:1070.516357421875\n",
      "1742/3000 train_loss: 43.36739730834961 test_loss:1061.057861328125\n",
      "1743/3000 train_loss: 42.332244873046875 test_loss:1075.320556640625\n",
      "1744/3000 train_loss: 43.33195114135742 test_loss:1084.284912109375\n",
      "1745/3000 train_loss: 35.61314010620117 test_loss:1097.9708251953125\n",
      "1746/3000 train_loss: 38.60062026977539 test_loss:1056.6241455078125\n",
      "1747/3000 train_loss: 39.31340026855469 test_loss:1048.460693359375\n",
      "1748/3000 train_loss: 41.505619049072266 test_loss:1018.2642211914062\n",
      "1749/3000 train_loss: 40.86351013183594 test_loss:1074.1708984375\n",
      "1750/3000 train_loss: 39.43216323852539 test_loss:1007.998779296875\n",
      "1751/3000 train_loss: 34.46160888671875 test_loss:1002.7485961914062\n",
      "1752/3000 train_loss: 49.99956512451172 test_loss:1010.2913208007812\n",
      "1753/3000 train_loss: 46.10527801513672 test_loss:969.2244873046875\n",
      "1754/3000 train_loss: 38.95573425292969 test_loss:1000.4788818359375\n",
      "1755/3000 train_loss: 41.38553237915039 test_loss:1033.893798828125\n",
      "1756/3000 train_loss: 37.85081100463867 test_loss:1081.20361328125\n",
      "1757/3000 train_loss: 41.01209259033203 test_loss:1036.307861328125\n",
      "1758/3000 train_loss: 39.159854888916016 test_loss:1023.24169921875\n",
      "1759/3000 train_loss: 37.44639205932617 test_loss:1036.314453125\n",
      "1760/3000 train_loss: 48.438350677490234 test_loss:1042.7508544921875\n",
      "1761/3000 train_loss: 37.558509826660156 test_loss:1009.2023315429688\n",
      "1762/3000 train_loss: 44.766910552978516 test_loss:1023.2745361328125\n",
      "1763/3000 train_loss: 49.448814392089844 test_loss:1020.5413208007812\n",
      "1764/3000 train_loss: 41.25020980834961 test_loss:1104.03759765625\n",
      "1765/3000 train_loss: 41.93368911743164 test_loss:1097.17529296875\n",
      "1766/3000 train_loss: 44.75382614135742 test_loss:1088.8720703125\n",
      "1767/3000 train_loss: 45.90556716918945 test_loss:1042.323486328125\n",
      "1768/3000 train_loss: 49.68505096435547 test_loss:978.4147338867188\n",
      "1769/3000 train_loss: 45.58486557006836 test_loss:1010.067626953125\n",
      "1770/3000 train_loss: 45.811214447021484 test_loss:997.8101806640625\n",
      "1771/3000 train_loss: 41.096412658691406 test_loss:1007.6663208007812\n",
      "1772/3000 train_loss: 39.639041900634766 test_loss:1004.8037109375\n",
      "1773/3000 train_loss: 42.480098724365234 test_loss:992.1778564453125\n",
      "1774/3000 train_loss: 43.59346008300781 test_loss:1029.713623046875\n",
      "1775/3000 train_loss: 45.10747146606445 test_loss:977.244140625\n",
      "1776/3000 train_loss: 43.980125427246094 test_loss:1002.6654052734375\n",
      "1777/3000 train_loss: 45.098690032958984 test_loss:998.635009765625\n",
      "1778/3000 train_loss: 41.700531005859375 test_loss:929.9976806640625\n",
      "1779/3000 train_loss: 42.4640007019043 test_loss:948.667724609375\n",
      "1780/3000 train_loss: 40.691192626953125 test_loss:941.867919921875\n",
      "1781/3000 train_loss: 42.54877853393555 test_loss:984.843017578125\n",
      "1782/3000 train_loss: 44.494171142578125 test_loss:944.803955078125\n",
      "1783/3000 train_loss: 41.1772575378418 test_loss:962.7691650390625\n",
      "1784/3000 train_loss: 37.4938850402832 test_loss:1007.6412963867188\n",
      "1785/3000 train_loss: 39.665802001953125 test_loss:1036.0291748046875\n",
      "1786/3000 train_loss: 38.096153259277344 test_loss:1016.4024658203125\n",
      "1787/3000 train_loss: 34.44260787963867 test_loss:984.470703125\n",
      "1788/3000 train_loss: 45.31381607055664 test_loss:967.7139892578125\n",
      "1789/3000 train_loss: 42.17980194091797 test_loss:952.6202392578125\n",
      "1790/3000 train_loss: 43.35887908935547 test_loss:918.4482421875\n",
      "1791/3000 train_loss: 44.14044189453125 test_loss:902.6286010742188\n",
      "1792/3000 train_loss: 41.778961181640625 test_loss:951.0223388671875\n",
      "1793/3000 train_loss: 40.979881286621094 test_loss:985.5559692382812\n",
      "1794/3000 train_loss: 41.372859954833984 test_loss:988.2755126953125\n",
      "1795/3000 train_loss: 42.181331634521484 test_loss:970.27392578125\n",
      "1796/3000 train_loss: 37.795467376708984 test_loss:956.85498046875\n",
      "1797/3000 train_loss: 41.24802780151367 test_loss:930.6302490234375\n",
      "1798/3000 train_loss: 34.245521545410156 test_loss:993.27001953125\n",
      "1799/3000 train_loss: 41.26801681518555 test_loss:967.010986328125\n",
      "1800/3000 train_loss: 38.138999938964844 test_loss:963.6132202148438\n",
      "1801/3000 train_loss: 41.825462341308594 test_loss:962.4425048828125\n",
      "1802/3000 train_loss: 45.85280990600586 test_loss:990.4088745117188\n",
      "1803/3000 train_loss: 37.23748016357422 test_loss:958.931396484375\n",
      "1804/3000 train_loss: 34.77086639404297 test_loss:960.1292724609375\n",
      "1805/3000 train_loss: 41.2952995300293 test_loss:913.3985595703125\n",
      "1806/3000 train_loss: 36.61378860473633 test_loss:956.8338623046875\n",
      "1807/3000 train_loss: 42.79752731323242 test_loss:1003.8710327148438\n",
      "1808/3000 train_loss: 40.892578125 test_loss:951.028564453125\n",
      "1809/3000 train_loss: 39.25768280029297 test_loss:1023.3815307617188\n",
      "1810/3000 train_loss: 46.747314453125 test_loss:1049.18115234375\n",
      "1811/3000 train_loss: 40.22358703613281 test_loss:958.7099609375\n",
      "1812/3000 train_loss: 41.71895980834961 test_loss:985.37890625\n",
      "1813/3000 train_loss: 42.044803619384766 test_loss:957.012939453125\n",
      "1814/3000 train_loss: 37.6882209777832 test_loss:952.3466796875\n",
      "1815/3000 train_loss: 47.2187614440918 test_loss:978.6026611328125\n",
      "1816/3000 train_loss: 37.436519622802734 test_loss:1003.235107421875\n",
      "1817/3000 train_loss: 43.672210693359375 test_loss:987.1595458984375\n",
      "1818/3000 train_loss: 49.03559875488281 test_loss:979.4780883789062\n",
      "1819/3000 train_loss: 52.88304138183594 test_loss:1102.357666015625\n",
      "1820/3000 train_loss: 48.81303405761719 test_loss:993.1281127929688\n",
      "1821/3000 train_loss: 43.71567153930664 test_loss:1054.6693115234375\n",
      "1822/3000 train_loss: 40.561954498291016 test_loss:1031.0999755859375\n",
      "1823/3000 train_loss: 43.09393310546875 test_loss:1014.462158203125\n",
      "1824/3000 train_loss: 40.32830810546875 test_loss:1027.787841796875\n",
      "1825/3000 train_loss: 45.95362854003906 test_loss:1060.56884765625\n",
      "1826/3000 train_loss: 41.107269287109375 test_loss:1033.95654296875\n",
      "1827/3000 train_loss: 39.4383544921875 test_loss:1023.4230346679688\n",
      "1828/3000 train_loss: 41.280189514160156 test_loss:1006.81298828125\n",
      "1829/3000 train_loss: 45.10570526123047 test_loss:991.9808959960938\n",
      "1830/3000 train_loss: 38.2969970703125 test_loss:963.5071411132812\n",
      "1831/3000 train_loss: 33.98782730102539 test_loss:938.147216796875\n",
      "1832/3000 train_loss: 41.8759880065918 test_loss:940.224609375\n",
      "1833/3000 train_loss: 42.818538665771484 test_loss:930.4376831054688\n",
      "1834/3000 train_loss: 42.084083557128906 test_loss:900.8427124023438\n",
      "1835/3000 train_loss: 42.58540725708008 test_loss:962.6619873046875\n",
      "1836/3000 train_loss: 40.15328598022461 test_loss:933.9261474609375\n",
      "1837/3000 train_loss: 36.72633361816406 test_loss:966.3826904296875\n",
      "1838/3000 train_loss: 38.00746536254883 test_loss:1004.3223876953125\n",
      "1839/3000 train_loss: 40.40263748168945 test_loss:960.408203125\n",
      "1840/3000 train_loss: 38.36323547363281 test_loss:1030.231201171875\n",
      "1841/3000 train_loss: 54.849449157714844 test_loss:1037.443359375\n",
      "1842/3000 train_loss: 46.27685546875 test_loss:1087.833740234375\n",
      "1843/3000 train_loss: 36.36830520629883 test_loss:1019.511962890625\n",
      "1844/3000 train_loss: 47.300899505615234 test_loss:1003.686767578125\n",
      "1845/3000 train_loss: 38.66304397583008 test_loss:954.3355712890625\n",
      "1846/3000 train_loss: 40.476470947265625 test_loss:1005.3352661132812\n",
      "1847/3000 train_loss: 44.25439453125 test_loss:1010.6015625\n",
      "1848/3000 train_loss: 38.90776443481445 test_loss:967.7880859375\n",
      "1849/3000 train_loss: 44.4757194519043 test_loss:1017.802001953125\n",
      "1850/3000 train_loss: 41.36129379272461 test_loss:904.83837890625\n",
      "1851/3000 train_loss: 42.041568756103516 test_loss:905.7510986328125\n",
      "1852/3000 train_loss: 39.34070587158203 test_loss:952.873779296875\n",
      "1853/3000 train_loss: 36.240211486816406 test_loss:964.2442016601562\n",
      "1854/3000 train_loss: 42.56074142456055 test_loss:988.4074096679688\n",
      "1855/3000 train_loss: 40.45375442504883 test_loss:986.6912841796875\n",
      "1856/3000 train_loss: 38.55130386352539 test_loss:968.0946655273438\n",
      "1857/3000 train_loss: 35.035430908203125 test_loss:958.1265258789062\n",
      "1858/3000 train_loss: 39.740020751953125 test_loss:1008.0831909179688\n",
      "1859/3000 train_loss: 37.933658599853516 test_loss:1031.1173095703125\n",
      "1860/3000 train_loss: 40.62376403808594 test_loss:976.7921142578125\n",
      "1861/3000 train_loss: 38.7893180847168 test_loss:965.9639892578125\n",
      "1862/3000 train_loss: 37.4410285949707 test_loss:944.4359130859375\n",
      "1863/3000 train_loss: 44.552703857421875 test_loss:933.812255859375\n",
      "1864/3000 train_loss: 42.78497314453125 test_loss:959.789794921875\n",
      "1865/3000 train_loss: 35.55044937133789 test_loss:995.04150390625\n",
      "1866/3000 train_loss: 38.22781753540039 test_loss:1009.3515625\n",
      "1867/3000 train_loss: 45.39781951904297 test_loss:940.955322265625\n",
      "1868/3000 train_loss: 36.99549102783203 test_loss:945.3344116210938\n",
      "1869/3000 train_loss: 33.265262603759766 test_loss:936.4172973632812\n",
      "1870/3000 train_loss: 39.55376434326172 test_loss:899.2022705078125\n",
      "1871/3000 train_loss: 34.46074295043945 test_loss:934.912841796875\n",
      "1872/3000 train_loss: 36.69913864135742 test_loss:936.461669921875\n",
      "1873/3000 train_loss: 37.439353942871094 test_loss:1016.0814208984375\n",
      "1874/3000 train_loss: 41.28087615966797 test_loss:1006.124267578125\n",
      "1875/3000 train_loss: 34.743614196777344 test_loss:1003.14453125\n",
      "1876/3000 train_loss: 38.74192810058594 test_loss:1051.0804443359375\n",
      "1877/3000 train_loss: 38.77054977416992 test_loss:1035.2548828125\n",
      "1878/3000 train_loss: 45.674991607666016 test_loss:1050.26611328125\n",
      "1879/3000 train_loss: 46.151885986328125 test_loss:1014.4939575195312\n",
      "1880/3000 train_loss: 36.26755142211914 test_loss:1015.3316650390625\n",
      "1881/3000 train_loss: 40.43718338012695 test_loss:931.3052978515625\n",
      "1882/3000 train_loss: 40.470890045166016 test_loss:969.7760009765625\n",
      "1883/3000 train_loss: 36.588966369628906 test_loss:969.7027587890625\n",
      "1884/3000 train_loss: 35.260826110839844 test_loss:1054.638671875\n",
      "1885/3000 train_loss: 43.49613571166992 test_loss:1022.990478515625\n",
      "1886/3000 train_loss: 35.3442497253418 test_loss:949.2342529296875\n",
      "1887/3000 train_loss: 36.891746520996094 test_loss:950.314697265625\n",
      "1888/3000 train_loss: 38.174129486083984 test_loss:1007.7638549804688\n",
      "1889/3000 train_loss: 42.2425651550293 test_loss:987.8865966796875\n",
      "1890/3000 train_loss: 41.71151351928711 test_loss:926.5606689453125\n",
      "1891/3000 train_loss: 39.5286979675293 test_loss:987.5115966796875\n",
      "1892/3000 train_loss: 56.83131408691406 test_loss:978.62744140625\n",
      "1893/3000 train_loss: 46.30931854248047 test_loss:1236.7545166015625\n",
      "1894/3000 train_loss: 45.61893081665039 test_loss:1191.48974609375\n",
      "1895/3000 train_loss: 44.52872085571289 test_loss:1076.6453857421875\n",
      "1896/3000 train_loss: 43.88314437866211 test_loss:1106.8072509765625\n",
      "1897/3000 train_loss: 39.65129089355469 test_loss:1091.2652587890625\n",
      "1898/3000 train_loss: 36.74689483642578 test_loss:1095.13330078125\n",
      "1899/3000 train_loss: 42.661109924316406 test_loss:1066.1043701171875\n",
      "1900/3000 train_loss: 36.83382034301758 test_loss:1040.405517578125\n",
      "1901/3000 train_loss: 33.741615295410156 test_loss:1039.82763671875\n",
      "1902/3000 train_loss: 41.625144958496094 test_loss:1055.223876953125\n",
      "1903/3000 train_loss: 36.88297653198242 test_loss:1011.9774169921875\n",
      "1904/3000 train_loss: 35.68400573730469 test_loss:988.1243896484375\n",
      "1905/3000 train_loss: 39.32724380493164 test_loss:1041.13330078125\n",
      "1906/3000 train_loss: 35.447410583496094 test_loss:1001.1688232421875\n",
      "1907/3000 train_loss: 35.64888381958008 test_loss:950.2886962890625\n",
      "1908/3000 train_loss: 40.32354736328125 test_loss:948.8143310546875\n",
      "1909/3000 train_loss: 42.05290222167969 test_loss:965.6107788085938\n",
      "1910/3000 train_loss: 46.33647155761719 test_loss:903.6612548828125\n",
      "1911/3000 train_loss: 35.62443542480469 test_loss:869.58837890625\n",
      "1912/3000 train_loss: 41.18514633178711 test_loss:897.9150390625\n",
      "1913/3000 train_loss: 37.266197204589844 test_loss:864.1363525390625\n",
      "1914/3000 train_loss: 35.43857192993164 test_loss:893.9169921875\n",
      "1915/3000 train_loss: 38.151432037353516 test_loss:945.8134155273438\n",
      "1916/3000 train_loss: 32.76624298095703 test_loss:946.2326049804688\n",
      "1917/3000 train_loss: 42.631229400634766 test_loss:962.1070556640625\n",
      "1918/3000 train_loss: 35.70058059692383 test_loss:989.2581787109375\n",
      "1919/3000 train_loss: 45.18776321411133 test_loss:1004.4448852539062\n",
      "1920/3000 train_loss: 34.5135612487793 test_loss:934.3360595703125\n",
      "1921/3000 train_loss: 40.6563606262207 test_loss:932.004638671875\n",
      "1922/3000 train_loss: 36.20438766479492 test_loss:933.9766845703125\n",
      "1923/3000 train_loss: 53.77204895019531 test_loss:950.1961669921875\n",
      "1924/3000 train_loss: 42.05492401123047 test_loss:967.0114135742188\n",
      "1925/3000 train_loss: 42.80781936645508 test_loss:942.566650390625\n",
      "1926/3000 train_loss: 33.489402770996094 test_loss:907.1651000976562\n",
      "1927/3000 train_loss: 38.0440559387207 test_loss:915.4530029296875\n",
      "1928/3000 train_loss: 36.571590423583984 test_loss:919.33056640625\n",
      "1929/3000 train_loss: 39.18769836425781 test_loss:963.1573486328125\n",
      "1930/3000 train_loss: 36.22677993774414 test_loss:967.3251953125\n",
      "1931/3000 train_loss: 36.16064453125 test_loss:942.9639892578125\n",
      "1932/3000 train_loss: 42.11405944824219 test_loss:970.7509155273438\n",
      "1933/3000 train_loss: 39.59537124633789 test_loss:968.874267578125\n",
      "1934/3000 train_loss: 40.980690002441406 test_loss:963.57421875\n",
      "1935/3000 train_loss: 41.865413665771484 test_loss:935.5730590820312\n",
      "1936/3000 train_loss: 36.43504333496094 test_loss:946.0413818359375\n",
      "1937/3000 train_loss: 37.888526916503906 test_loss:923.4197998046875\n",
      "1938/3000 train_loss: 43.44829177856445 test_loss:984.829345703125\n",
      "1939/3000 train_loss: 40.15120315551758 test_loss:976.223388671875\n",
      "1940/3000 train_loss: 38.39200973510742 test_loss:904.314453125\n",
      "1941/3000 train_loss: 39.92448806762695 test_loss:920.978759765625\n",
      "1942/3000 train_loss: 37.59852981567383 test_loss:938.86865234375\n",
      "1943/3000 train_loss: 39.18783187866211 test_loss:938.71435546875\n",
      "1944/3000 train_loss: 37.731658935546875 test_loss:981.0941162109375\n",
      "1945/3000 train_loss: 38.410728454589844 test_loss:977.61181640625\n",
      "1946/3000 train_loss: 45.705406188964844 test_loss:959.07275390625\n",
      "1947/3000 train_loss: 39.33750915527344 test_loss:969.3450927734375\n",
      "1948/3000 train_loss: 40.60048294067383 test_loss:998.3226928710938\n",
      "1949/3000 train_loss: 46.679481506347656 test_loss:998.165283203125\n",
      "1950/3000 train_loss: 37.25786209106445 test_loss:948.6427001953125\n",
      "1951/3000 train_loss: 37.97911834716797 test_loss:1010.8124389648438\n",
      "1952/3000 train_loss: 39.39104461669922 test_loss:1019.6859741210938\n",
      "1953/3000 train_loss: 38.5935173034668 test_loss:1001.6475830078125\n",
      "1954/3000 train_loss: 47.474273681640625 test_loss:1004.917236328125\n",
      "1955/3000 train_loss: 36.7379150390625 test_loss:994.6282958984375\n",
      "1956/3000 train_loss: 39.57022476196289 test_loss:964.3782958984375\n",
      "1957/3000 train_loss: 35.403202056884766 test_loss:1009.23291015625\n",
      "1958/3000 train_loss: 35.657371520996094 test_loss:992.6640625\n",
      "1959/3000 train_loss: 43.204315185546875 test_loss:1030.7783203125\n",
      "1960/3000 train_loss: 61.210479736328125 test_loss:1021.2327880859375\n",
      "1961/3000 train_loss: 43.76012420654297 test_loss:972.606689453125\n",
      "1962/3000 train_loss: 43.7631721496582 test_loss:954.8997802734375\n",
      "1963/3000 train_loss: 35.84721374511719 test_loss:935.4261474609375\n",
      "1964/3000 train_loss: 38.04166793823242 test_loss:981.4277954101562\n",
      "1965/3000 train_loss: 36.75783157348633 test_loss:960.7022705078125\n",
      "1966/3000 train_loss: 41.42583084106445 test_loss:1015.3212280273438\n",
      "1967/3000 train_loss: 41.54913330078125 test_loss:978.1322021484375\n",
      "1968/3000 train_loss: 36.68589401245117 test_loss:883.7766723632812\n",
      "1969/3000 train_loss: 37.04429626464844 test_loss:911.28857421875\n",
      "1970/3000 train_loss: 37.31644058227539 test_loss:896.5652465820312\n",
      "1971/3000 train_loss: 41.262081146240234 test_loss:882.2258911132812\n",
      "1972/3000 train_loss: 41.457420349121094 test_loss:889.5921630859375\n",
      "1973/3000 train_loss: 45.4427375793457 test_loss:943.063720703125\n",
      "1974/3000 train_loss: 36.39910888671875 test_loss:923.4635009765625\n",
      "1975/3000 train_loss: 43.64208984375 test_loss:969.6211547851562\n",
      "1976/3000 train_loss: 44.31645202636719 test_loss:1048.365478515625\n",
      "1977/3000 train_loss: 39.300418853759766 test_loss:1036.923828125\n",
      "1978/3000 train_loss: 42.44991683959961 test_loss:1030.413818359375\n",
      "1979/3000 train_loss: 39.95960998535156 test_loss:1001.025390625\n",
      "1980/3000 train_loss: 36.84439468383789 test_loss:1011.5880737304688\n",
      "1981/3000 train_loss: 37.43757629394531 test_loss:1008.7115478515625\n",
      "1982/3000 train_loss: 40.957298278808594 test_loss:967.133056640625\n",
      "1983/3000 train_loss: 44.31687927246094 test_loss:1000.75146484375\n",
      "1984/3000 train_loss: 35.50096893310547 test_loss:1038.107421875\n",
      "1985/3000 train_loss: 36.85078048706055 test_loss:1053.9969482421875\n",
      "1986/3000 train_loss: 46.24378204345703 test_loss:1037.7083740234375\n",
      "1987/3000 train_loss: 38.70891571044922 test_loss:1011.0550537109375\n",
      "1988/3000 train_loss: 33.508262634277344 test_loss:936.9927978515625\n",
      "1989/3000 train_loss: 41.61096954345703 test_loss:972.9068603515625\n",
      "1990/3000 train_loss: 38.08796310424805 test_loss:1010.6520385742188\n",
      "1991/3000 train_loss: 35.63278579711914 test_loss:987.9617309570312\n",
      "1992/3000 train_loss: 36.018856048583984 test_loss:1001.6912841796875\n",
      "1993/3000 train_loss: 34.91896438598633 test_loss:979.4881591796875\n",
      "1994/3000 train_loss: 39.9124870300293 test_loss:983.24169921875\n",
      "1995/3000 train_loss: 34.015132904052734 test_loss:1006.414306640625\n",
      "1996/3000 train_loss: 45.258583068847656 test_loss:914.7997436523438\n",
      "1997/3000 train_loss: 35.57470703125 test_loss:884.1292724609375\n",
      "1998/3000 train_loss: 39.04217529296875 test_loss:895.930419921875\n",
      "1999/3000 train_loss: 39.90683364868164 test_loss:905.900634765625\n",
      "2000/3000 train_loss: 39.64741134643555 test_loss:908.2939453125\n",
      "2001/3000 train_loss: 37.21981430053711 test_loss:925.8387451171875\n",
      "2002/3000 train_loss: 36.18056106567383 test_loss:964.0353393554688\n",
      "2003/3000 train_loss: 35.54057312011719 test_loss:947.6256713867188\n",
      "2004/3000 train_loss: 33.93692398071289 test_loss:959.0528564453125\n",
      "2005/3000 train_loss: 45.548553466796875 test_loss:895.520751953125\n",
      "2006/3000 train_loss: 37.36662292480469 test_loss:910.1615600585938\n",
      "2007/3000 train_loss: 38.761695861816406 test_loss:931.1331787109375\n",
      "2008/3000 train_loss: 38.24277877807617 test_loss:927.1997680664062\n",
      "2009/3000 train_loss: 40.71720886230469 test_loss:890.6055908203125\n",
      "2010/3000 train_loss: 37.48219680786133 test_loss:868.8792724609375\n",
      "2011/3000 train_loss: 38.22864532470703 test_loss:865.8697509765625\n",
      "2012/3000 train_loss: 41.219383239746094 test_loss:915.0980224609375\n",
      "2013/3000 train_loss: 36.153480529785156 test_loss:922.3817138671875\n",
      "2014/3000 train_loss: 44.60386657714844 test_loss:914.7586059570312\n",
      "2015/3000 train_loss: 36.79088592529297 test_loss:934.4962158203125\n",
      "2016/3000 train_loss: 39.77495193481445 test_loss:970.822265625\n",
      "2017/3000 train_loss: 35.50361633300781 test_loss:950.6522216796875\n",
      "2018/3000 train_loss: 36.21928787231445 test_loss:985.802001953125\n",
      "2019/3000 train_loss: 35.944236755371094 test_loss:1000.281005859375\n",
      "2020/3000 train_loss: 38.9945068359375 test_loss:1008.5935668945312\n",
      "2021/3000 train_loss: 34.367431640625 test_loss:964.103759765625\n",
      "2022/3000 train_loss: 39.678924560546875 test_loss:943.679931640625\n",
      "2023/3000 train_loss: 37.20392990112305 test_loss:916.3502197265625\n",
      "2024/3000 train_loss: 41.507904052734375 test_loss:983.020751953125\n",
      "2025/3000 train_loss: 36.66254425048828 test_loss:997.4508056640625\n",
      "2026/3000 train_loss: 45.1829833984375 test_loss:947.7584228515625\n",
      "2027/3000 train_loss: 44.48870086669922 test_loss:1011.949462890625\n",
      "2028/3000 train_loss: 44.32847595214844 test_loss:989.7183837890625\n",
      "2029/3000 train_loss: 32.845909118652344 test_loss:963.3516845703125\n",
      "2030/3000 train_loss: 35.17401123046875 test_loss:945.504150390625\n",
      "2031/3000 train_loss: 43.889408111572266 test_loss:916.4967041015625\n",
      "2032/3000 train_loss: 37.26547622680664 test_loss:939.966552734375\n",
      "2033/3000 train_loss: 34.26708221435547 test_loss:937.8707275390625\n",
      "2034/3000 train_loss: 31.381319046020508 test_loss:979.4877319335938\n",
      "2035/3000 train_loss: 35.955570220947266 test_loss:943.6660766601562\n",
      "2036/3000 train_loss: 36.0821647644043 test_loss:998.294189453125\n",
      "2037/3000 train_loss: 33.391056060791016 test_loss:1014.6026000976562\n",
      "2038/3000 train_loss: 43.421749114990234 test_loss:999.8898315429688\n",
      "2039/3000 train_loss: 37.23031997680664 test_loss:1037.2335205078125\n",
      "2040/3000 train_loss: 35.903404235839844 test_loss:995.6867065429688\n",
      "2041/3000 train_loss: 38.07561492919922 test_loss:967.865234375\n",
      "2042/3000 train_loss: 35.8794059753418 test_loss:982.083740234375\n",
      "2043/3000 train_loss: 34.47423553466797 test_loss:1009.3460083007812\n",
      "2044/3000 train_loss: 39.820247650146484 test_loss:964.9219360351562\n",
      "2045/3000 train_loss: 34.246856689453125 test_loss:957.5987548828125\n",
      "2046/3000 train_loss: 35.199951171875 test_loss:977.1795654296875\n",
      "2047/3000 train_loss: 37.11565399169922 test_loss:983.256591796875\n",
      "2048/3000 train_loss: 38.38523483276367 test_loss:973.741943359375\n",
      "2049/3000 train_loss: 37.41190719604492 test_loss:956.5664672851562\n",
      "2050/3000 train_loss: 36.72029113769531 test_loss:991.026123046875\n",
      "2051/3000 train_loss: 38.790130615234375 test_loss:1009.3157958984375\n",
      "2052/3000 train_loss: 41.60496520996094 test_loss:1047.77197265625\n",
      "2053/3000 train_loss: 34.61322021484375 test_loss:993.1988525390625\n",
      "2054/3000 train_loss: 37.43660354614258 test_loss:960.748291015625\n",
      "2055/3000 train_loss: 41.795955657958984 test_loss:1007.4256591796875\n",
      "2056/3000 train_loss: 41.71532440185547 test_loss:928.7308349609375\n",
      "2057/3000 train_loss: 37.41539001464844 test_loss:945.5609130859375\n",
      "2058/3000 train_loss: 38.728946685791016 test_loss:952.76025390625\n",
      "2059/3000 train_loss: 42.27424621582031 test_loss:996.4193115234375\n",
      "2060/3000 train_loss: 38.7459831237793 test_loss:953.2140502929688\n",
      "2061/3000 train_loss: 34.31037902832031 test_loss:942.776123046875\n",
      "2062/3000 train_loss: 36.12678527832031 test_loss:939.5260620117188\n",
      "2063/3000 train_loss: 33.47378158569336 test_loss:966.9893798828125\n",
      "2064/3000 train_loss: 34.831809997558594 test_loss:1019.646484375\n",
      "2065/3000 train_loss: 40.26906204223633 test_loss:981.6209716796875\n",
      "2066/3000 train_loss: 35.88835144042969 test_loss:999.8206787109375\n",
      "2067/3000 train_loss: 37.19668960571289 test_loss:985.7979736328125\n",
      "2068/3000 train_loss: 38.18096923828125 test_loss:1017.720947265625\n",
      "2069/3000 train_loss: 40.038047790527344 test_loss:1018.1658935546875\n",
      "2070/3000 train_loss: 44.14924240112305 test_loss:1013.7791137695312\n",
      "2071/3000 train_loss: 41.69426727294922 test_loss:992.5162963867188\n",
      "2072/3000 train_loss: 41.46107482910156 test_loss:974.025390625\n",
      "2073/3000 train_loss: 38.68730926513672 test_loss:975.7819213867188\n",
      "2074/3000 train_loss: 37.88720703125 test_loss:968.7620849609375\n",
      "2075/3000 train_loss: 33.163047790527344 test_loss:958.1236572265625\n",
      "2076/3000 train_loss: 39.04841613769531 test_loss:949.1883544921875\n",
      "2077/3000 train_loss: 35.090293884277344 test_loss:932.3782958984375\n",
      "2078/3000 train_loss: 37.86064529418945 test_loss:962.5377197265625\n",
      "2079/3000 train_loss: 37.11781692504883 test_loss:924.7481689453125\n",
      "2080/3000 train_loss: 33.9789924621582 test_loss:961.1864013671875\n",
      "2081/3000 train_loss: 40.12837600708008 test_loss:960.9759521484375\n",
      "2082/3000 train_loss: 36.749359130859375 test_loss:940.7611083984375\n",
      "2083/3000 train_loss: 38.37971115112305 test_loss:962.60595703125\n",
      "2084/3000 train_loss: 34.48845672607422 test_loss:939.9833984375\n",
      "2085/3000 train_loss: 38.31658935546875 test_loss:956.8990478515625\n",
      "2086/3000 train_loss: 36.49799728393555 test_loss:954.5706787109375\n",
      "2087/3000 train_loss: 34.550376892089844 test_loss:951.431884765625\n",
      "2088/3000 train_loss: 36.88618087768555 test_loss:950.80810546875\n",
      "2089/3000 train_loss: 38.09137725830078 test_loss:933.2654418945312\n",
      "2090/3000 train_loss: 36.078372955322266 test_loss:974.1376953125\n",
      "2091/3000 train_loss: 32.88820266723633 test_loss:914.3036499023438\n",
      "2092/3000 train_loss: 34.146915435791016 test_loss:944.79833984375\n",
      "2093/3000 train_loss: 38.77507400512695 test_loss:935.7283935546875\n",
      "2094/3000 train_loss: 37.2000617980957 test_loss:951.6648559570312\n",
      "2095/3000 train_loss: 37.01325607299805 test_loss:915.0523681640625\n",
      "2096/3000 train_loss: 33.182456970214844 test_loss:956.65966796875\n",
      "2097/3000 train_loss: 33.343238830566406 test_loss:899.0799560546875\n",
      "2098/3000 train_loss: 38.53769302368164 test_loss:912.332275390625\n",
      "2099/3000 train_loss: 37.90033721923828 test_loss:924.9539184570312\n",
      "2100/3000 train_loss: 36.931602478027344 test_loss:902.1434326171875\n",
      "2101/3000 train_loss: 39.24824142456055 test_loss:941.6083984375\n",
      "2102/3000 train_loss: 34.58665084838867 test_loss:957.8734741210938\n",
      "2103/3000 train_loss: 42.82037353515625 test_loss:966.898193359375\n",
      "2104/3000 train_loss: 35.162132263183594 test_loss:985.87841796875\n",
      "2105/3000 train_loss: 33.67897415161133 test_loss:949.733154296875\n",
      "2106/3000 train_loss: 39.21834182739258 test_loss:949.4488525390625\n",
      "2107/3000 train_loss: 34.97099304199219 test_loss:994.2735595703125\n",
      "2108/3000 train_loss: 29.42119026184082 test_loss:1017.3497314453125\n",
      "2109/3000 train_loss: 34.9036979675293 test_loss:1049.394287109375\n",
      "2110/3000 train_loss: 34.53361892700195 test_loss:1011.0469970703125\n",
      "2111/3000 train_loss: 38.57288360595703 test_loss:1049.2156982421875\n",
      "2112/3000 train_loss: 38.55131149291992 test_loss:1025.681640625\n",
      "2113/3000 train_loss: 37.270118713378906 test_loss:1046.99609375\n",
      "2114/3000 train_loss: 37.1251106262207 test_loss:1034.097900390625\n",
      "2115/3000 train_loss: 34.98012924194336 test_loss:1025.352783203125\n",
      "2116/3000 train_loss: 36.64033126831055 test_loss:977.4122314453125\n",
      "2117/3000 train_loss: 42.030399322509766 test_loss:980.3076171875\n",
      "2118/3000 train_loss: 37.427852630615234 test_loss:973.2159423828125\n",
      "2119/3000 train_loss: 35.8845329284668 test_loss:994.5191040039062\n",
      "2120/3000 train_loss: 34.07963943481445 test_loss:964.4140625\n",
      "2121/3000 train_loss: 37.57362747192383 test_loss:988.3175048828125\n",
      "2122/3000 train_loss: 35.75770568847656 test_loss:1025.5477294921875\n",
      "2123/3000 train_loss: 38.172874450683594 test_loss:1011.3509521484375\n",
      "2124/3000 train_loss: 42.69874954223633 test_loss:954.75537109375\n",
      "2125/3000 train_loss: 32.509639739990234 test_loss:978.0431518554688\n",
      "2126/3000 train_loss: 39.48466491699219 test_loss:980.075927734375\n",
      "2127/3000 train_loss: 38.19820022583008 test_loss:896.0950927734375\n",
      "2128/3000 train_loss: 34.51536560058594 test_loss:920.6461791992188\n",
      "2129/3000 train_loss: 38.346893310546875 test_loss:899.8468017578125\n",
      "2130/3000 train_loss: 34.80009460449219 test_loss:928.1971435546875\n",
      "2131/3000 train_loss: 32.885765075683594 test_loss:955.2745971679688\n",
      "2132/3000 train_loss: 32.900115966796875 test_loss:896.9044189453125\n",
      "2133/3000 train_loss: 38.36207962036133 test_loss:895.493896484375\n",
      "2134/3000 train_loss: 38.099700927734375 test_loss:960.1054077148438\n",
      "2135/3000 train_loss: 32.4365119934082 test_loss:957.431396484375\n",
      "2136/3000 train_loss: 40.97539138793945 test_loss:896.8046264648438\n",
      "2137/3000 train_loss: 38.47768783569336 test_loss:901.915771484375\n",
      "2138/3000 train_loss: 41.71929168701172 test_loss:906.9736328125\n",
      "2139/3000 train_loss: 35.236541748046875 test_loss:905.3502197265625\n",
      "2140/3000 train_loss: 39.84470748901367 test_loss:942.2697143554688\n",
      "2141/3000 train_loss: 34.30242156982422 test_loss:929.9229736328125\n",
      "2142/3000 train_loss: 41.86545944213867 test_loss:937.1812744140625\n",
      "2143/3000 train_loss: 36.67958068847656 test_loss:911.1051025390625\n",
      "2144/3000 train_loss: 41.436981201171875 test_loss:954.7945556640625\n",
      "2145/3000 train_loss: 32.8173713684082 test_loss:927.7503662109375\n",
      "2146/3000 train_loss: 36.325767517089844 test_loss:927.9019775390625\n",
      "2147/3000 train_loss: 46.7070198059082 test_loss:922.48876953125\n",
      "2148/3000 train_loss: 38.65837478637695 test_loss:928.815185546875\n",
      "2149/3000 train_loss: 48.92955780029297 test_loss:957.5686645507812\n",
      "2150/3000 train_loss: 39.29191970825195 test_loss:946.6624755859375\n",
      "2151/3000 train_loss: 33.694427490234375 test_loss:953.9600830078125\n",
      "2152/3000 train_loss: 40.40589904785156 test_loss:952.7756958007812\n",
      "2153/3000 train_loss: 40.13838195800781 test_loss:975.100341796875\n",
      "2154/3000 train_loss: 36.97257614135742 test_loss:972.6712646484375\n",
      "2155/3000 train_loss: 42.58313751220703 test_loss:924.116455078125\n",
      "2156/3000 train_loss: 38.30839157104492 test_loss:826.3672485351562\n",
      "2157/3000 train_loss: 38.433189392089844 test_loss:842.9794311523438\n",
      "2158/3000 train_loss: 34.80793762207031 test_loss:890.9085693359375\n",
      "2159/3000 train_loss: 29.83300018310547 test_loss:852.81201171875\n",
      "2160/3000 train_loss: 42.269805908203125 test_loss:873.7547607421875\n",
      "2161/3000 train_loss: 42.32020568847656 test_loss:932.7312622070312\n",
      "2162/3000 train_loss: 36.49763488769531 test_loss:916.2245483398438\n",
      "2163/3000 train_loss: 35.246253967285156 test_loss:873.87744140625\n",
      "2164/3000 train_loss: 35.92334747314453 test_loss:859.2595825195312\n",
      "2165/3000 train_loss: 36.18631362915039 test_loss:892.1375732421875\n",
      "2166/3000 train_loss: 38.442176818847656 test_loss:908.306640625\n",
      "2167/3000 train_loss: 38.51481628417969 test_loss:852.3526611328125\n",
      "2168/3000 train_loss: 41.948333740234375 test_loss:879.9569091796875\n",
      "2169/3000 train_loss: 37.646121978759766 test_loss:926.321533203125\n",
      "2170/3000 train_loss: 38.895233154296875 test_loss:928.909423828125\n",
      "2171/3000 train_loss: 33.39994812011719 test_loss:909.92041015625\n",
      "2172/3000 train_loss: 30.133150100708008 test_loss:947.35400390625\n",
      "2173/3000 train_loss: 30.266193389892578 test_loss:947.4557495117188\n",
      "2174/3000 train_loss: 37.43307113647461 test_loss:938.6871337890625\n",
      "2175/3000 train_loss: 36.30622100830078 test_loss:939.9686279296875\n",
      "2176/3000 train_loss: 33.87937927246094 test_loss:873.91064453125\n",
      "2177/3000 train_loss: 37.83741760253906 test_loss:886.69482421875\n",
      "2178/3000 train_loss: 38.06388854980469 test_loss:872.7457275390625\n",
      "2179/3000 train_loss: 34.9335823059082 test_loss:878.8233642578125\n",
      "2180/3000 train_loss: 39.48667526245117 test_loss:895.2229614257812\n",
      "2181/3000 train_loss: 31.519054412841797 test_loss:909.9226684570312\n",
      "2182/3000 train_loss: 33.35313034057617 test_loss:900.8313598632812\n",
      "2183/3000 train_loss: 33.29929733276367 test_loss:915.8160400390625\n",
      "2184/3000 train_loss: 36.07561492919922 test_loss:969.3885498046875\n",
      "2185/3000 train_loss: 34.53480529785156 test_loss:945.04931640625\n",
      "2186/3000 train_loss: 43.908119201660156 test_loss:949.236572265625\n",
      "2187/3000 train_loss: 50.76701354980469 test_loss:995.7057495117188\n",
      "2188/3000 train_loss: 44.38541793823242 test_loss:901.8721923828125\n",
      "2189/3000 train_loss: 43.938323974609375 test_loss:891.0098876953125\n",
      "2190/3000 train_loss: 39.63563537597656 test_loss:919.5519409179688\n",
      "2191/3000 train_loss: 37.25861740112305 test_loss:860.0117797851562\n",
      "2192/3000 train_loss: 36.42184829711914 test_loss:886.9776611328125\n",
      "2193/3000 train_loss: 36.532203674316406 test_loss:893.4286499023438\n",
      "2194/3000 train_loss: 33.837215423583984 test_loss:914.69677734375\n",
      "2195/3000 train_loss: 34.00788879394531 test_loss:923.3772583007812\n",
      "2196/3000 train_loss: 32.845027923583984 test_loss:896.6997680664062\n",
      "2197/3000 train_loss: 42.22244644165039 test_loss:882.8477783203125\n",
      "2198/3000 train_loss: 34.45963668823242 test_loss:953.1648559570312\n",
      "2199/3000 train_loss: 36.784996032714844 test_loss:925.031005859375\n",
      "2200/3000 train_loss: 36.70268249511719 test_loss:947.439697265625\n",
      "2201/3000 train_loss: 36.780025482177734 test_loss:982.4111328125\n",
      "2202/3000 train_loss: 35.40980529785156 test_loss:938.8038330078125\n",
      "2203/3000 train_loss: 38.679378509521484 test_loss:955.861083984375\n",
      "2204/3000 train_loss: 38.1926383972168 test_loss:922.5811767578125\n",
      "2205/3000 train_loss: 43.31455993652344 test_loss:1014.6544799804688\n",
      "2206/3000 train_loss: 43.448974609375 test_loss:1074.5540771484375\n",
      "2207/3000 train_loss: 42.640193939208984 test_loss:1067.695556640625\n",
      "2208/3000 train_loss: 35.90353012084961 test_loss:1002.5044555664062\n",
      "2209/3000 train_loss: 34.212955474853516 test_loss:1047.4586181640625\n",
      "2210/3000 train_loss: 35.406341552734375 test_loss:1027.1253662109375\n",
      "2211/3000 train_loss: 35.55540084838867 test_loss:1034.476806640625\n",
      "2212/3000 train_loss: 38.1406364440918 test_loss:1022.793701171875\n",
      "2213/3000 train_loss: 32.8436393737793 test_loss:989.9036254882812\n",
      "2214/3000 train_loss: 30.779714584350586 test_loss:982.4512939453125\n",
      "2215/3000 train_loss: 35.61037826538086 test_loss:981.0740966796875\n",
      "2216/3000 train_loss: 32.30866241455078 test_loss:999.3343505859375\n",
      "2217/3000 train_loss: 34.69050216674805 test_loss:952.1701049804688\n",
      "2218/3000 train_loss: 44.267120361328125 test_loss:933.0653076171875\n",
      "2219/3000 train_loss: 48.7082633972168 test_loss:978.1808471679688\n",
      "2220/3000 train_loss: 34.4080810546875 test_loss:971.03955078125\n",
      "2221/3000 train_loss: 33.795169830322266 test_loss:968.8504638671875\n",
      "2222/3000 train_loss: 37.87580871582031 test_loss:981.466064453125\n",
      "2223/3000 train_loss: 42.77357482910156 test_loss:991.7250366210938\n",
      "2224/3000 train_loss: 36.97516632080078 test_loss:1037.3155517578125\n",
      "2225/3000 train_loss: 36.611900329589844 test_loss:1029.846435546875\n",
      "2226/3000 train_loss: 34.281673431396484 test_loss:992.8577880859375\n",
      "2227/3000 train_loss: 38.28032302856445 test_loss:1035.5206298828125\n",
      "2228/3000 train_loss: 40.550254821777344 test_loss:1025.6905517578125\n",
      "2229/3000 train_loss: 32.37321090698242 test_loss:1010.747314453125\n",
      "2230/3000 train_loss: 42.863338470458984 test_loss:991.7708740234375\n",
      "2231/3000 train_loss: 33.354740142822266 test_loss:1017.656494140625\n",
      "2232/3000 train_loss: 40.008968353271484 test_loss:1048.955810546875\n",
      "2233/3000 train_loss: 38.599945068359375 test_loss:1016.7528076171875\n",
      "2234/3000 train_loss: 33.016231536865234 test_loss:1033.658447265625\n",
      "2235/3000 train_loss: 34.59075927734375 test_loss:1045.3072509765625\n",
      "2236/3000 train_loss: 36.617210388183594 test_loss:1046.9501953125\n",
      "2237/3000 train_loss: 30.915874481201172 test_loss:1060.5809326171875\n",
      "2238/3000 train_loss: 44.23780059814453 test_loss:979.496826171875\n",
      "2239/3000 train_loss: 34.587284088134766 test_loss:969.6044921875\n",
      "2240/3000 train_loss: 32.89426803588867 test_loss:950.724609375\n",
      "2241/3000 train_loss: 37.59680938720703 test_loss:980.5413818359375\n",
      "2242/3000 train_loss: 35.35142517089844 test_loss:920.93359375\n",
      "2243/3000 train_loss: 34.458831787109375 test_loss:969.7852172851562\n",
      "2244/3000 train_loss: 34.59033203125 test_loss:959.75439453125\n",
      "2245/3000 train_loss: 34.8675651550293 test_loss:905.9803466796875\n",
      "2246/3000 train_loss: 35.317569732666016 test_loss:941.277099609375\n",
      "2247/3000 train_loss: 41.03248596191406 test_loss:963.2363891601562\n",
      "2248/3000 train_loss: 34.83548355102539 test_loss:1005.6331787109375\n",
      "2249/3000 train_loss: 35.69127655029297 test_loss:977.6815185546875\n",
      "2250/3000 train_loss: 31.08136749267578 test_loss:959.2696533203125\n",
      "2251/3000 train_loss: 38.30828857421875 test_loss:966.636474609375\n",
      "2252/3000 train_loss: 37.953163146972656 test_loss:974.0806274414062\n",
      "2253/3000 train_loss: 40.142356872558594 test_loss:965.233642578125\n",
      "2254/3000 train_loss: 31.7105655670166 test_loss:936.2677612304688\n",
      "2255/3000 train_loss: 35.2284049987793 test_loss:975.7750854492188\n",
      "2256/3000 train_loss: 34.04194641113281 test_loss:984.18115234375\n",
      "2257/3000 train_loss: 32.635986328125 test_loss:993.2607421875\n",
      "2258/3000 train_loss: 34.05978775024414 test_loss:980.2941284179688\n",
      "2259/3000 train_loss: 33.36562728881836 test_loss:1019.0091552734375\n",
      "2260/3000 train_loss: 37.138893127441406 test_loss:981.36669921875\n",
      "2261/3000 train_loss: 35.48200607299805 test_loss:972.3392333984375\n",
      "2262/3000 train_loss: 40.493736267089844 test_loss:958.1151733398438\n",
      "2263/3000 train_loss: 30.530229568481445 test_loss:956.018798828125\n",
      "2264/3000 train_loss: 32.182395935058594 test_loss:997.4284057617188\n",
      "2265/3000 train_loss: 36.09780502319336 test_loss:1016.8754272460938\n",
      "2266/3000 train_loss: 33.411041259765625 test_loss:982.0052490234375\n",
      "2267/3000 train_loss: 39.01875305175781 test_loss:977.8997802734375\n",
      "2268/3000 train_loss: 36.558807373046875 test_loss:1010.0401611328125\n",
      "2269/3000 train_loss: 31.237947463989258 test_loss:1030.961181640625\n",
      "2270/3000 train_loss: 32.06575393676758 test_loss:995.1303100585938\n",
      "2271/3000 train_loss: 38.61140060424805 test_loss:983.9053955078125\n",
      "2272/3000 train_loss: 39.19169235229492 test_loss:1028.6610107421875\n",
      "2273/3000 train_loss: 56.195465087890625 test_loss:982.308349609375\n",
      "2274/3000 train_loss: 39.195465087890625 test_loss:918.8901977539062\n",
      "2275/3000 train_loss: 36.3194694519043 test_loss:902.6358642578125\n",
      "2276/3000 train_loss: 34.55841827392578 test_loss:870.9554443359375\n",
      "2277/3000 train_loss: 35.81266784667969 test_loss:913.623779296875\n",
      "2278/3000 train_loss: 30.880491256713867 test_loss:944.1529541015625\n",
      "2279/3000 train_loss: 39.121883392333984 test_loss:900.3826904296875\n",
      "2280/3000 train_loss: 36.1848030090332 test_loss:891.7462158203125\n",
      "2281/3000 train_loss: 35.680850982666016 test_loss:972.8367919921875\n",
      "2282/3000 train_loss: 29.039907455444336 test_loss:911.7212524414062\n",
      "2283/3000 train_loss: 36.777103424072266 test_loss:919.843505859375\n",
      "2284/3000 train_loss: 39.12543487548828 test_loss:935.5557861328125\n",
      "2285/3000 train_loss: 42.456871032714844 test_loss:914.1968383789062\n",
      "2286/3000 train_loss: 38.47824478149414 test_loss:931.4476318359375\n",
      "2287/3000 train_loss: 38.47057342529297 test_loss:935.5634765625\n",
      "2288/3000 train_loss: 35.473793029785156 test_loss:941.0126342773438\n",
      "2289/3000 train_loss: 43.38389587402344 test_loss:952.4852294921875\n",
      "2290/3000 train_loss: 36.60053253173828 test_loss:958.6586303710938\n",
      "2291/3000 train_loss: 32.927818298339844 test_loss:998.981201171875\n",
      "2292/3000 train_loss: 33.295692443847656 test_loss:980.84912109375\n",
      "2293/3000 train_loss: 34.89053726196289 test_loss:984.55224609375\n",
      "2294/3000 train_loss: 37.6655158996582 test_loss:987.9912109375\n",
      "2295/3000 train_loss: 36.30739212036133 test_loss:1017.4476318359375\n",
      "2296/3000 train_loss: 41.5943603515625 test_loss:1008.1324462890625\n",
      "2297/3000 train_loss: 33.22372055053711 test_loss:1013.7156982421875\n",
      "2298/3000 train_loss: 31.281326293945312 test_loss:1010.1890258789062\n",
      "2299/3000 train_loss: 32.65964126586914 test_loss:983.14697265625\n",
      "2300/3000 train_loss: 38.83369445800781 test_loss:993.1647338867188\n",
      "2301/3000 train_loss: 31.727909088134766 test_loss:1019.308349609375\n",
      "2302/3000 train_loss: 37.28078842163086 test_loss:1016.3753662109375\n",
      "2303/3000 train_loss: 35.96302032470703 test_loss:1007.4974975585938\n",
      "2304/3000 train_loss: 32.00680923461914 test_loss:993.050537109375\n",
      "2305/3000 train_loss: 36.15386962890625 test_loss:1010.666015625\n",
      "2306/3000 train_loss: 45.06181335449219 test_loss:1001.6181030273438\n",
      "2307/3000 train_loss: 35.22574996948242 test_loss:995.177490234375\n",
      "2308/3000 train_loss: 32.36601638793945 test_loss:948.7192993164062\n",
      "2309/3000 train_loss: 33.918556213378906 test_loss:974.0875854492188\n",
      "2310/3000 train_loss: 35.843963623046875 test_loss:930.0846557617188\n",
      "2311/3000 train_loss: 31.571025848388672 test_loss:957.3214721679688\n",
      "2312/3000 train_loss: 36.679954528808594 test_loss:970.784912109375\n",
      "2313/3000 train_loss: 42.75604248046875 test_loss:975.2164306640625\n",
      "2314/3000 train_loss: 36.518592834472656 test_loss:929.0478515625\n",
      "2315/3000 train_loss: 34.13936996459961 test_loss:944.8173217773438\n",
      "2316/3000 train_loss: 34.43585968017578 test_loss:973.878662109375\n",
      "2317/3000 train_loss: 30.385129928588867 test_loss:964.02099609375\n",
      "2318/3000 train_loss: 40.63859176635742 test_loss:990.48681640625\n",
      "2319/3000 train_loss: 39.44595718383789 test_loss:991.5888671875\n",
      "2320/3000 train_loss: 33.828128814697266 test_loss:1018.5833740234375\n",
      "2321/3000 train_loss: 33.38009262084961 test_loss:1024.031005859375\n",
      "2322/3000 train_loss: 34.75734329223633 test_loss:1018.9473876953125\n",
      "2323/3000 train_loss: 32.136451721191406 test_loss:1069.0545654296875\n",
      "2324/3000 train_loss: 31.327133178710938 test_loss:1022.554443359375\n",
      "2325/3000 train_loss: 33.516849517822266 test_loss:1011.678955078125\n",
      "2326/3000 train_loss: 41.200069427490234 test_loss:995.2608032226562\n",
      "2327/3000 train_loss: 32.68087387084961 test_loss:963.089599609375\n",
      "2328/3000 train_loss: 41.818180084228516 test_loss:945.337158203125\n",
      "2329/3000 train_loss: 38.682456970214844 test_loss:1013.7814331054688\n",
      "2330/3000 train_loss: 38.21028137207031 test_loss:1019.0867309570312\n",
      "2331/3000 train_loss: 33.652557373046875 test_loss:1009.9942626953125\n",
      "2332/3000 train_loss: 33.34013366699219 test_loss:1000.2852783203125\n",
      "2333/3000 train_loss: 33.80931854248047 test_loss:1006.6912841796875\n",
      "2334/3000 train_loss: 33.89360046386719 test_loss:968.28857421875\n",
      "2335/3000 train_loss: 34.730899810791016 test_loss:956.5785522460938\n",
      "2336/3000 train_loss: 33.10829162597656 test_loss:953.390625\n",
      "2337/3000 train_loss: 33.2547607421875 test_loss:959.900634765625\n",
      "2338/3000 train_loss: 33.40373229980469 test_loss:961.9697265625\n",
      "2339/3000 train_loss: 35.996971130371094 test_loss:946.1226806640625\n",
      "2340/3000 train_loss: 38.91353988647461 test_loss:979.877197265625\n",
      "2341/3000 train_loss: 38.097415924072266 test_loss:976.1618041992188\n",
      "2342/3000 train_loss: 44.42320251464844 test_loss:985.5033569335938\n",
      "2343/3000 train_loss: 35.83723449707031 test_loss:976.56298828125\n",
      "2344/3000 train_loss: 40.11531448364258 test_loss:914.9183349609375\n",
      "2345/3000 train_loss: 33.94416046142578 test_loss:893.3699951171875\n",
      "2346/3000 train_loss: 33.09917068481445 test_loss:943.2952880859375\n",
      "2347/3000 train_loss: 31.10291290283203 test_loss:962.566650390625\n",
      "2348/3000 train_loss: 39.773075103759766 test_loss:984.22705078125\n",
      "2349/3000 train_loss: 37.263423919677734 test_loss:1002.1025390625\n",
      "2350/3000 train_loss: 38.23328399658203 test_loss:1027.2838134765625\n",
      "2351/3000 train_loss: 35.096309661865234 test_loss:1025.4852294921875\n",
      "2352/3000 train_loss: 31.429615020751953 test_loss:998.64794921875\n",
      "2353/3000 train_loss: 33.05787658691406 test_loss:988.9783935546875\n",
      "2354/3000 train_loss: 40.141021728515625 test_loss:928.0196533203125\n",
      "2355/3000 train_loss: 38.67781448364258 test_loss:968.5039672851562\n",
      "2356/3000 train_loss: 32.90937423706055 test_loss:956.0663452148438\n",
      "2357/3000 train_loss: 41.80557632446289 test_loss:985.574462890625\n",
      "2358/3000 train_loss: 36.3807487487793 test_loss:964.7808227539062\n",
      "2359/3000 train_loss: 32.8441162109375 test_loss:943.7763671875\n",
      "2360/3000 train_loss: 36.851806640625 test_loss:928.33935546875\n",
      "2361/3000 train_loss: 35.83965301513672 test_loss:901.6934814453125\n",
      "2362/3000 train_loss: 37.01555252075195 test_loss:920.58544921875\n",
      "2363/3000 train_loss: 34.29972839355469 test_loss:957.7484130859375\n",
      "2364/3000 train_loss: 36.27564239501953 test_loss:990.1458740234375\n",
      "2365/3000 train_loss: 28.770097732543945 test_loss:989.04443359375\n",
      "2366/3000 train_loss: 31.403026580810547 test_loss:1003.614501953125\n",
      "2367/3000 train_loss: 35.08366012573242 test_loss:975.4453125\n",
      "2368/3000 train_loss: 35.63405990600586 test_loss:963.515869140625\n",
      "2369/3000 train_loss: 42.90425491333008 test_loss:972.870849609375\n",
      "2370/3000 train_loss: 46.21984100341797 test_loss:988.640380859375\n",
      "2371/3000 train_loss: 36.25716781616211 test_loss:1077.69482421875\n",
      "2372/3000 train_loss: 36.25920867919922 test_loss:1038.3941650390625\n",
      "2373/3000 train_loss: 36.45757293701172 test_loss:1054.1484375\n",
      "2374/3000 train_loss: 37.32413864135742 test_loss:1024.1778564453125\n",
      "2375/3000 train_loss: 37.04214859008789 test_loss:1052.814208984375\n",
      "2376/3000 train_loss: 35.704654693603516 test_loss:1042.2130126953125\n",
      "2377/3000 train_loss: 33.053401947021484 test_loss:1040.117431640625\n",
      "2378/3000 train_loss: 35.68135452270508 test_loss:1014.3624267578125\n",
      "2379/3000 train_loss: 39.056922912597656 test_loss:1033.18408203125\n",
      "2380/3000 train_loss: 35.49042892456055 test_loss:1063.639404296875\n",
      "2381/3000 train_loss: 36.40446472167969 test_loss:1063.3115234375\n",
      "2382/3000 train_loss: 32.692222595214844 test_loss:1083.55419921875\n",
      "2383/3000 train_loss: 33.41587829589844 test_loss:1054.4747314453125\n",
      "2384/3000 train_loss: 37.780277252197266 test_loss:1033.8846435546875\n",
      "2385/3000 train_loss: 35.30081558227539 test_loss:976.657958984375\n",
      "2386/3000 train_loss: 36.63325500488281 test_loss:1007.707763671875\n",
      "2387/3000 train_loss: 35.526371002197266 test_loss:981.50341796875\n",
      "2388/3000 train_loss: 35.5469856262207 test_loss:951.8841552734375\n",
      "2389/3000 train_loss: 34.78791809082031 test_loss:1000.4962158203125\n",
      "2390/3000 train_loss: 30.225358963012695 test_loss:1021.834228515625\n",
      "2391/3000 train_loss: 33.43851089477539 test_loss:1005.295654296875\n",
      "2392/3000 train_loss: 41.419185638427734 test_loss:972.260009765625\n",
      "2393/3000 train_loss: 36.934181213378906 test_loss:965.021728515625\n",
      "2394/3000 train_loss: 41.02723693847656 test_loss:927.4647216796875\n",
      "2395/3000 train_loss: 33.43769836425781 test_loss:972.5001220703125\n",
      "2396/3000 train_loss: 30.222299575805664 test_loss:935.342529296875\n",
      "2397/3000 train_loss: 33.2955436706543 test_loss:942.8968505859375\n",
      "2398/3000 train_loss: 38.37835693359375 test_loss:930.410888671875\n",
      "2399/3000 train_loss: 35.3802490234375 test_loss:956.5888671875\n",
      "2400/3000 train_loss: 35.60990524291992 test_loss:941.17822265625\n",
      "2401/3000 train_loss: 33.65825653076172 test_loss:907.0197143554688\n",
      "2402/3000 train_loss: 46.39729690551758 test_loss:914.6888427734375\n",
      "2403/3000 train_loss: 36.1066780090332 test_loss:929.336181640625\n",
      "2404/3000 train_loss: 34.870052337646484 test_loss:878.659912109375\n",
      "2405/3000 train_loss: 34.83931350708008 test_loss:863.1195068359375\n",
      "2406/3000 train_loss: 34.69411849975586 test_loss:914.8079833984375\n",
      "2407/3000 train_loss: 33.7073860168457 test_loss:894.0433959960938\n",
      "2408/3000 train_loss: 30.408029556274414 test_loss:892.401123046875\n",
      "2409/3000 train_loss: 32.02318572998047 test_loss:925.0531005859375\n",
      "2410/3000 train_loss: 34.961708068847656 test_loss:946.9166259765625\n",
      "2411/3000 train_loss: 35.80866622924805 test_loss:940.1654663085938\n",
      "2412/3000 train_loss: 32.39314270019531 test_loss:931.8861083984375\n",
      "2413/3000 train_loss: 34.74404525756836 test_loss:926.1751708984375\n",
      "2414/3000 train_loss: 37.796138763427734 test_loss:918.3430786132812\n",
      "2415/3000 train_loss: 29.635604858398438 test_loss:890.8026123046875\n",
      "2416/3000 train_loss: 31.91463279724121 test_loss:932.7364501953125\n",
      "2417/3000 train_loss: 31.989234924316406 test_loss:887.62939453125\n",
      "2418/3000 train_loss: 31.973806381225586 test_loss:930.7525024414062\n",
      "2419/3000 train_loss: 35.7139778137207 test_loss:923.92578125\n",
      "2420/3000 train_loss: 35.64937973022461 test_loss:849.853271484375\n",
      "2421/3000 train_loss: 32.8930549621582 test_loss:844.0657958984375\n",
      "2422/3000 train_loss: 33.954872131347656 test_loss:877.4725341796875\n",
      "2423/3000 train_loss: 33.38991165161133 test_loss:906.6646728515625\n",
      "2424/3000 train_loss: 33.82349395751953 test_loss:933.310302734375\n",
      "2425/3000 train_loss: 39.69356918334961 test_loss:934.2991943359375\n",
      "2426/3000 train_loss: 35.83796691894531 test_loss:920.0054931640625\n",
      "2427/3000 train_loss: 34.291038513183594 test_loss:961.8600463867188\n",
      "2428/3000 train_loss: 34.057952880859375 test_loss:955.0120849609375\n",
      "2429/3000 train_loss: 31.400428771972656 test_loss:961.9050903320312\n",
      "2430/3000 train_loss: 34.18761444091797 test_loss:1023.0175170898438\n",
      "2431/3000 train_loss: 31.984386444091797 test_loss:995.957763671875\n",
      "2432/3000 train_loss: 30.49822235107422 test_loss:986.8348388671875\n",
      "2433/3000 train_loss: 38.66292953491211 test_loss:952.97509765625\n",
      "2434/3000 train_loss: 38.92920684814453 test_loss:915.4573364257812\n",
      "2435/3000 train_loss: 39.246795654296875 test_loss:943.383056640625\n",
      "2436/3000 train_loss: 30.659025192260742 test_loss:901.4942626953125\n",
      "2437/3000 train_loss: 33.12080001831055 test_loss:909.1722412109375\n",
      "2438/3000 train_loss: 31.06896209716797 test_loss:909.0772094726562\n",
      "2439/3000 train_loss: 32.197669982910156 test_loss:927.4801025390625\n",
      "2440/3000 train_loss: 34.74492645263672 test_loss:946.906005859375\n",
      "2441/3000 train_loss: 31.547927856445312 test_loss:954.0260009765625\n",
      "2442/3000 train_loss: 36.69004821777344 test_loss:955.2777099609375\n",
      "2443/3000 train_loss: 35.99234390258789 test_loss:941.5294799804688\n",
      "2444/3000 train_loss: 31.75542449951172 test_loss:955.4293212890625\n",
      "2445/3000 train_loss: 30.919240951538086 test_loss:970.7392578125\n",
      "2446/3000 train_loss: 30.4130802154541 test_loss:935.3551025390625\n",
      "2447/3000 train_loss: 30.08211898803711 test_loss:911.4678344726562\n",
      "2448/3000 train_loss: 31.665803909301758 test_loss:900.50048828125\n",
      "2449/3000 train_loss: 33.649837493896484 test_loss:935.3427124023438\n",
      "2450/3000 train_loss: 32.07041931152344 test_loss:946.92626953125\n",
      "2451/3000 train_loss: 35.52367401123047 test_loss:902.5267944335938\n",
      "2452/3000 train_loss: 31.4746150970459 test_loss:909.8474731445312\n",
      "2453/3000 train_loss: 33.756980895996094 test_loss:950.6529541015625\n",
      "2454/3000 train_loss: 30.633045196533203 test_loss:952.134033203125\n",
      "2455/3000 train_loss: 33.50051498413086 test_loss:931.1051025390625\n",
      "2456/3000 train_loss: 29.19390106201172 test_loss:908.52685546875\n",
      "2457/3000 train_loss: 29.03460121154785 test_loss:914.768798828125\n",
      "2458/3000 train_loss: 36.06861877441406 test_loss:931.1146240234375\n",
      "2459/3000 train_loss: 35.11658477783203 test_loss:979.9373779296875\n",
      "2460/3000 train_loss: 32.89949417114258 test_loss:955.6962890625\n",
      "2461/3000 train_loss: 31.787813186645508 test_loss:949.66455078125\n",
      "2462/3000 train_loss: 38.31266784667969 test_loss:932.47265625\n",
      "2463/3000 train_loss: 32.845184326171875 test_loss:922.9639282226562\n",
      "2464/3000 train_loss: 33.361595153808594 test_loss:908.9476318359375\n",
      "2465/3000 train_loss: 36.79735565185547 test_loss:910.78271484375\n",
      "2466/3000 train_loss: 32.23652267456055 test_loss:888.5640869140625\n",
      "2467/3000 train_loss: 35.029815673828125 test_loss:876.556396484375\n",
      "2468/3000 train_loss: 34.67012405395508 test_loss:945.242431640625\n",
      "2469/3000 train_loss: 32.842323303222656 test_loss:949.825439453125\n",
      "2470/3000 train_loss: 29.675674438476562 test_loss:943.8262329101562\n",
      "2471/3000 train_loss: 32.03981399536133 test_loss:928.618896484375\n",
      "2472/3000 train_loss: 33.47978973388672 test_loss:888.63037109375\n",
      "2473/3000 train_loss: 37.42741012573242 test_loss:958.115478515625\n",
      "2474/3000 train_loss: 32.1446418762207 test_loss:937.56640625\n",
      "2475/3000 train_loss: 35.88738250732422 test_loss:948.0821533203125\n",
      "2476/3000 train_loss: 34.9573860168457 test_loss:972.4110107421875\n",
      "2477/3000 train_loss: 33.08029556274414 test_loss:939.6764526367188\n",
      "2478/3000 train_loss: 34.243656158447266 test_loss:950.4822998046875\n",
      "2479/3000 train_loss: 29.964977264404297 test_loss:955.1231689453125\n",
      "2480/3000 train_loss: 35.39906692504883 test_loss:914.1761474609375\n",
      "2481/3000 train_loss: 31.41946029663086 test_loss:909.2506103515625\n",
      "2482/3000 train_loss: 31.55145263671875 test_loss:920.1741333007812\n",
      "2483/3000 train_loss: 32.449462890625 test_loss:950.2161865234375\n",
      "2484/3000 train_loss: 31.810087203979492 test_loss:939.47265625\n",
      "2485/3000 train_loss: 42.51700210571289 test_loss:967.8460693359375\n",
      "2486/3000 train_loss: 37.03135299682617 test_loss:1018.5364990234375\n",
      "2487/3000 train_loss: 33.627193450927734 test_loss:992.527587890625\n",
      "2488/3000 train_loss: 31.291749954223633 test_loss:1008.47705078125\n",
      "2489/3000 train_loss: 32.116703033447266 test_loss:1005.6668701171875\n",
      "2490/3000 train_loss: 31.62039566040039 test_loss:1002.7047119140625\n",
      "2491/3000 train_loss: 34.73603820800781 test_loss:972.171142578125\n",
      "2492/3000 train_loss: 31.859397888183594 test_loss:967.64990234375\n",
      "2493/3000 train_loss: 39.60398483276367 test_loss:985.15869140625\n",
      "2494/3000 train_loss: 33.65292739868164 test_loss:1008.0799560546875\n",
      "2495/3000 train_loss: 37.64961624145508 test_loss:945.3408203125\n",
      "2496/3000 train_loss: 32.0279541015625 test_loss:971.9027099609375\n",
      "2497/3000 train_loss: 35.92882537841797 test_loss:992.5003662109375\n",
      "2498/3000 train_loss: 32.49928283691406 test_loss:994.1573486328125\n",
      "2499/3000 train_loss: 31.429718017578125 test_loss:987.28564453125\n",
      "2500/3000 train_loss: 39.85969543457031 test_loss:970.1158447265625\n",
      "2501/3000 train_loss: 37.460426330566406 test_loss:943.4805908203125\n",
      "2502/3000 train_loss: 33.519126892089844 test_loss:980.4417724609375\n",
      "2503/3000 train_loss: 30.84376335144043 test_loss:972.0184936523438\n",
      "2504/3000 train_loss: 32.23301696777344 test_loss:927.8082885742188\n",
      "2505/3000 train_loss: 36.80713653564453 test_loss:959.012451171875\n",
      "2506/3000 train_loss: 32.693138122558594 test_loss:992.69091796875\n",
      "2507/3000 train_loss: 38.003501892089844 test_loss:982.826171875\n",
      "2508/3000 train_loss: 28.82794189453125 test_loss:1010.2296752929688\n",
      "2509/3000 train_loss: 30.653226852416992 test_loss:945.685791015625\n",
      "2510/3000 train_loss: 31.418209075927734 test_loss:929.1468505859375\n",
      "2511/3000 train_loss: 36.37441635131836 test_loss:945.0164794921875\n",
      "2512/3000 train_loss: 30.74679183959961 test_loss:1003.7493896484375\n",
      "2513/3000 train_loss: 42.699283599853516 test_loss:988.0626220703125\n",
      "2514/3000 train_loss: 37.80389404296875 test_loss:954.900390625\n",
      "2515/3000 train_loss: 40.23790740966797 test_loss:942.5272216796875\n",
      "2516/3000 train_loss: 36.01088333129883 test_loss:934.3935546875\n",
      "2517/3000 train_loss: 36.94074630737305 test_loss:1016.29150390625\n",
      "2518/3000 train_loss: 36.58167266845703 test_loss:1056.884033203125\n",
      "2519/3000 train_loss: 40.88993453979492 test_loss:979.0877685546875\n",
      "2520/3000 train_loss: 41.10554885864258 test_loss:913.137451171875\n",
      "2521/3000 train_loss: 34.73079299926758 test_loss:921.95068359375\n",
      "2522/3000 train_loss: 38.648033142089844 test_loss:923.3331298828125\n",
      "2523/3000 train_loss: 33.20073699951172 test_loss:908.85400390625\n",
      "2524/3000 train_loss: 35.64967346191406 test_loss:912.8101196289062\n",
      "2525/3000 train_loss: 31.476486206054688 test_loss:905.103759765625\n",
      "2526/3000 train_loss: 35.9002799987793 test_loss:989.018798828125\n",
      "2527/3000 train_loss: 32.50226974487305 test_loss:967.8548583984375\n",
      "2528/3000 train_loss: 28.476085662841797 test_loss:965.91064453125\n",
      "2529/3000 train_loss: 32.655765533447266 test_loss:977.3270874023438\n",
      "2530/3000 train_loss: 34.80217742919922 test_loss:976.8602294921875\n",
      "2531/3000 train_loss: 29.51752471923828 test_loss:937.4526977539062\n",
      "2532/3000 train_loss: 31.33050537109375 test_loss:950.8251953125\n",
      "2533/3000 train_loss: 33.97712326049805 test_loss:927.0931396484375\n",
      "2534/3000 train_loss: 35.483367919921875 test_loss:911.1605224609375\n",
      "2535/3000 train_loss: 32.08413314819336 test_loss:899.6036376953125\n",
      "2536/3000 train_loss: 28.964445114135742 test_loss:908.8890380859375\n",
      "2537/3000 train_loss: 32.60448455810547 test_loss:932.28662109375\n",
      "2538/3000 train_loss: 32.5268669128418 test_loss:956.6800537109375\n",
      "2539/3000 train_loss: 32.234344482421875 test_loss:990.0895385742188\n",
      "2540/3000 train_loss: 32.032814025878906 test_loss:985.1490478515625\n",
      "2541/3000 train_loss: 32.28019332885742 test_loss:991.7569580078125\n",
      "2542/3000 train_loss: 29.273426055908203 test_loss:1015.7481689453125\n",
      "2543/3000 train_loss: 33.72813034057617 test_loss:1002.494384765625\n",
      "2544/3000 train_loss: 31.35184669494629 test_loss:1023.041259765625\n",
      "2545/3000 train_loss: 35.17100524902344 test_loss:974.7852172851562\n",
      "2546/3000 train_loss: 31.501693725585938 test_loss:986.4727172851562\n",
      "2547/3000 train_loss: 32.28285598754883 test_loss:979.0040283203125\n",
      "2548/3000 train_loss: 30.346155166625977 test_loss:1027.9437255859375\n",
      "2549/3000 train_loss: 31.238739013671875 test_loss:1010.5621948242188\n",
      "2550/3000 train_loss: 36.19941711425781 test_loss:989.7601928710938\n",
      "2551/3000 train_loss: 33.27814865112305 test_loss:974.8343505859375\n",
      "2552/3000 train_loss: 30.516193389892578 test_loss:945.905029296875\n",
      "2553/3000 train_loss: 31.16100311279297 test_loss:934.7417602539062\n",
      "2554/3000 train_loss: 29.71456527709961 test_loss:992.36181640625\n",
      "2555/3000 train_loss: 31.97443962097168 test_loss:1004.5648803710938\n",
      "2556/3000 train_loss: 31.949155807495117 test_loss:980.51123046875\n",
      "2557/3000 train_loss: 30.566471099853516 test_loss:960.4400634765625\n",
      "2558/3000 train_loss: 31.838504791259766 test_loss:959.4700317382812\n",
      "2559/3000 train_loss: 29.38935089111328 test_loss:963.4892578125\n",
      "2560/3000 train_loss: 36.00120162963867 test_loss:958.3821411132812\n",
      "2561/3000 train_loss: 36.609962463378906 test_loss:936.6551513671875\n",
      "2562/3000 train_loss: 35.1322021484375 test_loss:947.2540893554688\n",
      "2563/3000 train_loss: 31.92990493774414 test_loss:924.298583984375\n",
      "2564/3000 train_loss: 30.856061935424805 test_loss:916.625\n",
      "2565/3000 train_loss: 32.16783905029297 test_loss:941.2970581054688\n",
      "2566/3000 train_loss: 34.6912727355957 test_loss:1015.2257080078125\n",
      "2567/3000 train_loss: 29.582401275634766 test_loss:978.01708984375\n",
      "2568/3000 train_loss: 35.34620666503906 test_loss:984.3030395507812\n",
      "2569/3000 train_loss: 28.843246459960938 test_loss:967.07080078125\n",
      "2570/3000 train_loss: 27.52391815185547 test_loss:956.844482421875\n",
      "2571/3000 train_loss: 32.72787857055664 test_loss:974.880126953125\n",
      "2572/3000 train_loss: 37.24839782714844 test_loss:933.8963623046875\n",
      "2573/3000 train_loss: 29.329187393188477 test_loss:938.64208984375\n",
      "2574/3000 train_loss: 31.41241455078125 test_loss:972.593505859375\n",
      "2575/3000 train_loss: 36.93160629272461 test_loss:956.2022705078125\n",
      "2576/3000 train_loss: 31.506345748901367 test_loss:939.7265625\n",
      "2577/3000 train_loss: 30.888748168945312 test_loss:937.8587646484375\n",
      "2578/3000 train_loss: 40.73644256591797 test_loss:958.3140258789062\n",
      "2579/3000 train_loss: 32.85365676879883 test_loss:917.0439453125\n",
      "2580/3000 train_loss: 34.6813850402832 test_loss:901.2977294921875\n",
      "2581/3000 train_loss: 31.899066925048828 test_loss:910.6558837890625\n",
      "2582/3000 train_loss: 33.38942337036133 test_loss:897.0101318359375\n",
      "2583/3000 train_loss: 28.981307983398438 test_loss:864.6129150390625\n",
      "2584/3000 train_loss: 31.24305534362793 test_loss:898.65087890625\n",
      "2585/3000 train_loss: 33.52418899536133 test_loss:910.5799560546875\n",
      "2586/3000 train_loss: 39.153587341308594 test_loss:926.4503173828125\n",
      "2587/3000 train_loss: 36.593135833740234 test_loss:937.9298706054688\n",
      "2588/3000 train_loss: 29.019939422607422 test_loss:936.637451171875\n",
      "2589/3000 train_loss: 35.49631881713867 test_loss:879.35205078125\n",
      "2590/3000 train_loss: 36.67420959472656 test_loss:935.0123291015625\n",
      "2591/3000 train_loss: 30.519176483154297 test_loss:909.6004638671875\n",
      "2592/3000 train_loss: 33.098960876464844 test_loss:915.433837890625\n",
      "2593/3000 train_loss: 32.05523681640625 test_loss:906.3116455078125\n",
      "2594/3000 train_loss: 31.80185890197754 test_loss:946.523681640625\n",
      "2595/3000 train_loss: 29.7455997467041 test_loss:984.3258056640625\n",
      "2596/3000 train_loss: 28.123483657836914 test_loss:933.6039428710938\n",
      "2597/3000 train_loss: 34.331298828125 test_loss:960.5839233398438\n",
      "2598/3000 train_loss: 31.316795349121094 test_loss:976.635009765625\n",
      "2599/3000 train_loss: 31.709827423095703 test_loss:944.75927734375\n",
      "2600/3000 train_loss: 35.22584533691406 test_loss:919.4393920898438\n",
      "2601/3000 train_loss: 29.143077850341797 test_loss:945.5511474609375\n",
      "2602/3000 train_loss: 39.5012092590332 test_loss:885.6829833984375\n",
      "2603/3000 train_loss: 27.728235244750977 test_loss:842.2376708984375\n",
      "2604/3000 train_loss: 33.71339416503906 test_loss:925.2137451171875\n",
      "2605/3000 train_loss: 31.969839096069336 test_loss:909.5242309570312\n",
      "2606/3000 train_loss: 32.510005950927734 test_loss:926.4488525390625\n",
      "2607/3000 train_loss: 34.76591491699219 test_loss:902.996337890625\n",
      "2608/3000 train_loss: 31.875202178955078 test_loss:920.9656372070312\n",
      "2609/3000 train_loss: 36.818538665771484 test_loss:898.1962280273438\n",
      "2610/3000 train_loss: 33.616790771484375 test_loss:903.5650634765625\n",
      "2611/3000 train_loss: 42.823429107666016 test_loss:895.9703369140625\n",
      "2612/3000 train_loss: 31.96506690979004 test_loss:865.4913330078125\n",
      "2613/3000 train_loss: 33.06108474731445 test_loss:892.7171020507812\n",
      "2614/3000 train_loss: 37.079036712646484 test_loss:864.9766845703125\n",
      "2615/3000 train_loss: 39.298736572265625 test_loss:882.5576171875\n",
      "2616/3000 train_loss: 33.8112907409668 test_loss:924.2234497070312\n",
      "2617/3000 train_loss: 32.62994384765625 test_loss:900.6610717773438\n",
      "2618/3000 train_loss: 31.432231903076172 test_loss:904.5633544921875\n",
      "2619/3000 train_loss: 31.830543518066406 test_loss:929.103515625\n",
      "2620/3000 train_loss: 32.77571105957031 test_loss:896.0997314453125\n",
      "2621/3000 train_loss: 37.06265640258789 test_loss:916.9071044921875\n",
      "2622/3000 train_loss: 33.16038513183594 test_loss:972.1861572265625\n",
      "2623/3000 train_loss: 32.129756927490234 test_loss:947.91796875\n",
      "2624/3000 train_loss: 30.445541381835938 test_loss:945.1448974609375\n",
      "2625/3000 train_loss: 40.32000732421875 test_loss:948.5281982421875\n",
      "2626/3000 train_loss: 33.341224670410156 test_loss:987.7279663085938\n",
      "2627/3000 train_loss: 38.454071044921875 test_loss:1005.0517578125\n",
      "2628/3000 train_loss: 31.791109085083008 test_loss:950.4161376953125\n",
      "2629/3000 train_loss: 31.554269790649414 test_loss:999.2339477539062\n",
      "2630/3000 train_loss: 30.196048736572266 test_loss:942.1822509765625\n",
      "2631/3000 train_loss: 29.61225128173828 test_loss:935.455322265625\n",
      "2632/3000 train_loss: 28.952960968017578 test_loss:960.9608764648438\n",
      "2633/3000 train_loss: 34.3282470703125 test_loss:983.0447387695312\n",
      "2634/3000 train_loss: 37.21179962158203 test_loss:960.7611083984375\n",
      "2635/3000 train_loss: 30.459470748901367 test_loss:952.4901123046875\n",
      "2636/3000 train_loss: 30.434144973754883 test_loss:945.5113525390625\n",
      "2637/3000 train_loss: 33.73626708984375 test_loss:954.7334594726562\n",
      "2638/3000 train_loss: 33.32802963256836 test_loss:964.949951171875\n",
      "2639/3000 train_loss: 33.72325897216797 test_loss:994.9820556640625\n",
      "2640/3000 train_loss: 29.871294021606445 test_loss:1002.3106689453125\n",
      "2641/3000 train_loss: 33.345436096191406 test_loss:1088.7841796875\n",
      "2642/3000 train_loss: 32.30436706542969 test_loss:1002.100341796875\n",
      "2643/3000 train_loss: 30.18741226196289 test_loss:991.50537109375\n",
      "2644/3000 train_loss: 33.38636016845703 test_loss:996.762939453125\n",
      "2645/3000 train_loss: 28.275846481323242 test_loss:1022.9613037109375\n",
      "2646/3000 train_loss: 31.200523376464844 test_loss:997.2678833007812\n",
      "2647/3000 train_loss: 33.52527618408203 test_loss:957.1280517578125\n",
      "2648/3000 train_loss: 29.957124710083008 test_loss:973.85888671875\n",
      "2649/3000 train_loss: 35.787315368652344 test_loss:980.7470703125\n",
      "2650/3000 train_loss: 30.12211036682129 test_loss:950.3536987304688\n",
      "2651/3000 train_loss: 34.490516662597656 test_loss:967.9612426757812\n",
      "2652/3000 train_loss: 32.82204818725586 test_loss:973.43994140625\n",
      "2653/3000 train_loss: 30.951831817626953 test_loss:961.739501953125\n",
      "2654/3000 train_loss: 31.940393447875977 test_loss:955.2685546875\n",
      "2655/3000 train_loss: 30.016883850097656 test_loss:950.5977783203125\n",
      "2656/3000 train_loss: 31.376598358154297 test_loss:984.346923828125\n",
      "2657/3000 train_loss: 27.13402557373047 test_loss:1002.626708984375\n",
      "2658/3000 train_loss: 32.42104721069336 test_loss:1007.18017578125\n",
      "2659/3000 train_loss: 36.52764892578125 test_loss:999.391357421875\n",
      "2660/3000 train_loss: 33.135128021240234 test_loss:947.2517700195312\n",
      "2661/3000 train_loss: 37.62405014038086 test_loss:1006.336181640625\n",
      "2662/3000 train_loss: 37.55384826660156 test_loss:963.2528076171875\n",
      "2663/3000 train_loss: 31.97264862060547 test_loss:937.3695068359375\n",
      "2664/3000 train_loss: 31.918502807617188 test_loss:950.8345947265625\n",
      "2665/3000 train_loss: 30.82621955871582 test_loss:971.54443359375\n",
      "2666/3000 train_loss: 31.114688873291016 test_loss:941.4971313476562\n",
      "2667/3000 train_loss: 28.90300750732422 test_loss:908.443603515625\n",
      "2668/3000 train_loss: 28.336545944213867 test_loss:938.5546875\n",
      "2669/3000 train_loss: 31.20774269104004 test_loss:967.8343505859375\n",
      "2670/3000 train_loss: 32.51446533203125 test_loss:934.1202392578125\n",
      "2671/3000 train_loss: 32.229373931884766 test_loss:925.4920654296875\n",
      "2672/3000 train_loss: 30.11602783203125 test_loss:941.9024047851562\n",
      "2673/3000 train_loss: 29.029054641723633 test_loss:985.7568359375\n",
      "2674/3000 train_loss: 30.532028198242188 test_loss:946.3460083007812\n",
      "2675/3000 train_loss: 33.88978576660156 test_loss:910.51318359375\n",
      "2676/3000 train_loss: 31.523847579956055 test_loss:952.865478515625\n",
      "2677/3000 train_loss: 32.301387786865234 test_loss:943.4434204101562\n",
      "2678/3000 train_loss: 34.79263687133789 test_loss:950.194580078125\n",
      "2679/3000 train_loss: 35.37505340576172 test_loss:900.1223754882812\n",
      "2680/3000 train_loss: 34.7123908996582 test_loss:905.9012451171875\n",
      "2681/3000 train_loss: 32.07308578491211 test_loss:879.7720947265625\n",
      "2682/3000 train_loss: 35.61895751953125 test_loss:897.2039794921875\n",
      "2683/3000 train_loss: 33.23945999145508 test_loss:927.8276977539062\n",
      "2684/3000 train_loss: 32.01278305053711 test_loss:922.92578125\n",
      "2685/3000 train_loss: 40.802642822265625 test_loss:932.30126953125\n",
      "2686/3000 train_loss: 33.11160659790039 test_loss:994.3297119140625\n",
      "2687/3000 train_loss: 31.37704849243164 test_loss:953.301025390625\n",
      "2688/3000 train_loss: 31.10112953186035 test_loss:942.3233642578125\n",
      "2689/3000 train_loss: 40.6977653503418 test_loss:940.99658203125\n",
      "2690/3000 train_loss: 33.9398307800293 test_loss:904.5254516601562\n",
      "2691/3000 train_loss: 28.938724517822266 test_loss:935.895263671875\n",
      "2692/3000 train_loss: 29.423526763916016 test_loss:973.172607421875\n",
      "2693/3000 train_loss: 26.764366149902344 test_loss:947.8670654296875\n",
      "2694/3000 train_loss: 30.749143600463867 test_loss:947.7385864257812\n",
      "2695/3000 train_loss: 32.48814010620117 test_loss:950.0791015625\n",
      "2696/3000 train_loss: 37.273643493652344 test_loss:908.0867919921875\n",
      "2697/3000 train_loss: 29.96262550354004 test_loss:961.68896484375\n",
      "2698/3000 train_loss: 28.956783294677734 test_loss:962.0336303710938\n",
      "2699/3000 train_loss: 29.76433563232422 test_loss:917.9674072265625\n",
      "2700/3000 train_loss: 28.359790802001953 test_loss:893.7474975585938\n",
      "2701/3000 train_loss: 35.71141052246094 test_loss:923.9053955078125\n",
      "2702/3000 train_loss: 38.27761459350586 test_loss:890.88232421875\n",
      "2703/3000 train_loss: 35.63730239868164 test_loss:862.6891479492188\n",
      "2704/3000 train_loss: 34.36944580078125 test_loss:902.4766845703125\n",
      "2705/3000 train_loss: 31.119075775146484 test_loss:865.842041015625\n",
      "2706/3000 train_loss: 31.77184295654297 test_loss:880.511474609375\n",
      "2707/3000 train_loss: 29.901596069335938 test_loss:888.1610107421875\n",
      "2708/3000 train_loss: 33.47807312011719 test_loss:917.8067016601562\n",
      "2709/3000 train_loss: 34.51087951660156 test_loss:878.99755859375\n",
      "2710/3000 train_loss: 33.9567985534668 test_loss:927.96044921875\n",
      "2711/3000 train_loss: 32.147403717041016 test_loss:889.49365234375\n",
      "2712/3000 train_loss: 28.088010787963867 test_loss:897.2617797851562\n",
      "2713/3000 train_loss: 33.19456481933594 test_loss:882.484619140625\n",
      "2714/3000 train_loss: 33.02199172973633 test_loss:853.6876831054688\n",
      "2715/3000 train_loss: 30.94121551513672 test_loss:899.1158447265625\n",
      "2716/3000 train_loss: 28.898521423339844 test_loss:908.6097412109375\n",
      "2717/3000 train_loss: 36.36294937133789 test_loss:899.9064331054688\n",
      "2718/3000 train_loss: 32.81267166137695 test_loss:913.130126953125\n",
      "2719/3000 train_loss: 37.005859375 test_loss:917.5852661132812\n",
      "2720/3000 train_loss: 33.31528854370117 test_loss:880.94140625\n",
      "2721/3000 train_loss: 35.42255401611328 test_loss:884.4922485351562\n",
      "2722/3000 train_loss: 28.777559280395508 test_loss:897.2960815429688\n",
      "2723/3000 train_loss: 29.08300018310547 test_loss:901.9636840820312\n",
      "2724/3000 train_loss: 48.04020690917969 test_loss:893.8048706054688\n",
      "2725/3000 train_loss: 39.44211196899414 test_loss:891.4349365234375\n",
      "2726/3000 train_loss: 47.55126953125 test_loss:969.400634765625\n",
      "2727/3000 train_loss: 33.479759216308594 test_loss:1012.692138671875\n",
      "2728/3000 train_loss: 32.55785369873047 test_loss:967.99365234375\n",
      "2729/3000 train_loss: 37.81479263305664 test_loss:969.0731201171875\n",
      "2730/3000 train_loss: 32.8005485534668 test_loss:976.2608032226562\n",
      "2731/3000 train_loss: 32.277408599853516 test_loss:958.8540649414062\n",
      "2732/3000 train_loss: 32.37786102294922 test_loss:944.3660888671875\n",
      "2733/3000 train_loss: 33.12699508666992 test_loss:938.7366943359375\n",
      "2734/3000 train_loss: 46.61283493041992 test_loss:964.6773071289062\n",
      "2735/3000 train_loss: 34.71949005126953 test_loss:997.7982177734375\n",
      "2736/3000 train_loss: 33.424095153808594 test_loss:967.8026733398438\n",
      "2737/3000 train_loss: 31.94388198852539 test_loss:963.37255859375\n",
      "2738/3000 train_loss: 34.424739837646484 test_loss:1000.4599609375\n",
      "2739/3000 train_loss: 30.353656768798828 test_loss:1000.4207763671875\n",
      "2740/3000 train_loss: 29.412677764892578 test_loss:998.65380859375\n",
      "2741/3000 train_loss: 30.03525733947754 test_loss:984.918701171875\n",
      "2742/3000 train_loss: 30.090360641479492 test_loss:964.0452880859375\n",
      "2743/3000 train_loss: 37.205299377441406 test_loss:939.8804931640625\n",
      "2744/3000 train_loss: 28.40315818786621 test_loss:931.3328857421875\n",
      "2745/3000 train_loss: 28.929107666015625 test_loss:942.4965209960938\n",
      "2746/3000 train_loss: 28.317869186401367 test_loss:964.224853515625\n",
      "2747/3000 train_loss: 37.904205322265625 test_loss:937.504638671875\n",
      "2748/3000 train_loss: 29.815711975097656 test_loss:902.6394653320312\n",
      "2749/3000 train_loss: 36.158023834228516 test_loss:920.1431274414062\n",
      "2750/3000 train_loss: 34.91206359863281 test_loss:929.031982421875\n",
      "2751/3000 train_loss: 32.025360107421875 test_loss:944.989990234375\n",
      "2752/3000 train_loss: 28.910070419311523 test_loss:908.8005981445312\n",
      "2753/3000 train_loss: 26.21603775024414 test_loss:938.8572998046875\n",
      "2754/3000 train_loss: 33.18953323364258 test_loss:939.0330810546875\n",
      "2755/3000 train_loss: 36.65385818481445 test_loss:899.3360595703125\n",
      "2756/3000 train_loss: 30.861852645874023 test_loss:949.773681640625\n",
      "2757/3000 train_loss: 30.70288848876953 test_loss:959.144287109375\n",
      "2758/3000 train_loss: 29.5320987701416 test_loss:950.9955444335938\n",
      "2759/3000 train_loss: 29.932300567626953 test_loss:951.9034423828125\n",
      "2760/3000 train_loss: 31.627199172973633 test_loss:934.3336181640625\n",
      "2761/3000 train_loss: 30.996578216552734 test_loss:920.4457397460938\n",
      "2762/3000 train_loss: 28.6097469329834 test_loss:907.356201171875\n",
      "2763/3000 train_loss: 34.112144470214844 test_loss:924.9511108398438\n",
      "2764/3000 train_loss: 31.0963191986084 test_loss:942.10888671875\n",
      "2765/3000 train_loss: 31.014076232910156 test_loss:945.882568359375\n",
      "2766/3000 train_loss: 35.331199645996094 test_loss:932.0872802734375\n",
      "2767/3000 train_loss: 29.952322006225586 test_loss:945.9525146484375\n",
      "2768/3000 train_loss: 30.198993682861328 test_loss:920.8439331054688\n",
      "2769/3000 train_loss: 46.172760009765625 test_loss:934.41064453125\n",
      "2770/3000 train_loss: 38.713741302490234 test_loss:949.7157592773438\n",
      "2771/3000 train_loss: 51.050010681152344 test_loss:951.7647094726562\n",
      "2772/3000 train_loss: 47.18354415893555 test_loss:988.3475341796875\n",
      "2773/3000 train_loss: 40.246604919433594 test_loss:948.10888671875\n",
      "2774/3000 train_loss: 33.6702766418457 test_loss:957.5467529296875\n",
      "2775/3000 train_loss: 34.228153228759766 test_loss:922.1307373046875\n",
      "2776/3000 train_loss: 34.06592559814453 test_loss:927.08251953125\n",
      "2777/3000 train_loss: 32.468772888183594 test_loss:939.824951171875\n",
      "2778/3000 train_loss: 33.366119384765625 test_loss:928.8306884765625\n",
      "2779/3000 train_loss: 32.954986572265625 test_loss:878.015869140625\n",
      "2780/3000 train_loss: 31.794103622436523 test_loss:923.5712890625\n",
      "2781/3000 train_loss: 31.416135787963867 test_loss:886.7402954101562\n",
      "2782/3000 train_loss: 28.54259490966797 test_loss:875.535888671875\n",
      "2783/3000 train_loss: 29.075651168823242 test_loss:901.2750244140625\n",
      "2784/3000 train_loss: 27.90127944946289 test_loss:912.305419921875\n",
      "2785/3000 train_loss: 31.161771774291992 test_loss:917.801513671875\n",
      "2786/3000 train_loss: 29.650266647338867 test_loss:963.5435791015625\n",
      "2787/3000 train_loss: 32.49885940551758 test_loss:930.489013671875\n",
      "2788/3000 train_loss: 29.333438873291016 test_loss:920.188232421875\n",
      "2789/3000 train_loss: 28.87372398376465 test_loss:919.5889282226562\n",
      "2790/3000 train_loss: 32.06926727294922 test_loss:912.0299072265625\n",
      "2791/3000 train_loss: 32.32942581176758 test_loss:871.630615234375\n",
      "2792/3000 train_loss: 29.234981536865234 test_loss:922.77685546875\n",
      "2793/3000 train_loss: 30.79352569580078 test_loss:931.0200805664062\n",
      "2794/3000 train_loss: 34.95865249633789 test_loss:941.7513427734375\n",
      "2795/3000 train_loss: 32.38761901855469 test_loss:948.6890869140625\n",
      "2796/3000 train_loss: 31.146455764770508 test_loss:931.9595947265625\n",
      "2797/3000 train_loss: 42.51972198486328 test_loss:938.1080322265625\n",
      "2798/3000 train_loss: 37.429927825927734 test_loss:947.9893188476562\n",
      "2799/3000 train_loss: 37.24324417114258 test_loss:957.667724609375\n",
      "2800/3000 train_loss: 60.507286071777344 test_loss:976.260498046875\n",
      "2801/3000 train_loss: 40.827125549316406 test_loss:996.7943115234375\n",
      "2802/3000 train_loss: 31.769189834594727 test_loss:987.1197509765625\n",
      "2803/3000 train_loss: 37.393592834472656 test_loss:955.0418701171875\n",
      "2804/3000 train_loss: 33.68017578125 test_loss:940.2611694335938\n",
      "2805/3000 train_loss: 32.38278579711914 test_loss:904.90673828125\n",
      "2806/3000 train_loss: 33.873023986816406 test_loss:967.7681884765625\n",
      "2807/3000 train_loss: 29.439802169799805 test_loss:929.3673095703125\n",
      "2808/3000 train_loss: 43.69741439819336 test_loss:939.84912109375\n",
      "2809/3000 train_loss: 29.238563537597656 test_loss:984.3211059570312\n",
      "2810/3000 train_loss: 28.00992774963379 test_loss:971.616455078125\n",
      "2811/3000 train_loss: 30.171323776245117 test_loss:973.3089599609375\n",
      "2812/3000 train_loss: 26.615970611572266 test_loss:943.7821044921875\n",
      "2813/3000 train_loss: 32.38414764404297 test_loss:949.5303955078125\n",
      "2814/3000 train_loss: 33.47841262817383 test_loss:943.1697998046875\n",
      "2815/3000 train_loss: 37.39984130859375 test_loss:952.997314453125\n",
      "2816/3000 train_loss: 33.524253845214844 test_loss:938.6893920898438\n",
      "2817/3000 train_loss: 38.731136322021484 test_loss:952.14794921875\n",
      "2818/3000 train_loss: 28.82261085510254 test_loss:922.44384765625\n",
      "2819/3000 train_loss: 28.26496696472168 test_loss:969.5184326171875\n",
      "2820/3000 train_loss: 31.517162322998047 test_loss:930.657470703125\n",
      "2821/3000 train_loss: 31.041370391845703 test_loss:920.0248413085938\n",
      "2822/3000 train_loss: 32.445152282714844 test_loss:909.4013671875\n",
      "2823/3000 train_loss: 32.3814697265625 test_loss:897.7127075195312\n",
      "2824/3000 train_loss: 31.695674896240234 test_loss:898.6568603515625\n",
      "2825/3000 train_loss: 30.01567268371582 test_loss:938.619384765625\n",
      "2826/3000 train_loss: 29.46812629699707 test_loss:958.005126953125\n",
      "2827/3000 train_loss: 32.52214050292969 test_loss:939.753662109375\n",
      "2828/3000 train_loss: 36.70899200439453 test_loss:913.8125\n",
      "2829/3000 train_loss: 45.719078063964844 test_loss:955.453369140625\n",
      "2830/3000 train_loss: 33.73118591308594 test_loss:943.192138671875\n",
      "2831/3000 train_loss: 28.185279846191406 test_loss:975.5482177734375\n",
      "2832/3000 train_loss: 31.044851303100586 test_loss:939.144775390625\n",
      "2833/3000 train_loss: 33.64038848876953 test_loss:950.699462890625\n",
      "2834/3000 train_loss: 27.730043411254883 test_loss:959.80419921875\n",
      "2835/3000 train_loss: 28.856908798217773 test_loss:955.5653076171875\n",
      "2836/3000 train_loss: 29.141502380371094 test_loss:941.2567138671875\n",
      "2837/3000 train_loss: 34.43513107299805 test_loss:958.595458984375\n",
      "2838/3000 train_loss: 30.323537826538086 test_loss:931.15771484375\n",
      "2839/3000 train_loss: 27.437063217163086 test_loss:952.130859375\n",
      "2840/3000 train_loss: 35.242637634277344 test_loss:913.712158203125\n",
      "2841/3000 train_loss: 32.110260009765625 test_loss:941.0362548828125\n",
      "2842/3000 train_loss: 30.190265655517578 test_loss:929.9628295898438\n",
      "2843/3000 train_loss: 32.40318298339844 test_loss:948.070556640625\n",
      "2844/3000 train_loss: 29.30207633972168 test_loss:945.8206787109375\n",
      "2845/3000 train_loss: 30.750097274780273 test_loss:948.5408935546875\n",
      "2846/3000 train_loss: 29.68614959716797 test_loss:952.2640380859375\n",
      "2847/3000 train_loss: 27.240278244018555 test_loss:943.9998779296875\n",
      "2848/3000 train_loss: 29.64487648010254 test_loss:930.2861328125\n",
      "2849/3000 train_loss: 29.894609451293945 test_loss:960.1873779296875\n",
      "2850/3000 train_loss: 30.712100982666016 test_loss:953.859375\n",
      "2851/3000 train_loss: 34.335777282714844 test_loss:963.6409912109375\n",
      "2852/3000 train_loss: 39.15118408203125 test_loss:962.8814697265625\n",
      "2853/3000 train_loss: 31.25446319580078 test_loss:950.5888671875\n",
      "2854/3000 train_loss: 33.96413803100586 test_loss:940.6492919921875\n",
      "2855/3000 train_loss: 29.614395141601562 test_loss:945.7801513671875\n",
      "2856/3000 train_loss: 33.40224838256836 test_loss:939.835693359375\n",
      "2857/3000 train_loss: 30.7729434967041 test_loss:929.3933715820312\n",
      "2858/3000 train_loss: 38.31522750854492 test_loss:953.24560546875\n",
      "2859/3000 train_loss: 29.76053810119629 test_loss:914.3658447265625\n",
      "2860/3000 train_loss: 30.244457244873047 test_loss:907.9476318359375\n",
      "2861/3000 train_loss: 27.462553024291992 test_loss:951.708984375\n",
      "2862/3000 train_loss: 34.609405517578125 test_loss:933.5712890625\n",
      "2863/3000 train_loss: 29.247264862060547 test_loss:965.1241455078125\n",
      "2864/3000 train_loss: 29.227170944213867 test_loss:950.8729248046875\n",
      "2865/3000 train_loss: 35.375709533691406 test_loss:970.8936767578125\n",
      "2866/3000 train_loss: 31.882429122924805 test_loss:962.0091552734375\n",
      "2867/3000 train_loss: 30.76045799255371 test_loss:982.228759765625\n",
      "2868/3000 train_loss: 30.58079719543457 test_loss:966.5675659179688\n",
      "2869/3000 train_loss: 30.656335830688477 test_loss:953.6550903320312\n",
      "2870/3000 train_loss: 26.883445739746094 test_loss:959.6097412109375\n",
      "2871/3000 train_loss: 33.03724670410156 test_loss:1009.12451171875\n",
      "2872/3000 train_loss: 31.767154693603516 test_loss:993.9132080078125\n",
      "2873/3000 train_loss: 26.5093936920166 test_loss:1040.64697265625\n",
      "2874/3000 train_loss: 33.18709182739258 test_loss:1020.0408935546875\n",
      "2875/3000 train_loss: 31.8884334564209 test_loss:1051.824462890625\n",
      "2876/3000 train_loss: 41.090240478515625 test_loss:1036.423828125\n",
      "2877/3000 train_loss: 34.15064239501953 test_loss:1017.7791748046875\n",
      "2878/3000 train_loss: 30.9443359375 test_loss:1049.681884765625\n",
      "2879/3000 train_loss: 31.128034591674805 test_loss:1049.958984375\n",
      "2880/3000 train_loss: 33.871253967285156 test_loss:1036.533447265625\n",
      "2881/3000 train_loss: 35.5440673828125 test_loss:1002.9493408203125\n",
      "2882/3000 train_loss: 31.619903564453125 test_loss:972.2918701171875\n",
      "2883/3000 train_loss: 34.73383712768555 test_loss:991.2625122070312\n",
      "2884/3000 train_loss: 24.86725425720215 test_loss:989.80810546875\n",
      "2885/3000 train_loss: 31.139617919921875 test_loss:1016.8489379882812\n",
      "2886/3000 train_loss: 31.376211166381836 test_loss:980.1527709960938\n",
      "2887/3000 train_loss: 33.58580017089844 test_loss:980.34423828125\n",
      "2888/3000 train_loss: 28.333410263061523 test_loss:954.8556518554688\n",
      "2889/3000 train_loss: 35.88486862182617 test_loss:968.623779296875\n",
      "2890/3000 train_loss: 33.33468246459961 test_loss:962.7342529296875\n",
      "2891/3000 train_loss: 28.137317657470703 test_loss:971.360107421875\n",
      "2892/3000 train_loss: 35.33454513549805 test_loss:937.840576171875\n",
      "2893/3000 train_loss: 32.6566276550293 test_loss:977.3607177734375\n",
      "2894/3000 train_loss: 31.187389373779297 test_loss:927.7196655273438\n",
      "2895/3000 train_loss: 30.90020751953125 test_loss:935.1611328125\n",
      "2896/3000 train_loss: 27.958728790283203 test_loss:961.4122314453125\n",
      "2897/3000 train_loss: 27.69042205810547 test_loss:999.8619384765625\n",
      "2898/3000 train_loss: 29.810842514038086 test_loss:989.46337890625\n",
      "2899/3000 train_loss: 27.89733123779297 test_loss:968.4776611328125\n",
      "2900/3000 train_loss: 29.457914352416992 test_loss:970.6732788085938\n",
      "2901/3000 train_loss: 32.535614013671875 test_loss:947.0565185546875\n",
      "2902/3000 train_loss: 34.25893020629883 test_loss:965.7637939453125\n",
      "2903/3000 train_loss: 32.100791931152344 test_loss:940.4564208984375\n",
      "2904/3000 train_loss: 30.397851943969727 test_loss:926.0933227539062\n",
      "2905/3000 train_loss: 38.448150634765625 test_loss:932.9632568359375\n",
      "2906/3000 train_loss: 30.05789566040039 test_loss:964.18310546875\n",
      "2907/3000 train_loss: 30.44931983947754 test_loss:994.7974853515625\n",
      "2908/3000 train_loss: 27.611289978027344 test_loss:965.6329345703125\n",
      "2909/3000 train_loss: 33.0690803527832 test_loss:982.307861328125\n",
      "2910/3000 train_loss: 31.423633575439453 test_loss:1024.435546875\n",
      "2911/3000 train_loss: 34.66709899902344 test_loss:981.34228515625\n",
      "2912/3000 train_loss: 29.228010177612305 test_loss:996.054443359375\n",
      "2913/3000 train_loss: 29.56097984313965 test_loss:974.1634521484375\n",
      "2914/3000 train_loss: 33.737144470214844 test_loss:978.7769775390625\n",
      "2915/3000 train_loss: 33.532833099365234 test_loss:963.017578125\n",
      "2916/3000 train_loss: 32.17055892944336 test_loss:973.953125\n",
      "2917/3000 train_loss: 30.017940521240234 test_loss:958.3251953125\n",
      "2918/3000 train_loss: 27.29769515991211 test_loss:989.5637817382812\n",
      "2919/3000 train_loss: 29.243698120117188 test_loss:952.7864990234375\n",
      "2920/3000 train_loss: 32.759212493896484 test_loss:961.5933837890625\n",
      "2921/3000 train_loss: 33.443809509277344 test_loss:976.502685546875\n",
      "2922/3000 train_loss: 32.322410583496094 test_loss:939.3626098632812\n",
      "2923/3000 train_loss: 32.48210144042969 test_loss:957.5032958984375\n",
      "2924/3000 train_loss: 35.38732147216797 test_loss:957.43359375\n",
      "2925/3000 train_loss: 30.120380401611328 test_loss:947.596923828125\n",
      "2926/3000 train_loss: 31.389646530151367 test_loss:972.0455322265625\n",
      "2927/3000 train_loss: 26.011978149414062 test_loss:923.2042846679688\n",
      "2928/3000 train_loss: 29.527616500854492 test_loss:943.8613891601562\n",
      "2929/3000 train_loss: 36.51069641113281 test_loss:961.9209594726562\n",
      "2930/3000 train_loss: 37.01469039916992 test_loss:955.1192626953125\n",
      "2931/3000 train_loss: 32.956939697265625 test_loss:928.598876953125\n",
      "2932/3000 train_loss: 30.70001220703125 test_loss:959.4716796875\n",
      "2933/3000 train_loss: 32.08306884765625 test_loss:966.2476806640625\n",
      "2934/3000 train_loss: 32.807491302490234 test_loss:940.0762939453125\n",
      "2935/3000 train_loss: 30.85110092163086 test_loss:915.551025390625\n",
      "2936/3000 train_loss: 29.62474250793457 test_loss:939.9862060546875\n",
      "2937/3000 train_loss: 29.279258728027344 test_loss:950.1856689453125\n",
      "2938/3000 train_loss: 28.714902877807617 test_loss:940.8970947265625\n",
      "2939/3000 train_loss: 30.01547622680664 test_loss:921.5031127929688\n",
      "2940/3000 train_loss: 28.54884147644043 test_loss:944.1170043945312\n",
      "2941/3000 train_loss: 31.80848503112793 test_loss:1000.369873046875\n",
      "2942/3000 train_loss: 32.07075881958008 test_loss:947.0386962890625\n",
      "2943/3000 train_loss: 27.62594223022461 test_loss:936.00048828125\n",
      "2944/3000 train_loss: 32.1624755859375 test_loss:948.1809692382812\n",
      "2945/3000 train_loss: 30.230005264282227 test_loss:962.254638671875\n",
      "2946/3000 train_loss: 25.75020980834961 test_loss:948.544921875\n",
      "2947/3000 train_loss: 27.90567398071289 test_loss:942.3447875976562\n",
      "2948/3000 train_loss: 30.925159454345703 test_loss:947.9229736328125\n",
      "2949/3000 train_loss: 33.08113479614258 test_loss:972.8544921875\n",
      "2950/3000 train_loss: 30.686250686645508 test_loss:979.437744140625\n",
      "2951/3000 train_loss: 30.83190155029297 test_loss:972.3887939453125\n",
      "2952/3000 train_loss: 25.55129623413086 test_loss:946.1826171875\n",
      "2953/3000 train_loss: 31.182209014892578 test_loss:988.1173095703125\n",
      "2954/3000 train_loss: 32.180267333984375 test_loss:997.4730834960938\n",
      "2955/3000 train_loss: 32.36204528808594 test_loss:932.7224731445312\n",
      "2956/3000 train_loss: 29.939037322998047 test_loss:914.3817749023438\n",
      "2957/3000 train_loss: 29.474458694458008 test_loss:949.7627563476562\n",
      "2958/3000 train_loss: 32.09660720825195 test_loss:944.6370239257812\n",
      "2959/3000 train_loss: 26.2737979888916 test_loss:923.5953369140625\n",
      "2960/3000 train_loss: 25.051898956298828 test_loss:947.9700927734375\n",
      "2961/3000 train_loss: 29.531299591064453 test_loss:958.443603515625\n",
      "2962/3000 train_loss: 26.93756103515625 test_loss:945.90087890625\n",
      "2963/3000 train_loss: 36.843231201171875 test_loss:907.8294067382812\n",
      "2964/3000 train_loss: 50.790321350097656 test_loss:930.3428344726562\n",
      "2965/3000 train_loss: 30.307754516601562 test_loss:831.7278442382812\n",
      "2966/3000 train_loss: 29.213157653808594 test_loss:850.6817626953125\n",
      "2967/3000 train_loss: 36.6169548034668 test_loss:871.3040771484375\n",
      "2968/3000 train_loss: 33.91349792480469 test_loss:846.580322265625\n",
      "2969/3000 train_loss: 29.7078914642334 test_loss:871.03076171875\n",
      "2970/3000 train_loss: 31.968467712402344 test_loss:862.4031372070312\n",
      "2971/3000 train_loss: 32.19097137451172 test_loss:812.058837890625\n",
      "2972/3000 train_loss: 27.28134536743164 test_loss:844.5133056640625\n",
      "2973/3000 train_loss: 30.623090744018555 test_loss:871.4876098632812\n",
      "2974/3000 train_loss: 29.45867919921875 test_loss:905.1398315429688\n",
      "2975/3000 train_loss: 34.231929779052734 test_loss:941.6619873046875\n",
      "2976/3000 train_loss: 31.30959701538086 test_loss:915.95654296875\n",
      "2977/3000 train_loss: 35.95241928100586 test_loss:910.02490234375\n",
      "2978/3000 train_loss: 29.83514404296875 test_loss:907.16015625\n",
      "2979/3000 train_loss: 34.405853271484375 test_loss:908.8248291015625\n",
      "2980/3000 train_loss: 33.48270034790039 test_loss:953.47265625\n",
      "2981/3000 train_loss: 38.40559005737305 test_loss:950.904296875\n",
      "2982/3000 train_loss: 30.129749298095703 test_loss:899.2523193359375\n",
      "2983/3000 train_loss: 33.728450775146484 test_loss:948.834228515625\n",
      "2984/3000 train_loss: 28.367719650268555 test_loss:935.6639404296875\n",
      "2985/3000 train_loss: 35.345455169677734 test_loss:932.8256225585938\n",
      "2986/3000 train_loss: 28.576332092285156 test_loss:843.248046875\n",
      "2987/3000 train_loss: 31.416593551635742 test_loss:864.8131103515625\n",
      "2988/3000 train_loss: 28.582332611083984 test_loss:871.596923828125\n",
      "2989/3000 train_loss: 26.554969787597656 test_loss:873.3102416992188\n",
      "2990/3000 train_loss: 34.54451370239258 test_loss:872.171630859375\n",
      "2991/3000 train_loss: 33.979095458984375 test_loss:927.4475708007812\n",
      "2992/3000 train_loss: 32.41624069213867 test_loss:934.1659545898438\n",
      "2993/3000 train_loss: 29.013490676879883 test_loss:910.158447265625\n",
      "2994/3000 train_loss: 33.23017883300781 test_loss:913.15478515625\n",
      "2995/3000 train_loss: 32.60851287841797 test_loss:905.3886108398438\n",
      "2996/3000 train_loss: 33.40258026123047 test_loss:928.9360961914062\n",
      "2997/3000 train_loss: 30.437881469726562 test_loss:890.669189453125\n",
      "2998/3000 train_loss: 33.38951873779297 test_loss:918.71484375\n",
      "2999/3000 train_loss: 29.679912567138672 test_loss:906.16748046875\n",
      "3000/3000 train_loss: 32.89027404785156 test_loss:924.1749267578125\n"
     ]
    }
   ],
   "source": [
    "losses = train(model = unet, optimizer = optimizer, criterion=criterion, data_tr=train_data,\n",
    "               data_val = test_data, scheduler = scheduler,device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "6Ew7_F0-q7aL",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "85888ed2-c169-44f7-d514-ff1dffb1085d"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(924.1749)"
      ]
     },
     "metadata": {},
     "execution_count": 88
    }
   ],
   "source": [
    "avg_loss = 0\n",
    "# unet.eval()\n",
    "# unet.train()\n",
    "preds = []\n",
    "i = 0\n",
    "test_anomaly_losses = []\n",
    "test_normal_losses = []\n",
    "test_losses = []\n",
    "test_real = y_test.tolist()\n",
    "for batch in test_data:\n",
    "  with torch.no_grad():\n",
    "    # unet.train()\n",
    "    predictions = unet(batch.to(device)).cpu()\n",
    "    preds.append(predictions)\n",
    "  loss = criterion(predictions, batch.cpu())\n",
    "  for j in range(len(predictions)):\n",
    "    if int(y_test[i]) == 1:\n",
    "      test_anomaly_losses.append(float(criterion(predictions[j], batch[j])))\n",
    "    else:\n",
    "      test_normal_losses.append(float(criterion(predictions[j], batch[j])))\n",
    "    i += 1\n",
    "    test_losses.append(criterion(predictions[j], batch[j]))\n",
    "  # print(loss)\n",
    "  # print(loss)\n",
    "  avg_loss += loss / len(test_data)\n",
    "# avg_loss\n",
    "\n",
    "avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "VpDKorrRso9o",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "e77fd25c-a5d5-459c-841e-6ff36ba1be56"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1362.2417647355087, 91.97403500556946)"
      ]
     },
     "metadata": {},
     "execution_count": 89
    }
   ],
   "source": [
    "sum(test_anomaly_losses)/len(test_anomaly_losses) , sum(test_normal_losses)/len(test_normal_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5LEvbZKYuh7J",
    "outputId": "6863fb96-d6b8-4d51-caaa-0d7c9b43ca5c"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.9545104895104896\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "vals = np.arange(50, 1000, 0.5).tolist()\n",
    "for threshold in vals:\n",
    "  preds = []\n",
    "  for j in range(len(test_losses)):\n",
    "    if test_losses[j] > threshold:\n",
    "      preds.append(1)\n",
    "    else:\n",
    "      preds.append(0)\n",
    "  \n",
    "  results.append(roc_auc_score(test_real,preds))\n",
    "\n",
    "print(max(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QaSSqG8SbAw2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
