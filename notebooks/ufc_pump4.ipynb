{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "9SStKf4G0V5H"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "import torchaudio\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from skimage.util import img_as_ubyte\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import io\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_XtxbKLZq5KX",
    "outputId": "37023990-2b40-4e1f-e63e-1dba18c3acdb"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CYxHegIM0Z4i",
    "outputId": "f68f6fab-6d0f-4c1f-9d0a-527e166123f6"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive/\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "h9DATQwS0ivD"
   },
   "outputs": [],
   "source": [
    "class MimiiDataset(Dataset):\n",
    "    def __init__(self,audio_dir, n_fft = 1024, win_length = 1024,\n",
    "                 hop_length = 512,power = 2,n_mels = 128,pad_mode = 'reflect',\n",
    "                 sr = 16000,center = True,norm = None):\n",
    "      \n",
    "        super(MimiiDataset, self).__init__()\n",
    "        self.audio_dir = audio_dir\n",
    "        self.n_mels = n_mels\n",
    "        self.n_fft = n_fft\n",
    "        self.win_length = win_length\n",
    "        self.hop_length = hop_length\n",
    "        self.power = power\n",
    "        self.pad_mode = pad_mode\n",
    "        self.sr = sr\n",
    "        self.center = center\n",
    "        self.norm = norm\n",
    "\n",
    "    def get_files(self):\n",
    "       return self.train_files, self.test_files\n",
    "    \n",
    "    def get_data(self,device, id):\n",
    "        \n",
    "        self.train_files, self.train_labels = self._train_file_list(device, id)\n",
    "        self.test_files, self.test_labels = self._test_file_list(device, id)\n",
    "        \n",
    "        self.train_data = self.get_audios(self.train_files)\n",
    "        self.test_data = self.get_audios(self.test_files)\n",
    "        \n",
    "        return self.train_data, self.test_data, self.train_labels, self.test_labels\n",
    "    \n",
    "    def _train_file_list(self, device, id):\n",
    "        query = os.path.abspath(\n",
    "            f\"{self.audio_dir}/{device}/train/normal_id_0{id}*.wav\"\n",
    "        )\n",
    "        train_normal_files = sorted(glob.glob(query))\n",
    "        train_normal_labels = np.zeros(len(train_normal_files))\n",
    "        \n",
    "        query = os.path.abspath(\n",
    "                f\"{self.audio_dir}/{device}/train/anomaly_id_0{id}*.wav\"\n",
    "            )\n",
    "        train_anomaly_files = sorted(glob.glob(query))\n",
    "        train_anomaly_labels = np.ones(len(train_anomaly_files))\n",
    "        \n",
    "        train_file_list = np.concatenate((train_normal_files, train_anomaly_files), axis=0)\n",
    "        train_labels = np.concatenate((train_normal_labels, train_anomaly_labels), axis=0)\n",
    "        \n",
    "        return train_file_list, train_labels\n",
    "    \n",
    "    def _test_file_list(self, device, id):     \n",
    "        query = os.path.abspath(\n",
    "            f\"{self.audio_dir}/{device}/test/normal_id_0{id}*.wav\"\n",
    "            )\n",
    "        test_normal_files = sorted(glob.glob(query))\n",
    "        test_normal_labels = np.zeros(len(test_normal_files))\n",
    "        \n",
    "        query = os.path.abspath(\n",
    "            f\"{self.audio_dir}/{device}/test/anomaly_id_0{id}*.wav\"\n",
    "            )\n",
    "        test_anomaly_files = sorted(glob.glob(query))\n",
    "        test_anomaly_labels = np.ones(len(test_anomaly_files))\n",
    "        \n",
    "        test_file_list = np.concatenate((test_normal_files, \n",
    "                                          test_anomaly_files), axis=0)\n",
    "        test_labels = np.concatenate((test_normal_labels,\n",
    "                                      test_anomaly_labels), axis=0)\n",
    "          \n",
    "        return test_file_list, test_labels\n",
    "\n",
    "    def normalize(self,tensor):\n",
    "        tensor_minusmean = tensor - tensor.mean()\n",
    "        return tensor_minusmean/np.absolute(tensor_minusmean).max()\n",
    "\n",
    "    def make0min(self,tensornd):\n",
    "        tensor = tensornd.numpy()\n",
    "        res = np.where(tensor == 0, 1E-19 , tensor)\n",
    "        return torch.from_numpy(res)\n",
    "\n",
    "    def spectrogrameToImage(self,specgram):\n",
    "        # specgram = torchaudio.transforms.MelSpectrogram(n_fft=1024, win_length=1024, \n",
    "        #                                                 hop_length=512, power=2, \n",
    "        #                                                 normalized=True, n_mels=128)(waveform )\n",
    "        specgram= self.make0min(specgram)\n",
    "        specgram = specgram.log2()[0,:,:].numpy()\n",
    "        \n",
    "        tr2image = transforms.Compose([transforms.ToPILImage()])\n",
    "\n",
    "        specgram= self.normalize(specgram)\n",
    "        # specgram = img_as_ubyte(specgram)\n",
    "        specgramImage = tr2image(specgram)\n",
    "        return specgramImage\n",
    "\n",
    "    def get_logmelspectrogram(self, waveform):\n",
    "        melspec = librosa.feature.melspectrogram(\n",
    "          n_fft=self.n_fft, win_length=self.win_length, hop_length=self.hop_length,\n",
    "          power=self.power,n_mels=self.n_mels,pad_mode=self.pad_mode,sr=self.sr,\n",
    "          center=self.center,norm=self.norm,htk=True,\n",
    "          y = waveform.numpy()\n",
    "        )\n",
    "\n",
    "        logmelspec = librosa.power_to_db(melspec)\n",
    "\n",
    "        return logmelspec\n",
    "\n",
    "    def get_melspectrogram(self,waveform):\n",
    "        melspec = librosa.feature.melspectrogram(\n",
    "            n_fft=self.n_fft, win_length=self.win_length, hop_length=self.hop_length,\n",
    "            power=self.power,n_mels=self.n_mels,pad_mode=self.pad_mode,sr=self.sr,\n",
    "            center=self.center,norm=self.norm,htk=True,\n",
    "            y = waveform.numpy()\n",
    "        )\n",
    "\n",
    "        return melspec\n",
    "    \n",
    "    def get_mfcc(self,waveform):\n",
    "        mfcc = librosa.feature.mfcc(    \n",
    "            n_fft=self.n_fft, win_length=self.win_length, \n",
    "            hop_length=self.hop_length,pad_mode=self.pad_mode,sr=self.sr,\n",
    "            center=self.center,norm=self.norm,n_mfcc=40,\n",
    "            y = waveform.numpy()\n",
    "        )\n",
    "\n",
    "        return mfcc\n",
    "\n",
    "    def get_chroma_stft(self,waveform):\n",
    "        stft = librosa.feature.chroma_stft(\n",
    "            n_fft=self.n_fft, win_length=self.win_length, \n",
    "            hop_length=self.hop_length,pad_mode=self.pad_mode,sr=self.sr,\n",
    "            center=self.center,norm=self.norm,n_chroma=12,\n",
    "            y=waveform.numpy()\n",
    "        )\n",
    "\n",
    "        return stft\n",
    "\n",
    "    def get_spectral_contrast(self,waveform):\n",
    "        spec_contrast = librosa.feature.spectral_contrast(    \n",
    "            n_fft=self.n_fft, win_length=self.win_length,center=self.center,\n",
    "            hop_length=self.hop_length,pad_mode=self.pad_mode,sr=self.sr,\n",
    "            y = waveform.numpy()\n",
    "        )\n",
    "\n",
    "        return spec_contrast\n",
    "    \n",
    "    def get_tonnetz(self,waveform):\n",
    "        harmonic = librosa.effects.harmonic(waveform.numpy())\n",
    "        tonnetz = librosa.feature.tonnetz(y=harmonic,sr=self.sr)\n",
    "\n",
    "        return tonnetz\n",
    "\n",
    "    def get_audios(self, file_list):\n",
    "        data = []\n",
    "        for i in range(len(file_list)):\n",
    "          y, sr = torchaudio.load(file_list[i])  \n",
    "          data.append(y)\n",
    "\n",
    "        return data\n",
    "    def _derive_data(self, file_list):\n",
    "        train_data = []\n",
    "        test_data = []\n",
    "        train_mode = True\n",
    "        for file_list in [self.train_files, self.test_files]:\n",
    "          tr2tensor = transforms.Compose([transforms.PILToTensor()])\n",
    "          data = []\n",
    "          for j in range(len(file_list)):\n",
    "            y, sr = torchaudio.load(file_list[j])  \n",
    "            spec = self.get_melspectrogram(y)\n",
    "            spec = self.spectrogrameToImage(spec)\n",
    "            spec = spec.convert('RGB')\n",
    "            vectors = tr2tensor(spec)\n",
    "            if train_mode:     \n",
    "              train_data.append(vectors)\n",
    "            else:\n",
    "              test_data.append(vectors)\n",
    "            \n",
    "          train_mode = False\n",
    "                \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "S96soeIc0o13"
   },
   "outputs": [],
   "source": [
    "dataset = MimiiDataset('/content/drive/MyDrive/mimii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "Gn2zdn92doi1"
   },
   "outputs": [],
   "source": [
    "_, _, y_train, y_test = dataset.get_data('pump', 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "SgjpeWy_RV1C"
   },
   "outputs": [],
   "source": [
    "train_mixed_f = torch.load('/content/drive/MyDrive/mixed_features/train_mf_pump4.pt')\n",
    "test_mixed_f = torch.load('/content/drive/MyDrive/mixed_features/test_mf_pump4.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "jWMPVGu1qiEq"
   },
   "outputs": [],
   "source": [
    "train_data = DataLoader(train_mixed_f, batch_size=32, shuffle = True)\n",
    "test_data = DataLoader(test_mixed_f, batch_size = 32, shuffle= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "vNTBTRe6qnBq"
   },
   "outputs": [],
   "source": [
    "class UNet_FC(nn.Module):\n",
    "\n",
    "  def __init__(self, in_features):\n",
    "    super().__init__()\n",
    "    \n",
    "    self.bn = nn.BatchNorm1d(128)\n",
    "    self.relu = nn.ReLU()\n",
    "\n",
    "    self.fc0 = nn.Linear(in_features=in_features,out_features=in_features)\n",
    "\n",
    "    # encoder\n",
    "    self.fc1 = nn.Linear(in_features=in_features, out_features=128)\n",
    "    self.fc2 = nn.Linear(in_features=128,out_features=128)\n",
    "    self.fc3 = nn.Linear(in_features=128, out_features=128)\n",
    "    self.fc4 = nn.Linear(in_features=128, out_features=128)\n",
    "    self.fc5 = nn.Linear(in_features=128, out_features=8)\n",
    "\n",
    "    # decoder\n",
    "    self.fc6 = nn.Linear(in_features=8, out_features=128)\n",
    "    self.fc7 = nn.Linear(in_features=128*2, out_features=128)\n",
    "    self.fc8 = nn.Linear(in_features=128*2, out_features=128)\n",
    "    self.fc9 = nn.Linear(in_features=128*2, out_features=128)\n",
    "\n",
    "    self.out = nn.Linear(in_features=128*2, out_features=in_features)\n",
    "\n",
    "  def forward(self, x):\n",
    "    input = self.fc0(x)\n",
    "\n",
    "    x1 = self.relu(self.bn(self.fc1(input)))\n",
    "    x2 = self.relu(self.bn(self.fc2(x1)))\n",
    "    x3 = self.relu(self.bn(self.fc3(x2)))\n",
    "    x4 = self.relu(self.bn(self.fc4(x3)))\n",
    "    x5 = self.relu(self.fc5(x4))\n",
    "\n",
    "    xy = [x5, x4, x3, x2, x1]\n",
    "\n",
    "    x6 = self.relu(self.fc6(xy[0]))\n",
    "    con1 = torch.cat((x6,xy[1]), 1) \n",
    "    x7 = self.relu(self.bn(self.fc7(con1)))\n",
    "    con2 = torch.cat((x7,xy[2]), 1)\n",
    "    x8 = self.relu(self.bn(self.fc8(con2)))\n",
    "    con3 = torch.cat((x8,xy[3]), 1)\n",
    "    x9 = self.relu(self.bn(self.fc9(con3)))\n",
    "    con4 = torch.cat((x9,xy[4]), 1)\n",
    "\n",
    "    x10 = self.out(con4)\n",
    "\n",
    "    return x10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "ZfgcBtQ3qn5l"
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, data_tr, data_val, scheduler = None,\n",
    "          epochs = 3000, device = 'cpu'):\n",
    "    # X_val, Y_val = next(iter(data_val))\n",
    "    losses = []\n",
    "    prev_avg_loss = 100000\n",
    "    for epoch in range(epochs):\n",
    "        train_avg_loss = 0\n",
    "        test_avg_loss = 0\n",
    "        # model.train()  # train mode\n",
    "        for batch in data_tr:\n",
    "          # data to device\n",
    "          batch = batch.to(device)\n",
    "          # set parameter gradients to zero\n",
    "          optimizer.zero_grad()\n",
    "          # forward\n",
    "          # print(Y_batch.shape)\n",
    "          predictions = model(batch)\n",
    "          loss = criterion(predictions, batch)\n",
    "          loss.backward() # backward-pass\n",
    "          optimizer.step()  # update weights\n",
    "          # calculate loss to show the user\n",
    "          if scheduler:\n",
    "            scheduler.step(loss)\n",
    "          train_avg_loss += loss / len(data_tr)\n",
    "\n",
    "        # model.eval()\n",
    "        for batch in data_val:\n",
    "          with torch.no_grad():\n",
    "            preds = model(batch.to(device)).cpu()\n",
    "            loss = criterion(preds,batch)\n",
    "            test_avg_loss += loss / len(data_val)\n",
    "                    \n",
    "        losses.append(train_avg_loss.item())\n",
    "        # if (epoch+1)%50 == 0:\n",
    "        print(\"{}/{} train_loss: {} test_loss:{}\".format(epoch+1, epochs, train_avg_loss, test_avg_loss))\n",
    "        # if test_avg_loss < 70:\n",
    "        #   break\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "ptkVTF55quOL"
   },
   "outputs": [],
   "source": [
    "unet = UNet_FC(in_features=193).to(device)\n",
    "optimizer = Adam(params = unet.parameters(), lr = 10e-3)\n",
    "# optimizer = Adam(params = unet.parameters())\n",
    "criterion = nn.MSELoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, \n",
    "                                                       min_lr=10e-4, mode = 'min',\n",
    "                                                       patience = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QkfmYl9oXhcB",
    "outputId": "9c5113fd-5057-4f46-92a1-ccba27fdf43c"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1/3000 train_loss: 281437.84375 test_loss:278470.0\n",
      "2/3000 train_loss: 277263.84375 test_loss:273441.28125\n",
      "3/3000 train_loss: 270618.71875 test_loss:265524.59375\n",
      "4/3000 train_loss: 260645.71875 test_loss:253788.3125\n",
      "5/3000 train_loss: 247427.875 test_loss:238925.03125\n",
      "6/3000 train_loss: 231215.5 test_loss:221574.203125\n",
      "7/3000 train_loss: 212314.3125 test_loss:202073.59375\n",
      "8/3000 train_loss: 191725.890625 test_loss:180605.59375\n",
      "9/3000 train_loss: 169911.234375 test_loss:158011.703125\n",
      "10/3000 train_loss: 147992.921875 test_loss:137045.78125\n",
      "11/3000 train_loss: 124685.6953125 test_loss:113730.109375\n",
      "12/3000 train_loss: 102728.96875 test_loss:92726.3828125\n",
      "13/3000 train_loss: 82651.4765625 test_loss:74758.21875\n",
      "14/3000 train_loss: 64718.0390625 test_loss:57185.3125\n",
      "15/3000 train_loss: 48507.26953125 test_loss:42204.96484375\n",
      "16/3000 train_loss: 35237.640625 test_loss:30298.14453125\n",
      "17/3000 train_loss: 24596.705078125 test_loss:21281.33203125\n",
      "18/3000 train_loss: 16851.4609375 test_loss:14523.5458984375\n",
      "19/3000 train_loss: 11248.2607421875 test_loss:9964.0927734375\n",
      "20/3000 train_loss: 7378.02685546875 test_loss:6679.26708984375\n",
      "21/3000 train_loss: 4672.3837890625 test_loss:4320.12890625\n",
      "22/3000 train_loss: 3142.68115234375 test_loss:3435.751220703125\n",
      "23/3000 train_loss: 2204.514892578125 test_loss:2603.9140625\n",
      "24/3000 train_loss: 1485.380126953125 test_loss:2165.608154296875\n",
      "25/3000 train_loss: 1307.12353515625 test_loss:2068.932861328125\n",
      "26/3000 train_loss: 1052.49072265625 test_loss:1730.4493408203125\n",
      "27/3000 train_loss: 999.8470458984375 test_loss:1615.300537109375\n",
      "28/3000 train_loss: 880.1622314453125 test_loss:1563.4716796875\n",
      "29/3000 train_loss: 805.9013671875 test_loss:1549.85546875\n",
      "30/3000 train_loss: 775.84033203125 test_loss:1486.168212890625\n",
      "31/3000 train_loss: 770.1268920898438 test_loss:1459.1912841796875\n",
      "32/3000 train_loss: 828.2535400390625 test_loss:1491.40185546875\n",
      "33/3000 train_loss: 761.0119018554688 test_loss:1473.316650390625\n",
      "34/3000 train_loss: 715.3299560546875 test_loss:1426.5731201171875\n",
      "35/3000 train_loss: 722.0301513671875 test_loss:1415.079345703125\n",
      "36/3000 train_loss: 753.3439331054688 test_loss:1412.719482421875\n",
      "37/3000 train_loss: 700.9664306640625 test_loss:1408.75244140625\n",
      "38/3000 train_loss: 692.1881103515625 test_loss:1402.55322265625\n",
      "39/3000 train_loss: 695.3746948242188 test_loss:1400.763427734375\n",
      "40/3000 train_loss: 697.138916015625 test_loss:1402.4599609375\n",
      "41/3000 train_loss: 688.5761108398438 test_loss:1392.958740234375\n",
      "42/3000 train_loss: 697.1211547851562 test_loss:1392.01220703125\n",
      "43/3000 train_loss: 705.2860717773438 test_loss:1383.2833251953125\n",
      "44/3000 train_loss: 681.1896362304688 test_loss:1382.6094970703125\n",
      "45/3000 train_loss: 664.9412841796875 test_loss:1369.078369140625\n",
      "46/3000 train_loss: 705.29931640625 test_loss:1376.6875\n",
      "47/3000 train_loss: 659.2976684570312 test_loss:1371.276611328125\n",
      "48/3000 train_loss: 675.3756103515625 test_loss:1361.0457763671875\n",
      "49/3000 train_loss: 657.7784423828125 test_loss:1351.9871826171875\n",
      "50/3000 train_loss: 650.7902221679688 test_loss:1355.73193359375\n",
      "51/3000 train_loss: 647.2089233398438 test_loss:1357.1297607421875\n",
      "52/3000 train_loss: 643.6524658203125 test_loss:1344.36474609375\n",
      "53/3000 train_loss: 638.848388671875 test_loss:1342.7393798828125\n",
      "54/3000 train_loss: 629.8339233398438 test_loss:1334.3697509765625\n",
      "55/3000 train_loss: 622.5344848632812 test_loss:1333.806396484375\n",
      "56/3000 train_loss: 652.7825317382812 test_loss:1328.293212890625\n",
      "57/3000 train_loss: 642.1860961914062 test_loss:1323.5072021484375\n",
      "58/3000 train_loss: 628.3557739257812 test_loss:1318.626953125\n",
      "59/3000 train_loss: 598.8726196289062 test_loss:1312.0889892578125\n",
      "60/3000 train_loss: 634.8284301757812 test_loss:1316.818359375\n",
      "61/3000 train_loss: 618.2576904296875 test_loss:1310.275634765625\n",
      "62/3000 train_loss: 597.4476318359375 test_loss:1300.8524169921875\n",
      "63/3000 train_loss: 605.8948364257812 test_loss:1302.0380859375\n",
      "64/3000 train_loss: 591.5060424804688 test_loss:1291.986572265625\n",
      "65/3000 train_loss: 593.15869140625 test_loss:1283.3385009765625\n",
      "66/3000 train_loss: 597.836669921875 test_loss:1281.8770751953125\n",
      "67/3000 train_loss: 602.81494140625 test_loss:1283.709228515625\n",
      "68/3000 train_loss: 590.6710815429688 test_loss:1265.1820068359375\n",
      "69/3000 train_loss: 574.3878173828125 test_loss:1268.925537109375\n",
      "70/3000 train_loss: 575.7320556640625 test_loss:1268.1416015625\n",
      "71/3000 train_loss: 590.8536376953125 test_loss:1256.827880859375\n",
      "72/3000 train_loss: 570.5161743164062 test_loss:1258.2918701171875\n",
      "73/3000 train_loss: 581.660888671875 test_loss:1260.031494140625\n",
      "74/3000 train_loss: 576.5440673828125 test_loss:1244.803955078125\n",
      "75/3000 train_loss: 574.474853515625 test_loss:1243.39697265625\n",
      "76/3000 train_loss: 571.0587768554688 test_loss:1238.386474609375\n",
      "77/3000 train_loss: 568.0728759765625 test_loss:1233.62890625\n",
      "78/3000 train_loss: 555.7933959960938 test_loss:1227.179443359375\n",
      "79/3000 train_loss: 551.740966796875 test_loss:1235.40966796875\n",
      "80/3000 train_loss: 557.5216674804688 test_loss:1228.3677978515625\n",
      "81/3000 train_loss: 561.51171875 test_loss:1224.2562255859375\n",
      "82/3000 train_loss: 539.6019287109375 test_loss:1216.385009765625\n",
      "83/3000 train_loss: 549.5698852539062 test_loss:1221.68701171875\n",
      "84/3000 train_loss: 530.7410888671875 test_loss:1213.58203125\n",
      "85/3000 train_loss: 529.7180786132812 test_loss:1222.0819091796875\n",
      "86/3000 train_loss: 529.8014526367188 test_loss:1217.6317138671875\n",
      "87/3000 train_loss: 536.4442138671875 test_loss:1203.7861328125\n",
      "88/3000 train_loss: 524.92919921875 test_loss:1206.713134765625\n",
      "89/3000 train_loss: 535.4063720703125 test_loss:1201.52294921875\n",
      "90/3000 train_loss: 550.5128173828125 test_loss:1200.879638671875\n",
      "91/3000 train_loss: 523.3042602539062 test_loss:1197.01171875\n",
      "92/3000 train_loss: 527.53564453125 test_loss:1202.0714111328125\n",
      "93/3000 train_loss: 558.4984741210938 test_loss:1208.2684326171875\n",
      "94/3000 train_loss: 508.99395751953125 test_loss:1213.7418212890625\n",
      "95/3000 train_loss: 516.40673828125 test_loss:1229.025634765625\n",
      "96/3000 train_loss: 517.8600463867188 test_loss:1216.2303466796875\n",
      "97/3000 train_loss: 522.3916015625 test_loss:1209.1070556640625\n",
      "98/3000 train_loss: 537.1223754882812 test_loss:1214.2314453125\n",
      "99/3000 train_loss: 540.1011352539062 test_loss:1218.9718017578125\n",
      "100/3000 train_loss: 524.613525390625 test_loss:1218.068603515625\n",
      "101/3000 train_loss: 541.3292236328125 test_loss:1210.02490234375\n",
      "102/3000 train_loss: 519.4576416015625 test_loss:1204.8365478515625\n",
      "103/3000 train_loss: 528.9327392578125 test_loss:1199.4298095703125\n",
      "104/3000 train_loss: 514.9495239257812 test_loss:1191.272216796875\n",
      "105/3000 train_loss: 505.740966796875 test_loss:1182.087890625\n",
      "106/3000 train_loss: 501.85009765625 test_loss:1187.5589599609375\n",
      "107/3000 train_loss: 512.396484375 test_loss:1199.7578125\n",
      "108/3000 train_loss: 503.4298400878906 test_loss:1191.545166015625\n",
      "109/3000 train_loss: 500.158935546875 test_loss:1179.7520751953125\n",
      "110/3000 train_loss: 509.822021484375 test_loss:1191.50341796875\n",
      "111/3000 train_loss: 501.458251953125 test_loss:1185.5357666015625\n",
      "112/3000 train_loss: 497.8638916015625 test_loss:1177.451416015625\n",
      "113/3000 train_loss: 488.2416687011719 test_loss:1183.1416015625\n",
      "114/3000 train_loss: 486.1199645996094 test_loss:1194.4918212890625\n",
      "115/3000 train_loss: 507.69287109375 test_loss:1179.5225830078125\n",
      "116/3000 train_loss: 483.23919677734375 test_loss:1181.59619140625\n",
      "117/3000 train_loss: 493.21514892578125 test_loss:1178.8033447265625\n",
      "118/3000 train_loss: 502.974853515625 test_loss:1177.8187255859375\n",
      "119/3000 train_loss: 496.32806396484375 test_loss:1180.3199462890625\n",
      "120/3000 train_loss: 510.1143798828125 test_loss:1163.226318359375\n",
      "121/3000 train_loss: 487.6663818359375 test_loss:1172.0341796875\n",
      "122/3000 train_loss: 509.8543701171875 test_loss:1160.4549560546875\n",
      "123/3000 train_loss: 489.8968505859375 test_loss:1153.3868408203125\n",
      "124/3000 train_loss: 490.9569396972656 test_loss:1163.146484375\n",
      "125/3000 train_loss: 481.49951171875 test_loss:1160.6253662109375\n",
      "126/3000 train_loss: 477.2063903808594 test_loss:1163.8961181640625\n",
      "127/3000 train_loss: 471.6000671386719 test_loss:1152.272705078125\n",
      "128/3000 train_loss: 478.8312072753906 test_loss:1152.7821044921875\n",
      "129/3000 train_loss: 480.320068359375 test_loss:1155.3587646484375\n",
      "130/3000 train_loss: 492.2131042480469 test_loss:1150.46044921875\n",
      "131/3000 train_loss: 469.9238586425781 test_loss:1141.0634765625\n",
      "132/3000 train_loss: 469.20843505859375 test_loss:1140.1138916015625\n",
      "133/3000 train_loss: 469.7729187011719 test_loss:1135.72265625\n",
      "134/3000 train_loss: 492.0406494140625 test_loss:1141.686767578125\n",
      "135/3000 train_loss: 467.194580078125 test_loss:1144.1455078125\n",
      "136/3000 train_loss: 462.15411376953125 test_loss:1132.5855712890625\n",
      "137/3000 train_loss: 446.16375732421875 test_loss:1134.64453125\n",
      "138/3000 train_loss: 470.59814453125 test_loss:1128.661865234375\n",
      "139/3000 train_loss: 466.7344970703125 test_loss:1125.6064453125\n",
      "140/3000 train_loss: 456.9281311035156 test_loss:1119.9639892578125\n",
      "141/3000 train_loss: 454.38934326171875 test_loss:1119.12744140625\n",
      "142/3000 train_loss: 445.5309753417969 test_loss:1119.46533203125\n",
      "143/3000 train_loss: 444.42303466796875 test_loss:1120.2423095703125\n",
      "144/3000 train_loss: 449.24188232421875 test_loss:1115.6573486328125\n",
      "145/3000 train_loss: 476.40155029296875 test_loss:1113.811279296875\n",
      "146/3000 train_loss: 460.95068359375 test_loss:1105.3603515625\n",
      "147/3000 train_loss: 457.71771240234375 test_loss:1106.3734130859375\n",
      "148/3000 train_loss: 459.67120361328125 test_loss:1102.315673828125\n",
      "149/3000 train_loss: 461.7048645019531 test_loss:1109.584228515625\n",
      "150/3000 train_loss: 438.25689697265625 test_loss:1104.007568359375\n",
      "151/3000 train_loss: 463.95391845703125 test_loss:1101.7257080078125\n",
      "152/3000 train_loss: 449.3306579589844 test_loss:1102.836181640625\n",
      "153/3000 train_loss: 458.8332214355469 test_loss:1093.380615234375\n",
      "154/3000 train_loss: 451.892822265625 test_loss:1085.64599609375\n",
      "155/3000 train_loss: 456.4259033203125 test_loss:1084.7042236328125\n",
      "156/3000 train_loss: 444.18975830078125 test_loss:1096.0048828125\n",
      "157/3000 train_loss: 435.18218994140625 test_loss:1085.386474609375\n",
      "158/3000 train_loss: 427.7138671875 test_loss:1082.601806640625\n",
      "159/3000 train_loss: 428.9327697753906 test_loss:1097.482666015625\n",
      "160/3000 train_loss: 438.19598388671875 test_loss:1083.0069580078125\n",
      "161/3000 train_loss: 436.86431884765625 test_loss:1082.6256103515625\n",
      "162/3000 train_loss: 434.1883239746094 test_loss:1072.30908203125\n",
      "163/3000 train_loss: 425.2799377441406 test_loss:1075.078125\n",
      "164/3000 train_loss: 425.8238830566406 test_loss:1063.6202392578125\n",
      "165/3000 train_loss: 415.938232421875 test_loss:1062.7996826171875\n",
      "166/3000 train_loss: 414.3626708984375 test_loss:1062.540283203125\n",
      "167/3000 train_loss: 418.7911071777344 test_loss:1063.917724609375\n",
      "168/3000 train_loss: 427.24444580078125 test_loss:1054.308349609375\n",
      "169/3000 train_loss: 417.5754089355469 test_loss:1050.836669921875\n",
      "170/3000 train_loss: 417.9414978027344 test_loss:1054.115234375\n",
      "171/3000 train_loss: 403.5181884765625 test_loss:1052.89404296875\n",
      "172/3000 train_loss: 405.9705505371094 test_loss:1056.8919677734375\n",
      "173/3000 train_loss: 412.8771057128906 test_loss:1058.489013671875\n",
      "174/3000 train_loss: 407.5531005859375 test_loss:1060.34033203125\n",
      "175/3000 train_loss: 412.8577575683594 test_loss:1050.2724609375\n",
      "176/3000 train_loss: 410.21661376953125 test_loss:1041.510986328125\n",
      "177/3000 train_loss: 395.7326965332031 test_loss:1035.6573486328125\n",
      "178/3000 train_loss: 398.2227783203125 test_loss:1042.946044921875\n",
      "179/3000 train_loss: 390.349853515625 test_loss:1042.77099609375\n",
      "180/3000 train_loss: 409.95172119140625 test_loss:1028.6229248046875\n",
      "181/3000 train_loss: 412.69024658203125 test_loss:1043.1209716796875\n",
      "182/3000 train_loss: 381.8085021972656 test_loss:1035.986572265625\n",
      "183/3000 train_loss: 408.6743469238281 test_loss:1027.197509765625\n",
      "184/3000 train_loss: 402.8090515136719 test_loss:1027.98486328125\n",
      "185/3000 train_loss: 392.0068359375 test_loss:1027.62890625\n",
      "186/3000 train_loss: 392.92901611328125 test_loss:1026.1834716796875\n",
      "187/3000 train_loss: 380.7082214355469 test_loss:1021.9241943359375\n",
      "188/3000 train_loss: 389.8293762207031 test_loss:1036.32666015625\n",
      "189/3000 train_loss: 376.97613525390625 test_loss:1015.8902587890625\n",
      "190/3000 train_loss: 390.51934814453125 test_loss:1015.9976806640625\n",
      "191/3000 train_loss: 394.7622985839844 test_loss:1019.0458374023438\n",
      "192/3000 train_loss: 387.0604553222656 test_loss:1015.1708374023438\n",
      "193/3000 train_loss: 394.4954833984375 test_loss:999.5889892578125\n",
      "194/3000 train_loss: 392.8277587890625 test_loss:1000.4007568359375\n",
      "195/3000 train_loss: 393.74346923828125 test_loss:1001.077880859375\n",
      "196/3000 train_loss: 377.70477294921875 test_loss:1005.9486083984375\n",
      "197/3000 train_loss: 371.5977478027344 test_loss:993.7240600585938\n",
      "198/3000 train_loss: 376.65533447265625 test_loss:991.4644165039062\n",
      "199/3000 train_loss: 383.83416748046875 test_loss:994.2610473632812\n",
      "200/3000 train_loss: 368.82470703125 test_loss:989.703369140625\n",
      "201/3000 train_loss: 391.5059814453125 test_loss:991.3746337890625\n",
      "202/3000 train_loss: 382.0252380371094 test_loss:993.0489501953125\n",
      "203/3000 train_loss: 373.51861572265625 test_loss:986.412841796875\n",
      "204/3000 train_loss: 379.37298583984375 test_loss:996.0883178710938\n",
      "205/3000 train_loss: 377.02069091796875 test_loss:985.775634765625\n",
      "206/3000 train_loss: 368.5445251464844 test_loss:983.9395751953125\n",
      "207/3000 train_loss: 371.2472839355469 test_loss:1011.4633178710938\n",
      "208/3000 train_loss: 375.3506774902344 test_loss:988.008056640625\n",
      "209/3000 train_loss: 376.17041015625 test_loss:980.0426025390625\n",
      "210/3000 train_loss: 363.42852783203125 test_loss:970.470458984375\n",
      "211/3000 train_loss: 377.0111389160156 test_loss:986.693603515625\n",
      "212/3000 train_loss: 359.60528564453125 test_loss:968.5818481445312\n",
      "213/3000 train_loss: 356.3180847167969 test_loss:969.8682861328125\n",
      "214/3000 train_loss: 347.54888916015625 test_loss:967.5331420898438\n",
      "215/3000 train_loss: 354.15509033203125 test_loss:992.0794677734375\n",
      "216/3000 train_loss: 355.1152648925781 test_loss:986.4248046875\n",
      "217/3000 train_loss: 358.6323547363281 test_loss:974.8965454101562\n",
      "218/3000 train_loss: 360.8018493652344 test_loss:966.685791015625\n",
      "219/3000 train_loss: 366.944580078125 test_loss:966.412353515625\n",
      "220/3000 train_loss: 351.1641845703125 test_loss:956.5816650390625\n",
      "221/3000 train_loss: 348.9306335449219 test_loss:952.887939453125\n",
      "222/3000 train_loss: 356.8548278808594 test_loss:949.635986328125\n",
      "223/3000 train_loss: 340.6325988769531 test_loss:950.2080078125\n",
      "224/3000 train_loss: 336.163818359375 test_loss:944.6712646484375\n",
      "225/3000 train_loss: 329.96728515625 test_loss:950.359375\n",
      "226/3000 train_loss: 350.64105224609375 test_loss:949.8531494140625\n",
      "227/3000 train_loss: 340.9873046875 test_loss:942.8733520507812\n",
      "228/3000 train_loss: 335.4228210449219 test_loss:959.2411499023438\n",
      "229/3000 train_loss: 350.0469055175781 test_loss:951.350830078125\n",
      "230/3000 train_loss: 351.0582580566406 test_loss:941.349365234375\n",
      "231/3000 train_loss: 354.9570007324219 test_loss:941.2417602539062\n",
      "232/3000 train_loss: 342.85089111328125 test_loss:946.60400390625\n",
      "233/3000 train_loss: 337.0269775390625 test_loss:929.1671142578125\n",
      "234/3000 train_loss: 333.8957824707031 test_loss:941.0052490234375\n",
      "235/3000 train_loss: 332.52630615234375 test_loss:943.521484375\n",
      "236/3000 train_loss: 335.2217712402344 test_loss:938.1121215820312\n",
      "237/3000 train_loss: 342.29327392578125 test_loss:935.2034912109375\n",
      "238/3000 train_loss: 319.46295166015625 test_loss:930.9613037109375\n",
      "239/3000 train_loss: 329.4793395996094 test_loss:934.4130859375\n",
      "240/3000 train_loss: 323.8250732421875 test_loss:918.528564453125\n",
      "241/3000 train_loss: 329.71697998046875 test_loss:928.0543212890625\n",
      "242/3000 train_loss: 325.2275085449219 test_loss:928.188232421875\n",
      "243/3000 train_loss: 320.38433837890625 test_loss:913.7861328125\n",
      "244/3000 train_loss: 334.3687744140625 test_loss:916.5355224609375\n",
      "245/3000 train_loss: 328.97918701171875 test_loss:904.1516723632812\n",
      "246/3000 train_loss: 325.8017578125 test_loss:918.5847778320312\n",
      "247/3000 train_loss: 324.3753662109375 test_loss:906.9287109375\n",
      "248/3000 train_loss: 337.24749755859375 test_loss:911.30615234375\n",
      "249/3000 train_loss: 326.7567138671875 test_loss:918.202880859375\n",
      "250/3000 train_loss: 309.2383728027344 test_loss:914.7117309570312\n",
      "251/3000 train_loss: 307.8728942871094 test_loss:906.0159912109375\n",
      "252/3000 train_loss: 306.450439453125 test_loss:903.6290283203125\n",
      "253/3000 train_loss: 305.7006530761719 test_loss:903.6134033203125\n",
      "254/3000 train_loss: 327.90380859375 test_loss:894.8778076171875\n",
      "255/3000 train_loss: 316.88421630859375 test_loss:902.4935302734375\n",
      "256/3000 train_loss: 316.68011474609375 test_loss:894.6331176757812\n",
      "257/3000 train_loss: 307.8723449707031 test_loss:898.6355590820312\n",
      "258/3000 train_loss: 321.0079345703125 test_loss:895.0904541015625\n",
      "259/3000 train_loss: 302.8511962890625 test_loss:892.392822265625\n",
      "260/3000 train_loss: 299.44610595703125 test_loss:893.295166015625\n",
      "261/3000 train_loss: 302.8843994140625 test_loss:893.348388671875\n",
      "262/3000 train_loss: 298.5147705078125 test_loss:885.790771484375\n",
      "263/3000 train_loss: 302.40728759765625 test_loss:890.884521484375\n",
      "264/3000 train_loss: 305.3204345703125 test_loss:890.723876953125\n",
      "265/3000 train_loss: 305.1728515625 test_loss:879.017578125\n",
      "266/3000 train_loss: 291.2636413574219 test_loss:884.0780029296875\n",
      "267/3000 train_loss: 303.185791015625 test_loss:894.1405029296875\n",
      "268/3000 train_loss: 298.52716064453125 test_loss:881.10009765625\n",
      "269/3000 train_loss: 298.60882568359375 test_loss:878.5633544921875\n",
      "270/3000 train_loss: 295.1661376953125 test_loss:876.7664794921875\n",
      "271/3000 train_loss: 286.88177490234375 test_loss:879.8876342773438\n",
      "272/3000 train_loss: 296.5350341796875 test_loss:870.4528198242188\n",
      "273/3000 train_loss: 281.7699890136719 test_loss:871.1752319335938\n",
      "274/3000 train_loss: 289.7000427246094 test_loss:868.16259765625\n",
      "275/3000 train_loss: 285.97772216796875 test_loss:870.9898071289062\n",
      "276/3000 train_loss: 283.091552734375 test_loss:859.9586181640625\n",
      "277/3000 train_loss: 284.53936767578125 test_loss:867.1122436523438\n",
      "278/3000 train_loss: 272.90045166015625 test_loss:858.2765502929688\n",
      "279/3000 train_loss: 280.1490783691406 test_loss:852.8946533203125\n",
      "280/3000 train_loss: 268.30267333984375 test_loss:861.968505859375\n",
      "281/3000 train_loss: 268.701171875 test_loss:857.859130859375\n",
      "282/3000 train_loss: 278.18853759765625 test_loss:840.20703125\n",
      "283/3000 train_loss: 269.5423889160156 test_loss:857.103271484375\n",
      "284/3000 train_loss: 262.4648132324219 test_loss:848.251708984375\n",
      "285/3000 train_loss: 272.73583984375 test_loss:845.4647216796875\n",
      "286/3000 train_loss: 272.85498046875 test_loss:841.4951171875\n",
      "287/3000 train_loss: 264.36083984375 test_loss:836.7508544921875\n",
      "288/3000 train_loss: 269.0167541503906 test_loss:838.6390380859375\n",
      "289/3000 train_loss: 266.7957458496094 test_loss:851.6671142578125\n",
      "290/3000 train_loss: 277.0021667480469 test_loss:847.4998168945312\n",
      "291/3000 train_loss: 256.29571533203125 test_loss:834.4952392578125\n",
      "292/3000 train_loss: 271.75091552734375 test_loss:841.6851806640625\n",
      "293/3000 train_loss: 258.4131164550781 test_loss:845.6116333007812\n",
      "294/3000 train_loss: 268.008056640625 test_loss:849.4622802734375\n",
      "295/3000 train_loss: 259.4559631347656 test_loss:839.5513916015625\n",
      "296/3000 train_loss: 261.02716064453125 test_loss:834.3603515625\n",
      "297/3000 train_loss: 260.67852783203125 test_loss:837.1370849609375\n",
      "298/3000 train_loss: 263.1817932128906 test_loss:825.715087890625\n",
      "299/3000 train_loss: 261.055908203125 test_loss:833.6895751953125\n",
      "300/3000 train_loss: 252.00294494628906 test_loss:824.2313232421875\n",
      "301/3000 train_loss: 251.97418212890625 test_loss:826.646484375\n",
      "302/3000 train_loss: 274.75823974609375 test_loss:858.4962158203125\n",
      "303/3000 train_loss: 251.23306274414062 test_loss:825.4840087890625\n",
      "304/3000 train_loss: 252.39625549316406 test_loss:827.2579345703125\n",
      "305/3000 train_loss: 241.82508850097656 test_loss:816.55322265625\n",
      "306/3000 train_loss: 261.0570373535156 test_loss:822.794189453125\n",
      "307/3000 train_loss: 259.9656066894531 test_loss:820.482666015625\n",
      "308/3000 train_loss: 253.53274536132812 test_loss:821.740234375\n",
      "309/3000 train_loss: 265.7224426269531 test_loss:817.3645629882812\n",
      "310/3000 train_loss: 252.232421875 test_loss:810.1082153320312\n",
      "311/3000 train_loss: 235.7965545654297 test_loss:817.3004760742188\n",
      "312/3000 train_loss: 246.67300415039062 test_loss:806.787109375\n",
      "313/3000 train_loss: 236.8531494140625 test_loss:821.0090942382812\n",
      "314/3000 train_loss: 247.9042510986328 test_loss:820.052734375\n",
      "315/3000 train_loss: 241.591552734375 test_loss:818.4599609375\n",
      "316/3000 train_loss: 237.34825134277344 test_loss:813.150634765625\n",
      "317/3000 train_loss: 228.00196838378906 test_loss:813.6817626953125\n",
      "318/3000 train_loss: 232.7770538330078 test_loss:823.3419189453125\n",
      "319/3000 train_loss: 239.94119262695312 test_loss:814.5159912109375\n",
      "320/3000 train_loss: 228.7887420654297 test_loss:801.0760498046875\n",
      "321/3000 train_loss: 232.55625915527344 test_loss:807.452880859375\n",
      "322/3000 train_loss: 230.9408721923828 test_loss:806.0398559570312\n",
      "323/3000 train_loss: 247.00111389160156 test_loss:811.7528076171875\n",
      "324/3000 train_loss: 236.78196716308594 test_loss:808.4468994140625\n",
      "325/3000 train_loss: 232.61489868164062 test_loss:799.4986572265625\n",
      "326/3000 train_loss: 242.33590698242188 test_loss:811.9417724609375\n",
      "327/3000 train_loss: 228.77734375 test_loss:799.08544921875\n",
      "328/3000 train_loss: 222.1019744873047 test_loss:793.49951171875\n",
      "329/3000 train_loss: 228.41030883789062 test_loss:798.845947265625\n",
      "330/3000 train_loss: 227.6450653076172 test_loss:789.7528076171875\n",
      "331/3000 train_loss: 221.81210327148438 test_loss:801.2283935546875\n",
      "332/3000 train_loss: 226.87051391601562 test_loss:790.5714111328125\n",
      "333/3000 train_loss: 226.509521484375 test_loss:793.4581909179688\n",
      "334/3000 train_loss: 223.3218536376953 test_loss:788.098388671875\n",
      "335/3000 train_loss: 221.6603546142578 test_loss:798.6202392578125\n",
      "336/3000 train_loss: 230.64260864257812 test_loss:794.84326171875\n",
      "337/3000 train_loss: 221.7176513671875 test_loss:793.0153198242188\n",
      "338/3000 train_loss: 211.44662475585938 test_loss:787.0458984375\n",
      "339/3000 train_loss: 236.96405029296875 test_loss:806.5576171875\n",
      "340/3000 train_loss: 221.0644073486328 test_loss:802.653564453125\n",
      "341/3000 train_loss: 229.4719696044922 test_loss:795.986328125\n",
      "342/3000 train_loss: 222.83743286132812 test_loss:782.929443359375\n",
      "343/3000 train_loss: 216.4473419189453 test_loss:786.6422729492188\n",
      "344/3000 train_loss: 216.77963256835938 test_loss:790.6698608398438\n",
      "345/3000 train_loss: 223.81971740722656 test_loss:777.7347412109375\n",
      "346/3000 train_loss: 233.75131225585938 test_loss:778.321044921875\n",
      "347/3000 train_loss: 218.7893524169922 test_loss:783.3855590820312\n",
      "348/3000 train_loss: 230.954833984375 test_loss:775.379638671875\n",
      "349/3000 train_loss: 206.5308837890625 test_loss:774.15673828125\n",
      "350/3000 train_loss: 216.2844696044922 test_loss:789.65869140625\n",
      "351/3000 train_loss: 218.20394897460938 test_loss:783.1204833984375\n",
      "352/3000 train_loss: 207.5631103515625 test_loss:771.58984375\n",
      "353/3000 train_loss: 202.79241943359375 test_loss:769.9296875\n",
      "354/3000 train_loss: 227.58872985839844 test_loss:773.1085205078125\n",
      "355/3000 train_loss: 204.38282775878906 test_loss:770.6368408203125\n",
      "356/3000 train_loss: 201.61190795898438 test_loss:761.738525390625\n",
      "357/3000 train_loss: 207.781982421875 test_loss:769.1692504882812\n",
      "358/3000 train_loss: 212.4609832763672 test_loss:771.64404296875\n",
      "359/3000 train_loss: 199.06504821777344 test_loss:760.4508056640625\n",
      "360/3000 train_loss: 203.57687377929688 test_loss:760.5091552734375\n",
      "361/3000 train_loss: 202.50912475585938 test_loss:766.6121215820312\n",
      "362/3000 train_loss: 204.04808044433594 test_loss:765.951904296875\n",
      "363/3000 train_loss: 199.5834197998047 test_loss:768.557373046875\n",
      "364/3000 train_loss: 212.64895629882812 test_loss:764.0840454101562\n",
      "365/3000 train_loss: 202.7090606689453 test_loss:762.8052368164062\n",
      "366/3000 train_loss: 216.32188415527344 test_loss:765.1698608398438\n",
      "367/3000 train_loss: 210.89962768554688 test_loss:750.3779296875\n",
      "368/3000 train_loss: 202.5669708251953 test_loss:752.3387451171875\n",
      "369/3000 train_loss: 201.59022521972656 test_loss:758.377685546875\n",
      "370/3000 train_loss: 200.978759765625 test_loss:753.8994140625\n",
      "371/3000 train_loss: 188.69256591796875 test_loss:750.8466186523438\n",
      "372/3000 train_loss: 193.8630828857422 test_loss:750.3530883789062\n",
      "373/3000 train_loss: 196.31170654296875 test_loss:756.7218017578125\n",
      "374/3000 train_loss: 202.491455078125 test_loss:751.1808471679688\n",
      "375/3000 train_loss: 194.29037475585938 test_loss:750.332763671875\n",
      "376/3000 train_loss: 202.82577514648438 test_loss:754.0357666015625\n",
      "377/3000 train_loss: 202.334716796875 test_loss:745.1357421875\n",
      "378/3000 train_loss: 198.43377685546875 test_loss:752.8274536132812\n",
      "379/3000 train_loss: 197.45762634277344 test_loss:755.67529296875\n",
      "380/3000 train_loss: 190.24343872070312 test_loss:748.6378173828125\n",
      "381/3000 train_loss: 193.12042236328125 test_loss:748.4688110351562\n",
      "382/3000 train_loss: 193.21615600585938 test_loss:754.0320434570312\n",
      "383/3000 train_loss: 192.35643005371094 test_loss:750.461181640625\n",
      "384/3000 train_loss: 192.64041137695312 test_loss:754.4356689453125\n",
      "385/3000 train_loss: 194.297607421875 test_loss:742.74560546875\n",
      "386/3000 train_loss: 191.69093322753906 test_loss:750.634033203125\n",
      "387/3000 train_loss: 182.30941772460938 test_loss:739.8590087890625\n",
      "388/3000 train_loss: 192.35923767089844 test_loss:746.1130981445312\n",
      "389/3000 train_loss: 192.7709503173828 test_loss:743.8451538085938\n",
      "390/3000 train_loss: 183.03985595703125 test_loss:752.9751586914062\n",
      "391/3000 train_loss: 197.83995056152344 test_loss:758.8963623046875\n",
      "392/3000 train_loss: 192.29237365722656 test_loss:758.961181640625\n",
      "393/3000 train_loss: 194.0888671875 test_loss:753.111083984375\n",
      "394/3000 train_loss: 183.55397033691406 test_loss:747.5093994140625\n",
      "395/3000 train_loss: 180.9898681640625 test_loss:747.9771728515625\n",
      "396/3000 train_loss: 208.62338256835938 test_loss:746.1497802734375\n",
      "397/3000 train_loss: 183.96490478515625 test_loss:766.7863159179688\n",
      "398/3000 train_loss: 191.0017852783203 test_loss:747.861328125\n",
      "399/3000 train_loss: 189.03282165527344 test_loss:752.433837890625\n",
      "400/3000 train_loss: 184.34036254882812 test_loss:739.140380859375\n",
      "401/3000 train_loss: 174.91549682617188 test_loss:749.4254150390625\n",
      "402/3000 train_loss: 180.49205017089844 test_loss:741.5494995117188\n",
      "403/3000 train_loss: 187.5987548828125 test_loss:737.3243408203125\n",
      "404/3000 train_loss: 182.43209838867188 test_loss:745.8629150390625\n",
      "405/3000 train_loss: 184.65216064453125 test_loss:746.60986328125\n",
      "406/3000 train_loss: 174.20111083984375 test_loss:731.9442749023438\n",
      "407/3000 train_loss: 182.85479736328125 test_loss:732.0342407226562\n",
      "408/3000 train_loss: 179.34408569335938 test_loss:735.71826171875\n",
      "409/3000 train_loss: 177.44703674316406 test_loss:730.2536010742188\n",
      "410/3000 train_loss: 172.47555541992188 test_loss:735.8396606445312\n",
      "411/3000 train_loss: 176.1714324951172 test_loss:742.5401611328125\n",
      "412/3000 train_loss: 170.83792114257812 test_loss:740.1796264648438\n",
      "413/3000 train_loss: 172.25302124023438 test_loss:730.84326171875\n",
      "414/3000 train_loss: 172.00257873535156 test_loss:726.7598266601562\n",
      "415/3000 train_loss: 169.16513061523438 test_loss:725.539794921875\n",
      "416/3000 train_loss: 176.84889221191406 test_loss:728.57568359375\n",
      "417/3000 train_loss: 165.2490997314453 test_loss:728.9942626953125\n",
      "418/3000 train_loss: 171.69500732421875 test_loss:728.34130859375\n",
      "419/3000 train_loss: 174.5597686767578 test_loss:723.2540283203125\n",
      "420/3000 train_loss: 173.6294708251953 test_loss:724.7484130859375\n",
      "421/3000 train_loss: 166.1203155517578 test_loss:733.4808959960938\n",
      "422/3000 train_loss: 168.13702392578125 test_loss:725.1370849609375\n",
      "423/3000 train_loss: 166.0026397705078 test_loss:734.0066528320312\n",
      "424/3000 train_loss: 167.93447875976562 test_loss:730.9363403320312\n",
      "425/3000 train_loss: 164.42263793945312 test_loss:736.0241088867188\n",
      "426/3000 train_loss: 164.34751892089844 test_loss:733.2725830078125\n",
      "427/3000 train_loss: 167.2529754638672 test_loss:733.0498046875\n",
      "428/3000 train_loss: 169.08944702148438 test_loss:728.716796875\n",
      "429/3000 train_loss: 170.55075073242188 test_loss:729.7869262695312\n",
      "430/3000 train_loss: 173.48513793945312 test_loss:725.9193115234375\n",
      "431/3000 train_loss: 159.18336486816406 test_loss:734.4651489257812\n",
      "432/3000 train_loss: 167.24525451660156 test_loss:723.1483764648438\n",
      "433/3000 train_loss: 164.35231018066406 test_loss:714.67041015625\n",
      "434/3000 train_loss: 163.84646606445312 test_loss:722.91259765625\n",
      "435/3000 train_loss: 161.3811798095703 test_loss:721.4583740234375\n",
      "436/3000 train_loss: 168.54025268554688 test_loss:728.9435424804688\n",
      "437/3000 train_loss: 169.70843505859375 test_loss:722.0166015625\n",
      "438/3000 train_loss: 156.79689025878906 test_loss:717.2881469726562\n",
      "439/3000 train_loss: 151.87977600097656 test_loss:716.856201171875\n",
      "440/3000 train_loss: 154.31304931640625 test_loss:716.5383911132812\n",
      "441/3000 train_loss: 156.54434204101562 test_loss:715.1397705078125\n",
      "442/3000 train_loss: 157.5717010498047 test_loss:721.2055053710938\n",
      "443/3000 train_loss: 162.19586181640625 test_loss:723.9044189453125\n",
      "444/3000 train_loss: 158.82815551757812 test_loss:725.0150756835938\n",
      "445/3000 train_loss: 154.86117553710938 test_loss:712.4219970703125\n",
      "446/3000 train_loss: 160.4600372314453 test_loss:714.6617431640625\n",
      "447/3000 train_loss: 154.15545654296875 test_loss:720.94873046875\n",
      "448/3000 train_loss: 160.56373596191406 test_loss:710.2752685546875\n",
      "449/3000 train_loss: 161.383544921875 test_loss:711.93017578125\n",
      "450/3000 train_loss: 165.68881225585938 test_loss:718.4786376953125\n",
      "451/3000 train_loss: 159.90762329101562 test_loss:700.7853393554688\n",
      "452/3000 train_loss: 153.0890350341797 test_loss:725.11181640625\n",
      "453/3000 train_loss: 155.57110595703125 test_loss:714.6925048828125\n",
      "454/3000 train_loss: 153.8431396484375 test_loss:721.4158935546875\n",
      "455/3000 train_loss: 152.7472686767578 test_loss:722.38232421875\n",
      "456/3000 train_loss: 149.03012084960938 test_loss:709.7178955078125\n",
      "457/3000 train_loss: 157.1680908203125 test_loss:712.4696655273438\n",
      "458/3000 train_loss: 148.3253631591797 test_loss:721.6976318359375\n",
      "459/3000 train_loss: 142.18702697753906 test_loss:718.3772583007812\n",
      "460/3000 train_loss: 150.37640380859375 test_loss:709.5361938476562\n",
      "461/3000 train_loss: 154.12957763671875 test_loss:704.8920288085938\n",
      "462/3000 train_loss: 142.12989807128906 test_loss:705.4025268554688\n",
      "463/3000 train_loss: 142.83468627929688 test_loss:703.820556640625\n",
      "464/3000 train_loss: 146.79611206054688 test_loss:705.2717895507812\n",
      "465/3000 train_loss: 148.6217498779297 test_loss:704.56982421875\n",
      "466/3000 train_loss: 138.96603393554688 test_loss:706.968017578125\n",
      "467/3000 train_loss: 151.26771545410156 test_loss:708.1300659179688\n",
      "468/3000 train_loss: 152.52830505371094 test_loss:708.2708129882812\n",
      "469/3000 train_loss: 161.99290466308594 test_loss:708.3587036132812\n",
      "470/3000 train_loss: 148.58843994140625 test_loss:697.7149658203125\n",
      "471/3000 train_loss: 143.5049591064453 test_loss:708.6188354492188\n",
      "472/3000 train_loss: 142.12664794921875 test_loss:706.6544189453125\n",
      "473/3000 train_loss: 144.4690704345703 test_loss:698.0352783203125\n",
      "474/3000 train_loss: 138.5231170654297 test_loss:700.3014526367188\n",
      "475/3000 train_loss: 137.32901000976562 test_loss:704.9100341796875\n",
      "476/3000 train_loss: 142.8947296142578 test_loss:695.8502197265625\n",
      "477/3000 train_loss: 145.46156311035156 test_loss:694.442626953125\n",
      "478/3000 train_loss: 134.6364288330078 test_loss:705.4591674804688\n",
      "479/3000 train_loss: 147.92124938964844 test_loss:710.0585327148438\n",
      "480/3000 train_loss: 138.21441650390625 test_loss:701.55419921875\n",
      "481/3000 train_loss: 147.1951904296875 test_loss:710.2381591796875\n",
      "482/3000 train_loss: 135.45791625976562 test_loss:697.2452392578125\n",
      "483/3000 train_loss: 140.49673461914062 test_loss:699.8153076171875\n",
      "484/3000 train_loss: 140.2826385498047 test_loss:692.0205078125\n",
      "485/3000 train_loss: 127.99418640136719 test_loss:692.672607421875\n",
      "486/3000 train_loss: 140.84634399414062 test_loss:702.6873168945312\n",
      "487/3000 train_loss: 150.31948852539062 test_loss:703.1015014648438\n",
      "488/3000 train_loss: 137.77154541015625 test_loss:700.2373657226562\n",
      "489/3000 train_loss: 137.72056579589844 test_loss:694.1984252929688\n",
      "490/3000 train_loss: 153.63546752929688 test_loss:697.70361328125\n",
      "491/3000 train_loss: 134.34396362304688 test_loss:685.669921875\n",
      "492/3000 train_loss: 130.1099395751953 test_loss:689.7015380859375\n",
      "493/3000 train_loss: 127.67655944824219 test_loss:687.204833984375\n",
      "494/3000 train_loss: 123.98625183105469 test_loss:686.2294311523438\n",
      "495/3000 train_loss: 132.9087371826172 test_loss:688.457275390625\n",
      "496/3000 train_loss: 138.59927368164062 test_loss:690.0927734375\n",
      "497/3000 train_loss: 134.518310546875 test_loss:710.5966796875\n",
      "498/3000 train_loss: 139.84228515625 test_loss:684.5416259765625\n",
      "499/3000 train_loss: 127.20001220703125 test_loss:692.1165161132812\n",
      "500/3000 train_loss: 135.39276123046875 test_loss:684.157958984375\n",
      "501/3000 train_loss: 137.05564880371094 test_loss:696.14111328125\n",
      "502/3000 train_loss: 119.40153503417969 test_loss:695.646484375\n",
      "503/3000 train_loss: 145.28468322753906 test_loss:697.5852661132812\n",
      "504/3000 train_loss: 141.65284729003906 test_loss:682.7952880859375\n",
      "505/3000 train_loss: 135.84857177734375 test_loss:689.5324096679688\n",
      "506/3000 train_loss: 121.61376190185547 test_loss:690.2783813476562\n",
      "507/3000 train_loss: 142.56753540039062 test_loss:691.187255859375\n",
      "508/3000 train_loss: 121.59395599365234 test_loss:680.5543823242188\n",
      "509/3000 train_loss: 122.8801498413086 test_loss:677.9148559570312\n",
      "510/3000 train_loss: 127.70016479492188 test_loss:686.1624145507812\n",
      "511/3000 train_loss: 125.3547134399414 test_loss:684.0355224609375\n",
      "512/3000 train_loss: 130.4682159423828 test_loss:684.2951049804688\n",
      "513/3000 train_loss: 135.53076171875 test_loss:683.7266235351562\n",
      "514/3000 train_loss: 131.3902130126953 test_loss:681.4443359375\n",
      "515/3000 train_loss: 120.25333404541016 test_loss:671.4972534179688\n",
      "516/3000 train_loss: 125.15228271484375 test_loss:681.830322265625\n",
      "517/3000 train_loss: 123.67329406738281 test_loss:675.8854370117188\n",
      "518/3000 train_loss: 133.2520751953125 test_loss:689.518798828125\n",
      "519/3000 train_loss: 128.13436889648438 test_loss:680.2057495117188\n",
      "520/3000 train_loss: 125.59747314453125 test_loss:678.718994140625\n",
      "521/3000 train_loss: 130.5999298095703 test_loss:685.7393798828125\n",
      "522/3000 train_loss: 137.09400939941406 test_loss:677.3790283203125\n",
      "523/3000 train_loss: 131.5417938232422 test_loss:676.6149291992188\n",
      "524/3000 train_loss: 116.74868774414062 test_loss:683.655517578125\n",
      "525/3000 train_loss: 117.20233154296875 test_loss:680.2125244140625\n",
      "526/3000 train_loss: 128.6803436279297 test_loss:683.1090087890625\n",
      "527/3000 train_loss: 125.72071075439453 test_loss:678.981201171875\n",
      "528/3000 train_loss: 121.23590087890625 test_loss:676.6770629882812\n",
      "529/3000 train_loss: 118.86602783203125 test_loss:672.7092895507812\n",
      "530/3000 train_loss: 119.75371551513672 test_loss:686.3873291015625\n",
      "531/3000 train_loss: 128.68603515625 test_loss:673.1192626953125\n",
      "532/3000 train_loss: 122.78068542480469 test_loss:680.02783203125\n",
      "533/3000 train_loss: 128.73834228515625 test_loss:666.5804443359375\n",
      "534/3000 train_loss: 122.5435562133789 test_loss:675.3895263671875\n",
      "535/3000 train_loss: 128.3524932861328 test_loss:673.9744873046875\n",
      "536/3000 train_loss: 116.19710540771484 test_loss:676.5017700195312\n",
      "537/3000 train_loss: 132.28675842285156 test_loss:683.2477416992188\n",
      "538/3000 train_loss: 111.57168579101562 test_loss:673.11865234375\n",
      "539/3000 train_loss: 120.1623306274414 test_loss:667.91748046875\n",
      "540/3000 train_loss: 120.06082153320312 test_loss:673.0682983398438\n",
      "541/3000 train_loss: 122.21638488769531 test_loss:666.7279052734375\n",
      "542/3000 train_loss: 121.46383666992188 test_loss:668.231689453125\n",
      "543/3000 train_loss: 118.43194580078125 test_loss:665.7601928710938\n",
      "544/3000 train_loss: 119.54429626464844 test_loss:664.6669921875\n",
      "545/3000 train_loss: 117.21088409423828 test_loss:670.04296875\n",
      "546/3000 train_loss: 118.71082305908203 test_loss:658.089111328125\n",
      "547/3000 train_loss: 111.38662719726562 test_loss:665.52197265625\n",
      "548/3000 train_loss: 128.4961700439453 test_loss:688.42236328125\n",
      "549/3000 train_loss: 125.89768981933594 test_loss:662.3682250976562\n",
      "550/3000 train_loss: 122.12954711914062 test_loss:674.5174560546875\n",
      "551/3000 train_loss: 121.56744384765625 test_loss:675.1282958984375\n",
      "552/3000 train_loss: 117.0135726928711 test_loss:673.0186767578125\n",
      "553/3000 train_loss: 120.72649383544922 test_loss:667.6563720703125\n",
      "554/3000 train_loss: 128.4044189453125 test_loss:661.9724731445312\n",
      "555/3000 train_loss: 109.87355041503906 test_loss:656.7796630859375\n",
      "556/3000 train_loss: 124.7059326171875 test_loss:665.5428466796875\n",
      "557/3000 train_loss: 113.88542175292969 test_loss:659.2818603515625\n",
      "558/3000 train_loss: 113.09790802001953 test_loss:657.7459716796875\n",
      "559/3000 train_loss: 120.21831512451172 test_loss:655.9718017578125\n",
      "560/3000 train_loss: 125.06570434570312 test_loss:665.234130859375\n",
      "561/3000 train_loss: 127.1191177368164 test_loss:655.7162475585938\n",
      "562/3000 train_loss: 123.56121826171875 test_loss:663.6007080078125\n",
      "563/3000 train_loss: 117.4454345703125 test_loss:658.0665893554688\n",
      "564/3000 train_loss: 116.25434875488281 test_loss:658.328857421875\n",
      "565/3000 train_loss: 114.54200744628906 test_loss:669.6091918945312\n",
      "566/3000 train_loss: 113.25650024414062 test_loss:655.0663452148438\n",
      "567/3000 train_loss: 118.1192398071289 test_loss:658.1182250976562\n",
      "568/3000 train_loss: 109.92845153808594 test_loss:656.8262939453125\n",
      "569/3000 train_loss: 108.28727722167969 test_loss:664.7794189453125\n",
      "570/3000 train_loss: 118.68880462646484 test_loss:656.8656616210938\n",
      "571/3000 train_loss: 113.55919647216797 test_loss:655.8380126953125\n",
      "572/3000 train_loss: 108.9885025024414 test_loss:657.9942016601562\n",
      "573/3000 train_loss: 111.58094787597656 test_loss:656.3732299804688\n",
      "574/3000 train_loss: 112.3275146484375 test_loss:653.7809448242188\n",
      "575/3000 train_loss: 109.99056243896484 test_loss:655.22705078125\n",
      "576/3000 train_loss: 99.19774627685547 test_loss:653.1347045898438\n",
      "577/3000 train_loss: 113.47928619384766 test_loss:654.3134765625\n",
      "578/3000 train_loss: 129.0809326171875 test_loss:658.5244140625\n",
      "579/3000 train_loss: 107.01422119140625 test_loss:654.97802734375\n",
      "580/3000 train_loss: 105.38496398925781 test_loss:660.3645629882812\n",
      "581/3000 train_loss: 109.79983520507812 test_loss:655.1593627929688\n",
      "582/3000 train_loss: 110.12200164794922 test_loss:656.50537109375\n",
      "583/3000 train_loss: 111.55133056640625 test_loss:645.6627197265625\n",
      "584/3000 train_loss: 104.14717864990234 test_loss:650.1898193359375\n",
      "585/3000 train_loss: 104.59353637695312 test_loss:649.2574462890625\n",
      "586/3000 train_loss: 112.83961486816406 test_loss:655.0592041015625\n",
      "587/3000 train_loss: 104.52266693115234 test_loss:647.81787109375\n",
      "588/3000 train_loss: 104.7093505859375 test_loss:645.5639038085938\n",
      "589/3000 train_loss: 105.7038803100586 test_loss:652.9264526367188\n",
      "590/3000 train_loss: 110.52574920654297 test_loss:647.443115234375\n",
      "591/3000 train_loss: 113.34274291992188 test_loss:653.659423828125\n",
      "592/3000 train_loss: 104.6376953125 test_loss:644.4830932617188\n",
      "593/3000 train_loss: 103.6601791381836 test_loss:649.3094482421875\n",
      "594/3000 train_loss: 110.12610626220703 test_loss:650.0994873046875\n",
      "595/3000 train_loss: 107.94978332519531 test_loss:647.46435546875\n",
      "596/3000 train_loss: 113.7338638305664 test_loss:648.07861328125\n",
      "597/3000 train_loss: 120.92488861083984 test_loss:644.8048095703125\n",
      "598/3000 train_loss: 105.37091064453125 test_loss:650.5780639648438\n",
      "599/3000 train_loss: 105.0326156616211 test_loss:642.6743774414062\n",
      "600/3000 train_loss: 106.66094970703125 test_loss:650.9757690429688\n",
      "601/3000 train_loss: 104.10951232910156 test_loss:649.3665771484375\n",
      "602/3000 train_loss: 101.74043273925781 test_loss:639.4227294921875\n",
      "603/3000 train_loss: 101.68344116210938 test_loss:646.469482421875\n",
      "604/3000 train_loss: 103.41926574707031 test_loss:648.7891845703125\n",
      "605/3000 train_loss: 109.96394348144531 test_loss:640.196533203125\n",
      "606/3000 train_loss: 105.16970825195312 test_loss:652.220703125\n",
      "607/3000 train_loss: 108.33572387695312 test_loss:642.9368896484375\n",
      "608/3000 train_loss: 101.9242935180664 test_loss:660.8108520507812\n",
      "609/3000 train_loss: 99.28822326660156 test_loss:643.2617797851562\n",
      "610/3000 train_loss: 99.84956359863281 test_loss:645.989501953125\n",
      "611/3000 train_loss: 110.226318359375 test_loss:639.6273803710938\n",
      "612/3000 train_loss: 100.98410034179688 test_loss:643.3828735351562\n",
      "613/3000 train_loss: 109.54647064208984 test_loss:641.275634765625\n",
      "614/3000 train_loss: 104.1500244140625 test_loss:642.2391357421875\n",
      "615/3000 train_loss: 107.38243865966797 test_loss:633.0970458984375\n",
      "616/3000 train_loss: 101.35755157470703 test_loss:648.5648193359375\n",
      "617/3000 train_loss: 98.75741577148438 test_loss:636.185302734375\n",
      "618/3000 train_loss: 105.38172149658203 test_loss:639.658447265625\n",
      "619/3000 train_loss: 106.45193481445312 test_loss:641.5747680664062\n",
      "620/3000 train_loss: 102.28550720214844 test_loss:646.3056640625\n",
      "621/3000 train_loss: 106.33745574951172 test_loss:650.0068359375\n",
      "622/3000 train_loss: 103.506591796875 test_loss:646.4525146484375\n",
      "623/3000 train_loss: 120.75436401367188 test_loss:642.8609619140625\n",
      "624/3000 train_loss: 98.28191375732422 test_loss:646.4345703125\n",
      "625/3000 train_loss: 109.53436279296875 test_loss:642.7918701171875\n",
      "626/3000 train_loss: 99.72748565673828 test_loss:643.0946044921875\n",
      "627/3000 train_loss: 107.33454132080078 test_loss:642.1062622070312\n",
      "628/3000 train_loss: 101.86383819580078 test_loss:641.3590698242188\n",
      "629/3000 train_loss: 109.61937713623047 test_loss:643.1337890625\n",
      "630/3000 train_loss: 101.35533142089844 test_loss:640.5604248046875\n",
      "631/3000 train_loss: 95.14852905273438 test_loss:646.484375\n",
      "632/3000 train_loss: 98.62010192871094 test_loss:638.0765380859375\n",
      "633/3000 train_loss: 104.80158996582031 test_loss:641.6259765625\n",
      "634/3000 train_loss: 101.28514099121094 test_loss:643.6187744140625\n",
      "635/3000 train_loss: 109.77023315429688 test_loss:646.5667114257812\n",
      "636/3000 train_loss: 107.55011749267578 test_loss:635.8359375\n",
      "637/3000 train_loss: 98.9908676147461 test_loss:640.7318115234375\n",
      "638/3000 train_loss: 106.60291290283203 test_loss:641.1097412109375\n",
      "639/3000 train_loss: 102.93009185791016 test_loss:630.6435546875\n",
      "640/3000 train_loss: 107.60980224609375 test_loss:636.4937133789062\n",
      "641/3000 train_loss: 96.84944915771484 test_loss:638.1755981445312\n",
      "642/3000 train_loss: 106.40113067626953 test_loss:634.87353515625\n",
      "643/3000 train_loss: 103.88391876220703 test_loss:643.5921630859375\n",
      "644/3000 train_loss: 103.74359893798828 test_loss:638.9337158203125\n",
      "645/3000 train_loss: 105.01959228515625 test_loss:643.0506591796875\n",
      "646/3000 train_loss: 93.40484619140625 test_loss:633.593017578125\n",
      "647/3000 train_loss: 112.77086639404297 test_loss:642.810302734375\n",
      "648/3000 train_loss: 99.15467834472656 test_loss:631.55126953125\n",
      "649/3000 train_loss: 98.5907211303711 test_loss:634.6506958007812\n",
      "650/3000 train_loss: 100.2371597290039 test_loss:630.8173828125\n",
      "651/3000 train_loss: 92.64702606201172 test_loss:633.523681640625\n",
      "652/3000 train_loss: 91.10200500488281 test_loss:634.43798828125\n",
      "653/3000 train_loss: 97.35932159423828 test_loss:627.397705078125\n",
      "654/3000 train_loss: 96.42240142822266 test_loss:631.8275756835938\n",
      "655/3000 train_loss: 101.54828643798828 test_loss:634.5235595703125\n",
      "656/3000 train_loss: 96.74832153320312 test_loss:642.6642456054688\n",
      "657/3000 train_loss: 98.28340911865234 test_loss:631.5184936523438\n",
      "658/3000 train_loss: 95.30916595458984 test_loss:639.4144897460938\n",
      "659/3000 train_loss: 90.70161437988281 test_loss:636.82080078125\n",
      "660/3000 train_loss: 98.75100708007812 test_loss:629.1124877929688\n",
      "661/3000 train_loss: 96.50672912597656 test_loss:634.604248046875\n",
      "662/3000 train_loss: 104.69647979736328 test_loss:640.3460693359375\n",
      "663/3000 train_loss: 91.31184387207031 test_loss:639.4612426757812\n",
      "664/3000 train_loss: 95.3978042602539 test_loss:639.1351928710938\n",
      "665/3000 train_loss: 92.06757354736328 test_loss:636.166259765625\n",
      "666/3000 train_loss: 97.50222778320312 test_loss:632.05078125\n",
      "667/3000 train_loss: 97.37476348876953 test_loss:633.8406372070312\n",
      "668/3000 train_loss: 96.68756103515625 test_loss:631.2860107421875\n",
      "669/3000 train_loss: 97.0156478881836 test_loss:622.976806640625\n",
      "670/3000 train_loss: 94.46821594238281 test_loss:633.9596557617188\n",
      "671/3000 train_loss: 98.1939468383789 test_loss:627.3670654296875\n",
      "672/3000 train_loss: 94.14569091796875 test_loss:628.6383056640625\n",
      "673/3000 train_loss: 93.82884216308594 test_loss:627.5748291015625\n",
      "674/3000 train_loss: 94.03866577148438 test_loss:626.9559326171875\n",
      "675/3000 train_loss: 99.47527313232422 test_loss:629.7484130859375\n",
      "676/3000 train_loss: 98.26176452636719 test_loss:626.4102783203125\n",
      "677/3000 train_loss: 95.46351623535156 test_loss:632.770263671875\n",
      "678/3000 train_loss: 94.7198715209961 test_loss:630.1339111328125\n",
      "679/3000 train_loss: 95.76459503173828 test_loss:624.5228881835938\n",
      "680/3000 train_loss: 93.99235534667969 test_loss:622.7423706054688\n",
      "681/3000 train_loss: 88.1719741821289 test_loss:632.5617065429688\n",
      "682/3000 train_loss: 92.58283996582031 test_loss:621.5846557617188\n",
      "683/3000 train_loss: 96.2296142578125 test_loss:629.5958251953125\n",
      "684/3000 train_loss: 95.50379943847656 test_loss:622.3213500976562\n",
      "685/3000 train_loss: 87.55242919921875 test_loss:625.099365234375\n",
      "686/3000 train_loss: 101.73684692382812 test_loss:630.118408203125\n",
      "687/3000 train_loss: 105.01415252685547 test_loss:617.2769775390625\n",
      "688/3000 train_loss: 87.25428771972656 test_loss:628.7327270507812\n",
      "689/3000 train_loss: 88.51509857177734 test_loss:621.2036743164062\n",
      "690/3000 train_loss: 86.35074615478516 test_loss:624.399658203125\n",
      "691/3000 train_loss: 98.74671173095703 test_loss:627.5149536132812\n",
      "692/3000 train_loss: 98.18318176269531 test_loss:630.929443359375\n",
      "693/3000 train_loss: 98.92424774169922 test_loss:626.3099365234375\n",
      "694/3000 train_loss: 100.00751495361328 test_loss:631.0472412109375\n",
      "695/3000 train_loss: 100.11721801757812 test_loss:627.57080078125\n",
      "696/3000 train_loss: 98.5411605834961 test_loss:624.8655395507812\n",
      "697/3000 train_loss: 92.71820068359375 test_loss:629.3525390625\n",
      "698/3000 train_loss: 89.39168548583984 test_loss:624.8280029296875\n",
      "699/3000 train_loss: 93.33131408691406 test_loss:617.7404174804688\n",
      "700/3000 train_loss: 94.72782135009766 test_loss:619.9552001953125\n",
      "701/3000 train_loss: 92.29254913330078 test_loss:621.7285766601562\n",
      "702/3000 train_loss: 91.36936950683594 test_loss:610.5468139648438\n",
      "703/3000 train_loss: 85.28187561035156 test_loss:614.9238891601562\n",
      "704/3000 train_loss: 91.01679992675781 test_loss:632.481689453125\n",
      "705/3000 train_loss: 88.79685974121094 test_loss:621.14208984375\n",
      "706/3000 train_loss: 90.81341552734375 test_loss:613.3203125\n",
      "707/3000 train_loss: 94.60786437988281 test_loss:620.0370483398438\n",
      "708/3000 train_loss: 90.2301254272461 test_loss:618.9627685546875\n",
      "709/3000 train_loss: 87.24151611328125 test_loss:615.9718017578125\n",
      "710/3000 train_loss: 91.78995513916016 test_loss:621.692138671875\n",
      "711/3000 train_loss: 86.7911605834961 test_loss:616.378662109375\n",
      "712/3000 train_loss: 88.66670227050781 test_loss:622.148193359375\n",
      "713/3000 train_loss: 90.30044555664062 test_loss:620.4981689453125\n",
      "714/3000 train_loss: 93.26730346679688 test_loss:616.4222412109375\n",
      "715/3000 train_loss: 84.740478515625 test_loss:616.7039794921875\n",
      "716/3000 train_loss: 99.30012512207031 test_loss:610.7493896484375\n",
      "717/3000 train_loss: 92.83460998535156 test_loss:627.9727783203125\n",
      "718/3000 train_loss: 94.64225006103516 test_loss:618.5628051757812\n",
      "719/3000 train_loss: 90.86337280273438 test_loss:611.948974609375\n",
      "720/3000 train_loss: 87.45330810546875 test_loss:619.782470703125\n",
      "721/3000 train_loss: 88.93658447265625 test_loss:614.5784301757812\n",
      "722/3000 train_loss: 92.45973205566406 test_loss:614.4434204101562\n",
      "723/3000 train_loss: 87.05370330810547 test_loss:625.650390625\n",
      "724/3000 train_loss: 85.26419067382812 test_loss:609.9581909179688\n",
      "725/3000 train_loss: 98.77727508544922 test_loss:610.742919921875\n",
      "726/3000 train_loss: 84.80457305908203 test_loss:618.119140625\n",
      "727/3000 train_loss: 93.68626403808594 test_loss:604.4129638671875\n",
      "728/3000 train_loss: 83.07124328613281 test_loss:620.4669799804688\n",
      "729/3000 train_loss: 92.23236083984375 test_loss:611.33056640625\n",
      "730/3000 train_loss: 87.5159683227539 test_loss:600.1048583984375\n",
      "731/3000 train_loss: 88.8183822631836 test_loss:603.5790405273438\n",
      "732/3000 train_loss: 87.07160186767578 test_loss:612.8401489257812\n",
      "733/3000 train_loss: 83.78575897216797 test_loss:607.0554809570312\n",
      "734/3000 train_loss: 88.04000854492188 test_loss:607.1063232421875\n",
      "735/3000 train_loss: 96.53096771240234 test_loss:605.2622680664062\n",
      "736/3000 train_loss: 88.8674087524414 test_loss:612.647705078125\n",
      "737/3000 train_loss: 88.55474853515625 test_loss:603.2031860351562\n",
      "738/3000 train_loss: 79.56507873535156 test_loss:612.2523803710938\n",
      "739/3000 train_loss: 82.28353881835938 test_loss:603.6005859375\n",
      "740/3000 train_loss: 81.5350570678711 test_loss:603.1700439453125\n",
      "741/3000 train_loss: 88.93864440917969 test_loss:618.0106201171875\n",
      "742/3000 train_loss: 86.28491973876953 test_loss:601.8297119140625\n",
      "743/3000 train_loss: 81.43807983398438 test_loss:605.6416625976562\n",
      "744/3000 train_loss: 82.09290313720703 test_loss:596.8482666015625\n",
      "745/3000 train_loss: 102.02909088134766 test_loss:595.6724243164062\n",
      "746/3000 train_loss: 90.74435424804688 test_loss:605.230712890625\n",
      "747/3000 train_loss: 84.60031127929688 test_loss:602.5889282226562\n",
      "748/3000 train_loss: 85.15686798095703 test_loss:608.077880859375\n",
      "749/3000 train_loss: 82.03044891357422 test_loss:609.01416015625\n",
      "750/3000 train_loss: 81.96656799316406 test_loss:600.2198486328125\n",
      "751/3000 train_loss: 87.0877685546875 test_loss:600.613525390625\n",
      "752/3000 train_loss: 84.68287658691406 test_loss:603.6456909179688\n",
      "753/3000 train_loss: 79.98466491699219 test_loss:610.144775390625\n",
      "754/3000 train_loss: 92.54750061035156 test_loss:609.9241333007812\n",
      "755/3000 train_loss: 85.69658660888672 test_loss:607.70849609375\n",
      "756/3000 train_loss: 81.8127212524414 test_loss:604.7078247070312\n",
      "757/3000 train_loss: 86.80998229980469 test_loss:606.2463989257812\n",
      "758/3000 train_loss: 81.33392333984375 test_loss:601.0394287109375\n",
      "759/3000 train_loss: 90.7539291381836 test_loss:614.360107421875\n",
      "760/3000 train_loss: 90.58808898925781 test_loss:597.9515380859375\n",
      "761/3000 train_loss: 86.71869659423828 test_loss:607.314208984375\n",
      "762/3000 train_loss: 87.04304504394531 test_loss:606.7025756835938\n",
      "763/3000 train_loss: 88.91297912597656 test_loss:607.7952270507812\n",
      "764/3000 train_loss: 80.57642364501953 test_loss:602.9609985351562\n",
      "765/3000 train_loss: 83.57645416259766 test_loss:603.672607421875\n",
      "766/3000 train_loss: 79.9006576538086 test_loss:605.3552856445312\n",
      "767/3000 train_loss: 82.78702545166016 test_loss:603.3902587890625\n",
      "768/3000 train_loss: 90.79257202148438 test_loss:610.15966796875\n",
      "769/3000 train_loss: 90.73648071289062 test_loss:606.978515625\n",
      "770/3000 train_loss: 78.39817810058594 test_loss:601.6756591796875\n",
      "771/3000 train_loss: 86.1893310546875 test_loss:600.28759765625\n",
      "772/3000 train_loss: 84.50916290283203 test_loss:603.607421875\n",
      "773/3000 train_loss: 89.18629455566406 test_loss:602.018310546875\n",
      "774/3000 train_loss: 76.95631408691406 test_loss:605.9711303710938\n",
      "775/3000 train_loss: 85.20150756835938 test_loss:605.83544921875\n",
      "776/3000 train_loss: 79.16888427734375 test_loss:602.4108276367188\n",
      "777/3000 train_loss: 80.85818481445312 test_loss:600.5888671875\n",
      "778/3000 train_loss: 82.66615295410156 test_loss:613.993896484375\n",
      "779/3000 train_loss: 82.82434844970703 test_loss:597.0078125\n",
      "780/3000 train_loss: 78.5456314086914 test_loss:604.7532958984375\n",
      "781/3000 train_loss: 80.58737182617188 test_loss:589.1412963867188\n",
      "782/3000 train_loss: 81.1543197631836 test_loss:603.2908935546875\n",
      "783/3000 train_loss: 70.1741943359375 test_loss:603.1881103515625\n",
      "784/3000 train_loss: 87.42298889160156 test_loss:599.5376586914062\n",
      "785/3000 train_loss: 80.65042114257812 test_loss:592.7160034179688\n",
      "786/3000 train_loss: 81.6583251953125 test_loss:595.879638671875\n",
      "787/3000 train_loss: 82.97544860839844 test_loss:595.88720703125\n",
      "788/3000 train_loss: 81.44062805175781 test_loss:601.680908203125\n",
      "789/3000 train_loss: 80.48304748535156 test_loss:605.8108520507812\n",
      "790/3000 train_loss: 77.23560333251953 test_loss:606.9928588867188\n",
      "791/3000 train_loss: 80.85523986816406 test_loss:599.013427734375\n",
      "792/3000 train_loss: 73.9864273071289 test_loss:608.5177001953125\n",
      "793/3000 train_loss: 82.400146484375 test_loss:606.3604125976562\n",
      "794/3000 train_loss: 86.02379608154297 test_loss:607.2244873046875\n",
      "795/3000 train_loss: 82.31816101074219 test_loss:612.9737548828125\n",
      "796/3000 train_loss: 88.96382141113281 test_loss:604.8488159179688\n",
      "797/3000 train_loss: 82.07738494873047 test_loss:609.8662109375\n",
      "798/3000 train_loss: 74.37388610839844 test_loss:609.65625\n",
      "799/3000 train_loss: 75.43244934082031 test_loss:611.5576171875\n",
      "800/3000 train_loss: 76.48240661621094 test_loss:603.9644165039062\n",
      "801/3000 train_loss: 76.9708480834961 test_loss:596.911865234375\n",
      "802/3000 train_loss: 78.73851776123047 test_loss:604.4617919921875\n",
      "803/3000 train_loss: 75.83094787597656 test_loss:604.6197509765625\n",
      "804/3000 train_loss: 83.07217407226562 test_loss:612.4422607421875\n",
      "805/3000 train_loss: 73.72035217285156 test_loss:594.486572265625\n",
      "806/3000 train_loss: 78.81375885009766 test_loss:593.311767578125\n",
      "807/3000 train_loss: 78.94451141357422 test_loss:600.5321655273438\n",
      "808/3000 train_loss: 81.39752197265625 test_loss:593.858642578125\n",
      "809/3000 train_loss: 69.47392272949219 test_loss:589.7708740234375\n",
      "810/3000 train_loss: 74.56929779052734 test_loss:589.715087890625\n",
      "811/3000 train_loss: 79.28599548339844 test_loss:604.7876586914062\n",
      "812/3000 train_loss: 72.19310760498047 test_loss:591.5296020507812\n",
      "813/3000 train_loss: 75.05067443847656 test_loss:589.8888549804688\n",
      "814/3000 train_loss: 74.61166381835938 test_loss:594.1253662109375\n",
      "815/3000 train_loss: 72.47384643554688 test_loss:596.212890625\n",
      "816/3000 train_loss: 73.66777801513672 test_loss:599.649169921875\n",
      "817/3000 train_loss: 81.0848159790039 test_loss:591.3711547851562\n",
      "818/3000 train_loss: 80.54390716552734 test_loss:592.5161743164062\n",
      "819/3000 train_loss: 74.09503173828125 test_loss:587.6412353515625\n",
      "820/3000 train_loss: 89.15524291992188 test_loss:587.2708129882812\n",
      "821/3000 train_loss: 74.19286346435547 test_loss:603.0300903320312\n",
      "822/3000 train_loss: 79.21122741699219 test_loss:589.4891967773438\n",
      "823/3000 train_loss: 85.25564575195312 test_loss:583.609619140625\n",
      "824/3000 train_loss: 81.34886932373047 test_loss:596.778076171875\n",
      "825/3000 train_loss: 82.45196533203125 test_loss:592.686767578125\n",
      "826/3000 train_loss: 69.92787170410156 test_loss:585.2313842773438\n",
      "827/3000 train_loss: 76.50064849853516 test_loss:591.0911865234375\n",
      "828/3000 train_loss: 80.85721588134766 test_loss:593.6489868164062\n",
      "829/3000 train_loss: 80.28703308105469 test_loss:592.1907348632812\n",
      "830/3000 train_loss: 74.9965591430664 test_loss:591.852294921875\n",
      "831/3000 train_loss: 80.34458923339844 test_loss:597.545166015625\n",
      "832/3000 train_loss: 75.92244720458984 test_loss:582.0218505859375\n",
      "833/3000 train_loss: 77.43090057373047 test_loss:589.6903686523438\n",
      "834/3000 train_loss: 81.40436553955078 test_loss:582.9239501953125\n",
      "835/3000 train_loss: 72.76555633544922 test_loss:585.3052368164062\n",
      "836/3000 train_loss: 77.53974914550781 test_loss:599.34814453125\n",
      "837/3000 train_loss: 73.32493591308594 test_loss:590.018798828125\n",
      "838/3000 train_loss: 77.48168182373047 test_loss:588.395751953125\n",
      "839/3000 train_loss: 74.8062973022461 test_loss:588.8419189453125\n",
      "840/3000 train_loss: 68.20140075683594 test_loss:587.7400512695312\n",
      "841/3000 train_loss: 79.734130859375 test_loss:586.3695068359375\n",
      "842/3000 train_loss: 70.60498046875 test_loss:581.625\n",
      "843/3000 train_loss: 69.61946105957031 test_loss:582.59423828125\n",
      "844/3000 train_loss: 73.13009643554688 test_loss:590.7941284179688\n",
      "845/3000 train_loss: 68.77129364013672 test_loss:587.486083984375\n",
      "846/3000 train_loss: 77.48365020751953 test_loss:588.3904418945312\n",
      "847/3000 train_loss: 75.49430084228516 test_loss:590.798095703125\n",
      "848/3000 train_loss: 76.6585464477539 test_loss:590.5874633789062\n",
      "849/3000 train_loss: 73.9846420288086 test_loss:588.3681640625\n",
      "850/3000 train_loss: 82.69945526123047 test_loss:592.8648071289062\n",
      "851/3000 train_loss: 82.9281997680664 test_loss:589.1119384765625\n",
      "852/3000 train_loss: 77.99972534179688 test_loss:579.3532104492188\n",
      "853/3000 train_loss: 67.10028839111328 test_loss:596.147705078125\n",
      "854/3000 train_loss: 73.2913818359375 test_loss:577.7945556640625\n",
      "855/3000 train_loss: 77.83641052246094 test_loss:582.476806640625\n",
      "856/3000 train_loss: 73.6020278930664 test_loss:581.3505859375\n",
      "857/3000 train_loss: 74.90382385253906 test_loss:580.6060791015625\n",
      "858/3000 train_loss: 72.60623168945312 test_loss:574.3799438476562\n",
      "859/3000 train_loss: 77.3482437133789 test_loss:587.1698608398438\n",
      "860/3000 train_loss: 78.50910949707031 test_loss:570.5819091796875\n",
      "861/3000 train_loss: 76.91800689697266 test_loss:580.1625366210938\n",
      "862/3000 train_loss: 74.21934509277344 test_loss:594.8674926757812\n",
      "863/3000 train_loss: 74.69184112548828 test_loss:579.426513671875\n",
      "864/3000 train_loss: 68.86556243896484 test_loss:586.5475463867188\n",
      "865/3000 train_loss: 77.03020477294922 test_loss:594.426513671875\n",
      "866/3000 train_loss: 76.62973022460938 test_loss:576.27734375\n",
      "867/3000 train_loss: 75.91226959228516 test_loss:577.3936767578125\n",
      "868/3000 train_loss: 75.52104949951172 test_loss:570.904541015625\n",
      "869/3000 train_loss: 71.8747329711914 test_loss:600.7839965820312\n",
      "870/3000 train_loss: 69.5444107055664 test_loss:623.2393188476562\n",
      "871/3000 train_loss: 72.71865844726562 test_loss:584.8824462890625\n",
      "872/3000 train_loss: 78.30270385742188 test_loss:575.833740234375\n",
      "873/3000 train_loss: 75.78878021240234 test_loss:576.6957397460938\n",
      "874/3000 train_loss: 79.03655242919922 test_loss:658.4625854492188\n",
      "875/3000 train_loss: 86.62299346923828 test_loss:665.8369140625\n",
      "876/3000 train_loss: 77.70550537109375 test_loss:625.572021484375\n",
      "877/3000 train_loss: 67.6497573852539 test_loss:641.6546630859375\n",
      "878/3000 train_loss: 78.96665954589844 test_loss:587.52587890625\n",
      "879/3000 train_loss: 74.5130844116211 test_loss:563.824462890625\n",
      "880/3000 train_loss: 74.60771942138672 test_loss:581.5928955078125\n",
      "881/3000 train_loss: 80.52587890625 test_loss:580.720703125\n",
      "882/3000 train_loss: 72.47492980957031 test_loss:591.748779296875\n",
      "883/3000 train_loss: 73.2430191040039 test_loss:591.2617797851562\n",
      "884/3000 train_loss: 72.0011978149414 test_loss:582.2537231445312\n",
      "885/3000 train_loss: 66.16976928710938 test_loss:585.0394287109375\n",
      "886/3000 train_loss: 68.90594482421875 test_loss:579.5048217773438\n",
      "887/3000 train_loss: 70.15640258789062 test_loss:587.7481689453125\n",
      "888/3000 train_loss: 74.11884307861328 test_loss:585.38427734375\n",
      "889/3000 train_loss: 64.93389129638672 test_loss:575.9781494140625\n",
      "890/3000 train_loss: 65.61032104492188 test_loss:590.6649780273438\n",
      "891/3000 train_loss: 72.8449478149414 test_loss:585.7672119140625\n",
      "892/3000 train_loss: 65.39875030517578 test_loss:578.552001953125\n",
      "893/3000 train_loss: 68.97728729248047 test_loss:583.5919189453125\n",
      "894/3000 train_loss: 72.01961517333984 test_loss:580.9337158203125\n",
      "895/3000 train_loss: 72.72386169433594 test_loss:586.2315673828125\n",
      "896/3000 train_loss: 71.1331558227539 test_loss:578.4512939453125\n",
      "897/3000 train_loss: 70.06344604492188 test_loss:583.7188110351562\n",
      "898/3000 train_loss: 78.12185668945312 test_loss:583.0904541015625\n",
      "899/3000 train_loss: 65.83784484863281 test_loss:582.1677856445312\n",
      "900/3000 train_loss: 72.63428497314453 test_loss:576.8072509765625\n",
      "901/3000 train_loss: 70.83271789550781 test_loss:579.9251098632812\n",
      "902/3000 train_loss: 68.66773986816406 test_loss:580.1729736328125\n",
      "903/3000 train_loss: 72.67823791503906 test_loss:584.2236938476562\n",
      "904/3000 train_loss: 80.9161376953125 test_loss:583.4383544921875\n",
      "905/3000 train_loss: 73.09949493408203 test_loss:584.1017456054688\n",
      "906/3000 train_loss: 71.27044677734375 test_loss:583.84130859375\n",
      "907/3000 train_loss: 76.6482925415039 test_loss:587.7606201171875\n",
      "908/3000 train_loss: 67.25306701660156 test_loss:580.029296875\n",
      "909/3000 train_loss: 68.910400390625 test_loss:577.5390014648438\n",
      "910/3000 train_loss: 67.32054901123047 test_loss:578.5987548828125\n",
      "911/3000 train_loss: 65.30072784423828 test_loss:586.2344360351562\n",
      "912/3000 train_loss: 68.87505340576172 test_loss:585.1985473632812\n",
      "913/3000 train_loss: 70.01984405517578 test_loss:580.214599609375\n",
      "914/3000 train_loss: 69.1867904663086 test_loss:581.4826049804688\n",
      "915/3000 train_loss: 68.78892517089844 test_loss:578.1701049804688\n",
      "916/3000 train_loss: 66.6903305053711 test_loss:581.4563598632812\n",
      "917/3000 train_loss: 61.428810119628906 test_loss:582.76025390625\n",
      "918/3000 train_loss: 67.97416687011719 test_loss:584.93505859375\n",
      "919/3000 train_loss: 66.54357147216797 test_loss:577.7183227539062\n",
      "920/3000 train_loss: 74.0344009399414 test_loss:593.1353149414062\n",
      "921/3000 train_loss: 60.29188919067383 test_loss:584.0005493164062\n",
      "922/3000 train_loss: 69.58155822753906 test_loss:579.2720336914062\n",
      "923/3000 train_loss: 70.33645629882812 test_loss:578.904052734375\n",
      "924/3000 train_loss: 68.89865112304688 test_loss:584.4424438476562\n",
      "925/3000 train_loss: 65.95215606689453 test_loss:575.6185913085938\n",
      "926/3000 train_loss: 63.306758880615234 test_loss:575.6422729492188\n",
      "927/3000 train_loss: 66.54549407958984 test_loss:574.9569091796875\n",
      "928/3000 train_loss: 69.22745513916016 test_loss:584.1907348632812\n",
      "929/3000 train_loss: 66.45823669433594 test_loss:572.1749267578125\n",
      "930/3000 train_loss: 67.66151428222656 test_loss:583.667236328125\n",
      "931/3000 train_loss: 62.898841857910156 test_loss:576.12451171875\n",
      "932/3000 train_loss: 71.213134765625 test_loss:581.300537109375\n",
      "933/3000 train_loss: 69.0370864868164 test_loss:577.6541748046875\n",
      "934/3000 train_loss: 75.99463653564453 test_loss:579.373779296875\n",
      "935/3000 train_loss: 62.93037796020508 test_loss:575.8858032226562\n",
      "936/3000 train_loss: 64.95257568359375 test_loss:571.2904052734375\n",
      "937/3000 train_loss: 67.20228576660156 test_loss:579.69775390625\n",
      "938/3000 train_loss: 63.64451599121094 test_loss:576.05419921875\n",
      "939/3000 train_loss: 71.41255187988281 test_loss:580.9554443359375\n",
      "940/3000 train_loss: 69.55294036865234 test_loss:575.8527221679688\n",
      "941/3000 train_loss: 66.11345672607422 test_loss:574.4888916015625\n",
      "942/3000 train_loss: 61.20355224609375 test_loss:574.4622802734375\n",
      "943/3000 train_loss: 76.44528198242188 test_loss:575.5784301757812\n",
      "944/3000 train_loss: 64.54698944091797 test_loss:572.2596435546875\n",
      "945/3000 train_loss: 65.98186492919922 test_loss:568.941162109375\n",
      "946/3000 train_loss: 61.24776077270508 test_loss:575.7899780273438\n",
      "947/3000 train_loss: 72.29532623291016 test_loss:574.0673828125\n",
      "948/3000 train_loss: 72.77466583251953 test_loss:572.5205688476562\n",
      "949/3000 train_loss: 75.41786193847656 test_loss:574.1796875\n",
      "950/3000 train_loss: 66.30899810791016 test_loss:570.0780639648438\n",
      "951/3000 train_loss: 73.37602233886719 test_loss:569.80810546875\n",
      "952/3000 train_loss: 67.36405944824219 test_loss:575.4553833007812\n",
      "953/3000 train_loss: 74.05712890625 test_loss:574.2122802734375\n",
      "954/3000 train_loss: 58.64577865600586 test_loss:572.2200927734375\n",
      "955/3000 train_loss: 60.52821731567383 test_loss:566.7886962890625\n",
      "956/3000 train_loss: 63.69525909423828 test_loss:562.816162109375\n",
      "957/3000 train_loss: 68.94422912597656 test_loss:563.774169921875\n",
      "958/3000 train_loss: 65.449462890625 test_loss:568.5001220703125\n",
      "959/3000 train_loss: 65.63158416748047 test_loss:571.5021362304688\n",
      "960/3000 train_loss: 68.29458618164062 test_loss:572.94384765625\n",
      "961/3000 train_loss: 71.24530792236328 test_loss:569.3403930664062\n",
      "962/3000 train_loss: 68.93252563476562 test_loss:624.6919555664062\n",
      "963/3000 train_loss: 69.82958221435547 test_loss:582.7423095703125\n",
      "964/3000 train_loss: 66.141357421875 test_loss:573.9530029296875\n",
      "965/3000 train_loss: 70.40843200683594 test_loss:567.1497192382812\n",
      "966/3000 train_loss: 77.52440643310547 test_loss:591.6240844726562\n",
      "967/3000 train_loss: 62.800148010253906 test_loss:570.2764892578125\n",
      "968/3000 train_loss: 71.91251373291016 test_loss:576.2908935546875\n",
      "969/3000 train_loss: 72.31829071044922 test_loss:571.927734375\n",
      "970/3000 train_loss: 68.51049041748047 test_loss:561.029541015625\n",
      "971/3000 train_loss: 64.78314208984375 test_loss:575.5245361328125\n",
      "972/3000 train_loss: 61.69887924194336 test_loss:576.805908203125\n",
      "973/3000 train_loss: 69.53038787841797 test_loss:574.3087158203125\n",
      "974/3000 train_loss: 66.97791290283203 test_loss:574.5950927734375\n",
      "975/3000 train_loss: 65.77564239501953 test_loss:564.3479614257812\n",
      "976/3000 train_loss: 67.32587432861328 test_loss:573.0072631835938\n",
      "977/3000 train_loss: 61.415184020996094 test_loss:571.831298828125\n",
      "978/3000 train_loss: 71.87210845947266 test_loss:577.80712890625\n",
      "979/3000 train_loss: 68.80314636230469 test_loss:567.252685546875\n",
      "980/3000 train_loss: 62.802330017089844 test_loss:568.128173828125\n",
      "981/3000 train_loss: 74.87969207763672 test_loss:642.5897216796875\n",
      "982/3000 train_loss: 75.86639404296875 test_loss:570.833251953125\n",
      "983/3000 train_loss: 71.67845153808594 test_loss:568.324951171875\n",
      "984/3000 train_loss: 69.66617584228516 test_loss:566.0970458984375\n",
      "985/3000 train_loss: 72.84754180908203 test_loss:571.5684814453125\n",
      "986/3000 train_loss: 66.4026107788086 test_loss:570.7110595703125\n",
      "987/3000 train_loss: 61.615020751953125 test_loss:557.9965209960938\n",
      "988/3000 train_loss: 63.246707916259766 test_loss:570.6012573242188\n",
      "989/3000 train_loss: 68.75431823730469 test_loss:560.5663452148438\n",
      "990/3000 train_loss: 62.20122528076172 test_loss:570.7587890625\n",
      "991/3000 train_loss: 65.6260986328125 test_loss:572.1925048828125\n",
      "992/3000 train_loss: 64.4278793334961 test_loss:572.1446533203125\n",
      "993/3000 train_loss: 69.1148452758789 test_loss:570.7571411132812\n",
      "994/3000 train_loss: 75.06438446044922 test_loss:563.4313354492188\n",
      "995/3000 train_loss: 67.7139892578125 test_loss:573.6788940429688\n",
      "996/3000 train_loss: 68.40060424804688 test_loss:570.9619140625\n",
      "997/3000 train_loss: 66.49673461914062 test_loss:567.4002075195312\n",
      "998/3000 train_loss: 67.76746368408203 test_loss:563.8876342773438\n",
      "999/3000 train_loss: 72.18376922607422 test_loss:571.853759765625\n",
      "1000/3000 train_loss: 66.63859558105469 test_loss:574.9179077148438\n",
      "1001/3000 train_loss: 77.84149169921875 test_loss:572.2060546875\n",
      "1002/3000 train_loss: 71.8371810913086 test_loss:575.9345703125\n",
      "1003/3000 train_loss: 69.84422302246094 test_loss:575.8375244140625\n",
      "1004/3000 train_loss: 74.19953155517578 test_loss:573.8671264648438\n",
      "1005/3000 train_loss: 64.36222076416016 test_loss:562.2279052734375\n",
      "1006/3000 train_loss: 60.762168884277344 test_loss:565.9793090820312\n",
      "1007/3000 train_loss: 74.45368194580078 test_loss:574.7755737304688\n",
      "1008/3000 train_loss: 70.69242858886719 test_loss:565.5753173828125\n",
      "1009/3000 train_loss: 65.24506378173828 test_loss:566.4912719726562\n",
      "1010/3000 train_loss: 67.10643768310547 test_loss:575.09619140625\n",
      "1011/3000 train_loss: 65.24575805664062 test_loss:578.525146484375\n",
      "1012/3000 train_loss: 68.33770751953125 test_loss:557.627197265625\n",
      "1013/3000 train_loss: 61.92732620239258 test_loss:563.985107421875\n",
      "1014/3000 train_loss: 65.17345428466797 test_loss:566.647216796875\n",
      "1015/3000 train_loss: 66.77976989746094 test_loss:573.064697265625\n",
      "1016/3000 train_loss: 63.1055908203125 test_loss:567.34130859375\n",
      "1017/3000 train_loss: 68.95639038085938 test_loss:576.2216186523438\n",
      "1018/3000 train_loss: 64.13076782226562 test_loss:564.6702270507812\n",
      "1019/3000 train_loss: 65.92424011230469 test_loss:558.7821655273438\n",
      "1020/3000 train_loss: 62.01583480834961 test_loss:566.998046875\n",
      "1021/3000 train_loss: 61.054256439208984 test_loss:556.580810546875\n",
      "1022/3000 train_loss: 61.89552307128906 test_loss:567.9989624023438\n",
      "1023/3000 train_loss: 63.51984405517578 test_loss:556.1973266601562\n",
      "1024/3000 train_loss: 65.46577453613281 test_loss:569.2175903320312\n",
      "1025/3000 train_loss: 63.345924377441406 test_loss:565.233154296875\n",
      "1026/3000 train_loss: 61.82161331176758 test_loss:564.4071044921875\n",
      "1027/3000 train_loss: 67.31010437011719 test_loss:561.4761352539062\n",
      "1028/3000 train_loss: 62.74504470825195 test_loss:564.6634521484375\n",
      "1029/3000 train_loss: 66.970703125 test_loss:573.1580200195312\n",
      "1030/3000 train_loss: 62.6031494140625 test_loss:561.044677734375\n",
      "1031/3000 train_loss: 63.1154899597168 test_loss:568.4739990234375\n",
      "1032/3000 train_loss: 62.346923828125 test_loss:567.2045288085938\n",
      "1033/3000 train_loss: 69.94277954101562 test_loss:564.0811157226562\n",
      "1034/3000 train_loss: 59.38248062133789 test_loss:567.9940185546875\n",
      "1035/3000 train_loss: 67.79719543457031 test_loss:565.2683715820312\n",
      "1036/3000 train_loss: 72.42237091064453 test_loss:558.1487426757812\n",
      "1037/3000 train_loss: 55.95894241333008 test_loss:566.6657104492188\n",
      "1038/3000 train_loss: 64.12133026123047 test_loss:568.6033935546875\n",
      "1039/3000 train_loss: 61.412601470947266 test_loss:665.9317016601562\n",
      "1040/3000 train_loss: 71.831298828125 test_loss:570.5673217773438\n",
      "1041/3000 train_loss: 60.1183967590332 test_loss:562.3169555664062\n",
      "1042/3000 train_loss: 63.66476821899414 test_loss:564.71630859375\n",
      "1043/3000 train_loss: 59.149147033691406 test_loss:561.685791015625\n",
      "1044/3000 train_loss: 65.53861236572266 test_loss:565.545654296875\n",
      "1045/3000 train_loss: 59.549564361572266 test_loss:560.0682983398438\n",
      "1046/3000 train_loss: 66.98736572265625 test_loss:558.8438720703125\n",
      "1047/3000 train_loss: 60.12425231933594 test_loss:556.269287109375\n",
      "1048/3000 train_loss: 60.12311553955078 test_loss:553.5713500976562\n",
      "1049/3000 train_loss: 63.4365234375 test_loss:556.9295654296875\n",
      "1050/3000 train_loss: 62.39813232421875 test_loss:553.3926391601562\n",
      "1051/3000 train_loss: 64.09083557128906 test_loss:562.3928833007812\n",
      "1052/3000 train_loss: 64.02949523925781 test_loss:557.31396484375\n",
      "1053/3000 train_loss: 61.82109069824219 test_loss:566.7099609375\n",
      "1054/3000 train_loss: 66.26608276367188 test_loss:563.595458984375\n",
      "1055/3000 train_loss: 55.52534103393555 test_loss:556.04150390625\n",
      "1056/3000 train_loss: 62.51152420043945 test_loss:551.3716430664062\n",
      "1057/3000 train_loss: 67.16484069824219 test_loss:562.1276245117188\n",
      "1058/3000 train_loss: 63.40806198120117 test_loss:559.0982666015625\n",
      "1059/3000 train_loss: 66.61223602294922 test_loss:553.2973022460938\n",
      "1060/3000 train_loss: 62.275657653808594 test_loss:563.8721923828125\n",
      "1061/3000 train_loss: 64.04667663574219 test_loss:554.9417114257812\n",
      "1062/3000 train_loss: 64.12010192871094 test_loss:565.046875\n",
      "1063/3000 train_loss: 57.63469314575195 test_loss:555.9752197265625\n",
      "1064/3000 train_loss: 67.34452056884766 test_loss:570.8541259765625\n",
      "1065/3000 train_loss: 67.96066284179688 test_loss:558.4102783203125\n",
      "1066/3000 train_loss: 66.65011596679688 test_loss:557.4700317382812\n",
      "1067/3000 train_loss: 64.69083404541016 test_loss:554.196044921875\n",
      "1068/3000 train_loss: 57.94087600708008 test_loss:560.4840087890625\n",
      "1069/3000 train_loss: 64.84751892089844 test_loss:558.3626098632812\n",
      "1070/3000 train_loss: 63.965518951416016 test_loss:557.1370239257812\n",
      "1071/3000 train_loss: 64.12574005126953 test_loss:558.2154541015625\n",
      "1072/3000 train_loss: 59.523338317871094 test_loss:549.228515625\n",
      "1073/3000 train_loss: 61.441104888916016 test_loss:557.6466674804688\n",
      "1074/3000 train_loss: 59.05362319946289 test_loss:559.6949462890625\n",
      "1075/3000 train_loss: 57.691280364990234 test_loss:559.8521728515625\n",
      "1076/3000 train_loss: 63.15220642089844 test_loss:551.918701171875\n",
      "1077/3000 train_loss: 60.701934814453125 test_loss:558.44677734375\n",
      "1078/3000 train_loss: 69.55078887939453 test_loss:567.5230102539062\n",
      "1079/3000 train_loss: 64.74808502197266 test_loss:560.4501953125\n",
      "1080/3000 train_loss: 59.944576263427734 test_loss:553.69921875\n",
      "1081/3000 train_loss: 61.77855682373047 test_loss:558.2687377929688\n",
      "1082/3000 train_loss: 60.537254333496094 test_loss:560.3082885742188\n",
      "1083/3000 train_loss: 60.5676155090332 test_loss:567.3302612304688\n",
      "1084/3000 train_loss: 62.986507415771484 test_loss:560.4522705078125\n",
      "1085/3000 train_loss: 62.010799407958984 test_loss:558.7225952148438\n",
      "1086/3000 train_loss: 60.307186126708984 test_loss:561.4347534179688\n",
      "1087/3000 train_loss: 62.9697151184082 test_loss:557.5792236328125\n",
      "1088/3000 train_loss: 60.0379524230957 test_loss:556.2178344726562\n",
      "1089/3000 train_loss: 56.985992431640625 test_loss:565.0557861328125\n",
      "1090/3000 train_loss: 60.34847640991211 test_loss:558.0243530273438\n",
      "1091/3000 train_loss: 56.30232620239258 test_loss:561.5985107421875\n",
      "1092/3000 train_loss: 66.11238861083984 test_loss:557.4473876953125\n",
      "1093/3000 train_loss: 58.49988555908203 test_loss:544.9171752929688\n",
      "1094/3000 train_loss: 56.450904846191406 test_loss:553.8541870117188\n",
      "1095/3000 train_loss: 66.82989501953125 test_loss:564.8251953125\n",
      "1096/3000 train_loss: 55.95070266723633 test_loss:550.3135986328125\n",
      "1097/3000 train_loss: 60.219146728515625 test_loss:554.0164184570312\n",
      "1098/3000 train_loss: 61.83431625366211 test_loss:551.2919311523438\n",
      "1099/3000 train_loss: 58.888099670410156 test_loss:562.5494384765625\n",
      "1100/3000 train_loss: 58.206912994384766 test_loss:557.8480224609375\n",
      "1101/3000 train_loss: 59.411407470703125 test_loss:556.9627685546875\n",
      "1102/3000 train_loss: 58.01034927368164 test_loss:560.3244018554688\n",
      "1103/3000 train_loss: 62.59846496582031 test_loss:554.0675048828125\n",
      "1104/3000 train_loss: 57.77669906616211 test_loss:558.2628784179688\n",
      "1105/3000 train_loss: 54.10124206542969 test_loss:552.7581787109375\n",
      "1106/3000 train_loss: 53.246273040771484 test_loss:558.0418090820312\n",
      "1107/3000 train_loss: 56.6718635559082 test_loss:551.9384155273438\n",
      "1108/3000 train_loss: 63.34528732299805 test_loss:556.6503295898438\n",
      "1109/3000 train_loss: 61.34292221069336 test_loss:557.500244140625\n",
      "1110/3000 train_loss: 58.37409591674805 test_loss:550.469970703125\n",
      "1111/3000 train_loss: 59.176170349121094 test_loss:560.8700561523438\n",
      "1112/3000 train_loss: 61.49058532714844 test_loss:554.2230224609375\n",
      "1113/3000 train_loss: 55.905906677246094 test_loss:555.995849609375\n",
      "1114/3000 train_loss: 62.46395492553711 test_loss:557.0298461914062\n",
      "1115/3000 train_loss: 55.1262092590332 test_loss:556.5562744140625\n",
      "1116/3000 train_loss: 56.88703536987305 test_loss:556.9171752929688\n",
      "1117/3000 train_loss: 60.95491409301758 test_loss:562.56689453125\n",
      "1118/3000 train_loss: 60.382713317871094 test_loss:562.529541015625\n",
      "1119/3000 train_loss: 58.474735260009766 test_loss:551.3614501953125\n",
      "1120/3000 train_loss: 67.60722351074219 test_loss:570.3603515625\n",
      "1121/3000 train_loss: 63.543426513671875 test_loss:565.4679565429688\n",
      "1122/3000 train_loss: 58.56721496582031 test_loss:562.082275390625\n",
      "1123/3000 train_loss: 59.45900344848633 test_loss:557.5175170898438\n",
      "1124/3000 train_loss: 62.88314437866211 test_loss:552.3471069335938\n",
      "1125/3000 train_loss: 56.879024505615234 test_loss:552.6305541992188\n",
      "1126/3000 train_loss: 64.12140655517578 test_loss:553.4783325195312\n",
      "1127/3000 train_loss: 61.593231201171875 test_loss:547.9521484375\n",
      "1128/3000 train_loss: 56.57791519165039 test_loss:548.0595703125\n",
      "1129/3000 train_loss: 60.80464172363281 test_loss:549.9354858398438\n",
      "1130/3000 train_loss: 57.47058868408203 test_loss:562.945556640625\n",
      "1131/3000 train_loss: 56.341522216796875 test_loss:551.42822265625\n",
      "1132/3000 train_loss: 55.459388732910156 test_loss:555.0331420898438\n",
      "1133/3000 train_loss: 57.04161071777344 test_loss:552.99462890625\n",
      "1134/3000 train_loss: 60.344261169433594 test_loss:554.7122802734375\n",
      "1135/3000 train_loss: 57.28692626953125 test_loss:542.2432861328125\n",
      "1136/3000 train_loss: 55.982093811035156 test_loss:558.384521484375\n",
      "1137/3000 train_loss: 55.34641647338867 test_loss:550.1029052734375\n",
      "1138/3000 train_loss: 59.340484619140625 test_loss:554.0138549804688\n",
      "1139/3000 train_loss: 69.42784881591797 test_loss:551.7256469726562\n",
      "1140/3000 train_loss: 62.234474182128906 test_loss:552.770751953125\n",
      "1141/3000 train_loss: 55.650413513183594 test_loss:545.8792724609375\n",
      "1142/3000 train_loss: 57.62339401245117 test_loss:551.7908935546875\n",
      "1143/3000 train_loss: 58.461429595947266 test_loss:549.1398315429688\n",
      "1144/3000 train_loss: 58.289119720458984 test_loss:551.746337890625\n",
      "1145/3000 train_loss: 55.614322662353516 test_loss:552.974365234375\n",
      "1146/3000 train_loss: 59.40018844604492 test_loss:547.01318359375\n",
      "1147/3000 train_loss: 64.13536834716797 test_loss:552.4188232421875\n",
      "1148/3000 train_loss: 55.54222869873047 test_loss:543.5325317382812\n",
      "1149/3000 train_loss: 61.98905944824219 test_loss:550.381103515625\n",
      "1150/3000 train_loss: 52.49977493286133 test_loss:552.3590087890625\n",
      "1151/3000 train_loss: 55.5920295715332 test_loss:548.3074951171875\n",
      "1152/3000 train_loss: 54.7735481262207 test_loss:550.479736328125\n",
      "1153/3000 train_loss: 56.24633026123047 test_loss:548.78955078125\n",
      "1154/3000 train_loss: 54.98108673095703 test_loss:550.0057373046875\n",
      "1155/3000 train_loss: 55.85599136352539 test_loss:551.0786743164062\n",
      "1156/3000 train_loss: 60.50572967529297 test_loss:557.1119384765625\n",
      "1157/3000 train_loss: 57.12196350097656 test_loss:556.2119140625\n",
      "1158/3000 train_loss: 57.584102630615234 test_loss:551.56494140625\n",
      "1159/3000 train_loss: 56.39170837402344 test_loss:546.9549560546875\n",
      "1160/3000 train_loss: 53.071231842041016 test_loss:545.5888671875\n",
      "1161/3000 train_loss: 56.74693298339844 test_loss:547.9552001953125\n",
      "1162/3000 train_loss: 54.31521987915039 test_loss:555.3024291992188\n",
      "1163/3000 train_loss: 56.42033386230469 test_loss:554.74462890625\n",
      "1164/3000 train_loss: 60.257598876953125 test_loss:550.5106811523438\n",
      "1165/3000 train_loss: 56.24261474609375 test_loss:552.52880859375\n",
      "1166/3000 train_loss: 64.67655944824219 test_loss:545.9899291992188\n",
      "1167/3000 train_loss: 53.98468780517578 test_loss:548.517578125\n",
      "1168/3000 train_loss: 53.680519104003906 test_loss:554.7284545898438\n",
      "1169/3000 train_loss: 64.4610366821289 test_loss:545.745361328125\n",
      "1170/3000 train_loss: 55.117069244384766 test_loss:554.1302490234375\n",
      "1171/3000 train_loss: 50.47383499145508 test_loss:555.439697265625\n",
      "1172/3000 train_loss: 58.36062240600586 test_loss:550.32666015625\n",
      "1173/3000 train_loss: 51.29268264770508 test_loss:548.2950439453125\n",
      "1174/3000 train_loss: 57.17408752441406 test_loss:570.86572265625\n",
      "1175/3000 train_loss: 57.33502197265625 test_loss:571.9539794921875\n",
      "1176/3000 train_loss: 61.64471435546875 test_loss:550.9580078125\n",
      "1177/3000 train_loss: 56.325706481933594 test_loss:555.349365234375\n",
      "1178/3000 train_loss: 54.8099365234375 test_loss:542.5975952148438\n",
      "1179/3000 train_loss: 56.896949768066406 test_loss:550.29296875\n",
      "1180/3000 train_loss: 51.45259094238281 test_loss:544.2938232421875\n",
      "1181/3000 train_loss: 55.00465393066406 test_loss:559.270751953125\n",
      "1182/3000 train_loss: 51.86232376098633 test_loss:557.011474609375\n",
      "1183/3000 train_loss: 50.91952896118164 test_loss:549.641357421875\n",
      "1184/3000 train_loss: 57.0389289855957 test_loss:556.92822265625\n",
      "1185/3000 train_loss: 56.03083801269531 test_loss:546.9779052734375\n",
      "1186/3000 train_loss: 55.653141021728516 test_loss:560.631103515625\n",
      "1187/3000 train_loss: 59.927490234375 test_loss:553.8568725585938\n",
      "1188/3000 train_loss: 59.21038818359375 test_loss:558.5785522460938\n",
      "1189/3000 train_loss: 56.30409240722656 test_loss:549.4254150390625\n",
      "1190/3000 train_loss: 55.35080337524414 test_loss:558.775146484375\n",
      "1191/3000 train_loss: 51.363887786865234 test_loss:549.4617919921875\n",
      "1192/3000 train_loss: 54.508670806884766 test_loss:561.1116333007812\n",
      "1193/3000 train_loss: 57.624053955078125 test_loss:548.902099609375\n",
      "1194/3000 train_loss: 53.948509216308594 test_loss:553.0565185546875\n",
      "1195/3000 train_loss: 57.2401237487793 test_loss:546.656494140625\n",
      "1196/3000 train_loss: 56.971275329589844 test_loss:546.5164184570312\n",
      "1197/3000 train_loss: 57.92823028564453 test_loss:565.6328735351562\n",
      "1198/3000 train_loss: 61.13679885864258 test_loss:560.1409912109375\n",
      "1199/3000 train_loss: 57.199134826660156 test_loss:554.5202026367188\n",
      "1200/3000 train_loss: 56.704490661621094 test_loss:547.82568359375\n",
      "1201/3000 train_loss: 57.692626953125 test_loss:552.6885375976562\n",
      "1202/3000 train_loss: 52.09703826904297 test_loss:550.606201171875\n",
      "1203/3000 train_loss: 49.414493560791016 test_loss:547.0726318359375\n",
      "1204/3000 train_loss: 61.64371871948242 test_loss:536.16748046875\n",
      "1205/3000 train_loss: 54.57759094238281 test_loss:549.2546997070312\n",
      "1206/3000 train_loss: 56.54673767089844 test_loss:550.2395629882812\n",
      "1207/3000 train_loss: 55.06126022338867 test_loss:543.5765991210938\n",
      "1208/3000 train_loss: 54.9637336730957 test_loss:548.2200927734375\n",
      "1209/3000 train_loss: 58.03720474243164 test_loss:559.0224609375\n",
      "1210/3000 train_loss: 58.14809799194336 test_loss:542.632080078125\n",
      "1211/3000 train_loss: 60.282527923583984 test_loss:557.4625244140625\n",
      "1212/3000 train_loss: 57.010562896728516 test_loss:556.32470703125\n",
      "1213/3000 train_loss: 54.406578063964844 test_loss:553.0421142578125\n",
      "1214/3000 train_loss: 56.86786651611328 test_loss:549.1318359375\n",
      "1215/3000 train_loss: 55.50587463378906 test_loss:555.72216796875\n",
      "1216/3000 train_loss: 57.88861846923828 test_loss:550.0709838867188\n",
      "1217/3000 train_loss: 51.507591247558594 test_loss:557.5066528320312\n",
      "1218/3000 train_loss: 60.92064666748047 test_loss:550.9454345703125\n",
      "1219/3000 train_loss: 60.26169204711914 test_loss:555.4241333007812\n",
      "1220/3000 train_loss: 61.630332946777344 test_loss:560.8785400390625\n",
      "1221/3000 train_loss: 60.84877014160156 test_loss:537.8575439453125\n",
      "1222/3000 train_loss: 57.15210723876953 test_loss:549.4577026367188\n",
      "1223/3000 train_loss: 56.172264099121094 test_loss:550.1202392578125\n",
      "1224/3000 train_loss: 60.10277557373047 test_loss:548.2244873046875\n",
      "1225/3000 train_loss: 55.435874938964844 test_loss:551.5372924804688\n",
      "1226/3000 train_loss: 55.70506286621094 test_loss:548.2225952148438\n",
      "1227/3000 train_loss: 57.889793395996094 test_loss:569.3831176757812\n",
      "1228/3000 train_loss: 50.6077766418457 test_loss:542.1935424804688\n",
      "1229/3000 train_loss: 47.191165924072266 test_loss:548.2183837890625\n",
      "1230/3000 train_loss: 52.011077880859375 test_loss:546.0253295898438\n",
      "1231/3000 train_loss: 52.28786849975586 test_loss:546.8858032226562\n",
      "1232/3000 train_loss: 54.05527114868164 test_loss:550.6353149414062\n",
      "1233/3000 train_loss: 50.62178421020508 test_loss:550.1025390625\n",
      "1234/3000 train_loss: 53.54302978515625 test_loss:543.6698608398438\n",
      "1235/3000 train_loss: 61.64147186279297 test_loss:551.1846923828125\n",
      "1236/3000 train_loss: 56.376792907714844 test_loss:544.2560424804688\n",
      "1237/3000 train_loss: 53.9918212890625 test_loss:547.7508544921875\n",
      "1238/3000 train_loss: 55.225547790527344 test_loss:541.4635620117188\n",
      "1239/3000 train_loss: 50.984493255615234 test_loss:550.1838989257812\n",
      "1240/3000 train_loss: 66.62782287597656 test_loss:541.3004760742188\n",
      "1241/3000 train_loss: 51.78117752075195 test_loss:547.5744018554688\n",
      "1242/3000 train_loss: 55.2838134765625 test_loss:546.4822998046875\n",
      "1243/3000 train_loss: 53.64214324951172 test_loss:550.9179077148438\n",
      "1244/3000 train_loss: 51.713844299316406 test_loss:545.892822265625\n",
      "1245/3000 train_loss: 58.538455963134766 test_loss:545.7123413085938\n",
      "1246/3000 train_loss: 46.85256576538086 test_loss:556.7537841796875\n",
      "1247/3000 train_loss: 51.061187744140625 test_loss:554.8074340820312\n",
      "1248/3000 train_loss: 50.186546325683594 test_loss:553.29150390625\n",
      "1249/3000 train_loss: 62.94156265258789 test_loss:553.1131591796875\n",
      "1250/3000 train_loss: 54.33354568481445 test_loss:550.5579833984375\n",
      "1251/3000 train_loss: 52.94134521484375 test_loss:550.6049194335938\n",
      "1252/3000 train_loss: 56.056034088134766 test_loss:544.932373046875\n",
      "1253/3000 train_loss: 51.388519287109375 test_loss:543.5269165039062\n",
      "1254/3000 train_loss: 54.258750915527344 test_loss:546.9408569335938\n",
      "1255/3000 train_loss: 55.2767448425293 test_loss:541.759765625\n",
      "1256/3000 train_loss: 54.597557067871094 test_loss:537.06005859375\n",
      "1257/3000 train_loss: 55.412479400634766 test_loss:547.0548095703125\n",
      "1258/3000 train_loss: 53.35001754760742 test_loss:541.619140625\n",
      "1259/3000 train_loss: 57.21207046508789 test_loss:540.1407470703125\n",
      "1260/3000 train_loss: 57.74769592285156 test_loss:543.3140869140625\n",
      "1261/3000 train_loss: 55.74159240722656 test_loss:546.5715942382812\n",
      "1262/3000 train_loss: 51.51283645629883 test_loss:544.605712890625\n",
      "1263/3000 train_loss: 54.23171615600586 test_loss:548.439453125\n",
      "1264/3000 train_loss: 58.606117248535156 test_loss:548.214111328125\n",
      "1265/3000 train_loss: 52.93333053588867 test_loss:540.9785766601562\n",
      "1266/3000 train_loss: 58.129310607910156 test_loss:546.7247314453125\n",
      "1267/3000 train_loss: 55.62074661254883 test_loss:549.545654296875\n",
      "1268/3000 train_loss: 50.461795806884766 test_loss:540.5181884765625\n",
      "1269/3000 train_loss: 46.35464859008789 test_loss:541.4732666015625\n",
      "1270/3000 train_loss: 55.31276321411133 test_loss:543.8424072265625\n",
      "1271/3000 train_loss: 51.39339065551758 test_loss:539.3696899414062\n",
      "1272/3000 train_loss: 53.11738967895508 test_loss:547.5064697265625\n",
      "1273/3000 train_loss: 56.41659164428711 test_loss:540.5504150390625\n",
      "1274/3000 train_loss: 55.04597091674805 test_loss:547.970703125\n",
      "1275/3000 train_loss: 57.80413818359375 test_loss:547.779541015625\n",
      "1276/3000 train_loss: 54.21469497680664 test_loss:542.8895263671875\n",
      "1277/3000 train_loss: 49.380828857421875 test_loss:547.7662353515625\n",
      "1278/3000 train_loss: 51.28399658203125 test_loss:534.45947265625\n",
      "1279/3000 train_loss: 48.96352767944336 test_loss:547.265625\n",
      "1280/3000 train_loss: 49.05949783325195 test_loss:541.301025390625\n",
      "1281/3000 train_loss: 51.11079406738281 test_loss:542.3242797851562\n",
      "1282/3000 train_loss: 63.55482482910156 test_loss:550.4526977539062\n",
      "1283/3000 train_loss: 56.014259338378906 test_loss:534.6228637695312\n",
      "1284/3000 train_loss: 50.134220123291016 test_loss:549.1652221679688\n",
      "1285/3000 train_loss: 56.14228820800781 test_loss:529.2650756835938\n",
      "1286/3000 train_loss: 52.31601333618164 test_loss:539.1314086914062\n",
      "1287/3000 train_loss: 47.425323486328125 test_loss:546.2695922851562\n",
      "1288/3000 train_loss: 52.695674896240234 test_loss:540.7103271484375\n",
      "1289/3000 train_loss: 48.528175354003906 test_loss:538.33349609375\n",
      "1290/3000 train_loss: 50.66233444213867 test_loss:542.6163940429688\n",
      "1291/3000 train_loss: 55.920936584472656 test_loss:539.3784790039062\n",
      "1292/3000 train_loss: 54.50885009765625 test_loss:551.8360595703125\n",
      "1293/3000 train_loss: 50.36552429199219 test_loss:537.1508178710938\n",
      "1294/3000 train_loss: 55.28493881225586 test_loss:539.9468383789062\n",
      "1295/3000 train_loss: 48.74623489379883 test_loss:591.07568359375\n",
      "1296/3000 train_loss: 47.79084777832031 test_loss:536.6102294921875\n",
      "1297/3000 train_loss: 53.496490478515625 test_loss:619.6905517578125\n",
      "1298/3000 train_loss: 52.493709564208984 test_loss:538.1505737304688\n",
      "1299/3000 train_loss: 56.45191192626953 test_loss:546.6612548828125\n",
      "1300/3000 train_loss: 57.808349609375 test_loss:548.7429809570312\n",
      "1301/3000 train_loss: 59.49093246459961 test_loss:550.1547241210938\n",
      "1302/3000 train_loss: 54.82316207885742 test_loss:541.6325073242188\n",
      "1303/3000 train_loss: 58.6045036315918 test_loss:543.8633422851562\n",
      "1304/3000 train_loss: 51.00492858886719 test_loss:545.6179809570312\n",
      "1305/3000 train_loss: 51.25713348388672 test_loss:541.4178466796875\n",
      "1306/3000 train_loss: 49.24187469482422 test_loss:541.156494140625\n",
      "1307/3000 train_loss: 48.49271774291992 test_loss:533.0747680664062\n",
      "1308/3000 train_loss: 57.10028839111328 test_loss:545.8682250976562\n",
      "1309/3000 train_loss: 54.16612243652344 test_loss:554.1478271484375\n",
      "1310/3000 train_loss: 50.11325454711914 test_loss:545.7512817382812\n",
      "1311/3000 train_loss: 51.27293395996094 test_loss:542.966796875\n",
      "1312/3000 train_loss: 50.4998779296875 test_loss:543.3184814453125\n",
      "1313/3000 train_loss: 54.937652587890625 test_loss:538.7415161132812\n",
      "1314/3000 train_loss: 50.218658447265625 test_loss:542.92333984375\n",
      "1315/3000 train_loss: 46.47536849975586 test_loss:543.5073852539062\n",
      "1316/3000 train_loss: 48.2862663269043 test_loss:533.3698120117188\n",
      "1317/3000 train_loss: 51.82379913330078 test_loss:540.6270751953125\n",
      "1318/3000 train_loss: 51.53115463256836 test_loss:543.180908203125\n",
      "1319/3000 train_loss: 57.12898254394531 test_loss:536.4927978515625\n",
      "1320/3000 train_loss: 52.35028839111328 test_loss:546.97412109375\n",
      "1321/3000 train_loss: 48.48561096191406 test_loss:537.9757690429688\n",
      "1322/3000 train_loss: 51.117332458496094 test_loss:541.2435302734375\n",
      "1323/3000 train_loss: 49.25849151611328 test_loss:546.5448608398438\n",
      "1324/3000 train_loss: 46.829429626464844 test_loss:537.2831420898438\n",
      "1325/3000 train_loss: 55.419898986816406 test_loss:543.3123779296875\n",
      "1326/3000 train_loss: 54.45663070678711 test_loss:537.4939575195312\n",
      "1327/3000 train_loss: 56.27163314819336 test_loss:552.5498657226562\n",
      "1328/3000 train_loss: 51.57574462890625 test_loss:549.38134765625\n",
      "1329/3000 train_loss: 52.85761260986328 test_loss:540.631591796875\n",
      "1330/3000 train_loss: 49.761634826660156 test_loss:542.9957275390625\n",
      "1331/3000 train_loss: 52.192562103271484 test_loss:549.8832397460938\n",
      "1332/3000 train_loss: 55.42479705810547 test_loss:545.2430419921875\n",
      "1333/3000 train_loss: 48.70466613769531 test_loss:536.6414794921875\n",
      "1334/3000 train_loss: 52.96683883666992 test_loss:537.6873779296875\n",
      "1335/3000 train_loss: 51.73283386230469 test_loss:555.6881103515625\n",
      "1336/3000 train_loss: 49.981849670410156 test_loss:535.3760986328125\n",
      "1337/3000 train_loss: 54.00281524658203 test_loss:546.879638671875\n",
      "1338/3000 train_loss: 49.230918884277344 test_loss:539.6054077148438\n",
      "1339/3000 train_loss: 54.719573974609375 test_loss:544.2760620117188\n",
      "1340/3000 train_loss: 57.9735107421875 test_loss:547.6370849609375\n",
      "1341/3000 train_loss: 55.773887634277344 test_loss:547.0181274414062\n",
      "1342/3000 train_loss: 57.2813720703125 test_loss:543.3718872070312\n",
      "1343/3000 train_loss: 52.29769515991211 test_loss:544.276123046875\n",
      "1344/3000 train_loss: 53.162654876708984 test_loss:556.6473999023438\n",
      "1345/3000 train_loss: 53.32832336425781 test_loss:540.0841064453125\n",
      "1346/3000 train_loss: 50.28438186645508 test_loss:551.7999267578125\n",
      "1347/3000 train_loss: 59.378787994384766 test_loss:547.11669921875\n",
      "1348/3000 train_loss: 50.188758850097656 test_loss:541.4537963867188\n",
      "1349/3000 train_loss: 50.22983932495117 test_loss:546.0623779296875\n",
      "1350/3000 train_loss: 58.611915588378906 test_loss:540.4663696289062\n",
      "1351/3000 train_loss: 58.963016510009766 test_loss:549.9044189453125\n",
      "1352/3000 train_loss: 50.88833236694336 test_loss:547.971435546875\n",
      "1353/3000 train_loss: 58.235862731933594 test_loss:552.3974609375\n",
      "1354/3000 train_loss: 52.33953094482422 test_loss:544.4442749023438\n",
      "1355/3000 train_loss: 51.26325988769531 test_loss:545.0224609375\n",
      "1356/3000 train_loss: 59.7347526550293 test_loss:550.714111328125\n",
      "1357/3000 train_loss: 48.140968322753906 test_loss:538.0489501953125\n",
      "1358/3000 train_loss: 51.52130126953125 test_loss:551.951904296875\n",
      "1359/3000 train_loss: 48.04179382324219 test_loss:539.9630126953125\n",
      "1360/3000 train_loss: 47.574798583984375 test_loss:548.4613647460938\n",
      "1361/3000 train_loss: 49.606475830078125 test_loss:544.9923095703125\n",
      "1362/3000 train_loss: 55.72768020629883 test_loss:546.4277954101562\n",
      "1363/3000 train_loss: 51.04904556274414 test_loss:551.3321533203125\n",
      "1364/3000 train_loss: 47.31517028808594 test_loss:540.5897216796875\n",
      "1365/3000 train_loss: 52.8212776184082 test_loss:549.36328125\n",
      "1366/3000 train_loss: 54.17156982421875 test_loss:543.298828125\n",
      "1367/3000 train_loss: 54.41054153442383 test_loss:544.1619873046875\n",
      "1368/3000 train_loss: 53.590980529785156 test_loss:550.1224365234375\n",
      "1369/3000 train_loss: 45.421669006347656 test_loss:549.0975341796875\n",
      "1370/3000 train_loss: 58.15288543701172 test_loss:549.4519653320312\n",
      "1371/3000 train_loss: 53.27479553222656 test_loss:548.761962890625\n",
      "1372/3000 train_loss: 57.23978805541992 test_loss:552.2940673828125\n",
      "1373/3000 train_loss: 62.183982849121094 test_loss:543.7784423828125\n",
      "1374/3000 train_loss: 51.49837112426758 test_loss:550.88623046875\n",
      "1375/3000 train_loss: 49.188194274902344 test_loss:546.508056640625\n",
      "1376/3000 train_loss: 49.62668991088867 test_loss:550.5153198242188\n",
      "1377/3000 train_loss: 49.589691162109375 test_loss:544.9849243164062\n",
      "1378/3000 train_loss: 48.85197830200195 test_loss:549.0924072265625\n",
      "1379/3000 train_loss: 49.74208450317383 test_loss:545.5792236328125\n",
      "1380/3000 train_loss: 53.25178146362305 test_loss:547.4973754882812\n",
      "1381/3000 train_loss: 57.588951110839844 test_loss:558.9253540039062\n",
      "1382/3000 train_loss: 52.90058898925781 test_loss:549.6676025390625\n",
      "1383/3000 train_loss: 52.36056137084961 test_loss:550.3265991210938\n",
      "1384/3000 train_loss: 45.2962646484375 test_loss:543.4102783203125\n",
      "1385/3000 train_loss: 54.43595504760742 test_loss:549.904296875\n",
      "1386/3000 train_loss: 51.847129821777344 test_loss:552.989990234375\n",
      "1387/3000 train_loss: 49.04884338378906 test_loss:551.8607788085938\n",
      "1388/3000 train_loss: 48.2182731628418 test_loss:545.9180297851562\n",
      "1389/3000 train_loss: 53.2860221862793 test_loss:548.3130493164062\n",
      "1390/3000 train_loss: 48.84551239013672 test_loss:549.8533325195312\n",
      "1391/3000 train_loss: 57.20219421386719 test_loss:556.5772094726562\n",
      "1392/3000 train_loss: 57.78191375732422 test_loss:554.94140625\n",
      "1393/3000 train_loss: 50.40185546875 test_loss:557.738037109375\n",
      "1394/3000 train_loss: 53.23138427734375 test_loss:555.7311401367188\n",
      "1395/3000 train_loss: 59.04706573486328 test_loss:548.0202026367188\n",
      "1396/3000 train_loss: 46.00132369995117 test_loss:540.349609375\n",
      "1397/3000 train_loss: 50.773536682128906 test_loss:549.1106567382812\n",
      "1398/3000 train_loss: 48.0010871887207 test_loss:538.481689453125\n",
      "1399/3000 train_loss: 55.26902389526367 test_loss:551.8217163085938\n",
      "1400/3000 train_loss: 48.949058532714844 test_loss:547.6176147460938\n",
      "1401/3000 train_loss: 48.5130729675293 test_loss:552.1304321289062\n",
      "1402/3000 train_loss: 46.032997131347656 test_loss:551.516845703125\n",
      "1403/3000 train_loss: 58.82457733154297 test_loss:547.18505859375\n",
      "1404/3000 train_loss: 47.44015121459961 test_loss:542.8937377929688\n",
      "1405/3000 train_loss: 55.55016326904297 test_loss:547.4765014648438\n",
      "1406/3000 train_loss: 56.24519729614258 test_loss:543.742431640625\n",
      "1407/3000 train_loss: 49.94900894165039 test_loss:546.4005126953125\n",
      "1408/3000 train_loss: 48.148353576660156 test_loss:536.42822265625\n",
      "1409/3000 train_loss: 54.09345626831055 test_loss:554.7442016601562\n",
      "1410/3000 train_loss: 50.864723205566406 test_loss:535.2717895507812\n",
      "1411/3000 train_loss: 54.46632766723633 test_loss:550.4959716796875\n",
      "1412/3000 train_loss: 51.5980224609375 test_loss:538.990478515625\n",
      "1413/3000 train_loss: 53.1546630859375 test_loss:541.7252807617188\n",
      "1414/3000 train_loss: 50.22008514404297 test_loss:543.6622924804688\n",
      "1415/3000 train_loss: 48.237335205078125 test_loss:545.778564453125\n",
      "1416/3000 train_loss: 49.194862365722656 test_loss:532.194091796875\n",
      "1417/3000 train_loss: 48.35908508300781 test_loss:546.5350341796875\n",
      "1418/3000 train_loss: 45.77105712890625 test_loss:550.2266235351562\n",
      "1419/3000 train_loss: 48.763206481933594 test_loss:542.8837890625\n",
      "1420/3000 train_loss: 50.16151809692383 test_loss:542.0197143554688\n",
      "1421/3000 train_loss: 58.22249221801758 test_loss:540.8795776367188\n",
      "1422/3000 train_loss: 45.38523483276367 test_loss:536.9869995117188\n",
      "1423/3000 train_loss: 43.547508239746094 test_loss:541.5335083007812\n",
      "1424/3000 train_loss: 47.91502380371094 test_loss:546.250732421875\n",
      "1425/3000 train_loss: 53.41167449951172 test_loss:549.008056640625\n",
      "1426/3000 train_loss: 46.3416633605957 test_loss:550.7738037109375\n",
      "1427/3000 train_loss: 48.0230827331543 test_loss:541.9063720703125\n",
      "1428/3000 train_loss: 46.8021125793457 test_loss:547.8491821289062\n",
      "1429/3000 train_loss: 58.11371612548828 test_loss:546.103759765625\n",
      "1430/3000 train_loss: 52.040184020996094 test_loss:537.7967529296875\n",
      "1431/3000 train_loss: 49.60573196411133 test_loss:546.0004272460938\n",
      "1432/3000 train_loss: 55.91524124145508 test_loss:541.3081665039062\n",
      "1433/3000 train_loss: 51.04554748535156 test_loss:543.3572998046875\n",
      "1434/3000 train_loss: 44.5120735168457 test_loss:537.6317138671875\n",
      "1435/3000 train_loss: 50.91031265258789 test_loss:545.2467041015625\n",
      "1436/3000 train_loss: 44.89061737060547 test_loss:532.3134155273438\n",
      "1437/3000 train_loss: 52.815391540527344 test_loss:544.2899169921875\n",
      "1438/3000 train_loss: 46.8661003112793 test_loss:542.2333984375\n",
      "1439/3000 train_loss: 48.44289016723633 test_loss:541.6070556640625\n",
      "1440/3000 train_loss: 47.62961959838867 test_loss:552.4354248046875\n",
      "1441/3000 train_loss: 49.03770065307617 test_loss:533.58056640625\n",
      "1442/3000 train_loss: 57.618553161621094 test_loss:549.4296875\n",
      "1443/3000 train_loss: 45.21817398071289 test_loss:543.9317626953125\n",
      "1444/3000 train_loss: 49.31342315673828 test_loss:555.8612060546875\n",
      "1445/3000 train_loss: 45.692256927490234 test_loss:542.3157348632812\n",
      "1446/3000 train_loss: 46.47783660888672 test_loss:550.7490234375\n",
      "1447/3000 train_loss: 57.7695426940918 test_loss:537.7183227539062\n",
      "1448/3000 train_loss: 49.55794143676758 test_loss:545.7025146484375\n",
      "1449/3000 train_loss: 49.11137390136719 test_loss:547.5408325195312\n",
      "1450/3000 train_loss: 49.19324493408203 test_loss:546.0897827148438\n",
      "1451/3000 train_loss: 50.00251770019531 test_loss:543.0145263671875\n",
      "1452/3000 train_loss: 44.84392166137695 test_loss:550.71484375\n",
      "1453/3000 train_loss: 48.6160774230957 test_loss:547.8030395507812\n",
      "1454/3000 train_loss: 51.87581253051758 test_loss:540.6574096679688\n",
      "1455/3000 train_loss: 49.54643249511719 test_loss:543.5281982421875\n",
      "1456/3000 train_loss: 55.48796844482422 test_loss:541.8976440429688\n",
      "1457/3000 train_loss: 45.711830139160156 test_loss:552.902099609375\n",
      "1458/3000 train_loss: 53.58002853393555 test_loss:542.9546508789062\n",
      "1459/3000 train_loss: 49.10515213012695 test_loss:548.7172241210938\n",
      "1460/3000 train_loss: 47.654197692871094 test_loss:540.274169921875\n",
      "1461/3000 train_loss: 47.13814926147461 test_loss:550.9508056640625\n",
      "1462/3000 train_loss: 46.69634246826172 test_loss:545.2318725585938\n",
      "1463/3000 train_loss: 53.895416259765625 test_loss:553.552734375\n",
      "1464/3000 train_loss: 48.32030487060547 test_loss:543.4344482421875\n",
      "1465/3000 train_loss: 53.66764450073242 test_loss:546.4097900390625\n",
      "1466/3000 train_loss: 49.36009216308594 test_loss:536.7822265625\n",
      "1467/3000 train_loss: 48.60368728637695 test_loss:538.9931030273438\n",
      "1468/3000 train_loss: 56.10906219482422 test_loss:537.5301513671875\n",
      "1469/3000 train_loss: 50.06651306152344 test_loss:540.851806640625\n",
      "1470/3000 train_loss: 52.593814849853516 test_loss:539.7694091796875\n",
      "1471/3000 train_loss: 51.78413009643555 test_loss:534.166748046875\n",
      "1472/3000 train_loss: 47.04228973388672 test_loss:540.6207275390625\n",
      "1473/3000 train_loss: 44.50399398803711 test_loss:529.6947021484375\n",
      "1474/3000 train_loss: 57.09632110595703 test_loss:547.1971435546875\n",
      "1475/3000 train_loss: 48.86095428466797 test_loss:539.644775390625\n",
      "1476/3000 train_loss: 53.6461296081543 test_loss:532.9594116210938\n",
      "1477/3000 train_loss: 45.8982048034668 test_loss:542.7158813476562\n",
      "1478/3000 train_loss: 51.468544006347656 test_loss:535.947998046875\n",
      "1479/3000 train_loss: 56.23476028442383 test_loss:540.31005859375\n",
      "1480/3000 train_loss: 48.85368728637695 test_loss:539.5575561523438\n",
      "1481/3000 train_loss: 47.55084228515625 test_loss:554.4083862304688\n",
      "1482/3000 train_loss: 50.718658447265625 test_loss:537.1765747070312\n",
      "1483/3000 train_loss: 46.620765686035156 test_loss:538.453125\n",
      "1484/3000 train_loss: 46.96726608276367 test_loss:551.7742919921875\n",
      "1485/3000 train_loss: 50.05109786987305 test_loss:540.2611083984375\n",
      "1486/3000 train_loss: 48.11948013305664 test_loss:540.7105712890625\n",
      "1487/3000 train_loss: 56.44409942626953 test_loss:554.58740234375\n",
      "1488/3000 train_loss: 48.06208038330078 test_loss:529.7435302734375\n",
      "1489/3000 train_loss: 53.10902404785156 test_loss:543.3562622070312\n",
      "1490/3000 train_loss: 44.45208740234375 test_loss:543.9907836914062\n",
      "1491/3000 train_loss: 46.450435638427734 test_loss:537.7969360351562\n",
      "1492/3000 train_loss: 51.421417236328125 test_loss:543.624267578125\n",
      "1493/3000 train_loss: 51.00676727294922 test_loss:542.8751220703125\n",
      "1494/3000 train_loss: 48.093833923339844 test_loss:544.6862182617188\n",
      "1495/3000 train_loss: 52.391624450683594 test_loss:541.9555053710938\n",
      "1496/3000 train_loss: 46.191741943359375 test_loss:549.5139770507812\n",
      "1497/3000 train_loss: 46.453800201416016 test_loss:542.297607421875\n",
      "1498/3000 train_loss: 50.237770080566406 test_loss:539.2081298828125\n",
      "1499/3000 train_loss: 43.67768096923828 test_loss:548.1953735351562\n",
      "1500/3000 train_loss: 45.40436553955078 test_loss:542.6432495117188\n",
      "1501/3000 train_loss: 49.1615104675293 test_loss:540.5089721679688\n",
      "1502/3000 train_loss: 46.40973663330078 test_loss:548.03857421875\n",
      "1503/3000 train_loss: 43.67107009887695 test_loss:543.3865966796875\n",
      "1504/3000 train_loss: 50.21403503417969 test_loss:544.8968505859375\n",
      "1505/3000 train_loss: 50.9561653137207 test_loss:549.9422607421875\n",
      "1506/3000 train_loss: 49.68480682373047 test_loss:547.1343383789062\n",
      "1507/3000 train_loss: 47.255645751953125 test_loss:533.0438232421875\n",
      "1508/3000 train_loss: 50.125144958496094 test_loss:549.9863891601562\n",
      "1509/3000 train_loss: 52.74794006347656 test_loss:544.4776611328125\n",
      "1510/3000 train_loss: 51.54193878173828 test_loss:539.9268798828125\n",
      "1511/3000 train_loss: 50.29551696777344 test_loss:544.1468505859375\n",
      "1512/3000 train_loss: 48.41482925415039 test_loss:543.6796264648438\n",
      "1513/3000 train_loss: 50.26352310180664 test_loss:548.2464599609375\n",
      "1514/3000 train_loss: 50.017635345458984 test_loss:539.0208129882812\n",
      "1515/3000 train_loss: 49.18498229980469 test_loss:546.7664794921875\n",
      "1516/3000 train_loss: 49.61836242675781 test_loss:549.0701904296875\n",
      "1517/3000 train_loss: 42.93443298339844 test_loss:538.9141845703125\n",
      "1518/3000 train_loss: 49.609161376953125 test_loss:549.0560302734375\n",
      "1519/3000 train_loss: 47.133033752441406 test_loss:547.8259887695312\n",
      "1520/3000 train_loss: 48.5104866027832 test_loss:543.763916015625\n",
      "1521/3000 train_loss: 52.97906494140625 test_loss:542.6096801757812\n",
      "1522/3000 train_loss: 49.537696838378906 test_loss:546.135009765625\n",
      "1523/3000 train_loss: 47.43476104736328 test_loss:545.245849609375\n",
      "1524/3000 train_loss: 50.10126495361328 test_loss:542.8226318359375\n",
      "1525/3000 train_loss: 45.89148712158203 test_loss:544.6819458007812\n",
      "1526/3000 train_loss: 48.006378173828125 test_loss:550.59130859375\n",
      "1527/3000 train_loss: 50.31438446044922 test_loss:538.153076171875\n",
      "1528/3000 train_loss: 44.56511306762695 test_loss:540.393310546875\n",
      "1529/3000 train_loss: 47.280277252197266 test_loss:542.2374267578125\n",
      "1530/3000 train_loss: 50.69378662109375 test_loss:540.94580078125\n",
      "1531/3000 train_loss: 47.96746063232422 test_loss:539.9910888671875\n",
      "1532/3000 train_loss: 43.40283966064453 test_loss:546.05224609375\n",
      "1533/3000 train_loss: 47.389034271240234 test_loss:539.3663330078125\n",
      "1534/3000 train_loss: 48.42892837524414 test_loss:541.1019287109375\n",
      "1535/3000 train_loss: 42.8447151184082 test_loss:550.3544921875\n",
      "1536/3000 train_loss: 40.741607666015625 test_loss:544.042236328125\n",
      "1537/3000 train_loss: 45.57431411743164 test_loss:546.8316650390625\n",
      "1538/3000 train_loss: 40.22315979003906 test_loss:541.012451171875\n",
      "1539/3000 train_loss: 45.56060028076172 test_loss:548.1373901367188\n",
      "1540/3000 train_loss: 50.601810455322266 test_loss:541.2691040039062\n",
      "1541/3000 train_loss: 43.99469757080078 test_loss:544.44091796875\n",
      "1542/3000 train_loss: 49.19642639160156 test_loss:542.9985961914062\n",
      "1543/3000 train_loss: 49.2900276184082 test_loss:543.009521484375\n",
      "1544/3000 train_loss: 52.37590026855469 test_loss:538.046875\n",
      "1545/3000 train_loss: 46.146690368652344 test_loss:539.5962524414062\n",
      "1546/3000 train_loss: 45.60615539550781 test_loss:539.2496948242188\n",
      "1547/3000 train_loss: 48.86921691894531 test_loss:546.2974853515625\n",
      "1548/3000 train_loss: 42.68794250488281 test_loss:533.8228759765625\n",
      "1549/3000 train_loss: 47.52244567871094 test_loss:538.114501953125\n",
      "1550/3000 train_loss: 52.875186920166016 test_loss:548.0274658203125\n",
      "1551/3000 train_loss: 49.287628173828125 test_loss:537.2293090820312\n",
      "1552/3000 train_loss: 51.40433120727539 test_loss:549.0709228515625\n",
      "1553/3000 train_loss: 48.74638748168945 test_loss:541.0257568359375\n",
      "1554/3000 train_loss: 44.17323303222656 test_loss:543.9744262695312\n",
      "1555/3000 train_loss: 45.82087707519531 test_loss:533.8765869140625\n",
      "1556/3000 train_loss: 48.88470458984375 test_loss:539.326904296875\n",
      "1557/3000 train_loss: 43.803245544433594 test_loss:543.6923828125\n",
      "1558/3000 train_loss: 48.443572998046875 test_loss:538.1957397460938\n",
      "1559/3000 train_loss: 45.87971878051758 test_loss:544.4982299804688\n",
      "1560/3000 train_loss: 50.081336975097656 test_loss:555.95849609375\n",
      "1561/3000 train_loss: 45.97489547729492 test_loss:543.9827880859375\n",
      "1562/3000 train_loss: 46.70491409301758 test_loss:541.700439453125\n",
      "1563/3000 train_loss: 46.09098815917969 test_loss:533.6292724609375\n",
      "1564/3000 train_loss: 46.83031463623047 test_loss:545.4568481445312\n",
      "1565/3000 train_loss: 48.432132720947266 test_loss:551.635986328125\n",
      "1566/3000 train_loss: 46.09354019165039 test_loss:542.3931274414062\n",
      "1567/3000 train_loss: 46.58857345581055 test_loss:541.9107055664062\n",
      "1568/3000 train_loss: 49.1115608215332 test_loss:547.7055053710938\n",
      "1569/3000 train_loss: 45.2453727722168 test_loss:551.2442626953125\n",
      "1570/3000 train_loss: 41.83574676513672 test_loss:550.011474609375\n",
      "1571/3000 train_loss: 49.24483871459961 test_loss:548.5371704101562\n",
      "1572/3000 train_loss: 48.65836715698242 test_loss:551.44091796875\n",
      "1573/3000 train_loss: 49.42095947265625 test_loss:550.629150390625\n",
      "1574/3000 train_loss: 52.210365295410156 test_loss:548.8761596679688\n",
      "1575/3000 train_loss: 47.21574783325195 test_loss:548.7462158203125\n",
      "1576/3000 train_loss: 43.26771545410156 test_loss:553.7569580078125\n",
      "1577/3000 train_loss: 45.13227462768555 test_loss:538.0864868164062\n",
      "1578/3000 train_loss: 45.460697174072266 test_loss:546.2176513671875\n",
      "1579/3000 train_loss: 46.62706756591797 test_loss:557.5794067382812\n",
      "1580/3000 train_loss: 39.2687873840332 test_loss:546.8477783203125\n",
      "1581/3000 train_loss: 49.93547058105469 test_loss:559.6937866210938\n",
      "1582/3000 train_loss: 49.51150894165039 test_loss:556.70703125\n",
      "1583/3000 train_loss: 46.739681243896484 test_loss:543.4292602539062\n",
      "1584/3000 train_loss: 47.91466522216797 test_loss:546.0111694335938\n",
      "1585/3000 train_loss: 55.43826675415039 test_loss:547.7086181640625\n",
      "1586/3000 train_loss: 47.56142807006836 test_loss:551.679443359375\n",
      "1587/3000 train_loss: 46.59246063232422 test_loss:549.3406982421875\n",
      "1588/3000 train_loss: 45.892616271972656 test_loss:548.2112426757812\n",
      "1589/3000 train_loss: 46.80184555053711 test_loss:563.24072265625\n",
      "1590/3000 train_loss: 45.15463638305664 test_loss:549.7684326171875\n",
      "1591/3000 train_loss: 50.410400390625 test_loss:547.2503051757812\n",
      "1592/3000 train_loss: 42.127716064453125 test_loss:551.312255859375\n",
      "1593/3000 train_loss: 47.15435791015625 test_loss:552.4616088867188\n",
      "1594/3000 train_loss: 45.41061782836914 test_loss:557.8140869140625\n",
      "1595/3000 train_loss: 53.684730529785156 test_loss:551.1526489257812\n",
      "1596/3000 train_loss: 56.65110397338867 test_loss:556.080322265625\n",
      "1597/3000 train_loss: 44.97945785522461 test_loss:557.9657592773438\n",
      "1598/3000 train_loss: 39.2622184753418 test_loss:550.2643432617188\n",
      "1599/3000 train_loss: 44.75429153442383 test_loss:561.2918701171875\n",
      "1600/3000 train_loss: 47.678916931152344 test_loss:551.5404052734375\n",
      "1601/3000 train_loss: 52.70730972290039 test_loss:560.378173828125\n",
      "1602/3000 train_loss: 46.08148193359375 test_loss:546.8901977539062\n",
      "1603/3000 train_loss: 44.87550354003906 test_loss:550.697998046875\n",
      "1604/3000 train_loss: 46.57801818847656 test_loss:555.9833374023438\n",
      "1605/3000 train_loss: 53.04533386230469 test_loss:550.201416015625\n",
      "1606/3000 train_loss: 43.4116325378418 test_loss:559.0235595703125\n",
      "1607/3000 train_loss: 46.67012405395508 test_loss:558.21728515625\n",
      "1608/3000 train_loss: 49.17694854736328 test_loss:560.8524169921875\n",
      "1609/3000 train_loss: 46.54478454589844 test_loss:556.6466674804688\n",
      "1610/3000 train_loss: 48.56212615966797 test_loss:551.154052734375\n",
      "1611/3000 train_loss: 47.55644607543945 test_loss:553.1537475585938\n",
      "1612/3000 train_loss: 40.975528717041016 test_loss:553.8338623046875\n",
      "1613/3000 train_loss: 48.84358596801758 test_loss:555.3433837890625\n",
      "1614/3000 train_loss: 42.21029281616211 test_loss:547.9785766601562\n",
      "1615/3000 train_loss: 48.2403678894043 test_loss:550.695068359375\n",
      "1616/3000 train_loss: 44.24003601074219 test_loss:546.2965087890625\n",
      "1617/3000 train_loss: 48.078372955322266 test_loss:548.99951171875\n",
      "1618/3000 train_loss: 40.67689514160156 test_loss:555.6099853515625\n",
      "1619/3000 train_loss: 47.18336486816406 test_loss:544.4166870117188\n",
      "1620/3000 train_loss: 44.19196319580078 test_loss:552.3400268554688\n",
      "1621/3000 train_loss: 48.58116912841797 test_loss:547.6109008789062\n",
      "1622/3000 train_loss: 46.26823425292969 test_loss:552.8958740234375\n",
      "1623/3000 train_loss: 47.44609069824219 test_loss:556.147216796875\n",
      "1624/3000 train_loss: 43.216224670410156 test_loss:552.4356689453125\n",
      "1625/3000 train_loss: 55.655147552490234 test_loss:552.239990234375\n",
      "1626/3000 train_loss: 46.61143112182617 test_loss:548.6156616210938\n",
      "1627/3000 train_loss: 48.8667106628418 test_loss:541.0932006835938\n",
      "1628/3000 train_loss: 44.1170654296875 test_loss:548.91845703125\n",
      "1629/3000 train_loss: 49.71577835083008 test_loss:548.7787475585938\n",
      "1630/3000 train_loss: 52.43916320800781 test_loss:549.1312255859375\n",
      "1631/3000 train_loss: 46.09401321411133 test_loss:546.5263061523438\n",
      "1632/3000 train_loss: 47.60757064819336 test_loss:543.156982421875\n",
      "1633/3000 train_loss: 48.041351318359375 test_loss:552.5641479492188\n",
      "1634/3000 train_loss: 49.80279541015625 test_loss:563.8092651367188\n",
      "1635/3000 train_loss: 46.20350646972656 test_loss:555.5910034179688\n",
      "1636/3000 train_loss: 47.70143508911133 test_loss:557.9021606445312\n",
      "1637/3000 train_loss: 50.33881759643555 test_loss:552.12353515625\n",
      "1638/3000 train_loss: 46.08898162841797 test_loss:552.7357177734375\n",
      "1639/3000 train_loss: 42.81251907348633 test_loss:550.1369018554688\n",
      "1640/3000 train_loss: 43.117130279541016 test_loss:552.8923950195312\n",
      "1641/3000 train_loss: 48.886714935302734 test_loss:553.2313232421875\n",
      "1642/3000 train_loss: 42.9799690246582 test_loss:545.7326049804688\n",
      "1643/3000 train_loss: 38.338768005371094 test_loss:552.8399658203125\n",
      "1644/3000 train_loss: 42.42409133911133 test_loss:546.824951171875\n",
      "1645/3000 train_loss: 44.97030258178711 test_loss:552.2510986328125\n",
      "1646/3000 train_loss: 42.85480880737305 test_loss:545.2056274414062\n",
      "1647/3000 train_loss: 47.95026397705078 test_loss:545.447509765625\n",
      "1648/3000 train_loss: 46.066001892089844 test_loss:547.6148681640625\n",
      "1649/3000 train_loss: 38.69676208496094 test_loss:559.7769165039062\n",
      "1650/3000 train_loss: 52.55930709838867 test_loss:542.8456420898438\n",
      "1651/3000 train_loss: 45.826045989990234 test_loss:561.4522094726562\n",
      "1652/3000 train_loss: 44.10495376586914 test_loss:551.085205078125\n",
      "1653/3000 train_loss: 45.6575927734375 test_loss:550.04931640625\n",
      "1654/3000 train_loss: 49.614715576171875 test_loss:564.0908813476562\n",
      "1655/3000 train_loss: 52.69367218017578 test_loss:550.4158935546875\n",
      "1656/3000 train_loss: 44.01979446411133 test_loss:546.6149291992188\n",
      "1657/3000 train_loss: 46.350547790527344 test_loss:553.3927001953125\n",
      "1658/3000 train_loss: 50.19011688232422 test_loss:551.0263671875\n",
      "1659/3000 train_loss: 48.44462203979492 test_loss:549.427734375\n",
      "1660/3000 train_loss: 50.65264129638672 test_loss:549.2164916992188\n",
      "1661/3000 train_loss: 47.38169860839844 test_loss:547.0367431640625\n",
      "1662/3000 train_loss: 47.50159454345703 test_loss:546.8704833984375\n",
      "1663/3000 train_loss: 45.59819030761719 test_loss:547.8590087890625\n",
      "1664/3000 train_loss: 46.33212661743164 test_loss:547.474365234375\n",
      "1665/3000 train_loss: 42.23152542114258 test_loss:549.6199951171875\n",
      "1666/3000 train_loss: 41.556053161621094 test_loss:549.138671875\n",
      "1667/3000 train_loss: 45.79104232788086 test_loss:553.6536254882812\n",
      "1668/3000 train_loss: 39.832645416259766 test_loss:547.7169189453125\n",
      "1669/3000 train_loss: 47.43376541137695 test_loss:551.57666015625\n",
      "1670/3000 train_loss: 41.10543441772461 test_loss:540.4293212890625\n",
      "1671/3000 train_loss: 39.897369384765625 test_loss:545.0809326171875\n",
      "1672/3000 train_loss: 42.35206604003906 test_loss:548.3213500976562\n",
      "1673/3000 train_loss: 46.2152214050293 test_loss:546.3370971679688\n",
      "1674/3000 train_loss: 47.4962158203125 test_loss:538.8811645507812\n",
      "1675/3000 train_loss: 45.564109802246094 test_loss:537.77294921875\n",
      "1676/3000 train_loss: 47.31269454956055 test_loss:548.1632080078125\n",
      "1677/3000 train_loss: 44.837547302246094 test_loss:541.7200927734375\n",
      "1678/3000 train_loss: 45.40668487548828 test_loss:553.8656005859375\n",
      "1679/3000 train_loss: 41.6658935546875 test_loss:537.6505737304688\n",
      "1680/3000 train_loss: 45.688751220703125 test_loss:547.2638549804688\n",
      "1681/3000 train_loss: 43.85358810424805 test_loss:549.632080078125\n",
      "1682/3000 train_loss: 44.915584564208984 test_loss:544.7403564453125\n",
      "1683/3000 train_loss: 42.656551361083984 test_loss:544.9531860351562\n",
      "1684/3000 train_loss: 45.14037322998047 test_loss:540.58984375\n",
      "1685/3000 train_loss: 44.81173324584961 test_loss:537.2034912109375\n",
      "1686/3000 train_loss: 44.05415725708008 test_loss:539.6092529296875\n",
      "1687/3000 train_loss: 47.19779968261719 test_loss:552.1976318359375\n",
      "1688/3000 train_loss: 43.873748779296875 test_loss:551.3071899414062\n",
      "1689/3000 train_loss: 46.85150909423828 test_loss:547.78955078125\n",
      "1690/3000 train_loss: 44.90153503417969 test_loss:543.4437255859375\n",
      "1691/3000 train_loss: 43.86574172973633 test_loss:556.6828002929688\n",
      "1692/3000 train_loss: 46.182918548583984 test_loss:550.1627807617188\n",
      "1693/3000 train_loss: 41.32266616821289 test_loss:546.755126953125\n",
      "1694/3000 train_loss: 45.1431999206543 test_loss:547.6421508789062\n",
      "1695/3000 train_loss: 44.67736053466797 test_loss:543.7648315429688\n",
      "1696/3000 train_loss: 39.28312683105469 test_loss:549.4317016601562\n",
      "1697/3000 train_loss: 48.085445404052734 test_loss:540.28271484375\n",
      "1698/3000 train_loss: 46.10478210449219 test_loss:539.48828125\n",
      "1699/3000 train_loss: 46.751365661621094 test_loss:543.8584594726562\n",
      "1700/3000 train_loss: 46.099815368652344 test_loss:536.53271484375\n",
      "1701/3000 train_loss: 48.36305236816406 test_loss:544.667724609375\n",
      "1702/3000 train_loss: 48.521949768066406 test_loss:536.8155517578125\n",
      "1703/3000 train_loss: 39.515830993652344 test_loss:539.0999755859375\n",
      "1704/3000 train_loss: 37.673187255859375 test_loss:543.8258666992188\n",
      "1705/3000 train_loss: 43.98655700683594 test_loss:541.2365112304688\n",
      "1706/3000 train_loss: 44.703712463378906 test_loss:543.5975341796875\n",
      "1707/3000 train_loss: 42.14799118041992 test_loss:525.248779296875\n",
      "1708/3000 train_loss: 44.070011138916016 test_loss:544.9033203125\n",
      "1709/3000 train_loss: 50.541839599609375 test_loss:541.9970092773438\n",
      "1710/3000 train_loss: 48.09873962402344 test_loss:535.2054443359375\n",
      "1711/3000 train_loss: 50.84877014160156 test_loss:551.3905029296875\n",
      "1712/3000 train_loss: 42.967613220214844 test_loss:541.469970703125\n",
      "1713/3000 train_loss: 51.45123291015625 test_loss:530.9996337890625\n",
      "1714/3000 train_loss: 44.912925720214844 test_loss:545.3426513671875\n",
      "1715/3000 train_loss: 45.27878952026367 test_loss:530.5151977539062\n",
      "1716/3000 train_loss: 45.799930572509766 test_loss:536.0437622070312\n",
      "1717/3000 train_loss: 45.766963958740234 test_loss:539.3346557617188\n",
      "1718/3000 train_loss: 50.03669738769531 test_loss:544.5443115234375\n",
      "1719/3000 train_loss: 49.60462188720703 test_loss:531.3247680664062\n",
      "1720/3000 train_loss: 47.43864059448242 test_loss:527.4613647460938\n",
      "1721/3000 train_loss: 49.784637451171875 test_loss:536.5074462890625\n",
      "1722/3000 train_loss: 41.219356536865234 test_loss:545.6142578125\n",
      "1723/3000 train_loss: 49.479026794433594 test_loss:534.16357421875\n",
      "1724/3000 train_loss: 49.75371551513672 test_loss:544.5068359375\n",
      "1725/3000 train_loss: 49.383670806884766 test_loss:538.8600463867188\n",
      "1726/3000 train_loss: 44.332862854003906 test_loss:537.2866821289062\n",
      "1727/3000 train_loss: 48.34944152832031 test_loss:539.0164184570312\n",
      "1728/3000 train_loss: 42.55009841918945 test_loss:539.5737915039062\n",
      "1729/3000 train_loss: 43.605247497558594 test_loss:531.3358764648438\n",
      "1730/3000 train_loss: 46.563175201416016 test_loss:540.468505859375\n",
      "1731/3000 train_loss: 44.435791015625 test_loss:545.9259643554688\n",
      "1732/3000 train_loss: 44.6491813659668 test_loss:538.0748291015625\n",
      "1733/3000 train_loss: 48.11057662963867 test_loss:531.004150390625\n",
      "1734/3000 train_loss: 44.7556266784668 test_loss:547.779052734375\n",
      "1735/3000 train_loss: 37.681766510009766 test_loss:542.4764404296875\n",
      "1736/3000 train_loss: 52.40863800048828 test_loss:545.577880859375\n",
      "1737/3000 train_loss: 45.34736251831055 test_loss:541.6423950195312\n",
      "1738/3000 train_loss: 42.00909423828125 test_loss:549.224365234375\n",
      "1739/3000 train_loss: 43.815460205078125 test_loss:549.1803588867188\n",
      "1740/3000 train_loss: 45.755027770996094 test_loss:547.7228393554688\n",
      "1741/3000 train_loss: 44.709754943847656 test_loss:542.8911743164062\n",
      "1742/3000 train_loss: 46.36119079589844 test_loss:551.6134033203125\n",
      "1743/3000 train_loss: 48.62275314331055 test_loss:541.5580444335938\n",
      "1744/3000 train_loss: 43.59385681152344 test_loss:544.7906494140625\n",
      "1745/3000 train_loss: 47.444847106933594 test_loss:542.705322265625\n",
      "1746/3000 train_loss: 50.435726165771484 test_loss:542.9208984375\n",
      "1747/3000 train_loss: 40.66495132446289 test_loss:551.7301635742188\n",
      "1748/3000 train_loss: 43.33226013183594 test_loss:538.5773315429688\n",
      "1749/3000 train_loss: 42.79341506958008 test_loss:538.1409912109375\n",
      "1750/3000 train_loss: 44.16752624511719 test_loss:543.0034790039062\n",
      "1751/3000 train_loss: 47.84634780883789 test_loss:535.23828125\n",
      "1752/3000 train_loss: 45.574432373046875 test_loss:536.826171875\n",
      "1753/3000 train_loss: 42.11326217651367 test_loss:531.1090087890625\n",
      "1754/3000 train_loss: 42.44327163696289 test_loss:535.5633544921875\n",
      "1755/3000 train_loss: 40.77019500732422 test_loss:528.026611328125\n",
      "1756/3000 train_loss: 44.469024658203125 test_loss:536.6917114257812\n",
      "1757/3000 train_loss: 43.42291259765625 test_loss:536.49755859375\n",
      "1758/3000 train_loss: 42.490657806396484 test_loss:538.0560302734375\n",
      "1759/3000 train_loss: 40.314453125 test_loss:544.0266723632812\n",
      "1760/3000 train_loss: 39.695369720458984 test_loss:533.7703857421875\n",
      "1761/3000 train_loss: 47.65849685668945 test_loss:536.8003540039062\n",
      "1762/3000 train_loss: 48.169490814208984 test_loss:533.9976196289062\n",
      "1763/3000 train_loss: 41.82786178588867 test_loss:541.3844604492188\n",
      "1764/3000 train_loss: 41.9284553527832 test_loss:540.9915161132812\n",
      "1765/3000 train_loss: 46.774818420410156 test_loss:543.8787231445312\n",
      "1766/3000 train_loss: 39.13967514038086 test_loss:533.7984619140625\n",
      "1767/3000 train_loss: 40.97838592529297 test_loss:538.2445068359375\n",
      "1768/3000 train_loss: 42.325408935546875 test_loss:537.4158325195312\n",
      "1769/3000 train_loss: 42.61381912231445 test_loss:543.0712890625\n",
      "1770/3000 train_loss: 41.70738220214844 test_loss:538.9920654296875\n",
      "1771/3000 train_loss: 42.421607971191406 test_loss:540.7278442382812\n",
      "1772/3000 train_loss: 50.29255676269531 test_loss:535.455078125\n",
      "1773/3000 train_loss: 46.69020462036133 test_loss:542.0235595703125\n",
      "1774/3000 train_loss: 40.74623107910156 test_loss:535.8924560546875\n",
      "1775/3000 train_loss: 46.09427261352539 test_loss:542.914794921875\n",
      "1776/3000 train_loss: 39.42873764038086 test_loss:530.0220947265625\n",
      "1777/3000 train_loss: 43.0596923828125 test_loss:541.348388671875\n",
      "1778/3000 train_loss: 42.97201919555664 test_loss:529.42578125\n",
      "1779/3000 train_loss: 44.14329528808594 test_loss:536.677490234375\n",
      "1780/3000 train_loss: 49.15884780883789 test_loss:543.826171875\n",
      "1781/3000 train_loss: 46.574485778808594 test_loss:537.8803100585938\n",
      "1782/3000 train_loss: 45.37882995605469 test_loss:545.0731201171875\n",
      "1783/3000 train_loss: 41.8471565246582 test_loss:546.8458251953125\n",
      "1784/3000 train_loss: 45.86493682861328 test_loss:539.9005737304688\n",
      "1785/3000 train_loss: 43.63433074951172 test_loss:544.160888671875\n",
      "1786/3000 train_loss: 40.817283630371094 test_loss:542.7642822265625\n",
      "1787/3000 train_loss: 38.80091857910156 test_loss:544.9417724609375\n",
      "1788/3000 train_loss: 39.229286193847656 test_loss:540.4292602539062\n",
      "1789/3000 train_loss: 46.62570571899414 test_loss:553.6762084960938\n",
      "1790/3000 train_loss: 42.54691696166992 test_loss:531.6585693359375\n",
      "1791/3000 train_loss: 42.56263732910156 test_loss:545.07177734375\n",
      "1792/3000 train_loss: 45.080909729003906 test_loss:540.6097412109375\n",
      "1793/3000 train_loss: 46.461402893066406 test_loss:547.3477783203125\n",
      "1794/3000 train_loss: 43.98331832885742 test_loss:557.80615234375\n",
      "1795/3000 train_loss: 42.5749626159668 test_loss:542.103515625\n",
      "1796/3000 train_loss: 43.453857421875 test_loss:553.6635131835938\n",
      "1797/3000 train_loss: 41.201438903808594 test_loss:549.6309814453125\n",
      "1798/3000 train_loss: 42.92710494995117 test_loss:540.2435302734375\n",
      "1799/3000 train_loss: 41.823036193847656 test_loss:539.2908935546875\n",
      "1800/3000 train_loss: 46.27854537963867 test_loss:542.7353515625\n",
      "1801/3000 train_loss: 47.034053802490234 test_loss:545.610595703125\n",
      "1802/3000 train_loss: 43.608070373535156 test_loss:538.1072998046875\n",
      "1803/3000 train_loss: 41.04499053955078 test_loss:537.4851684570312\n",
      "1804/3000 train_loss: 46.30263900756836 test_loss:546.1207275390625\n",
      "1805/3000 train_loss: 41.42225646972656 test_loss:539.2977294921875\n",
      "1806/3000 train_loss: 42.63321304321289 test_loss:546.3968505859375\n",
      "1807/3000 train_loss: 44.18939971923828 test_loss:541.7542724609375\n",
      "1808/3000 train_loss: 44.653743743896484 test_loss:555.2569580078125\n",
      "1809/3000 train_loss: 43.047584533691406 test_loss:539.4990844726562\n",
      "1810/3000 train_loss: 44.39472198486328 test_loss:546.2376708984375\n",
      "1811/3000 train_loss: 34.89716339111328 test_loss:542.4873046875\n",
      "1812/3000 train_loss: 46.25307846069336 test_loss:542.2514038085938\n",
      "1813/3000 train_loss: 39.20916748046875 test_loss:543.606689453125\n",
      "1814/3000 train_loss: 39.823753356933594 test_loss:547.8060913085938\n",
      "1815/3000 train_loss: 41.83261489868164 test_loss:546.8740844726562\n",
      "1816/3000 train_loss: 47.527427673339844 test_loss:555.227783203125\n",
      "1817/3000 train_loss: 40.379981994628906 test_loss:545.3367919921875\n",
      "1818/3000 train_loss: 41.81277847290039 test_loss:536.0237426757812\n",
      "1819/3000 train_loss: 39.75904083251953 test_loss:533.9608154296875\n",
      "1820/3000 train_loss: 41.123130798339844 test_loss:546.0010375976562\n",
      "1821/3000 train_loss: 41.94480895996094 test_loss:535.0863037109375\n",
      "1822/3000 train_loss: 45.225311279296875 test_loss:538.1989135742188\n",
      "1823/3000 train_loss: 38.379573822021484 test_loss:539.7094116210938\n",
      "1824/3000 train_loss: 47.708858489990234 test_loss:550.41162109375\n",
      "1825/3000 train_loss: 43.88325881958008 test_loss:538.65673828125\n",
      "1826/3000 train_loss: 40.59640884399414 test_loss:542.8738403320312\n",
      "1827/3000 train_loss: 46.96359634399414 test_loss:543.5341186523438\n",
      "1828/3000 train_loss: 49.05765151977539 test_loss:550.6114501953125\n",
      "1829/3000 train_loss: 45.18826675415039 test_loss:549.5802001953125\n",
      "1830/3000 train_loss: 41.517520904541016 test_loss:539.8314208984375\n",
      "1831/3000 train_loss: 40.47504806518555 test_loss:550.3987426757812\n",
      "1832/3000 train_loss: 45.83744430541992 test_loss:550.318603515625\n",
      "1833/3000 train_loss: 45.018150329589844 test_loss:544.4749145507812\n",
      "1834/3000 train_loss: 41.913795471191406 test_loss:548.1278076171875\n",
      "1835/3000 train_loss: 49.192447662353516 test_loss:536.3693237304688\n",
      "1836/3000 train_loss: 41.19628143310547 test_loss:537.6136474609375\n",
      "1837/3000 train_loss: 39.287357330322266 test_loss:544.6575927734375\n",
      "1838/3000 train_loss: 44.49375915527344 test_loss:553.853515625\n",
      "1839/3000 train_loss: 44.87846374511719 test_loss:544.315673828125\n",
      "1840/3000 train_loss: 44.618560791015625 test_loss:545.1220092773438\n",
      "1841/3000 train_loss: 48.75276565551758 test_loss:544.140625\n",
      "1842/3000 train_loss: 41.04021453857422 test_loss:537.2719116210938\n",
      "1843/3000 train_loss: 39.899864196777344 test_loss:542.051025390625\n",
      "1844/3000 train_loss: 39.66632080078125 test_loss:530.974365234375\n",
      "1845/3000 train_loss: 39.24836349487305 test_loss:541.44482421875\n",
      "1846/3000 train_loss: 46.802574157714844 test_loss:539.9039916992188\n",
      "1847/3000 train_loss: 45.740726470947266 test_loss:531.7028198242188\n",
      "1848/3000 train_loss: 47.57761001586914 test_loss:549.854736328125\n",
      "1849/3000 train_loss: 43.35638427734375 test_loss:550.7821044921875\n",
      "1850/3000 train_loss: 42.61529541015625 test_loss:549.8898315429688\n",
      "1851/3000 train_loss: 39.66865539550781 test_loss:547.1227416992188\n",
      "1852/3000 train_loss: 45.43339538574219 test_loss:547.1270751953125\n",
      "1853/3000 train_loss: 45.92129898071289 test_loss:547.0081787109375\n",
      "1854/3000 train_loss: 39.860321044921875 test_loss:538.52001953125\n",
      "1855/3000 train_loss: 42.348182678222656 test_loss:541.2327880859375\n",
      "1856/3000 train_loss: 40.757083892822266 test_loss:542.9190673828125\n",
      "1857/3000 train_loss: 41.02608871459961 test_loss:549.1826782226562\n",
      "1858/3000 train_loss: 44.77463912963867 test_loss:537.89013671875\n",
      "1859/3000 train_loss: 48.73137283325195 test_loss:543.8463134765625\n",
      "1860/3000 train_loss: 41.010719299316406 test_loss:535.1176147460938\n",
      "1861/3000 train_loss: 47.63972854614258 test_loss:544.62841796875\n",
      "1862/3000 train_loss: 40.32136535644531 test_loss:536.9559326171875\n",
      "1863/3000 train_loss: 35.872562408447266 test_loss:533.3059692382812\n",
      "1864/3000 train_loss: 41.31565475463867 test_loss:542.3863525390625\n",
      "1865/3000 train_loss: 45.78999328613281 test_loss:549.3504638671875\n",
      "1866/3000 train_loss: 42.348209381103516 test_loss:546.45703125\n",
      "1867/3000 train_loss: 51.952423095703125 test_loss:545.4274291992188\n",
      "1868/3000 train_loss: 39.79785919189453 test_loss:542.615966796875\n",
      "1869/3000 train_loss: 48.3255615234375 test_loss:550.0769653320312\n",
      "1870/3000 train_loss: 42.987422943115234 test_loss:538.1356811523438\n",
      "1871/3000 train_loss: 40.108787536621094 test_loss:548.3765258789062\n",
      "1872/3000 train_loss: 42.62013244628906 test_loss:539.2864990234375\n",
      "1873/3000 train_loss: 43.22051239013672 test_loss:552.376953125\n",
      "1874/3000 train_loss: 41.81163787841797 test_loss:540.1001586914062\n",
      "1875/3000 train_loss: 42.86051559448242 test_loss:553.2054443359375\n",
      "1876/3000 train_loss: 38.893585205078125 test_loss:551.2211303710938\n",
      "1877/3000 train_loss: 49.390174865722656 test_loss:539.2937622070312\n",
      "1878/3000 train_loss: 48.17050552368164 test_loss:546.902099609375\n",
      "1879/3000 train_loss: 42.13450241088867 test_loss:535.185791015625\n",
      "1880/3000 train_loss: 44.26597213745117 test_loss:546.5052490234375\n",
      "1881/3000 train_loss: 43.548736572265625 test_loss:535.7296142578125\n",
      "1882/3000 train_loss: 43.11039733886719 test_loss:543.1949462890625\n",
      "1883/3000 train_loss: 42.32542037963867 test_loss:540.7216796875\n",
      "1884/3000 train_loss: 38.70746612548828 test_loss:551.2431640625\n",
      "1885/3000 train_loss: 46.71728515625 test_loss:535.6888427734375\n",
      "1886/3000 train_loss: 44.25071334838867 test_loss:547.5372314453125\n",
      "1887/3000 train_loss: 41.21932601928711 test_loss:543.58447265625\n",
      "1888/3000 train_loss: 41.97261047363281 test_loss:543.2310791015625\n",
      "1889/3000 train_loss: 41.409183502197266 test_loss:538.5277709960938\n",
      "1890/3000 train_loss: 39.096923828125 test_loss:541.962890625\n",
      "1891/3000 train_loss: 43.059783935546875 test_loss:542.8053588867188\n",
      "1892/3000 train_loss: 43.30382537841797 test_loss:541.7526245117188\n",
      "1893/3000 train_loss: 45.3897705078125 test_loss:538.5145263671875\n",
      "1894/3000 train_loss: 39.64710235595703 test_loss:535.532958984375\n",
      "1895/3000 train_loss: 40.52224349975586 test_loss:545.2719116210938\n",
      "1896/3000 train_loss: 45.04234313964844 test_loss:538.5158081054688\n",
      "1897/3000 train_loss: 38.27042007446289 test_loss:537.6475830078125\n",
      "1898/3000 train_loss: 36.705322265625 test_loss:540.62744140625\n",
      "1899/3000 train_loss: 45.81220626831055 test_loss:541.8383178710938\n",
      "1900/3000 train_loss: 42.60205841064453 test_loss:527.8460693359375\n",
      "1901/3000 train_loss: 42.40792465209961 test_loss:540.1927490234375\n",
      "1902/3000 train_loss: 40.9812126159668 test_loss:536.887939453125\n",
      "1903/3000 train_loss: 41.84720993041992 test_loss:540.0352172851562\n",
      "1904/3000 train_loss: 39.02898025512695 test_loss:536.8788452148438\n",
      "1905/3000 train_loss: 43.34727478027344 test_loss:535.5948486328125\n",
      "1906/3000 train_loss: 41.8623161315918 test_loss:539.5859985351562\n",
      "1907/3000 train_loss: 45.43421173095703 test_loss:539.9081420898438\n",
      "1908/3000 train_loss: 42.28666305541992 test_loss:532.9754028320312\n",
      "1909/3000 train_loss: 47.11358642578125 test_loss:537.72314453125\n",
      "1910/3000 train_loss: 46.87419509887695 test_loss:543.552978515625\n",
      "1911/3000 train_loss: 42.581687927246094 test_loss:537.4917602539062\n",
      "1912/3000 train_loss: 43.19862747192383 test_loss:540.5142822265625\n",
      "1913/3000 train_loss: 37.375526428222656 test_loss:546.7783203125\n",
      "1914/3000 train_loss: 46.264183044433594 test_loss:544.4451904296875\n",
      "1915/3000 train_loss: 42.876617431640625 test_loss:548.5230102539062\n",
      "1916/3000 train_loss: 43.680908203125 test_loss:553.3414306640625\n",
      "1917/3000 train_loss: 42.38390350341797 test_loss:550.051025390625\n",
      "1918/3000 train_loss: 43.86296463012695 test_loss:547.40869140625\n",
      "1919/3000 train_loss: 36.41380310058594 test_loss:560.2137451171875\n",
      "1920/3000 train_loss: 41.68829345703125 test_loss:554.2255859375\n",
      "1921/3000 train_loss: 41.87824249267578 test_loss:549.657958984375\n",
      "1922/3000 train_loss: 40.33815383911133 test_loss:545.0563354492188\n",
      "1923/3000 train_loss: 53.39204406738281 test_loss:551.2733154296875\n",
      "1924/3000 train_loss: 40.676353454589844 test_loss:551.676513671875\n",
      "1925/3000 train_loss: 44.7781982421875 test_loss:542.9490966796875\n",
      "1926/3000 train_loss: 41.39549255371094 test_loss:547.553955078125\n",
      "1927/3000 train_loss: 41.99724197387695 test_loss:529.443359375\n",
      "1928/3000 train_loss: 43.0191535949707 test_loss:544.0592041015625\n",
      "1929/3000 train_loss: 38.77128601074219 test_loss:542.8772583007812\n",
      "1930/3000 train_loss: 44.082271575927734 test_loss:536.4432373046875\n",
      "1931/3000 train_loss: 43.60749816894531 test_loss:536.013427734375\n",
      "1932/3000 train_loss: 42.99163818359375 test_loss:547.6246337890625\n",
      "1933/3000 train_loss: 38.93667984008789 test_loss:537.0072021484375\n",
      "1934/3000 train_loss: 44.52250289916992 test_loss:548.3681640625\n",
      "1935/3000 train_loss: 42.41389083862305 test_loss:551.9013671875\n",
      "1936/3000 train_loss: 41.26300048828125 test_loss:545.694580078125\n",
      "1937/3000 train_loss: 41.559165954589844 test_loss:546.4775390625\n",
      "1938/3000 train_loss: 40.933231353759766 test_loss:547.3770141601562\n",
      "1939/3000 train_loss: 37.630775451660156 test_loss:543.4422607421875\n",
      "1940/3000 train_loss: 41.764610290527344 test_loss:547.9544677734375\n",
      "1941/3000 train_loss: 43.2140998840332 test_loss:552.796142578125\n",
      "1942/3000 train_loss: 35.705841064453125 test_loss:539.073974609375\n",
      "1943/3000 train_loss: 42.359291076660156 test_loss:538.1699829101562\n",
      "1944/3000 train_loss: 47.189823150634766 test_loss:541.3173217773438\n",
      "1945/3000 train_loss: 42.31326675415039 test_loss:542.310546875\n",
      "1946/3000 train_loss: 42.59573745727539 test_loss:542.3344116210938\n",
      "1947/3000 train_loss: 42.45404052734375 test_loss:549.295654296875\n",
      "1948/3000 train_loss: 43.517581939697266 test_loss:555.5243530273438\n",
      "1949/3000 train_loss: 51.424835205078125 test_loss:554.1332397460938\n",
      "1950/3000 train_loss: 47.25444030761719 test_loss:564.8212890625\n",
      "1951/3000 train_loss: 46.149192810058594 test_loss:539.7698974609375\n",
      "1952/3000 train_loss: 47.74150466918945 test_loss:550.190673828125\n",
      "1953/3000 train_loss: 40.99232482910156 test_loss:541.453369140625\n",
      "1954/3000 train_loss: 44.90447998046875 test_loss:553.1790771484375\n",
      "1955/3000 train_loss: 44.8812370300293 test_loss:546.9688720703125\n",
      "1956/3000 train_loss: 44.537559509277344 test_loss:539.3490600585938\n",
      "1957/3000 train_loss: 45.48616027832031 test_loss:554.1574096679688\n",
      "1958/3000 train_loss: 45.537567138671875 test_loss:541.8960571289062\n",
      "1959/3000 train_loss: 46.73353958129883 test_loss:543.9560546875\n",
      "1960/3000 train_loss: 48.67475509643555 test_loss:535.7154541015625\n",
      "1961/3000 train_loss: 40.0341911315918 test_loss:534.4810180664062\n",
      "1962/3000 train_loss: 38.456748962402344 test_loss:541.7371826171875\n",
      "1963/3000 train_loss: 36.6254997253418 test_loss:534.4752807617188\n",
      "1964/3000 train_loss: 38.83069610595703 test_loss:543.5792236328125\n",
      "1965/3000 train_loss: 41.56095886230469 test_loss:539.6334838867188\n",
      "1966/3000 train_loss: 42.22587203979492 test_loss:540.876953125\n",
      "1967/3000 train_loss: 41.040767669677734 test_loss:542.2969970703125\n",
      "1968/3000 train_loss: 45.428836822509766 test_loss:543.2333374023438\n",
      "1969/3000 train_loss: 39.40985870361328 test_loss:539.6722412109375\n",
      "1970/3000 train_loss: 41.39789581298828 test_loss:539.5514526367188\n",
      "1971/3000 train_loss: 40.12770462036133 test_loss:539.2870483398438\n",
      "1972/3000 train_loss: 39.48699951171875 test_loss:541.3472290039062\n",
      "1973/3000 train_loss: 39.82596206665039 test_loss:546.300537109375\n",
      "1974/3000 train_loss: 42.46831512451172 test_loss:543.2265625\n",
      "1975/3000 train_loss: 41.299278259277344 test_loss:535.791015625\n",
      "1976/3000 train_loss: 40.78525161743164 test_loss:540.1082153320312\n",
      "1977/3000 train_loss: 43.9704704284668 test_loss:543.8866577148438\n",
      "1978/3000 train_loss: 37.72107696533203 test_loss:536.8588256835938\n",
      "1979/3000 train_loss: 40.98888397216797 test_loss:548.6039428710938\n",
      "1980/3000 train_loss: 38.035362243652344 test_loss:541.0122680664062\n",
      "1981/3000 train_loss: 39.86171340942383 test_loss:546.7548828125\n",
      "1982/3000 train_loss: 38.88710403442383 test_loss:548.1513061523438\n",
      "1983/3000 train_loss: 38.31098556518555 test_loss:542.018798828125\n",
      "1984/3000 train_loss: 41.760929107666016 test_loss:552.3905639648438\n",
      "1985/3000 train_loss: 41.35411834716797 test_loss:541.2606811523438\n",
      "1986/3000 train_loss: 35.51466369628906 test_loss:549.126220703125\n",
      "1987/3000 train_loss: 39.331787109375 test_loss:551.3222045898438\n",
      "1988/3000 train_loss: 45.292388916015625 test_loss:560.6514892578125\n",
      "1989/3000 train_loss: 35.149559020996094 test_loss:554.8370361328125\n",
      "1990/3000 train_loss: 38.7779655456543 test_loss:554.3822021484375\n",
      "1991/3000 train_loss: 41.730262756347656 test_loss:554.1119384765625\n",
      "1992/3000 train_loss: 44.54890060424805 test_loss:557.707275390625\n",
      "1993/3000 train_loss: 41.95566940307617 test_loss:555.0625\n",
      "1994/3000 train_loss: 38.59540557861328 test_loss:549.359619140625\n",
      "1995/3000 train_loss: 42.225624084472656 test_loss:551.6557006835938\n",
      "1996/3000 train_loss: 41.045684814453125 test_loss:553.688720703125\n",
      "1997/3000 train_loss: 40.691650390625 test_loss:556.8973999023438\n",
      "1998/3000 train_loss: 45.698665618896484 test_loss:551.9915771484375\n",
      "1999/3000 train_loss: 37.7410888671875 test_loss:546.5811767578125\n",
      "2000/3000 train_loss: 38.37742233276367 test_loss:550.8739624023438\n",
      "2001/3000 train_loss: 39.36160659790039 test_loss:550.161865234375\n",
      "2002/3000 train_loss: 35.69656753540039 test_loss:544.1139526367188\n",
      "2003/3000 train_loss: 36.97723388671875 test_loss:556.342529296875\n",
      "2004/3000 train_loss: 41.304161071777344 test_loss:547.0936889648438\n",
      "2005/3000 train_loss: 39.07636642456055 test_loss:550.2911376953125\n",
      "2006/3000 train_loss: 45.78003692626953 test_loss:543.3838500976562\n",
      "2007/3000 train_loss: 37.628318786621094 test_loss:533.409912109375\n",
      "2008/3000 train_loss: 42.9087028503418 test_loss:535.882568359375\n",
      "2009/3000 train_loss: 39.36602020263672 test_loss:541.1617431640625\n",
      "2010/3000 train_loss: 40.632720947265625 test_loss:533.2557983398438\n",
      "2011/3000 train_loss: 35.76753234863281 test_loss:543.6854248046875\n",
      "2012/3000 train_loss: 40.811424255371094 test_loss:543.2667236328125\n",
      "2013/3000 train_loss: 46.509822845458984 test_loss:536.669921875\n",
      "2014/3000 train_loss: 36.43593978881836 test_loss:537.7103271484375\n",
      "2015/3000 train_loss: 36.50045394897461 test_loss:532.20947265625\n",
      "2016/3000 train_loss: 37.320796966552734 test_loss:546.786865234375\n",
      "2017/3000 train_loss: 40.20370864868164 test_loss:540.460205078125\n",
      "2018/3000 train_loss: 38.30573272705078 test_loss:541.230224609375\n",
      "2019/3000 train_loss: 43.59904098510742 test_loss:539.0321044921875\n",
      "2020/3000 train_loss: 44.022789001464844 test_loss:545.3922729492188\n",
      "2021/3000 train_loss: 38.23828125 test_loss:529.4653930664062\n",
      "2022/3000 train_loss: 42.21696472167969 test_loss:537.55615234375\n",
      "2023/3000 train_loss: 42.46658706665039 test_loss:538.4412841796875\n",
      "2024/3000 train_loss: 37.52582931518555 test_loss:543.303466796875\n",
      "2025/3000 train_loss: 39.03180694580078 test_loss:535.0277099609375\n",
      "2026/3000 train_loss: 42.81877136230469 test_loss:535.4945678710938\n",
      "2027/3000 train_loss: 41.28775405883789 test_loss:541.77978515625\n",
      "2028/3000 train_loss: 48.67768096923828 test_loss:536.1556396484375\n",
      "2029/3000 train_loss: 38.77821731567383 test_loss:549.1175537109375\n",
      "2030/3000 train_loss: 40.348976135253906 test_loss:542.9395751953125\n",
      "2031/3000 train_loss: 39.976646423339844 test_loss:554.8800048828125\n",
      "2032/3000 train_loss: 41.461097717285156 test_loss:541.731201171875\n",
      "2033/3000 train_loss: 40.623348236083984 test_loss:546.8824462890625\n",
      "2034/3000 train_loss: 41.01768493652344 test_loss:547.5845336914062\n",
      "2035/3000 train_loss: 46.50254440307617 test_loss:543.0657958984375\n",
      "2036/3000 train_loss: 38.47416305541992 test_loss:537.9037475585938\n",
      "2037/3000 train_loss: 40.21303176879883 test_loss:547.3800048828125\n",
      "2038/3000 train_loss: 39.017818450927734 test_loss:549.1884765625\n",
      "2039/3000 train_loss: 39.50362777709961 test_loss:547.45703125\n",
      "2040/3000 train_loss: 44.848365783691406 test_loss:553.1053466796875\n",
      "2041/3000 train_loss: 37.930580139160156 test_loss:545.7713623046875\n",
      "2042/3000 train_loss: 34.12607192993164 test_loss:545.6112060546875\n",
      "2043/3000 train_loss: 37.41510772705078 test_loss:545.7673950195312\n",
      "2044/3000 train_loss: 33.2906608581543 test_loss:548.370361328125\n",
      "2045/3000 train_loss: 41.14857482910156 test_loss:544.9041748046875\n",
      "2046/3000 train_loss: 42.939998626708984 test_loss:548.5787353515625\n",
      "2047/3000 train_loss: 39.938480377197266 test_loss:545.5596313476562\n",
      "2048/3000 train_loss: 38.53632354736328 test_loss:546.67236328125\n",
      "2049/3000 train_loss: 34.77399826049805 test_loss:546.2377319335938\n",
      "2050/3000 train_loss: 45.35835647583008 test_loss:546.510009765625\n",
      "2051/3000 train_loss: 43.31153869628906 test_loss:550.1807861328125\n",
      "2052/3000 train_loss: 36.785125732421875 test_loss:553.3231811523438\n",
      "2053/3000 train_loss: 39.8380126953125 test_loss:548.3914794921875\n",
      "2054/3000 train_loss: 41.825584411621094 test_loss:555.2476806640625\n",
      "2055/3000 train_loss: 41.06011962890625 test_loss:548.0731811523438\n",
      "2056/3000 train_loss: 37.507869720458984 test_loss:543.6527709960938\n",
      "2057/3000 train_loss: 43.73398971557617 test_loss:552.7709350585938\n",
      "2058/3000 train_loss: 39.74163818359375 test_loss:549.4928588867188\n",
      "2059/3000 train_loss: 42.33306121826172 test_loss:550.5401611328125\n",
      "2060/3000 train_loss: 41.08501434326172 test_loss:546.5228271484375\n",
      "2061/3000 train_loss: 47.543033599853516 test_loss:544.65673828125\n",
      "2062/3000 train_loss: 43.189266204833984 test_loss:547.450927734375\n",
      "2063/3000 train_loss: 40.763275146484375 test_loss:544.2323608398438\n",
      "2064/3000 train_loss: 38.800045013427734 test_loss:548.7794189453125\n",
      "2065/3000 train_loss: 34.10319137573242 test_loss:547.7935791015625\n",
      "2066/3000 train_loss: 38.91703414916992 test_loss:551.0552978515625\n",
      "2067/3000 train_loss: 38.961822509765625 test_loss:535.0169067382812\n",
      "2068/3000 train_loss: 39.37261199951172 test_loss:548.9736328125\n",
      "2069/3000 train_loss: 37.91170120239258 test_loss:550.825439453125\n",
      "2070/3000 train_loss: 37.96587371826172 test_loss:540.4403076171875\n",
      "2071/3000 train_loss: 34.724021911621094 test_loss:554.4401245117188\n",
      "2072/3000 train_loss: 40.48154067993164 test_loss:546.6406860351562\n",
      "2073/3000 train_loss: 36.248497009277344 test_loss:547.2666625976562\n",
      "2074/3000 train_loss: 39.96510696411133 test_loss:551.1302490234375\n",
      "2075/3000 train_loss: 41.34069061279297 test_loss:551.413818359375\n",
      "2076/3000 train_loss: 44.26991271972656 test_loss:557.3130493164062\n",
      "2077/3000 train_loss: 49.37933349609375 test_loss:539.0323486328125\n",
      "2078/3000 train_loss: 39.47077941894531 test_loss:548.0747680664062\n",
      "2079/3000 train_loss: 42.63679504394531 test_loss:552.7957763671875\n",
      "2080/3000 train_loss: 37.87921905517578 test_loss:546.2380981445312\n",
      "2081/3000 train_loss: 42.25200271606445 test_loss:565.6885986328125\n",
      "2082/3000 train_loss: 43.80522537231445 test_loss:548.8697509765625\n",
      "2083/3000 train_loss: 47.8660774230957 test_loss:551.5223999023438\n",
      "2084/3000 train_loss: 44.392696380615234 test_loss:546.9312133789062\n",
      "2085/3000 train_loss: 43.11798858642578 test_loss:557.0629272460938\n",
      "2086/3000 train_loss: 41.90559387207031 test_loss:554.2110595703125\n",
      "2087/3000 train_loss: 37.27312469482422 test_loss:549.0140380859375\n",
      "2088/3000 train_loss: 40.57010269165039 test_loss:541.8490600585938\n",
      "2089/3000 train_loss: 42.98507308959961 test_loss:552.297607421875\n",
      "2090/3000 train_loss: 42.51801681518555 test_loss:535.0421142578125\n",
      "2091/3000 train_loss: 42.664798736572266 test_loss:546.7487182617188\n",
      "2092/3000 train_loss: 43.05408477783203 test_loss:539.0532836914062\n",
      "2093/3000 train_loss: 43.88478469848633 test_loss:553.0563354492188\n",
      "2094/3000 train_loss: 37.41936492919922 test_loss:537.996337890625\n",
      "2095/3000 train_loss: 38.661407470703125 test_loss:541.960205078125\n",
      "2096/3000 train_loss: 42.51507568359375 test_loss:527.4862670898438\n",
      "2097/3000 train_loss: 37.829795837402344 test_loss:537.9186401367188\n",
      "2098/3000 train_loss: 50.20274353027344 test_loss:538.35009765625\n",
      "2099/3000 train_loss: 40.32477951049805 test_loss:540.7134399414062\n",
      "2100/3000 train_loss: 38.139137268066406 test_loss:541.897705078125\n",
      "2101/3000 train_loss: 37.45914077758789 test_loss:541.14501953125\n",
      "2102/3000 train_loss: 46.83657455444336 test_loss:535.7777709960938\n",
      "2103/3000 train_loss: 40.20081329345703 test_loss:550.0516357421875\n",
      "2104/3000 train_loss: 39.8932991027832 test_loss:546.5055541992188\n",
      "2105/3000 train_loss: 38.49869155883789 test_loss:541.1651611328125\n",
      "2106/3000 train_loss: 36.18164825439453 test_loss:547.7626342773438\n",
      "2107/3000 train_loss: 37.64793014526367 test_loss:544.6190185546875\n",
      "2108/3000 train_loss: 31.56415367126465 test_loss:537.4718017578125\n",
      "2109/3000 train_loss: 39.87242126464844 test_loss:542.22705078125\n",
      "2110/3000 train_loss: 42.40483856201172 test_loss:543.3079833984375\n",
      "2111/3000 train_loss: 35.56052780151367 test_loss:537.8079223632812\n",
      "2112/3000 train_loss: 36.7052001953125 test_loss:542.7926025390625\n",
      "2113/3000 train_loss: 37.55196762084961 test_loss:532.5089721679688\n",
      "2114/3000 train_loss: 37.558555603027344 test_loss:541.14501953125\n",
      "2115/3000 train_loss: 43.49034881591797 test_loss:549.3963623046875\n",
      "2116/3000 train_loss: 40.96908950805664 test_loss:539.5943603515625\n",
      "2117/3000 train_loss: 35.7149658203125 test_loss:531.0901489257812\n",
      "2118/3000 train_loss: 40.1371955871582 test_loss:543.9666748046875\n",
      "2119/3000 train_loss: 47.577659606933594 test_loss:540.481689453125\n",
      "2120/3000 train_loss: 38.51905822753906 test_loss:551.2464599609375\n",
      "2121/3000 train_loss: 38.97460174560547 test_loss:535.8482055664062\n",
      "2122/3000 train_loss: 41.113792419433594 test_loss:540.2705688476562\n",
      "2123/3000 train_loss: 40.492279052734375 test_loss:547.833984375\n",
      "2124/3000 train_loss: 39.91675567626953 test_loss:537.4506225585938\n",
      "2125/3000 train_loss: 35.725223541259766 test_loss:545.0841064453125\n",
      "2126/3000 train_loss: 50.337135314941406 test_loss:543.20556640625\n",
      "2127/3000 train_loss: 33.247371673583984 test_loss:542.0703125\n",
      "2128/3000 train_loss: 43.04658508300781 test_loss:546.6010131835938\n",
      "2129/3000 train_loss: 38.34510040283203 test_loss:549.5338134765625\n",
      "2130/3000 train_loss: 38.86439514160156 test_loss:546.7434692382812\n",
      "2131/3000 train_loss: 38.14811706542969 test_loss:546.076904296875\n",
      "2132/3000 train_loss: 37.613670349121094 test_loss:546.8853759765625\n",
      "2133/3000 train_loss: 38.17884826660156 test_loss:548.272705078125\n",
      "2134/3000 train_loss: 34.99407196044922 test_loss:550.5169067382812\n",
      "2135/3000 train_loss: 38.45429611206055 test_loss:544.5986938476562\n",
      "2136/3000 train_loss: 39.953521728515625 test_loss:544.683349609375\n",
      "2137/3000 train_loss: 37.82966613769531 test_loss:545.1388549804688\n",
      "2138/3000 train_loss: 45.50274658203125 test_loss:552.3955078125\n",
      "2139/3000 train_loss: 36.908714294433594 test_loss:543.4775390625\n",
      "2140/3000 train_loss: 44.28522491455078 test_loss:549.738525390625\n",
      "2141/3000 train_loss: 39.32746124267578 test_loss:544.147216796875\n",
      "2142/3000 train_loss: 41.38023376464844 test_loss:544.319580078125\n",
      "2143/3000 train_loss: 45.37616729736328 test_loss:556.964111328125\n",
      "2144/3000 train_loss: 43.68104934692383 test_loss:540.614501953125\n",
      "2145/3000 train_loss: 46.43163299560547 test_loss:544.592529296875\n",
      "2146/3000 train_loss: 46.56770324707031 test_loss:532.545166015625\n",
      "2147/3000 train_loss: 40.60479736328125 test_loss:547.7116088867188\n",
      "2148/3000 train_loss: 41.66637420654297 test_loss:546.8973388671875\n",
      "2149/3000 train_loss: 36.33382797241211 test_loss:549.1334838867188\n",
      "2150/3000 train_loss: 34.60926818847656 test_loss:542.0052490234375\n",
      "2151/3000 train_loss: 36.8254508972168 test_loss:547.550048828125\n",
      "2152/3000 train_loss: 35.90589904785156 test_loss:548.0928955078125\n",
      "2153/3000 train_loss: 47.36693572998047 test_loss:544.2490234375\n",
      "2154/3000 train_loss: 37.39450454711914 test_loss:547.0879516601562\n",
      "2155/3000 train_loss: 37.63845443725586 test_loss:542.707275390625\n",
      "2156/3000 train_loss: 37.457950592041016 test_loss:548.6021118164062\n",
      "2157/3000 train_loss: 33.396148681640625 test_loss:545.452392578125\n",
      "2158/3000 train_loss: 39.83153533935547 test_loss:548.4075927734375\n",
      "2159/3000 train_loss: 40.068199157714844 test_loss:550.6041259765625\n",
      "2160/3000 train_loss: 42.094215393066406 test_loss:547.72509765625\n",
      "2161/3000 train_loss: 39.32939910888672 test_loss:550.3717041015625\n",
      "2162/3000 train_loss: 37.540794372558594 test_loss:548.6639404296875\n",
      "2163/3000 train_loss: 36.06637954711914 test_loss:539.4073486328125\n",
      "2164/3000 train_loss: 41.888458251953125 test_loss:541.4303588867188\n",
      "2165/3000 train_loss: 38.4439697265625 test_loss:544.5701904296875\n",
      "2166/3000 train_loss: 40.379024505615234 test_loss:538.4617919921875\n",
      "2167/3000 train_loss: 37.8249397277832 test_loss:538.939697265625\n",
      "2168/3000 train_loss: 36.35749435424805 test_loss:529.707275390625\n",
      "2169/3000 train_loss: 43.776893615722656 test_loss:546.2507934570312\n",
      "2170/3000 train_loss: 43.262996673583984 test_loss:560.6337890625\n",
      "2171/3000 train_loss: 41.01388168334961 test_loss:533.587158203125\n",
      "2172/3000 train_loss: 39.53676223754883 test_loss:540.6197509765625\n",
      "2173/3000 train_loss: 41.973716735839844 test_loss:538.2748413085938\n",
      "2174/3000 train_loss: 37.75730895996094 test_loss:541.8924560546875\n",
      "2175/3000 train_loss: 40.11465835571289 test_loss:543.9625854492188\n",
      "2176/3000 train_loss: 37.99496078491211 test_loss:535.7293090820312\n",
      "2177/3000 train_loss: 36.93214797973633 test_loss:540.563720703125\n",
      "2178/3000 train_loss: 42.12146759033203 test_loss:547.6996459960938\n",
      "2179/3000 train_loss: 36.781272888183594 test_loss:542.13671875\n",
      "2180/3000 train_loss: 47.543582916259766 test_loss:550.5914306640625\n",
      "2181/3000 train_loss: 41.974639892578125 test_loss:558.673095703125\n",
      "2182/3000 train_loss: 40.84455871582031 test_loss:547.6548461914062\n",
      "2183/3000 train_loss: 38.682167053222656 test_loss:556.768798828125\n",
      "2184/3000 train_loss: 41.922203063964844 test_loss:548.7889404296875\n",
      "2185/3000 train_loss: 37.3620719909668 test_loss:548.4934692382812\n",
      "2186/3000 train_loss: 39.43842315673828 test_loss:552.520263671875\n",
      "2187/3000 train_loss: 39.162044525146484 test_loss:556.3499145507812\n",
      "2188/3000 train_loss: 42.85945129394531 test_loss:551.6380004882812\n",
      "2189/3000 train_loss: 38.396080017089844 test_loss:548.6844482421875\n",
      "2190/3000 train_loss: 41.788326263427734 test_loss:542.7196044921875\n",
      "2191/3000 train_loss: 45.299583435058594 test_loss:545.3424072265625\n",
      "2192/3000 train_loss: 43.16838073730469 test_loss:544.2139892578125\n",
      "2193/3000 train_loss: 36.40857696533203 test_loss:539.2332153320312\n",
      "2194/3000 train_loss: 40.794708251953125 test_loss:549.8207397460938\n",
      "2195/3000 train_loss: 38.25004959106445 test_loss:537.689697265625\n",
      "2196/3000 train_loss: 42.07341766357422 test_loss:545.3685302734375\n",
      "2197/3000 train_loss: 42.35394287109375 test_loss:546.0360107421875\n",
      "2198/3000 train_loss: 43.24855422973633 test_loss:533.9515380859375\n",
      "2199/3000 train_loss: 40.16366195678711 test_loss:540.0700073242188\n",
      "2200/3000 train_loss: 35.19605255126953 test_loss:543.3286743164062\n",
      "2201/3000 train_loss: 37.1931266784668 test_loss:546.9188232421875\n",
      "2202/3000 train_loss: 34.04899978637695 test_loss:548.28076171875\n",
      "2203/3000 train_loss: 36.39087677001953 test_loss:544.9855346679688\n",
      "2204/3000 train_loss: 31.627775192260742 test_loss:543.0724487304688\n",
      "2205/3000 train_loss: 39.83382797241211 test_loss:553.997314453125\n",
      "2206/3000 train_loss: 46.96638870239258 test_loss:551.01904296875\n",
      "2207/3000 train_loss: 36.6953125 test_loss:548.6383056640625\n",
      "2208/3000 train_loss: 37.53037643432617 test_loss:551.7786254882812\n",
      "2209/3000 train_loss: 40.40655517578125 test_loss:551.7283935546875\n",
      "2210/3000 train_loss: 37.02548599243164 test_loss:548.4188842773438\n",
      "2211/3000 train_loss: 37.608524322509766 test_loss:545.745849609375\n",
      "2212/3000 train_loss: 38.788902282714844 test_loss:547.5148315429688\n",
      "2213/3000 train_loss: 38.25619888305664 test_loss:555.3927001953125\n",
      "2214/3000 train_loss: 35.555702209472656 test_loss:547.0128173828125\n",
      "2215/3000 train_loss: 35.28310012817383 test_loss:548.358154296875\n",
      "2216/3000 train_loss: 37.30216598510742 test_loss:545.41455078125\n",
      "2217/3000 train_loss: 39.664794921875 test_loss:545.7987060546875\n",
      "2218/3000 train_loss: 41.796749114990234 test_loss:540.2463989257812\n",
      "2219/3000 train_loss: 37.84571838378906 test_loss:547.7477416992188\n",
      "2220/3000 train_loss: 41.282745361328125 test_loss:546.7854614257812\n",
      "2221/3000 train_loss: 33.744873046875 test_loss:547.77880859375\n",
      "2222/3000 train_loss: 41.78009796142578 test_loss:557.43359375\n",
      "2223/3000 train_loss: 40.69304656982422 test_loss:545.2579956054688\n",
      "2224/3000 train_loss: 44.45440673828125 test_loss:549.92333984375\n",
      "2225/3000 train_loss: 45.84526824951172 test_loss:563.3021240234375\n",
      "2226/3000 train_loss: 37.028560638427734 test_loss:547.3411865234375\n",
      "2227/3000 train_loss: 36.7505989074707 test_loss:542.511474609375\n",
      "2228/3000 train_loss: 36.1942138671875 test_loss:558.9457397460938\n",
      "2229/3000 train_loss: 36.430789947509766 test_loss:549.232421875\n",
      "2230/3000 train_loss: 36.07096481323242 test_loss:552.7871704101562\n",
      "2231/3000 train_loss: 37.26192092895508 test_loss:547.8264770507812\n",
      "2232/3000 train_loss: 37.07887268066406 test_loss:544.3318481445312\n",
      "2233/3000 train_loss: 36.91614532470703 test_loss:550.3773803710938\n",
      "2234/3000 train_loss: 37.17532730102539 test_loss:541.0154418945312\n",
      "2235/3000 train_loss: 39.86205291748047 test_loss:554.9385986328125\n",
      "2236/3000 train_loss: 44.29436492919922 test_loss:548.62646484375\n",
      "2237/3000 train_loss: 40.106727600097656 test_loss:546.0938110351562\n",
      "2238/3000 train_loss: 37.7401008605957 test_loss:549.0977783203125\n",
      "2239/3000 train_loss: 35.52891159057617 test_loss:547.2489013671875\n",
      "2240/3000 train_loss: 35.7418327331543 test_loss:537.5093994140625\n",
      "2241/3000 train_loss: 42.11248016357422 test_loss:552.7198486328125\n",
      "2242/3000 train_loss: 41.309837341308594 test_loss:536.6427001953125\n",
      "2243/3000 train_loss: 37.92922592163086 test_loss:544.338134765625\n",
      "2244/3000 train_loss: 38.231868743896484 test_loss:550.6790161132812\n",
      "2245/3000 train_loss: 35.4422607421875 test_loss:546.3843994140625\n",
      "2246/3000 train_loss: 40.53120422363281 test_loss:545.2048950195312\n",
      "2247/3000 train_loss: 37.85758590698242 test_loss:545.1834106445312\n",
      "2248/3000 train_loss: 40.10392379760742 test_loss:538.6719970703125\n",
      "2249/3000 train_loss: 35.470027923583984 test_loss:539.52685546875\n",
      "2250/3000 train_loss: 38.89274597167969 test_loss:548.342041015625\n",
      "2251/3000 train_loss: 40.85226821899414 test_loss:540.5396728515625\n",
      "2252/3000 train_loss: 34.55170440673828 test_loss:548.8404541015625\n",
      "2253/3000 train_loss: 36.47171401977539 test_loss:543.56787109375\n",
      "2254/3000 train_loss: 37.56330108642578 test_loss:544.4283447265625\n",
      "2255/3000 train_loss: 38.07008361816406 test_loss:537.388427734375\n",
      "2256/3000 train_loss: 43.184913635253906 test_loss:553.7050170898438\n",
      "2257/3000 train_loss: 44.253013610839844 test_loss:543.6292724609375\n",
      "2258/3000 train_loss: 40.16570281982422 test_loss:555.614501953125\n",
      "2259/3000 train_loss: 42.92851257324219 test_loss:546.0724487304688\n",
      "2260/3000 train_loss: 36.71565246582031 test_loss:552.2486572265625\n",
      "2261/3000 train_loss: 40.08761215209961 test_loss:551.3126831054688\n",
      "2262/3000 train_loss: 39.87282180786133 test_loss:555.6383056640625\n",
      "2263/3000 train_loss: 39.17723083496094 test_loss:553.2667236328125\n",
      "2264/3000 train_loss: 39.74908447265625 test_loss:543.5560913085938\n",
      "2265/3000 train_loss: 33.88515853881836 test_loss:546.84521484375\n",
      "2266/3000 train_loss: 31.894540786743164 test_loss:545.5750732421875\n",
      "2267/3000 train_loss: 43.36858367919922 test_loss:545.98193359375\n",
      "2268/3000 train_loss: 39.32456970214844 test_loss:538.7886962890625\n",
      "2269/3000 train_loss: 37.74293899536133 test_loss:544.69384765625\n",
      "2270/3000 train_loss: 40.380149841308594 test_loss:538.3956909179688\n",
      "2271/3000 train_loss: 39.28350830078125 test_loss:538.3869018554688\n",
      "2272/3000 train_loss: 40.62942123413086 test_loss:547.8587646484375\n",
      "2273/3000 train_loss: 40.07279968261719 test_loss:537.9237060546875\n",
      "2274/3000 train_loss: 43.56814956665039 test_loss:544.929931640625\n",
      "2275/3000 train_loss: 38.1320686340332 test_loss:534.525390625\n",
      "2276/3000 train_loss: 45.713722229003906 test_loss:550.7205200195312\n",
      "2277/3000 train_loss: 43.22697830200195 test_loss:539.380126953125\n",
      "2278/3000 train_loss: 41.32991027832031 test_loss:549.9502563476562\n",
      "2279/3000 train_loss: 39.96370315551758 test_loss:543.5509643554688\n",
      "2280/3000 train_loss: 35.44336700439453 test_loss:545.2780151367188\n",
      "2281/3000 train_loss: 41.87352752685547 test_loss:540.2321166992188\n",
      "2282/3000 train_loss: 39.67332077026367 test_loss:538.8052368164062\n",
      "2283/3000 train_loss: 37.5640869140625 test_loss:539.05712890625\n",
      "2284/3000 train_loss: 41.94669723510742 test_loss:542.093505859375\n",
      "2285/3000 train_loss: 38.89579391479492 test_loss:546.5189208984375\n",
      "2286/3000 train_loss: 37.98273849487305 test_loss:538.0504150390625\n",
      "2287/3000 train_loss: 39.83079147338867 test_loss:546.2565307617188\n",
      "2288/3000 train_loss: 39.84923553466797 test_loss:540.7847900390625\n",
      "2289/3000 train_loss: 40.357364654541016 test_loss:546.58349609375\n",
      "2290/3000 train_loss: 38.23014831542969 test_loss:536.8486328125\n",
      "2291/3000 train_loss: 40.83953857421875 test_loss:537.2408447265625\n",
      "2292/3000 train_loss: 41.340354919433594 test_loss:536.1640625\n",
      "2293/3000 train_loss: 35.017059326171875 test_loss:545.4166870117188\n",
      "2294/3000 train_loss: 35.929283142089844 test_loss:548.493408203125\n",
      "2295/3000 train_loss: 38.25218200683594 test_loss:539.18212890625\n",
      "2296/3000 train_loss: 36.823585510253906 test_loss:538.2249755859375\n",
      "2297/3000 train_loss: 41.86560821533203 test_loss:546.50048828125\n",
      "2298/3000 train_loss: 37.89125442504883 test_loss:545.6859741210938\n",
      "2299/3000 train_loss: 36.041954040527344 test_loss:545.5440063476562\n",
      "2300/3000 train_loss: 41.33415222167969 test_loss:552.1142578125\n",
      "2301/3000 train_loss: 40.90152359008789 test_loss:537.9795532226562\n",
      "2302/3000 train_loss: 42.080623626708984 test_loss:556.5117797851562\n",
      "2303/3000 train_loss: 34.74385452270508 test_loss:550.6544799804688\n",
      "2304/3000 train_loss: 38.24179458618164 test_loss:551.9978637695312\n",
      "2305/3000 train_loss: 40.40385055541992 test_loss:539.7409057617188\n",
      "2306/3000 train_loss: 33.22873306274414 test_loss:535.1627197265625\n",
      "2307/3000 train_loss: 40.35590744018555 test_loss:542.2203369140625\n",
      "2308/3000 train_loss: 34.22752380371094 test_loss:546.3683471679688\n",
      "2309/3000 train_loss: 34.45857238769531 test_loss:536.8487548828125\n",
      "2310/3000 train_loss: 38.605735778808594 test_loss:547.8694458007812\n",
      "2311/3000 train_loss: 36.49272537231445 test_loss:534.145263671875\n",
      "2312/3000 train_loss: 35.83544921875 test_loss:532.9434204101562\n",
      "2313/3000 train_loss: 44.4919319152832 test_loss:544.5094604492188\n",
      "2314/3000 train_loss: 41.05283737182617 test_loss:545.31201171875\n",
      "2315/3000 train_loss: 31.771854400634766 test_loss:540.9752807617188\n",
      "2316/3000 train_loss: 41.75291061401367 test_loss:545.521240234375\n",
      "2317/3000 train_loss: 41.00065612792969 test_loss:551.786376953125\n",
      "2318/3000 train_loss: 36.27619171142578 test_loss:547.8890380859375\n",
      "2319/3000 train_loss: 41.04216003417969 test_loss:547.6281127929688\n",
      "2320/3000 train_loss: 36.5297966003418 test_loss:548.7080078125\n",
      "2321/3000 train_loss: 38.549530029296875 test_loss:552.318603515625\n",
      "2322/3000 train_loss: 36.027183532714844 test_loss:540.6288452148438\n",
      "2323/3000 train_loss: 37.26104736328125 test_loss:540.8580322265625\n",
      "2324/3000 train_loss: 40.216712951660156 test_loss:535.3486328125\n",
      "2325/3000 train_loss: 38.06730651855469 test_loss:541.4683837890625\n",
      "2326/3000 train_loss: 35.31718063354492 test_loss:539.7047119140625\n",
      "2327/3000 train_loss: 38.55482864379883 test_loss:535.0455322265625\n",
      "2328/3000 train_loss: 37.56730651855469 test_loss:545.48046875\n",
      "2329/3000 train_loss: 40.12945556640625 test_loss:548.1900024414062\n",
      "2330/3000 train_loss: 41.9498291015625 test_loss:547.1263427734375\n",
      "2331/3000 train_loss: 37.547569274902344 test_loss:551.7836303710938\n",
      "2332/3000 train_loss: 36.437313079833984 test_loss:552.5789184570312\n",
      "2333/3000 train_loss: 37.324058532714844 test_loss:534.3887939453125\n",
      "2334/3000 train_loss: 39.37979507446289 test_loss:545.2630004882812\n",
      "2335/3000 train_loss: 39.45570755004883 test_loss:542.0032348632812\n",
      "2336/3000 train_loss: 38.99045181274414 test_loss:536.6010131835938\n",
      "2337/3000 train_loss: 33.13570785522461 test_loss:542.0248413085938\n",
      "2338/3000 train_loss: 30.483509063720703 test_loss:539.4699096679688\n",
      "2339/3000 train_loss: 38.024662017822266 test_loss:526.102294921875\n",
      "2340/3000 train_loss: 35.06163024902344 test_loss:553.6976928710938\n",
      "2341/3000 train_loss: 38.990299224853516 test_loss:550.5662231445312\n",
      "2342/3000 train_loss: 37.92554473876953 test_loss:539.4341430664062\n",
      "2343/3000 train_loss: 36.90214157104492 test_loss:551.0849609375\n",
      "2344/3000 train_loss: 40.818634033203125 test_loss:542.083740234375\n",
      "2345/3000 train_loss: 40.185001373291016 test_loss:545.7742309570312\n",
      "2346/3000 train_loss: 37.277122497558594 test_loss:546.90966796875\n",
      "2347/3000 train_loss: 39.8745231628418 test_loss:547.6244506835938\n",
      "2348/3000 train_loss: 38.64010238647461 test_loss:540.18603515625\n",
      "2349/3000 train_loss: 38.151004791259766 test_loss:546.2353515625\n",
      "2350/3000 train_loss: 40.33906173706055 test_loss:541.555419921875\n",
      "2351/3000 train_loss: 33.726585388183594 test_loss:540.62353515625\n",
      "2352/3000 train_loss: 37.579994201660156 test_loss:538.161865234375\n",
      "2353/3000 train_loss: 36.318702697753906 test_loss:538.3338623046875\n",
      "2354/3000 train_loss: 37.40837860107422 test_loss:546.9562377929688\n",
      "2355/3000 train_loss: 37.87569808959961 test_loss:536.6029663085938\n",
      "2356/3000 train_loss: 41.49579620361328 test_loss:539.8116455078125\n",
      "2357/3000 train_loss: 42.07583236694336 test_loss:539.1420288085938\n",
      "2358/3000 train_loss: 37.52530288696289 test_loss:546.8353271484375\n",
      "2359/3000 train_loss: 45.323211669921875 test_loss:544.1434936523438\n",
      "2360/3000 train_loss: 40.90289306640625 test_loss:553.4473876953125\n",
      "2361/3000 train_loss: 40.9315185546875 test_loss:543.1720581054688\n",
      "2362/3000 train_loss: 38.426048278808594 test_loss:551.2002563476562\n",
      "2363/3000 train_loss: 37.813419342041016 test_loss:538.7445068359375\n",
      "2364/3000 train_loss: 37.837074279785156 test_loss:542.136962890625\n",
      "2365/3000 train_loss: 40.83500671386719 test_loss:548.8673095703125\n",
      "2366/3000 train_loss: 37.546024322509766 test_loss:545.4625244140625\n",
      "2367/3000 train_loss: 39.985565185546875 test_loss:533.87646484375\n",
      "2368/3000 train_loss: 41.06362533569336 test_loss:540.4618530273438\n",
      "2369/3000 train_loss: 43.686180114746094 test_loss:551.396240234375\n",
      "2370/3000 train_loss: 35.140262603759766 test_loss:549.1590576171875\n",
      "2371/3000 train_loss: 37.3060302734375 test_loss:544.0690307617188\n",
      "2372/3000 train_loss: 39.772422790527344 test_loss:547.9723510742188\n",
      "2373/3000 train_loss: 41.466033935546875 test_loss:551.5697631835938\n",
      "2374/3000 train_loss: 36.68315887451172 test_loss:550.895751953125\n",
      "2375/3000 train_loss: 36.82961654663086 test_loss:542.4197998046875\n",
      "2376/3000 train_loss: 39.2850341796875 test_loss:550.692138671875\n",
      "2377/3000 train_loss: 39.20759582519531 test_loss:549.0082397460938\n",
      "2378/3000 train_loss: 38.43024444580078 test_loss:551.731201171875\n",
      "2379/3000 train_loss: 38.26934051513672 test_loss:550.0637817382812\n",
      "2380/3000 train_loss: 41.19401550292969 test_loss:546.06787109375\n",
      "2381/3000 train_loss: 36.41497802734375 test_loss:566.4309692382812\n",
      "2382/3000 train_loss: 39.96953582763672 test_loss:547.5849609375\n",
      "2383/3000 train_loss: 40.414390563964844 test_loss:550.3802490234375\n",
      "2384/3000 train_loss: 35.54616165161133 test_loss:549.3296508789062\n",
      "2385/3000 train_loss: 36.98723220825195 test_loss:553.9996337890625\n",
      "2386/3000 train_loss: 37.6854248046875 test_loss:544.803466796875\n",
      "2387/3000 train_loss: 45.092811584472656 test_loss:555.391845703125\n",
      "2388/3000 train_loss: 41.872718811035156 test_loss:533.842529296875\n",
      "2389/3000 train_loss: 35.006591796875 test_loss:548.638916015625\n",
      "2390/3000 train_loss: 34.510887145996094 test_loss:547.263916015625\n",
      "2391/3000 train_loss: 37.5917854309082 test_loss:541.1472778320312\n",
      "2392/3000 train_loss: 38.274085998535156 test_loss:547.1624145507812\n",
      "2393/3000 train_loss: 39.08536148071289 test_loss:543.4697875976562\n",
      "2394/3000 train_loss: 37.5244255065918 test_loss:548.3860473632812\n",
      "2395/3000 train_loss: 35.045387268066406 test_loss:544.9983520507812\n",
      "2396/3000 train_loss: 37.11232376098633 test_loss:552.4713745117188\n",
      "2397/3000 train_loss: 37.91704177856445 test_loss:554.93896484375\n",
      "2398/3000 train_loss: 35.219764709472656 test_loss:546.5458984375\n",
      "2399/3000 train_loss: 36.23798370361328 test_loss:541.7957763671875\n",
      "2400/3000 train_loss: 33.93056869506836 test_loss:544.0283203125\n",
      "2401/3000 train_loss: 40.68278884887695 test_loss:546.4612426757812\n",
      "2402/3000 train_loss: 33.91590118408203 test_loss:538.9415893554688\n",
      "2403/3000 train_loss: 35.53192138671875 test_loss:541.1321411132812\n",
      "2404/3000 train_loss: 40.989837646484375 test_loss:547.1226806640625\n",
      "2405/3000 train_loss: 38.027713775634766 test_loss:540.1901245117188\n",
      "2406/3000 train_loss: 35.1363639831543 test_loss:544.9335327148438\n",
      "2407/3000 train_loss: 46.39689254760742 test_loss:546.40478515625\n",
      "2408/3000 train_loss: 35.66386795043945 test_loss:545.9705200195312\n",
      "2409/3000 train_loss: 41.43272399902344 test_loss:541.3165283203125\n",
      "2410/3000 train_loss: 39.19355773925781 test_loss:547.09814453125\n",
      "2411/3000 train_loss: 36.13861083984375 test_loss:547.4828491210938\n",
      "2412/3000 train_loss: 36.48811340332031 test_loss:543.48291015625\n",
      "2413/3000 train_loss: 33.37284851074219 test_loss:544.9669189453125\n",
      "2414/3000 train_loss: 32.22821044921875 test_loss:545.4601440429688\n",
      "2415/3000 train_loss: 36.056358337402344 test_loss:546.1841430664062\n",
      "2416/3000 train_loss: 35.53193664550781 test_loss:554.1458740234375\n",
      "2417/3000 train_loss: 45.398536682128906 test_loss:547.2716064453125\n",
      "2418/3000 train_loss: 39.64616012573242 test_loss:549.3253173828125\n",
      "2419/3000 train_loss: 35.962379455566406 test_loss:551.3892822265625\n",
      "2420/3000 train_loss: 36.94968795776367 test_loss:548.9705810546875\n",
      "2421/3000 train_loss: 40.57961654663086 test_loss:552.3013916015625\n",
      "2422/3000 train_loss: 33.380062103271484 test_loss:554.1189575195312\n",
      "2423/3000 train_loss: 41.188621520996094 test_loss:556.114013671875\n",
      "2424/3000 train_loss: 32.95982360839844 test_loss:545.4309692382812\n",
      "2425/3000 train_loss: 37.197723388671875 test_loss:549.857421875\n",
      "2426/3000 train_loss: 36.07158660888672 test_loss:548.62646484375\n",
      "2427/3000 train_loss: 31.7774658203125 test_loss:552.838134765625\n",
      "2428/3000 train_loss: 39.89558792114258 test_loss:545.6194458007812\n",
      "2429/3000 train_loss: 38.1799201965332 test_loss:548.74462890625\n",
      "2430/3000 train_loss: 33.281524658203125 test_loss:543.1300048828125\n",
      "2431/3000 train_loss: 37.65697479248047 test_loss:548.3785400390625\n",
      "2432/3000 train_loss: 35.65803527832031 test_loss:549.6956176757812\n",
      "2433/3000 train_loss: 41.462223052978516 test_loss:543.6366577148438\n",
      "2434/3000 train_loss: 44.944889068603516 test_loss:542.7227783203125\n",
      "2435/3000 train_loss: 35.906150817871094 test_loss:538.7432250976562\n",
      "2436/3000 train_loss: 36.3593635559082 test_loss:539.9154663085938\n",
      "2437/3000 train_loss: 39.970542907714844 test_loss:539.142578125\n",
      "2438/3000 train_loss: 40.241939544677734 test_loss:544.5487670898438\n",
      "2439/3000 train_loss: 38.645233154296875 test_loss:540.5781860351562\n",
      "2440/3000 train_loss: 35.345829010009766 test_loss:532.2808837890625\n",
      "2441/3000 train_loss: 40.88241958618164 test_loss:538.2974243164062\n",
      "2442/3000 train_loss: 35.49185562133789 test_loss:547.1341552734375\n",
      "2443/3000 train_loss: 37.203941345214844 test_loss:536.17578125\n",
      "2444/3000 train_loss: 36.165958404541016 test_loss:550.4216918945312\n",
      "2445/3000 train_loss: 32.40231704711914 test_loss:543.4567260742188\n",
      "2446/3000 train_loss: 42.311622619628906 test_loss:541.8693237304688\n",
      "2447/3000 train_loss: 38.38130569458008 test_loss:547.6253051757812\n",
      "2448/3000 train_loss: 39.53521728515625 test_loss:553.0485229492188\n",
      "2449/3000 train_loss: 32.51526641845703 test_loss:552.0828247070312\n",
      "2450/3000 train_loss: 43.12970733642578 test_loss:548.137451171875\n",
      "2451/3000 train_loss: 38.908424377441406 test_loss:542.2437133789062\n",
      "2452/3000 train_loss: 33.776466369628906 test_loss:537.0548095703125\n",
      "2453/3000 train_loss: 42.2374153137207 test_loss:530.8150634765625\n",
      "2454/3000 train_loss: 43.4764404296875 test_loss:541.8001708984375\n",
      "2455/3000 train_loss: 36.174224853515625 test_loss:536.9256591796875\n",
      "2456/3000 train_loss: 39.85195541381836 test_loss:548.64404296875\n",
      "2457/3000 train_loss: 37.24639892578125 test_loss:541.1329345703125\n",
      "2458/3000 train_loss: 39.348060607910156 test_loss:545.9642333984375\n",
      "2459/3000 train_loss: 35.715450286865234 test_loss:543.9354248046875\n",
      "2460/3000 train_loss: 34.74507141113281 test_loss:538.395263671875\n",
      "2461/3000 train_loss: 34.10771942138672 test_loss:546.7152709960938\n",
      "2462/3000 train_loss: 34.779876708984375 test_loss:544.40185546875\n",
      "2463/3000 train_loss: 39.61605453491211 test_loss:543.55615234375\n",
      "2464/3000 train_loss: 41.561222076416016 test_loss:558.06005859375\n",
      "2465/3000 train_loss: 38.0612678527832 test_loss:542.0032348632812\n",
      "2466/3000 train_loss: 37.46615982055664 test_loss:547.6912231445312\n",
      "2467/3000 train_loss: 35.932926177978516 test_loss:547.5711059570312\n",
      "2468/3000 train_loss: 41.0062255859375 test_loss:552.964599609375\n",
      "2469/3000 train_loss: 39.94660186767578 test_loss:543.6411743164062\n",
      "2470/3000 train_loss: 36.98167419433594 test_loss:546.7022705078125\n",
      "2471/3000 train_loss: 37.187232971191406 test_loss:550.1243286132812\n",
      "2472/3000 train_loss: 40.874427795410156 test_loss:553.0873413085938\n",
      "2473/3000 train_loss: 35.87105178833008 test_loss:536.3800659179688\n",
      "2474/3000 train_loss: 41.94993591308594 test_loss:540.9332885742188\n",
      "2475/3000 train_loss: 41.25613784790039 test_loss:550.9654541015625\n",
      "2476/3000 train_loss: 41.37810134887695 test_loss:537.4176025390625\n",
      "2477/3000 train_loss: 36.781131744384766 test_loss:541.33837890625\n",
      "2478/3000 train_loss: 35.085548400878906 test_loss:543.6436767578125\n",
      "2479/3000 train_loss: 37.39824676513672 test_loss:538.9489135742188\n",
      "2480/3000 train_loss: 34.963436126708984 test_loss:540.505126953125\n",
      "2481/3000 train_loss: 41.5074348449707 test_loss:543.8323974609375\n",
      "2482/3000 train_loss: 35.48951721191406 test_loss:542.1095581054688\n",
      "2483/3000 train_loss: 34.93517303466797 test_loss:544.529296875\n",
      "2484/3000 train_loss: 33.80543899536133 test_loss:538.384033203125\n",
      "2485/3000 train_loss: 35.26346969604492 test_loss:543.0906372070312\n",
      "2486/3000 train_loss: 38.123226165771484 test_loss:548.646728515625\n",
      "2487/3000 train_loss: 34.88874816894531 test_loss:545.3231201171875\n",
      "2488/3000 train_loss: 38.67943572998047 test_loss:549.1473388671875\n",
      "2489/3000 train_loss: 37.29844665527344 test_loss:545.3587646484375\n",
      "2490/3000 train_loss: 38.72748565673828 test_loss:540.0089111328125\n",
      "2491/3000 train_loss: 33.8656120300293 test_loss:547.7800903320312\n",
      "2492/3000 train_loss: 39.12020492553711 test_loss:534.2048950195312\n",
      "2493/3000 train_loss: 37.71400451660156 test_loss:534.4273071289062\n",
      "2494/3000 train_loss: 34.04877853393555 test_loss:533.1018676757812\n",
      "2495/3000 train_loss: 41.62548065185547 test_loss:532.6246948242188\n",
      "2496/3000 train_loss: 34.030250549316406 test_loss:543.0438842773438\n",
      "2497/3000 train_loss: 35.48827362060547 test_loss:541.7749633789062\n",
      "2498/3000 train_loss: 32.10821533203125 test_loss:527.8977661132812\n",
      "2499/3000 train_loss: 36.839195251464844 test_loss:535.5901489257812\n",
      "2500/3000 train_loss: 38.94657516479492 test_loss:543.7783203125\n",
      "2501/3000 train_loss: 33.605979919433594 test_loss:548.4481201171875\n",
      "2502/3000 train_loss: 35.1807746887207 test_loss:535.4400024414062\n",
      "2503/3000 train_loss: 35.59297561645508 test_loss:537.4029541015625\n",
      "2504/3000 train_loss: 37.77943420410156 test_loss:536.9931030273438\n",
      "2505/3000 train_loss: 33.1980094909668 test_loss:540.1031494140625\n",
      "2506/3000 train_loss: 35.49137878417969 test_loss:535.4059448242188\n",
      "2507/3000 train_loss: 37.61474609375 test_loss:543.5248413085938\n",
      "2508/3000 train_loss: 35.87729263305664 test_loss:531.3253173828125\n",
      "2509/3000 train_loss: 33.687538146972656 test_loss:544.8466796875\n",
      "2510/3000 train_loss: 32.55400085449219 test_loss:536.0006103515625\n",
      "2511/3000 train_loss: 37.42496109008789 test_loss:540.8760986328125\n",
      "2512/3000 train_loss: 34.016990661621094 test_loss:543.0696411132812\n",
      "2513/3000 train_loss: 37.706180572509766 test_loss:536.6248168945312\n",
      "2514/3000 train_loss: 38.14535903930664 test_loss:542.7413940429688\n",
      "2515/3000 train_loss: 39.644954681396484 test_loss:548.467041015625\n",
      "2516/3000 train_loss: 38.897315979003906 test_loss:537.9254150390625\n",
      "2517/3000 train_loss: 38.999202728271484 test_loss:548.2325439453125\n",
      "2518/3000 train_loss: 36.1479606628418 test_loss:537.7500610351562\n",
      "2519/3000 train_loss: 35.926368713378906 test_loss:542.361328125\n",
      "2520/3000 train_loss: 36.83863830566406 test_loss:533.5909423828125\n",
      "2521/3000 train_loss: 34.61098861694336 test_loss:522.392333984375\n",
      "2522/3000 train_loss: 38.44731521606445 test_loss:542.338623046875\n",
      "2523/3000 train_loss: 32.953895568847656 test_loss:534.5701904296875\n",
      "2524/3000 train_loss: 38.349082946777344 test_loss:537.863037109375\n",
      "2525/3000 train_loss: 38.6365852355957 test_loss:542.604248046875\n",
      "2526/3000 train_loss: 33.634910583496094 test_loss:540.282958984375\n",
      "2527/3000 train_loss: 34.22581481933594 test_loss:532.8973999023438\n",
      "2528/3000 train_loss: 33.465049743652344 test_loss:544.702392578125\n",
      "2529/3000 train_loss: 37.76183319091797 test_loss:546.9189453125\n",
      "2530/3000 train_loss: 38.61933135986328 test_loss:540.0286865234375\n",
      "2531/3000 train_loss: 36.951236724853516 test_loss:537.7599487304688\n",
      "2532/3000 train_loss: 40.57681655883789 test_loss:541.1498413085938\n",
      "2533/3000 train_loss: 35.654415130615234 test_loss:539.185791015625\n",
      "2534/3000 train_loss: 38.9264030456543 test_loss:541.5784912109375\n",
      "2535/3000 train_loss: 34.759849548339844 test_loss:543.6798706054688\n",
      "2536/3000 train_loss: 37.83755111694336 test_loss:546.3773193359375\n",
      "2537/3000 train_loss: 34.65822219848633 test_loss:550.8320922851562\n",
      "2538/3000 train_loss: 42.43558883666992 test_loss:537.2356567382812\n",
      "2539/3000 train_loss: 40.29239273071289 test_loss:547.822265625\n",
      "2540/3000 train_loss: 38.849273681640625 test_loss:539.6217041015625\n",
      "2541/3000 train_loss: 35.16163635253906 test_loss:541.352783203125\n",
      "2542/3000 train_loss: 37.230045318603516 test_loss:552.1442260742188\n",
      "2543/3000 train_loss: 37.17856979370117 test_loss:540.0435791015625\n",
      "2544/3000 train_loss: 34.200340270996094 test_loss:549.9893798828125\n",
      "2545/3000 train_loss: 36.900146484375 test_loss:544.8486328125\n",
      "2546/3000 train_loss: 39.210872650146484 test_loss:538.5714111328125\n",
      "2547/3000 train_loss: 36.624359130859375 test_loss:542.0135498046875\n",
      "2548/3000 train_loss: 36.45356369018555 test_loss:533.299560546875\n",
      "2549/3000 train_loss: 38.602481842041016 test_loss:541.6814575195312\n",
      "2550/3000 train_loss: 36.1159782409668 test_loss:533.228515625\n",
      "2551/3000 train_loss: 38.807899475097656 test_loss:541.151123046875\n",
      "2552/3000 train_loss: 44.305274963378906 test_loss:540.2731323242188\n",
      "2553/3000 train_loss: 34.5467414855957 test_loss:546.2130737304688\n",
      "2554/3000 train_loss: 37.61603546142578 test_loss:545.5762939453125\n",
      "2555/3000 train_loss: 36.27960205078125 test_loss:538.92333984375\n",
      "2556/3000 train_loss: 42.09831237792969 test_loss:549.726806640625\n",
      "2557/3000 train_loss: 39.13456726074219 test_loss:532.1754760742188\n",
      "2558/3000 train_loss: 37.91065216064453 test_loss:534.5280151367188\n",
      "2559/3000 train_loss: 36.810699462890625 test_loss:538.788818359375\n",
      "2560/3000 train_loss: 35.1285514831543 test_loss:542.0704345703125\n",
      "2561/3000 train_loss: 33.75382614135742 test_loss:534.74658203125\n",
      "2562/3000 train_loss: 39.818809509277344 test_loss:536.2626342773438\n",
      "2563/3000 train_loss: 33.44940948486328 test_loss:543.8291625976562\n",
      "2564/3000 train_loss: 39.66246795654297 test_loss:545.4224243164062\n",
      "2565/3000 train_loss: 40.66383743286133 test_loss:533.4583740234375\n",
      "2566/3000 train_loss: 37.834049224853516 test_loss:549.6419067382812\n",
      "2567/3000 train_loss: 37.8449821472168 test_loss:535.74267578125\n",
      "2568/3000 train_loss: 32.41569137573242 test_loss:528.9667358398438\n",
      "2569/3000 train_loss: 39.80332946777344 test_loss:541.8245849609375\n",
      "2570/3000 train_loss: 40.4193115234375 test_loss:540.4341430664062\n",
      "2571/3000 train_loss: 35.131004333496094 test_loss:541.5964965820312\n",
      "2572/3000 train_loss: 39.766544342041016 test_loss:532.0046997070312\n",
      "2573/3000 train_loss: 41.087215423583984 test_loss:539.303955078125\n",
      "2574/3000 train_loss: 37.708221435546875 test_loss:532.0283203125\n",
      "2575/3000 train_loss: 34.60340881347656 test_loss:533.3562622070312\n",
      "2576/3000 train_loss: 41.56371307373047 test_loss:542.9617309570312\n",
      "2577/3000 train_loss: 35.613101959228516 test_loss:532.1236572265625\n",
      "2578/3000 train_loss: 33.0172004699707 test_loss:536.3162841796875\n",
      "2579/3000 train_loss: 34.140865325927734 test_loss:538.0963134765625\n",
      "2580/3000 train_loss: 39.10056686401367 test_loss:535.9158935546875\n",
      "2581/3000 train_loss: 39.990699768066406 test_loss:543.931396484375\n",
      "2582/3000 train_loss: 36.464561462402344 test_loss:543.4786987304688\n",
      "2583/3000 train_loss: 33.30854415893555 test_loss:545.176025390625\n",
      "2584/3000 train_loss: 35.996910095214844 test_loss:548.7347412109375\n",
      "2585/3000 train_loss: 36.60292434692383 test_loss:544.2139892578125\n",
      "2586/3000 train_loss: 35.3728141784668 test_loss:533.44921875\n",
      "2587/3000 train_loss: 40.11627960205078 test_loss:552.9241333007812\n",
      "2588/3000 train_loss: 38.39616394042969 test_loss:540.331298828125\n",
      "2589/3000 train_loss: 34.21422576904297 test_loss:536.3748779296875\n",
      "2590/3000 train_loss: 35.8924560546875 test_loss:544.1004638671875\n",
      "2591/3000 train_loss: 32.239742279052734 test_loss:541.4771728515625\n",
      "2592/3000 train_loss: 37.13385772705078 test_loss:540.25390625\n",
      "2593/3000 train_loss: 38.77643585205078 test_loss:549.8155517578125\n",
      "2594/3000 train_loss: 35.093143463134766 test_loss:540.9080200195312\n",
      "2595/3000 train_loss: 31.145469665527344 test_loss:540.1270751953125\n",
      "2596/3000 train_loss: 37.47535705566406 test_loss:546.7364501953125\n",
      "2597/3000 train_loss: 40.10813522338867 test_loss:534.8447265625\n",
      "2598/3000 train_loss: 43.73359298706055 test_loss:540.1072998046875\n",
      "2599/3000 train_loss: 34.457462310791016 test_loss:546.8682861328125\n",
      "2600/3000 train_loss: 35.15577697753906 test_loss:545.9962158203125\n",
      "2601/3000 train_loss: 43.690521240234375 test_loss:544.8640747070312\n",
      "2602/3000 train_loss: 40.763465881347656 test_loss:532.1134643554688\n",
      "2603/3000 train_loss: 36.60697937011719 test_loss:537.9110717773438\n",
      "2604/3000 train_loss: 34.69305419921875 test_loss:551.5784912109375\n",
      "2605/3000 train_loss: 36.09373092651367 test_loss:546.4681396484375\n",
      "2606/3000 train_loss: 40.359046936035156 test_loss:538.9027099609375\n",
      "2607/3000 train_loss: 40.52032470703125 test_loss:543.541748046875\n",
      "2608/3000 train_loss: 40.65190887451172 test_loss:541.233642578125\n",
      "2609/3000 train_loss: 31.953609466552734 test_loss:541.8184204101562\n",
      "2610/3000 train_loss: 37.37224578857422 test_loss:545.694580078125\n",
      "2611/3000 train_loss: 36.63397216796875 test_loss:537.125244140625\n",
      "2612/3000 train_loss: 38.988685607910156 test_loss:541.3587646484375\n",
      "2613/3000 train_loss: 39.21622848510742 test_loss:544.0846557617188\n",
      "2614/3000 train_loss: 28.245038986206055 test_loss:544.8133544921875\n",
      "2615/3000 train_loss: 34.253997802734375 test_loss:535.4354858398438\n",
      "2616/3000 train_loss: 36.49873733520508 test_loss:544.50732421875\n",
      "2617/3000 train_loss: 36.463951110839844 test_loss:539.10498046875\n",
      "2618/3000 train_loss: 36.13431930541992 test_loss:537.992919921875\n",
      "2619/3000 train_loss: 36.345829010009766 test_loss:539.3336181640625\n",
      "2620/3000 train_loss: 39.889312744140625 test_loss:540.1674194335938\n",
      "2621/3000 train_loss: 38.63855743408203 test_loss:531.0584716796875\n",
      "2622/3000 train_loss: 39.22799301147461 test_loss:540.15185546875\n",
      "2623/3000 train_loss: 32.0815544128418 test_loss:542.6460571289062\n",
      "2624/3000 train_loss: 37.418907165527344 test_loss:534.18017578125\n",
      "2625/3000 train_loss: 33.095191955566406 test_loss:537.7021484375\n",
      "2626/3000 train_loss: 36.601829528808594 test_loss:542.5296020507812\n",
      "2627/3000 train_loss: 32.61019515991211 test_loss:541.5150146484375\n",
      "2628/3000 train_loss: 35.569087982177734 test_loss:548.1118774414062\n",
      "2629/3000 train_loss: 32.262264251708984 test_loss:542.9312744140625\n",
      "2630/3000 train_loss: 32.39746856689453 test_loss:541.345947265625\n",
      "2631/3000 train_loss: 39.67734909057617 test_loss:546.8287963867188\n",
      "2632/3000 train_loss: 34.78835678100586 test_loss:542.2816162109375\n",
      "2633/3000 train_loss: 35.005645751953125 test_loss:540.0365600585938\n",
      "2634/3000 train_loss: 36.527435302734375 test_loss:555.0130615234375\n",
      "2635/3000 train_loss: 38.807373046875 test_loss:542.9385375976562\n",
      "2636/3000 train_loss: 35.29587173461914 test_loss:545.654052734375\n",
      "2637/3000 train_loss: 38.87973403930664 test_loss:538.6435546875\n",
      "2638/3000 train_loss: 35.517967224121094 test_loss:542.7398681640625\n",
      "2639/3000 train_loss: 33.7859992980957 test_loss:546.9037475585938\n",
      "2640/3000 train_loss: 35.55266571044922 test_loss:540.7252807617188\n",
      "2641/3000 train_loss: 33.92599868774414 test_loss:551.1140747070312\n",
      "2642/3000 train_loss: 39.26228713989258 test_loss:539.5427856445312\n",
      "2643/3000 train_loss: 36.49101257324219 test_loss:535.901611328125\n",
      "2644/3000 train_loss: 34.10334014892578 test_loss:543.358154296875\n",
      "2645/3000 train_loss: 36.852439880371094 test_loss:541.472900390625\n",
      "2646/3000 train_loss: 37.35346984863281 test_loss:544.022705078125\n",
      "2647/3000 train_loss: 37.57474136352539 test_loss:543.7747192382812\n",
      "2648/3000 train_loss: 35.92797088623047 test_loss:546.78955078125\n",
      "2649/3000 train_loss: 35.86854934692383 test_loss:547.1890258789062\n",
      "2650/3000 train_loss: 35.594966888427734 test_loss:539.5866088867188\n",
      "2651/3000 train_loss: 40.646663665771484 test_loss:551.109619140625\n",
      "2652/3000 train_loss: 38.54294967651367 test_loss:551.4585571289062\n",
      "2653/3000 train_loss: 40.89200210571289 test_loss:551.8519897460938\n",
      "2654/3000 train_loss: 32.377742767333984 test_loss:547.8386840820312\n",
      "2655/3000 train_loss: 32.735740661621094 test_loss:541.6686401367188\n",
      "2656/3000 train_loss: 39.45372009277344 test_loss:543.89794921875\n",
      "2657/3000 train_loss: 36.65852737426758 test_loss:537.0640869140625\n",
      "2658/3000 train_loss: 37.024452209472656 test_loss:535.1336059570312\n",
      "2659/3000 train_loss: 37.010353088378906 test_loss:536.5699462890625\n",
      "2660/3000 train_loss: 37.6780891418457 test_loss:538.734375\n",
      "2661/3000 train_loss: 36.99686050415039 test_loss:546.50390625\n",
      "2662/3000 train_loss: 36.445411682128906 test_loss:540.6370849609375\n",
      "2663/3000 train_loss: 35.70154571533203 test_loss:533.5508422851562\n",
      "2664/3000 train_loss: 32.12470245361328 test_loss:539.2197265625\n",
      "2665/3000 train_loss: 34.52060317993164 test_loss:537.406494140625\n",
      "2666/3000 train_loss: 38.754798889160156 test_loss:542.2244262695312\n",
      "2667/3000 train_loss: 34.240074157714844 test_loss:542.060546875\n",
      "2668/3000 train_loss: 37.12073516845703 test_loss:543.1168212890625\n",
      "2669/3000 train_loss: 34.4145622253418 test_loss:541.0586547851562\n",
      "2670/3000 train_loss: 42.062255859375 test_loss:536.4244384765625\n",
      "2671/3000 train_loss: 37.923099517822266 test_loss:540.1917724609375\n",
      "2672/3000 train_loss: 34.91697692871094 test_loss:532.5137329101562\n",
      "2673/3000 train_loss: 33.67353057861328 test_loss:533.1524658203125\n",
      "2674/3000 train_loss: 36.94729995727539 test_loss:537.07958984375\n",
      "2675/3000 train_loss: 36.14200210571289 test_loss:541.3416137695312\n",
      "2676/3000 train_loss: 33.60261917114258 test_loss:537.2874145507812\n",
      "2677/3000 train_loss: 38.17640686035156 test_loss:546.4517822265625\n",
      "2678/3000 train_loss: 31.16272735595703 test_loss:536.52294921875\n",
      "2679/3000 train_loss: 34.367408752441406 test_loss:536.9019775390625\n",
      "2680/3000 train_loss: 32.102169036865234 test_loss:539.1482543945312\n",
      "2681/3000 train_loss: 42.80842971801758 test_loss:542.102783203125\n",
      "2682/3000 train_loss: 32.022945404052734 test_loss:547.8196411132812\n",
      "2683/3000 train_loss: 36.91864013671875 test_loss:539.474609375\n",
      "2684/3000 train_loss: 34.02230453491211 test_loss:546.54345703125\n",
      "2685/3000 train_loss: 35.07481002807617 test_loss:537.431640625\n",
      "2686/3000 train_loss: 31.467103958129883 test_loss:542.0537109375\n",
      "2687/3000 train_loss: 36.72684860229492 test_loss:538.90625\n",
      "2688/3000 train_loss: 34.73811721801758 test_loss:542.5730590820312\n",
      "2689/3000 train_loss: 36.092132568359375 test_loss:528.97412109375\n",
      "2690/3000 train_loss: 34.76988220214844 test_loss:545.5606689453125\n",
      "2691/3000 train_loss: 39.99228286743164 test_loss:535.7651977539062\n",
      "2692/3000 train_loss: 37.86972427368164 test_loss:532.4628295898438\n",
      "2693/3000 train_loss: 36.26426315307617 test_loss:536.639404296875\n",
      "2694/3000 train_loss: 39.457984924316406 test_loss:538.26025390625\n",
      "2695/3000 train_loss: 35.560298919677734 test_loss:544.021728515625\n",
      "2696/3000 train_loss: 38.72439193725586 test_loss:530.9254760742188\n",
      "2697/3000 train_loss: 34.082210540771484 test_loss:540.0999145507812\n",
      "2698/3000 train_loss: 35.90793228149414 test_loss:531.8108520507812\n",
      "2699/3000 train_loss: 35.166160583496094 test_loss:542.3675537109375\n",
      "2700/3000 train_loss: 37.35709762573242 test_loss:542.5789794921875\n",
      "2701/3000 train_loss: 32.21497344970703 test_loss:536.88232421875\n",
      "2702/3000 train_loss: 38.069026947021484 test_loss:534.2607421875\n",
      "2703/3000 train_loss: 40.924659729003906 test_loss:549.2977905273438\n",
      "2704/3000 train_loss: 34.40964126586914 test_loss:530.3271484375\n",
      "2705/3000 train_loss: 37.18935775756836 test_loss:536.86279296875\n",
      "2706/3000 train_loss: 32.63943862915039 test_loss:539.0779418945312\n",
      "2707/3000 train_loss: 36.99253463745117 test_loss:542.8695068359375\n",
      "2708/3000 train_loss: 31.674951553344727 test_loss:539.112060546875\n",
      "2709/3000 train_loss: 36.9689826965332 test_loss:544.7942504882812\n",
      "2710/3000 train_loss: 33.642974853515625 test_loss:537.98828125\n",
      "2711/3000 train_loss: 34.12369155883789 test_loss:526.6857299804688\n",
      "2712/3000 train_loss: 36.58011245727539 test_loss:542.2203369140625\n",
      "2713/3000 train_loss: 31.856830596923828 test_loss:537.2129516601562\n",
      "2714/3000 train_loss: 38.61837387084961 test_loss:540.2421264648438\n",
      "2715/3000 train_loss: 39.4307746887207 test_loss:542.108642578125\n",
      "2716/3000 train_loss: 31.776399612426758 test_loss:536.4695434570312\n",
      "2717/3000 train_loss: 39.1152229309082 test_loss:528.8177490234375\n",
      "2718/3000 train_loss: 37.75665283203125 test_loss:534.6858520507812\n",
      "2719/3000 train_loss: 36.446319580078125 test_loss:526.7235717773438\n",
      "2720/3000 train_loss: 34.53117370605469 test_loss:536.9562377929688\n",
      "2721/3000 train_loss: 34.50037384033203 test_loss:535.6972045898438\n",
      "2722/3000 train_loss: 33.42918014526367 test_loss:535.81591796875\n",
      "2723/3000 train_loss: 36.678157806396484 test_loss:534.22216796875\n",
      "2724/3000 train_loss: 43.196876525878906 test_loss:540.887451171875\n",
      "2725/3000 train_loss: 38.41632843017578 test_loss:537.9989013671875\n",
      "2726/3000 train_loss: 31.956180572509766 test_loss:527.9803466796875\n",
      "2727/3000 train_loss: 32.7059440612793 test_loss:538.8292846679688\n",
      "2728/3000 train_loss: 36.403709411621094 test_loss:537.855224609375\n",
      "2729/3000 train_loss: 34.1312141418457 test_loss:530.2274169921875\n",
      "2730/3000 train_loss: 36.856319427490234 test_loss:533.0257568359375\n",
      "2731/3000 train_loss: 35.736873626708984 test_loss:540.807373046875\n",
      "2732/3000 train_loss: 35.44948959350586 test_loss:535.7939453125\n",
      "2733/3000 train_loss: 36.410770416259766 test_loss:526.888671875\n",
      "2734/3000 train_loss: 34.68633270263672 test_loss:528.5303955078125\n",
      "2735/3000 train_loss: 35.72163391113281 test_loss:535.849853515625\n",
      "2736/3000 train_loss: 31.69768524169922 test_loss:532.8797607421875\n",
      "2737/3000 train_loss: 39.57188034057617 test_loss:527.7086791992188\n",
      "2738/3000 train_loss: 35.16398239135742 test_loss:535.6715698242188\n",
      "2739/3000 train_loss: 35.50430679321289 test_loss:536.0811767578125\n",
      "2740/3000 train_loss: 40.7684326171875 test_loss:530.5035400390625\n",
      "2741/3000 train_loss: 35.29769515991211 test_loss:545.0462646484375\n",
      "2742/3000 train_loss: 46.568668365478516 test_loss:538.273681640625\n",
      "2743/3000 train_loss: 33.9173583984375 test_loss:544.6102294921875\n",
      "2744/3000 train_loss: 40.06632995605469 test_loss:536.3992309570312\n",
      "2745/3000 train_loss: 36.4727783203125 test_loss:543.8968505859375\n",
      "2746/3000 train_loss: 32.77208709716797 test_loss:530.6026611328125\n",
      "2747/3000 train_loss: 35.00391387939453 test_loss:538.9227294921875\n",
      "2748/3000 train_loss: 39.52617263793945 test_loss:535.2605590820312\n",
      "2749/3000 train_loss: 32.96418380737305 test_loss:531.593017578125\n",
      "2750/3000 train_loss: 32.12129211425781 test_loss:546.4690551757812\n",
      "2751/3000 train_loss: 36.37619400024414 test_loss:538.8867797851562\n",
      "2752/3000 train_loss: 33.1140022277832 test_loss:538.6212158203125\n",
      "2753/3000 train_loss: 33.67391586303711 test_loss:534.3169555664062\n",
      "2754/3000 train_loss: 35.379573822021484 test_loss:538.4068603515625\n",
      "2755/3000 train_loss: 41.38710021972656 test_loss:533.8226318359375\n",
      "2756/3000 train_loss: 36.76197052001953 test_loss:538.126708984375\n",
      "2757/3000 train_loss: 35.197853088378906 test_loss:539.9727172851562\n",
      "2758/3000 train_loss: 34.948001861572266 test_loss:531.299560546875\n",
      "2759/3000 train_loss: 41.80756759643555 test_loss:542.1591796875\n",
      "2760/3000 train_loss: 32.52138137817383 test_loss:529.6148681640625\n",
      "2761/3000 train_loss: 36.21633529663086 test_loss:530.4276733398438\n",
      "2762/3000 train_loss: 41.34984588623047 test_loss:530.4230346679688\n",
      "2763/3000 train_loss: 37.83959197998047 test_loss:535.0067138671875\n",
      "2764/3000 train_loss: 34.28072738647461 test_loss:533.5223999023438\n",
      "2765/3000 train_loss: 33.166725158691406 test_loss:537.1455078125\n",
      "2766/3000 train_loss: 35.33919906616211 test_loss:535.8480834960938\n",
      "2767/3000 train_loss: 34.57485580444336 test_loss:541.125732421875\n",
      "2768/3000 train_loss: 34.25139236450195 test_loss:537.6666870117188\n",
      "2769/3000 train_loss: 29.980838775634766 test_loss:546.9862670898438\n",
      "2770/3000 train_loss: 37.2154541015625 test_loss:535.4848022460938\n",
      "2771/3000 train_loss: 30.807010650634766 test_loss:540.9462890625\n",
      "2772/3000 train_loss: 30.187761306762695 test_loss:537.4058837890625\n",
      "2773/3000 train_loss: 34.897987365722656 test_loss:532.9172973632812\n",
      "2774/3000 train_loss: 33.74927520751953 test_loss:538.580078125\n",
      "2775/3000 train_loss: 30.5284366607666 test_loss:544.03564453125\n",
      "2776/3000 train_loss: 34.390235900878906 test_loss:535.8177490234375\n",
      "2777/3000 train_loss: 33.621185302734375 test_loss:539.2237548828125\n",
      "2778/3000 train_loss: 40.892311096191406 test_loss:550.3758544921875\n",
      "2779/3000 train_loss: 42.632198333740234 test_loss:534.8941040039062\n",
      "2780/3000 train_loss: 33.077816009521484 test_loss:537.7040405273438\n",
      "2781/3000 train_loss: 35.77222442626953 test_loss:540.2031860351562\n",
      "2782/3000 train_loss: 34.15787887573242 test_loss:535.8001708984375\n",
      "2783/3000 train_loss: 38.50733947753906 test_loss:542.549560546875\n",
      "2784/3000 train_loss: 34.746219635009766 test_loss:538.3843994140625\n",
      "2785/3000 train_loss: 37.038902282714844 test_loss:538.0120849609375\n",
      "2786/3000 train_loss: 32.213462829589844 test_loss:532.48876953125\n",
      "2787/3000 train_loss: 35.87704086303711 test_loss:541.4708251953125\n",
      "2788/3000 train_loss: 37.44759750366211 test_loss:533.0872802734375\n",
      "2789/3000 train_loss: 35.83106994628906 test_loss:540.1028442382812\n",
      "2790/3000 train_loss: 35.93207931518555 test_loss:527.7539672851562\n",
      "2791/3000 train_loss: 36.78702163696289 test_loss:538.2736206054688\n",
      "2792/3000 train_loss: 32.20930480957031 test_loss:533.01953125\n",
      "2793/3000 train_loss: 37.468040466308594 test_loss:536.09521484375\n",
      "2794/3000 train_loss: 34.433658599853516 test_loss:537.0114135742188\n",
      "2795/3000 train_loss: 31.087947845458984 test_loss:530.5176391601562\n",
      "2796/3000 train_loss: 39.2278938293457 test_loss:546.70654296875\n",
      "2797/3000 train_loss: 35.80561447143555 test_loss:524.830322265625\n",
      "2798/3000 train_loss: 31.615123748779297 test_loss:536.5691528320312\n",
      "2799/3000 train_loss: 36.238746643066406 test_loss:528.3416748046875\n",
      "2800/3000 train_loss: 34.946651458740234 test_loss:528.9404296875\n",
      "2801/3000 train_loss: 41.01202392578125 test_loss:535.0135498046875\n",
      "2802/3000 train_loss: 36.15115737915039 test_loss:526.7756958007812\n",
      "2803/3000 train_loss: 34.44240188598633 test_loss:529.6103515625\n",
      "2804/3000 train_loss: 34.79826354980469 test_loss:535.6660766601562\n",
      "2805/3000 train_loss: 37.190643310546875 test_loss:540.3296508789062\n",
      "2806/3000 train_loss: 37.93165588378906 test_loss:530.8718872070312\n",
      "2807/3000 train_loss: 35.8245849609375 test_loss:536.27783203125\n",
      "2808/3000 train_loss: 35.65815353393555 test_loss:531.3916625976562\n",
      "2809/3000 train_loss: 38.20920181274414 test_loss:535.8145141601562\n",
      "2810/3000 train_loss: 34.03019332885742 test_loss:531.4880981445312\n",
      "2811/3000 train_loss: 36.17903137207031 test_loss:540.5509033203125\n",
      "2812/3000 train_loss: 37.16354751586914 test_loss:537.4720458984375\n",
      "2813/3000 train_loss: 32.569435119628906 test_loss:534.6150512695312\n",
      "2814/3000 train_loss: 32.95897674560547 test_loss:530.4593505859375\n",
      "2815/3000 train_loss: 40.85684585571289 test_loss:529.327392578125\n",
      "2816/3000 train_loss: 34.23114776611328 test_loss:531.064453125\n",
      "2817/3000 train_loss: 33.452877044677734 test_loss:535.8985595703125\n",
      "2818/3000 train_loss: 31.77448081970215 test_loss:537.221435546875\n",
      "2819/3000 train_loss: 32.930015563964844 test_loss:538.8303833007812\n",
      "2820/3000 train_loss: 35.16107940673828 test_loss:526.303466796875\n",
      "2821/3000 train_loss: 34.428287506103516 test_loss:534.946044921875\n",
      "2822/3000 train_loss: 29.227022171020508 test_loss:535.3334350585938\n",
      "2823/3000 train_loss: 40.5899772644043 test_loss:536.3564453125\n",
      "2824/3000 train_loss: 34.7050895690918 test_loss:527.38037109375\n",
      "2825/3000 train_loss: 33.272830963134766 test_loss:535.2452392578125\n",
      "2826/3000 train_loss: 30.289859771728516 test_loss:584.0899047851562\n",
      "2827/3000 train_loss: 33.51926040649414 test_loss:535.51806640625\n",
      "2828/3000 train_loss: 34.34510040283203 test_loss:541.1084594726562\n",
      "2829/3000 train_loss: 34.87167739868164 test_loss:531.650634765625\n",
      "2830/3000 train_loss: 32.48900604248047 test_loss:523.9666748046875\n",
      "2831/3000 train_loss: 36.915130615234375 test_loss:534.819580078125\n",
      "2832/3000 train_loss: 33.327735900878906 test_loss:531.507568359375\n",
      "2833/3000 train_loss: 31.912687301635742 test_loss:531.5030517578125\n",
      "2834/3000 train_loss: 33.649356842041016 test_loss:531.6773681640625\n",
      "2835/3000 train_loss: 30.701799392700195 test_loss:529.1320190429688\n",
      "2836/3000 train_loss: 30.197063446044922 test_loss:534.418701171875\n",
      "2837/3000 train_loss: 34.13315200805664 test_loss:527.87109375\n",
      "2838/3000 train_loss: 37.91433334350586 test_loss:530.9886474609375\n",
      "2839/3000 train_loss: 40.27970886230469 test_loss:533.1009521484375\n",
      "2840/3000 train_loss: 35.37106704711914 test_loss:537.4650268554688\n",
      "2841/3000 train_loss: 32.735897064208984 test_loss:535.967529296875\n",
      "2842/3000 train_loss: 38.24686050415039 test_loss:531.8555297851562\n",
      "2843/3000 train_loss: 34.23802185058594 test_loss:529.1773681640625\n",
      "2844/3000 train_loss: 38.693241119384766 test_loss:527.4989013671875\n",
      "2845/3000 train_loss: 32.43356704711914 test_loss:532.8811645507812\n",
      "2846/3000 train_loss: 35.067691802978516 test_loss:530.1007080078125\n",
      "2847/3000 train_loss: 29.77277374267578 test_loss:533.2523193359375\n",
      "2848/3000 train_loss: 34.68748092651367 test_loss:537.0864868164062\n",
      "2849/3000 train_loss: 32.08748245239258 test_loss:522.0691528320312\n",
      "2850/3000 train_loss: 31.473058700561523 test_loss:532.189208984375\n",
      "2851/3000 train_loss: 34.744041442871094 test_loss:529.9000244140625\n",
      "2852/3000 train_loss: 42.67911911010742 test_loss:536.46875\n",
      "2853/3000 train_loss: 33.65620422363281 test_loss:538.7052001953125\n",
      "2854/3000 train_loss: 35.8719482421875 test_loss:533.9521484375\n",
      "2855/3000 train_loss: 30.540042877197266 test_loss:526.2454833984375\n",
      "2856/3000 train_loss: 33.68238830566406 test_loss:534.6087646484375\n",
      "2857/3000 train_loss: 32.82605743408203 test_loss:530.7606201171875\n",
      "2858/3000 train_loss: 34.56780242919922 test_loss:536.99267578125\n",
      "2859/3000 train_loss: 35.10191345214844 test_loss:531.681884765625\n",
      "2860/3000 train_loss: 37.995723724365234 test_loss:533.9254150390625\n",
      "2861/3000 train_loss: 40.576900482177734 test_loss:539.073486328125\n",
      "2862/3000 train_loss: 31.832889556884766 test_loss:529.5287475585938\n",
      "2863/3000 train_loss: 39.38276290893555 test_loss:534.8417358398438\n",
      "2864/3000 train_loss: 34.125885009765625 test_loss:535.650146484375\n",
      "2865/3000 train_loss: 32.207191467285156 test_loss:528.392333984375\n",
      "2866/3000 train_loss: 37.97416305541992 test_loss:531.88671875\n",
      "2867/3000 train_loss: 32.584877014160156 test_loss:526.49267578125\n",
      "2868/3000 train_loss: 34.16303253173828 test_loss:524.9859008789062\n",
      "2869/3000 train_loss: 29.06375503540039 test_loss:533.95703125\n",
      "2870/3000 train_loss: 37.37797546386719 test_loss:538.1442260742188\n",
      "2871/3000 train_loss: 33.767173767089844 test_loss:519.6064453125\n",
      "2872/3000 train_loss: 31.924177169799805 test_loss:522.0851440429688\n",
      "2873/3000 train_loss: 36.005104064941406 test_loss:524.1747436523438\n",
      "2874/3000 train_loss: 31.278255462646484 test_loss:530.4979248046875\n",
      "2875/3000 train_loss: 32.78744125366211 test_loss:535.368896484375\n",
      "2876/3000 train_loss: 38.81192398071289 test_loss:522.63525390625\n",
      "2877/3000 train_loss: 32.127960205078125 test_loss:535.5342407226562\n",
      "2878/3000 train_loss: 34.47214126586914 test_loss:540.3487548828125\n",
      "2879/3000 train_loss: 34.884422302246094 test_loss:533.2568359375\n",
      "2880/3000 train_loss: 29.194976806640625 test_loss:538.3992919921875\n",
      "2881/3000 train_loss: 38.18074417114258 test_loss:535.681884765625\n",
      "2882/3000 train_loss: 41.77145004272461 test_loss:537.725830078125\n",
      "2883/3000 train_loss: 36.9484977722168 test_loss:539.7365112304688\n",
      "2884/3000 train_loss: 34.415523529052734 test_loss:541.4705810546875\n",
      "2885/3000 train_loss: 35.00082015991211 test_loss:539.2107543945312\n",
      "2886/3000 train_loss: 31.519712448120117 test_loss:536.2008056640625\n",
      "2887/3000 train_loss: 33.54318618774414 test_loss:541.6557006835938\n",
      "2888/3000 train_loss: 35.66972732543945 test_loss:544.7652587890625\n",
      "2889/3000 train_loss: 35.003135681152344 test_loss:538.880126953125\n",
      "2890/3000 train_loss: 37.928733825683594 test_loss:540.5519409179688\n",
      "2891/3000 train_loss: 30.52088165283203 test_loss:546.9629516601562\n",
      "2892/3000 train_loss: 35.53502655029297 test_loss:538.4634399414062\n",
      "2893/3000 train_loss: 34.18070602416992 test_loss:545.5902709960938\n",
      "2894/3000 train_loss: 33.5069465637207 test_loss:541.5184326171875\n",
      "2895/3000 train_loss: 34.027862548828125 test_loss:530.7718505859375\n",
      "2896/3000 train_loss: 36.96919631958008 test_loss:536.7831420898438\n",
      "2897/3000 train_loss: 34.75667190551758 test_loss:541.27490234375\n",
      "2898/3000 train_loss: 31.622068405151367 test_loss:537.9838256835938\n",
      "2899/3000 train_loss: 30.016687393188477 test_loss:535.2066650390625\n",
      "2900/3000 train_loss: 36.72874069213867 test_loss:533.1675415039062\n",
      "2901/3000 train_loss: 39.58567810058594 test_loss:533.8238525390625\n",
      "2902/3000 train_loss: 35.25634765625 test_loss:531.5614013671875\n",
      "2903/3000 train_loss: 31.009763717651367 test_loss:539.9884643554688\n",
      "2904/3000 train_loss: 34.25569534301758 test_loss:539.3286743164062\n",
      "2905/3000 train_loss: 38.25506591796875 test_loss:592.9271240234375\n",
      "2906/3000 train_loss: 29.49753761291504 test_loss:540.137939453125\n",
      "2907/3000 train_loss: 33.49602508544922 test_loss:529.5425415039062\n",
      "2908/3000 train_loss: 35.57979965209961 test_loss:537.9029541015625\n",
      "2909/3000 train_loss: 31.024124145507812 test_loss:536.7578125\n",
      "2910/3000 train_loss: 34.62345886230469 test_loss:541.5513305664062\n",
      "2911/3000 train_loss: 37.072418212890625 test_loss:542.4536743164062\n",
      "2912/3000 train_loss: 31.538846969604492 test_loss:538.7078857421875\n",
      "2913/3000 train_loss: 34.38114929199219 test_loss:541.317138671875\n",
      "2914/3000 train_loss: 33.86858367919922 test_loss:534.1873168945312\n",
      "2915/3000 train_loss: 34.130245208740234 test_loss:536.6795654296875\n",
      "2916/3000 train_loss: 36.03694534301758 test_loss:535.0379638671875\n",
      "2917/3000 train_loss: 32.15605163574219 test_loss:534.7262573242188\n",
      "2918/3000 train_loss: 35.48151397705078 test_loss:536.4893188476562\n",
      "2919/3000 train_loss: 35.157779693603516 test_loss:546.2298583984375\n",
      "2920/3000 train_loss: 36.40702438354492 test_loss:534.3334350585938\n",
      "2921/3000 train_loss: 37.10422897338867 test_loss:538.2208251953125\n",
      "2922/3000 train_loss: 31.80314064025879 test_loss:539.6917724609375\n",
      "2923/3000 train_loss: 33.68946075439453 test_loss:544.10498046875\n",
      "2924/3000 train_loss: 32.54816818237305 test_loss:530.0454711914062\n",
      "2925/3000 train_loss: 35.57128143310547 test_loss:543.751953125\n",
      "2926/3000 train_loss: 30.756080627441406 test_loss:535.7345581054688\n",
      "2927/3000 train_loss: 33.3573112487793 test_loss:542.9122314453125\n",
      "2928/3000 train_loss: 36.9402961730957 test_loss:542.48486328125\n",
      "2929/3000 train_loss: 32.67669677734375 test_loss:536.063232421875\n",
      "2930/3000 train_loss: 32.892372131347656 test_loss:539.19873046875\n",
      "2931/3000 train_loss: 35.361106872558594 test_loss:544.7392578125\n",
      "2932/3000 train_loss: 35.42748260498047 test_loss:533.75146484375\n",
      "2933/3000 train_loss: 34.03914260864258 test_loss:529.8187255859375\n",
      "2934/3000 train_loss: 32.09657287597656 test_loss:537.9539794921875\n",
      "2935/3000 train_loss: 30.401865005493164 test_loss:544.8775024414062\n",
      "2936/3000 train_loss: 35.30934524536133 test_loss:544.77001953125\n",
      "2937/3000 train_loss: 32.75949478149414 test_loss:535.22509765625\n",
      "2938/3000 train_loss: 32.189754486083984 test_loss:540.0750122070312\n",
      "2939/3000 train_loss: 36.191375732421875 test_loss:537.7025756835938\n",
      "2940/3000 train_loss: 31.415294647216797 test_loss:532.3201904296875\n",
      "2941/3000 train_loss: 35.620574951171875 test_loss:538.9663696289062\n",
      "2942/3000 train_loss: 31.73981475830078 test_loss:535.38330078125\n",
      "2943/3000 train_loss: 30.508386611938477 test_loss:538.4107666015625\n",
      "2944/3000 train_loss: 31.4321346282959 test_loss:539.32421875\n",
      "2945/3000 train_loss: 34.546810150146484 test_loss:532.0176391601562\n",
      "2946/3000 train_loss: 34.856136322021484 test_loss:623.0338134765625\n",
      "2947/3000 train_loss: 36.1993408203125 test_loss:626.193359375\n",
      "2948/3000 train_loss: 36.47368240356445 test_loss:536.8634033203125\n",
      "2949/3000 train_loss: 31.540489196777344 test_loss:653.80419921875\n",
      "2950/3000 train_loss: 31.818052291870117 test_loss:620.7638549804688\n",
      "2951/3000 train_loss: 28.602510452270508 test_loss:617.7269287109375\n",
      "2952/3000 train_loss: 35.31631088256836 test_loss:608.3037109375\n",
      "2953/3000 train_loss: 34.84004211425781 test_loss:630.482666015625\n",
      "2954/3000 train_loss: 31.713590621948242 test_loss:606.8050537109375\n",
      "2955/3000 train_loss: 33.06058120727539 test_loss:576.8480834960938\n",
      "2956/3000 train_loss: 36.39934539794922 test_loss:604.3509521484375\n",
      "2957/3000 train_loss: 34.093971252441406 test_loss:616.5550537109375\n",
      "2958/3000 train_loss: 35.43589782714844 test_loss:585.745361328125\n",
      "2959/3000 train_loss: 34.11307144165039 test_loss:539.12890625\n",
      "2960/3000 train_loss: 33.55889892578125 test_loss:613.7093505859375\n",
      "2961/3000 train_loss: 36.1490592956543 test_loss:547.3663330078125\n",
      "2962/3000 train_loss: 33.945621490478516 test_loss:536.4036254882812\n",
      "2963/3000 train_loss: 37.30773162841797 test_loss:537.1368408203125\n",
      "2964/3000 train_loss: 32.50703430175781 test_loss:542.9505615234375\n",
      "2965/3000 train_loss: 30.494848251342773 test_loss:535.5493774414062\n",
      "2966/3000 train_loss: 34.590415954589844 test_loss:536.5280151367188\n",
      "2967/3000 train_loss: 37.247657775878906 test_loss:532.298828125\n",
      "2968/3000 train_loss: 35.763545989990234 test_loss:533.7431640625\n",
      "2969/3000 train_loss: 41.469669342041016 test_loss:534.059326171875\n",
      "2970/3000 train_loss: 34.61580276489258 test_loss:622.3161010742188\n",
      "2971/3000 train_loss: 33.422462463378906 test_loss:552.406982421875\n",
      "2972/3000 train_loss: 30.19086456298828 test_loss:527.3488159179688\n",
      "2973/3000 train_loss: 32.71029281616211 test_loss:644.52099609375\n",
      "2974/3000 train_loss: 36.93278121948242 test_loss:528.5233154296875\n",
      "2975/3000 train_loss: 33.563480377197266 test_loss:593.259765625\n",
      "2976/3000 train_loss: 35.646934509277344 test_loss:534.1900634765625\n",
      "2977/3000 train_loss: 30.961008071899414 test_loss:537.7203979492188\n",
      "2978/3000 train_loss: 32.83561706542969 test_loss:536.5335693359375\n",
      "2979/3000 train_loss: 34.63798522949219 test_loss:536.029296875\n",
      "2980/3000 train_loss: 34.7160758972168 test_loss:643.4226684570312\n",
      "2981/3000 train_loss: 32.540985107421875 test_loss:656.5623779296875\n",
      "2982/3000 train_loss: 32.5448112487793 test_loss:636.0506591796875\n",
      "2983/3000 train_loss: 36.7579460144043 test_loss:538.8367309570312\n",
      "2984/3000 train_loss: 32.14652633666992 test_loss:538.4523315429688\n",
      "2985/3000 train_loss: 39.85993194580078 test_loss:577.167236328125\n",
      "2986/3000 train_loss: 33.49518585205078 test_loss:546.28515625\n",
      "2987/3000 train_loss: 37.22486877441406 test_loss:537.8861694335938\n",
      "2988/3000 train_loss: 33.610225677490234 test_loss:534.362060546875\n",
      "2989/3000 train_loss: 31.617828369140625 test_loss:537.877685546875\n",
      "2990/3000 train_loss: 34.78570556640625 test_loss:543.0636596679688\n",
      "2991/3000 train_loss: 30.173370361328125 test_loss:538.7977294921875\n",
      "2992/3000 train_loss: 32.48200988769531 test_loss:536.73681640625\n",
      "2993/3000 train_loss: 36.76152038574219 test_loss:543.4893798828125\n",
      "2994/3000 train_loss: 33.293670654296875 test_loss:532.5913696289062\n",
      "2995/3000 train_loss: 33.80852508544922 test_loss:536.707763671875\n",
      "2996/3000 train_loss: 35.644187927246094 test_loss:531.3836059570312\n",
      "2997/3000 train_loss: 34.59646224975586 test_loss:540.53369140625\n",
      "2998/3000 train_loss: 33.276187896728516 test_loss:538.46044921875\n",
      "2999/3000 train_loss: 38.53068542480469 test_loss:543.0560302734375\n",
      "3000/3000 train_loss: 36.023441314697266 test_loss:538.23876953125\n"
     ]
    }
   ],
   "source": [
    "losses = train(model = unet, optimizer = optimizer, criterion=criterion, data_tr=train_data,\n",
    "               data_val = test_data, scheduler = scheduler,device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "6Ew7_F0-q7aL",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "98089e90-61b2-4a64-dc6f-5ba2cb35e881"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(538.2388)"
      ]
     },
     "metadata": {},
     "execution_count": 98
    }
   ],
   "source": [
    "avg_loss = 0\n",
    "# unet.eval()\n",
    "# unet.train()\n",
    "preds = []\n",
    "i = 0\n",
    "test_anomaly_losses = []\n",
    "test_normal_losses = []\n",
    "test_losses = []\n",
    "test_real = y_test.tolist()\n",
    "for batch in test_data:\n",
    "  with torch.no_grad():\n",
    "    # unet.train()\n",
    "    predictions = unet(batch.to(device)).cpu()\n",
    "    preds.append(predictions)\n",
    "  loss = criterion(predictions, batch.cpu())\n",
    "  for j in range(len(predictions)):\n",
    "    if int(y_test[i]) == 1:\n",
    "      test_anomaly_losses.append(float(criterion(predictions[j], batch[j])))\n",
    "    else:\n",
    "      test_normal_losses.append(float(criterion(predictions[j], batch[j])))\n",
    "    i += 1\n",
    "    test_losses.append(criterion(predictions[j], batch[j]))\n",
    "  # print(loss)\n",
    "  # print(loss)\n",
    "  avg_loss += loss / len(test_data)\n",
    "# avg_loss\n",
    "\n",
    "avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "VpDKorrRso9o",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "465b9662-5789-4cb0-d840-9b485b27904c"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(804.6374017333984, 57.98713988304138)"
      ]
     },
     "metadata": {},
     "execution_count": 99
    }
   ],
   "source": [
    "sum(test_anomaly_losses)/len(test_anomaly_losses) , sum(test_normal_losses)/len(test_normal_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5LEvbZKYuh7J",
    "outputId": "55632f42-eb34-4e0c-f661-c55244dd7e4d"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "vals = np.arange(50, 1000, 0.5).tolist()\n",
    "for threshold in vals:\n",
    "  preds = []\n",
    "  for j in range(len(test_losses)):\n",
    "    if test_losses[j] > threshold:\n",
    "      preds.append(1)\n",
    "    else:\n",
    "      preds.append(0)\n",
    "  \n",
    "  results.append(roc_auc_score(test_real,preds))\n",
    "\n",
    "print(max(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QaSSqG8SbAw2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
