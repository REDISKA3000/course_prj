{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "9SStKf4G0V5H"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "import torchaudio\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from skimage.util import img_as_ubyte\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import io\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_XtxbKLZq5KX",
    "outputId": "0c6de58c-f736-440b-85ce-a6ad2ae77c29"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CYxHegIM0Z4i",
    "outputId": "8b35e1ba-3d36-4aca-c569-e94fbad47a14"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive/\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "h9DATQwS0ivD"
   },
   "outputs": [],
   "source": [
    "class MimiiDataset(Dataset):\n",
    "    def __init__(self,audio_dir, n_fft = 1024, win_length = 1024,\n",
    "                 hop_length = 512,power = 2,n_mels = 128,pad_mode = 'reflect',\n",
    "                 sr = 16000,center = True,norm = None):\n",
    "      \n",
    "        super(MimiiDataset, self).__init__()\n",
    "        self.audio_dir = audio_dir\n",
    "        self.n_mels = n_mels\n",
    "        self.n_fft = n_fft\n",
    "        self.win_length = win_length\n",
    "        self.hop_length = hop_length\n",
    "        self.power = power\n",
    "        self.pad_mode = pad_mode\n",
    "        self.sr = sr\n",
    "        self.center = center\n",
    "        self.norm = norm\n",
    "\n",
    "    def get_files(self):\n",
    "       return self.train_files, self.test_files\n",
    "    \n",
    "    def get_data(self,device, id):\n",
    "        \n",
    "        self.train_files, self.train_labels = self._train_file_list(device, id)\n",
    "        self.test_files, self.test_labels = self._test_file_list(device, id)\n",
    "        \n",
    "        self.train_data = self.get_audios(self.train_files)\n",
    "        self.test_data = self.get_audios(self.test_files)\n",
    "        \n",
    "        return self.train_data, self.test_data, self.train_labels, self.test_labels\n",
    "    \n",
    "    def _train_file_list(self, device, id):\n",
    "        query = os.path.abspath(\n",
    "            f\"{self.audio_dir}/{device}/train/normal_id_0{id}*.wav\"\n",
    "        )\n",
    "        train_normal_files = sorted(glob.glob(query))\n",
    "        train_normal_labels = np.zeros(len(train_normal_files))\n",
    "        \n",
    "        query = os.path.abspath(\n",
    "                f\"{self.audio_dir}/{device}/train/anomaly_id_0{id}*.wav\"\n",
    "            )\n",
    "        train_anomaly_files = sorted(glob.glob(query))\n",
    "        train_anomaly_labels = np.ones(len(train_anomaly_files))\n",
    "        \n",
    "        train_file_list = np.concatenate((train_normal_files, train_anomaly_files), axis=0)\n",
    "        train_labels = np.concatenate((train_normal_labels, train_anomaly_labels), axis=0)\n",
    "        \n",
    "        return train_file_list, train_labels\n",
    "    \n",
    "    def _test_file_list(self, device, id):     \n",
    "        query = os.path.abspath(\n",
    "            f\"{self.audio_dir}/{device}/test/normal_id_0{id}*.wav\"\n",
    "            )\n",
    "        test_normal_files = sorted(glob.glob(query))\n",
    "        test_normal_labels = np.zeros(len(test_normal_files))\n",
    "        \n",
    "        query = os.path.abspath(\n",
    "            f\"{self.audio_dir}/{device}/test/anomaly_id_0{id}*.wav\"\n",
    "            )\n",
    "        test_anomaly_files = sorted(glob.glob(query))\n",
    "        test_anomaly_labels = np.ones(len(test_anomaly_files))\n",
    "        \n",
    "        test_file_list = np.concatenate((test_normal_files, \n",
    "                                          test_anomaly_files), axis=0)\n",
    "        test_labels = np.concatenate((test_normal_labels,\n",
    "                                      test_anomaly_labels), axis=0)\n",
    "          \n",
    "        return test_file_list, test_labels\n",
    "\n",
    "    def normalize(self,tensor):\n",
    "        tensor_minusmean = tensor - tensor.mean()\n",
    "        return tensor_minusmean/np.absolute(tensor_minusmean).max()\n",
    "\n",
    "    def make0min(self,tensornd):\n",
    "        tensor = tensornd.numpy()\n",
    "        res = np.where(tensor == 0, 1E-19 , tensor)\n",
    "        return torch.from_numpy(res)\n",
    "\n",
    "    def spectrogrameToImage(self,specgram):\n",
    "        # specgram = torchaudio.transforms.MelSpectrogram(n_fft=1024, win_length=1024, \n",
    "        #                                                 hop_length=512, power=2, \n",
    "        #                                                 normalized=True, n_mels=128)(waveform )\n",
    "        specgram= self.make0min(specgram)\n",
    "        specgram = specgram.log2()[0,:,:].numpy()\n",
    "        \n",
    "        tr2image = transforms.Compose([transforms.ToPILImage()])\n",
    "\n",
    "        specgram= self.normalize(specgram)\n",
    "        # specgram = img_as_ubyte(specgram)\n",
    "        specgramImage = tr2image(specgram)\n",
    "        return specgramImage\n",
    "\n",
    "    def get_logmelspectrogram(self, waveform):\n",
    "        melspec = librosa.feature.melspectrogram(\n",
    "          n_fft=self.n_fft, win_length=self.win_length, hop_length=self.hop_length,\n",
    "          power=self.power,n_mels=self.n_mels,pad_mode=self.pad_mode,sr=self.sr,\n",
    "          center=self.center,norm=self.norm,htk=True,\n",
    "          y = waveform.numpy()\n",
    "        )\n",
    "\n",
    "        logmelspec = librosa.power_to_db(melspec)\n",
    "\n",
    "        return logmelspec\n",
    "\n",
    "    def get_melspectrogram(self,waveform):\n",
    "        melspec = librosa.feature.melspectrogram(\n",
    "            n_fft=self.n_fft, win_length=self.win_length, hop_length=self.hop_length,\n",
    "            power=self.power,n_mels=self.n_mels,pad_mode=self.pad_mode,sr=self.sr,\n",
    "            center=self.center,norm=self.norm,htk=True,\n",
    "            y = waveform.numpy()\n",
    "        )\n",
    "\n",
    "        return melspec\n",
    "    \n",
    "    def get_mfcc(self,waveform):\n",
    "        mfcc = librosa.feature.mfcc(    \n",
    "            n_fft=self.n_fft, win_length=self.win_length, \n",
    "            hop_length=self.hop_length,pad_mode=self.pad_mode,sr=self.sr,\n",
    "            center=self.center,norm=self.norm,n_mfcc=40,\n",
    "            y = waveform.numpy()\n",
    "        )\n",
    "\n",
    "        return mfcc\n",
    "\n",
    "    def get_chroma_stft(self,waveform):\n",
    "        stft = librosa.feature.chroma_stft(\n",
    "            n_fft=self.n_fft, win_length=self.win_length, \n",
    "            hop_length=self.hop_length,pad_mode=self.pad_mode,sr=self.sr,\n",
    "            center=self.center,norm=self.norm,n_chroma=12,\n",
    "            y=waveform.numpy()\n",
    "        )\n",
    "\n",
    "        return stft\n",
    "\n",
    "    def get_spectral_contrast(self,waveform):\n",
    "        spec_contrast = librosa.feature.spectral_contrast(    \n",
    "            n_fft=self.n_fft, win_length=self.win_length,center=self.center,\n",
    "            hop_length=self.hop_length,pad_mode=self.pad_mode,sr=self.sr,\n",
    "            y = waveform.numpy()\n",
    "        )\n",
    "\n",
    "        return spec_contrast\n",
    "    \n",
    "    def get_tonnetz(self,waveform):\n",
    "        harmonic = librosa.effects.harmonic(waveform.numpy())\n",
    "        tonnetz = librosa.feature.tonnetz(y=harmonic,sr=self.sr)\n",
    "\n",
    "        return tonnetz\n",
    "\n",
    "    def get_audios(self, file_list):\n",
    "        data = []\n",
    "        for i in range(len(file_list)):\n",
    "          y, sr = torchaudio.load(file_list[i])  \n",
    "          data.append(y)\n",
    "\n",
    "        return data\n",
    "    def _derive_data(self, file_list):\n",
    "        train_data = []\n",
    "        test_data = []\n",
    "        train_mode = True\n",
    "        for file_list in [self.train_files, self.test_files]:\n",
    "          tr2tensor = transforms.Compose([transforms.PILToTensor()])\n",
    "          data = []\n",
    "          for j in range(len(file_list)):\n",
    "            y, sr = torchaudio.load(file_list[j])  \n",
    "            spec = self.get_melspectrogram(y)\n",
    "            spec = self.spectrogrameToImage(spec)\n",
    "            spec = spec.convert('RGB')\n",
    "            vectors = tr2tensor(spec)\n",
    "            if train_mode:     \n",
    "              train_data.append(vectors)\n",
    "            else:\n",
    "              test_data.append(vectors)\n",
    "            \n",
    "          train_mode = False\n",
    "                \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "S96soeIc0o13"
   },
   "outputs": [],
   "source": [
    "dataset = MimiiDataset('/content/drive/MyDrive/mimii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "Gn2zdn92doi1"
   },
   "outputs": [],
   "source": [
    "_, _, y_train, y_test = dataset.get_data('slider', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "SgjpeWy_RV1C"
   },
   "outputs": [],
   "source": [
    "train_mixed_f = torch.load('/content/drive/MyDrive/mixed_features/train_mf_slider2.pt')\n",
    "test_mixed_f = torch.load('/content/drive/MyDrive/mixed_features/test_mf_slider2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "jWMPVGu1qiEq"
   },
   "outputs": [],
   "source": [
    "train_data = DataLoader(train_mixed_f, batch_size=64, shuffle = True)\n",
    "test_data = DataLoader(test_mixed_f, batch_size = 64, shuffle= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "vNTBTRe6qnBq"
   },
   "outputs": [],
   "source": [
    "class UNet_FC(nn.Module):\n",
    "\n",
    "  def __init__(self, in_features):\n",
    "    super().__init__()\n",
    "    \n",
    "    self.bn = nn.BatchNorm1d(128)\n",
    "    self.relu = nn.ReLU()\n",
    "\n",
    "    self.fc0 = nn.Linear(in_features=in_features,out_features=in_features)\n",
    "\n",
    "    # encoder\n",
    "    self.fc1 = nn.Linear(in_features=in_features, out_features=128)\n",
    "    self.fc2 = nn.Linear(in_features=128,out_features=128)\n",
    "    self.fc3 = nn.Linear(in_features=128, out_features=128)\n",
    "    self.fc4 = nn.Linear(in_features=128, out_features=128)\n",
    "    self.fc5 = nn.Linear(in_features=128, out_features=8)\n",
    "\n",
    "    # decoder\n",
    "    self.fc6 = nn.Linear(in_features=8, out_features=128)\n",
    "    self.fc7 = nn.Linear(in_features=128*2, out_features=128)\n",
    "    self.fc8 = nn.Linear(in_features=128*2, out_features=128)\n",
    "    self.fc9 = nn.Linear(in_features=128*2, out_features=128)\n",
    "\n",
    "    self.out = nn.Linear(in_features=128*2, out_features=in_features)\n",
    "\n",
    "  def forward(self, x):\n",
    "    input = self.fc0(x)\n",
    "\n",
    "    x1 = self.relu(self.bn(self.fc1(input)))\n",
    "    x2 = self.relu(self.bn(self.fc2(x1)))\n",
    "    x3 = self.relu(self.bn(self.fc3(x2)))\n",
    "    x4 = self.relu(self.bn(self.fc4(x3)))\n",
    "    x5 = self.relu(self.fc5(x4))\n",
    "\n",
    "    xy = [x5, x4, x3, x2, x1]\n",
    "\n",
    "    x6 = self.relu(self.fc6(xy[0]))\n",
    "    con1 = torch.cat((x6,xy[1]), 1) \n",
    "    x7 = self.relu(self.bn(self.fc7(con1)))\n",
    "    con2 = torch.cat((x7,xy[2]), 1)\n",
    "    x8 = self.relu(self.bn(self.fc8(con2)))\n",
    "    con3 = torch.cat((x8,xy[3]), 1)\n",
    "    x9 = self.relu(self.bn(self.fc9(con3)))\n",
    "    con4 = torch.cat((x9,xy[4]), 1)\n",
    "\n",
    "    x10 = self.out(con4)\n",
    "\n",
    "    return x10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ZfgcBtQ3qn5l"
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, data_tr, data_val, scheduler = None,\n",
    "          epochs = 3000, device = 'cpu'):\n",
    "    # X_val, Y_val = next(iter(data_val))\n",
    "    losses = []\n",
    "    prev_avg_loss = 100000\n",
    "    for epoch in range(epochs):\n",
    "        train_avg_loss = 0\n",
    "        test_avg_loss = 0\n",
    "        # model.train()  # train mode\n",
    "        for batch in data_tr:\n",
    "          # data to device\n",
    "          batch = batch.to(device)\n",
    "          # set parameter gradients to zero\n",
    "          optimizer.zero_grad()\n",
    "          # forward\n",
    "          # print(Y_batch.shape)\n",
    "          predictions = model(batch)\n",
    "          loss = criterion(predictions, batch)\n",
    "          loss.backward() # backward-pass\n",
    "          optimizer.step()  # update weights\n",
    "          # calculate loss to show the user\n",
    "          if scheduler:\n",
    "            scheduler.step(loss)\n",
    "          train_avg_loss += loss / len(data_tr)\n",
    "\n",
    "        # model.eval()\n",
    "        for batch in data_val:\n",
    "          with torch.no_grad():\n",
    "            preds = model(batch.to(device)).cpu()\n",
    "            loss = criterion(preds,batch)\n",
    "            test_avg_loss += loss / len(data_val)\n",
    "                    \n",
    "        losses.append(train_avg_loss.item())\n",
    "        # if (epoch+1)%50 == 0:\n",
    "        print(\"{}/{} train_loss: {} test_loss:{}\".format(epoch+1, epochs, train_avg_loss, test_avg_loss))\n",
    "        # if test_avg_loss < 70:\n",
    "        #   break\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "ptkVTF55quOL"
   },
   "outputs": [],
   "source": [
    "unet = UNet_FC(in_features=193).to(device)\n",
    "optimizer = Adam(params = unet.parameters(), lr = 10e-3)\n",
    "# optimizer = Adam(params = unet.parameters())\n",
    "criterion = nn.MSELoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, \n",
    "                                                       min_lr=10e-4, mode = 'min',\n",
    "                                                       patience = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QkfmYl9oXhcB",
    "outputId": "6435257b-76b9-4793-8713-5018c288f29d"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1/3000 train_loss: 339990.71875 test_loss:310919.5\n",
      "2/3000 train_loss: 334865.09375 test_loss:306780.75\n",
      "3/3000 train_loss: 330587.5625 test_loss:300572.21875\n",
      "4/3000 train_loss: 323718.875 test_loss:292148.75\n",
      "5/3000 train_loss: 315028.3125 test_loss:286755.03125\n",
      "6/3000 train_loss: 309773.78125 test_loss:280483.625\n",
      "7/3000 train_loss: 301889.375 test_loss:273601.875\n",
      "8/3000 train_loss: 294156.46875 test_loss:266612.78125\n",
      "9/3000 train_loss: 286470.0625 test_loss:259485.4375\n",
      "10/3000 train_loss: 278452.5 test_loss:251601.453125\n",
      "11/3000 train_loss: 271152.59375 test_loss:243710.96875\n",
      "12/3000 train_loss: 261836.625 test_loss:236102.234375\n",
      "13/3000 train_loss: 252451.9375 test_loss:226937.78125\n",
      "14/3000 train_loss: 243941.21875 test_loss:219080.46875\n",
      "15/3000 train_loss: 234279.515625 test_loss:209667.109375\n",
      "16/3000 train_loss: 224970.203125 test_loss:200290.921875\n",
      "17/3000 train_loss: 214451.0 test_loss:191471.90625\n",
      "18/3000 train_loss: 204846.78125 test_loss:182069.21875\n",
      "19/3000 train_loss: 194827.09375 test_loss:172207.828125\n",
      "20/3000 train_loss: 185369.625 test_loss:163283.84375\n",
      "21/3000 train_loss: 175366.515625 test_loss:154131.296875\n",
      "22/3000 train_loss: 164972.03125 test_loss:145193.03125\n",
      "23/3000 train_loss: 155145.59375 test_loss:135104.03125\n",
      "24/3000 train_loss: 145104.109375 test_loss:125743.8515625\n",
      "25/3000 train_loss: 134948.671875 test_loss:117197.65625\n",
      "26/3000 train_loss: 126293.3515625 test_loss:108435.90625\n",
      "27/3000 train_loss: 117693.84375 test_loss:100102.2109375\n",
      "28/3000 train_loss: 107921.578125 test_loss:91417.0546875\n",
      "29/3000 train_loss: 99405.9140625 test_loss:82772.6015625\n",
      "30/3000 train_loss: 90074.8046875 test_loss:75048.546875\n",
      "31/3000 train_loss: 82158.1875 test_loss:68391.5234375\n",
      "32/3000 train_loss: 74786.046875 test_loss:61151.3359375\n",
      "33/3000 train_loss: 67612.4375 test_loss:55272.0546875\n",
      "34/3000 train_loss: 60181.23828125 test_loss:48669.2734375\n",
      "35/3000 train_loss: 53953.60546875 test_loss:43046.57421875\n",
      "36/3000 train_loss: 48256.01171875 test_loss:37762.8515625\n",
      "37/3000 train_loss: 42608.43359375 test_loss:33319.27734375\n",
      "38/3000 train_loss: 37680.5546875 test_loss:28845.810546875\n",
      "39/3000 train_loss: 33685.04296875 test_loss:25272.640625\n",
      "40/3000 train_loss: 28936.63671875 test_loss:21314.82421875\n",
      "41/3000 train_loss: 25206.98828125 test_loss:18504.791015625\n",
      "42/3000 train_loss: 21655.7421875 test_loss:15697.2919921875\n",
      "43/3000 train_loss: 18712.466796875 test_loss:13499.8115234375\n",
      "44/3000 train_loss: 16144.166015625 test_loss:11319.556640625\n",
      "45/3000 train_loss: 13949.5224609375 test_loss:9662.916015625\n",
      "46/3000 train_loss: 12036.01953125 test_loss:7847.57275390625\n",
      "47/3000 train_loss: 10109.3671875 test_loss:6733.0263671875\n",
      "48/3000 train_loss: 8572.70703125 test_loss:5488.32080078125\n",
      "49/3000 train_loss: 7202.12109375 test_loss:4753.61865234375\n",
      "50/3000 train_loss: 6056.97802734375 test_loss:4133.11669921875\n",
      "51/3000 train_loss: 5448.59228515625 test_loss:3237.383056640625\n",
      "52/3000 train_loss: 4425.77197265625 test_loss:3017.877685546875\n",
      "53/3000 train_loss: 3547.675048828125 test_loss:2331.16552734375\n",
      "54/3000 train_loss: 3251.430908203125 test_loss:2227.35595703125\n",
      "55/3000 train_loss: 2678.668701171875 test_loss:1947.965576171875\n",
      "56/3000 train_loss: 2294.860107421875 test_loss:1735.8543701171875\n",
      "57/3000 train_loss: 1993.72802734375 test_loss:1734.3782958984375\n",
      "58/3000 train_loss: 1847.3018798828125 test_loss:1646.1998291015625\n",
      "59/3000 train_loss: 1611.591064453125 test_loss:1534.66845703125\n",
      "60/3000 train_loss: 1415.36572265625 test_loss:1559.651611328125\n",
      "61/3000 train_loss: 1297.18603515625 test_loss:1552.66748046875\n",
      "62/3000 train_loss: 1259.8004150390625 test_loss:1702.6380615234375\n",
      "63/3000 train_loss: 1094.459228515625 test_loss:1693.45166015625\n",
      "64/3000 train_loss: 1122.2127685546875 test_loss:1702.8741455078125\n",
      "65/3000 train_loss: 1035.206787109375 test_loss:1655.996337890625\n",
      "66/3000 train_loss: 965.5668334960938 test_loss:1538.8704833984375\n",
      "67/3000 train_loss: 921.9495239257812 test_loss:1590.23583984375\n",
      "68/3000 train_loss: 815.2938842773438 test_loss:1589.265380859375\n",
      "69/3000 train_loss: 897.9796142578125 test_loss:1614.9876708984375\n",
      "70/3000 train_loss: 828.5077514648438 test_loss:1579.7398681640625\n",
      "71/3000 train_loss: 792.1746215820312 test_loss:1556.2354736328125\n",
      "72/3000 train_loss: 802.01904296875 test_loss:1606.351806640625\n",
      "73/3000 train_loss: 767.5276489257812 test_loss:1591.144287109375\n",
      "74/3000 train_loss: 781.2681274414062 test_loss:1580.572998046875\n",
      "75/3000 train_loss: 783.742431640625 test_loss:1619.030029296875\n",
      "76/3000 train_loss: 818.0332641601562 test_loss:1595.834716796875\n",
      "77/3000 train_loss: 806.9874267578125 test_loss:1580.561279296875\n",
      "78/3000 train_loss: 766.4411010742188 test_loss:1609.8094482421875\n",
      "79/3000 train_loss: 771.7098999023438 test_loss:1597.17919921875\n",
      "80/3000 train_loss: 883.4428100585938 test_loss:1615.46142578125\n",
      "81/3000 train_loss: 768.2081909179688 test_loss:1609.1221923828125\n",
      "82/3000 train_loss: 743.2869873046875 test_loss:1594.671142578125\n",
      "83/3000 train_loss: 713.6514282226562 test_loss:1623.9046630859375\n",
      "84/3000 train_loss: 726.0155029296875 test_loss:1606.166015625\n",
      "85/3000 train_loss: 761.0156860351562 test_loss:1578.0418701171875\n",
      "86/3000 train_loss: 736.828369140625 test_loss:1598.1649169921875\n",
      "87/3000 train_loss: 732.8983764648438 test_loss:1588.01611328125\n",
      "88/3000 train_loss: 701.1819458007812 test_loss:1625.5673828125\n",
      "89/3000 train_loss: 717.2393798828125 test_loss:1594.259521484375\n",
      "90/3000 train_loss: 698.5665893554688 test_loss:1610.178466796875\n",
      "91/3000 train_loss: 685.4678344726562 test_loss:1611.14990234375\n",
      "92/3000 train_loss: 693.7041625976562 test_loss:1630.0657958984375\n",
      "93/3000 train_loss: 703.75390625 test_loss:1593.298828125\n",
      "94/3000 train_loss: 662.7207641601562 test_loss:1614.93798828125\n",
      "95/3000 train_loss: 658.1226806640625 test_loss:1605.3265380859375\n",
      "96/3000 train_loss: 665.4993896484375 test_loss:1632.55712890625\n",
      "97/3000 train_loss: 671.1109008789062 test_loss:1609.2733154296875\n",
      "98/3000 train_loss: 702.5263671875 test_loss:1648.454345703125\n",
      "99/3000 train_loss: 660.0137939453125 test_loss:1596.9385986328125\n",
      "100/3000 train_loss: 680.1753540039062 test_loss:1656.5494384765625\n",
      "101/3000 train_loss: 671.4066162109375 test_loss:1640.960205078125\n",
      "102/3000 train_loss: 647.9403076171875 test_loss:1651.452880859375\n",
      "103/3000 train_loss: 674.5928344726562 test_loss:1659.26611328125\n",
      "104/3000 train_loss: 676.5339965820312 test_loss:1643.5699462890625\n",
      "105/3000 train_loss: 670.1083984375 test_loss:1752.421630859375\n",
      "106/3000 train_loss: 689.0408325195312 test_loss:1635.7314453125\n",
      "107/3000 train_loss: 612.0048217773438 test_loss:1682.5458984375\n",
      "108/3000 train_loss: 639.7843627929688 test_loss:1613.6246337890625\n",
      "109/3000 train_loss: 627.3462524414062 test_loss:1679.03857421875\n",
      "110/3000 train_loss: 631.2445678710938 test_loss:1615.13134765625\n",
      "111/3000 train_loss: 644.8740234375 test_loss:1671.693359375\n",
      "112/3000 train_loss: 645.2437744140625 test_loss:1633.720703125\n",
      "113/3000 train_loss: 629.1057739257812 test_loss:1660.30615234375\n",
      "114/3000 train_loss: 620.3060913085938 test_loss:1664.856201171875\n",
      "115/3000 train_loss: 614.3187866210938 test_loss:1643.517333984375\n",
      "116/3000 train_loss: 608.9395141601562 test_loss:1721.4754638671875\n",
      "117/3000 train_loss: 644.74267578125 test_loss:1656.794677734375\n",
      "118/3000 train_loss: 615.2918701171875 test_loss:1793.844482421875\n",
      "119/3000 train_loss: 617.1492919921875 test_loss:1634.892822265625\n",
      "120/3000 train_loss: 603.2616577148438 test_loss:1715.58837890625\n",
      "121/3000 train_loss: 629.476318359375 test_loss:1678.044921875\n",
      "122/3000 train_loss: 594.2142333984375 test_loss:1683.0679931640625\n",
      "123/3000 train_loss: 581.5181274414062 test_loss:1704.764404296875\n",
      "124/3000 train_loss: 599.1300659179688 test_loss:1634.968505859375\n",
      "125/3000 train_loss: 583.2981567382812 test_loss:1696.819091796875\n",
      "126/3000 train_loss: 653.5747680664062 test_loss:1666.7274169921875\n",
      "127/3000 train_loss: 601.822509765625 test_loss:1668.9354248046875\n",
      "128/3000 train_loss: 593.765869140625 test_loss:1623.809326171875\n",
      "129/3000 train_loss: 569.117431640625 test_loss:1685.4334716796875\n",
      "130/3000 train_loss: 579.5258178710938 test_loss:1673.3355712890625\n",
      "131/3000 train_loss: 593.1827392578125 test_loss:1635.050537109375\n",
      "132/3000 train_loss: 578.35498046875 test_loss:1672.042724609375\n",
      "133/3000 train_loss: 644.9254150390625 test_loss:1635.8895263671875\n",
      "134/3000 train_loss: 563.567626953125 test_loss:1679.962890625\n",
      "135/3000 train_loss: 599.4921875 test_loss:1642.1220703125\n",
      "136/3000 train_loss: 586.245849609375 test_loss:1683.45849609375\n",
      "137/3000 train_loss: 577.904052734375 test_loss:1652.013427734375\n",
      "138/3000 train_loss: 550.16162109375 test_loss:1605.604248046875\n",
      "139/3000 train_loss: 585.358642578125 test_loss:1669.980712890625\n",
      "140/3000 train_loss: 575.8204345703125 test_loss:1619.771484375\n",
      "141/3000 train_loss: 630.8995971679688 test_loss:1651.8870849609375\n",
      "142/3000 train_loss: 567.7533569335938 test_loss:1638.9949951171875\n",
      "143/3000 train_loss: 554.62451171875 test_loss:1632.552978515625\n",
      "144/3000 train_loss: 568.6483764648438 test_loss:1641.3853759765625\n",
      "145/3000 train_loss: 538.6731567382812 test_loss:1641.1134033203125\n",
      "146/3000 train_loss: 573.8097534179688 test_loss:1643.5361328125\n",
      "147/3000 train_loss: 566.247802734375 test_loss:1646.90966796875\n",
      "148/3000 train_loss: 536.4616088867188 test_loss:1618.6854248046875\n",
      "149/3000 train_loss: 540.9951171875 test_loss:1629.6199951171875\n",
      "150/3000 train_loss: 575.4412841796875 test_loss:1643.471435546875\n",
      "151/3000 train_loss: 541.5877075195312 test_loss:1588.4918212890625\n",
      "152/3000 train_loss: 544.7973022460938 test_loss:1631.7271728515625\n",
      "153/3000 train_loss: 519.8486938476562 test_loss:1634.1192626953125\n",
      "154/3000 train_loss: 707.3522338867188 test_loss:1611.953369140625\n",
      "155/3000 train_loss: 542.6407470703125 test_loss:1686.479248046875\n",
      "156/3000 train_loss: 524.9757080078125 test_loss:1591.4669189453125\n",
      "157/3000 train_loss: 523.5924682617188 test_loss:1590.41796875\n",
      "158/3000 train_loss: 517.671875 test_loss:1580.742919921875\n",
      "159/3000 train_loss: 536.9666748046875 test_loss:1613.431884765625\n",
      "160/3000 train_loss: 520.2015991210938 test_loss:1601.39697265625\n",
      "161/3000 train_loss: 509.872802734375 test_loss:1589.771240234375\n",
      "162/3000 train_loss: 508.9099426269531 test_loss:1594.31396484375\n",
      "163/3000 train_loss: 512.2984619140625 test_loss:1622.2581787109375\n",
      "164/3000 train_loss: 511.62249755859375 test_loss:1614.07080078125\n",
      "165/3000 train_loss: 493.7031555175781 test_loss:1584.471923828125\n",
      "166/3000 train_loss: 530.7621459960938 test_loss:1572.238037109375\n",
      "167/3000 train_loss: 509.7956848144531 test_loss:1609.7939453125\n",
      "168/3000 train_loss: 495.7098693847656 test_loss:1565.31494140625\n",
      "169/3000 train_loss: 515.74560546875 test_loss:1577.8814697265625\n",
      "170/3000 train_loss: 488.45697021484375 test_loss:1594.979248046875\n",
      "171/3000 train_loss: 485.2279968261719 test_loss:1541.3779296875\n",
      "172/3000 train_loss: 486.0190124511719 test_loss:1565.2509765625\n",
      "173/3000 train_loss: 544.2587890625 test_loss:1590.0284423828125\n",
      "174/3000 train_loss: 521.9107666015625 test_loss:1594.418212890625\n",
      "175/3000 train_loss: 471.36273193359375 test_loss:1583.7078857421875\n",
      "176/3000 train_loss: 489.3016052246094 test_loss:1588.107421875\n",
      "177/3000 train_loss: 509.7892150878906 test_loss:1564.4095458984375\n",
      "178/3000 train_loss: 499.47308349609375 test_loss:1587.5810546875\n",
      "179/3000 train_loss: 508.6484375 test_loss:1580.52392578125\n",
      "180/3000 train_loss: 500.30242919921875 test_loss:1582.15087890625\n",
      "181/3000 train_loss: 506.78765869140625 test_loss:1607.5452880859375\n",
      "182/3000 train_loss: 455.7749328613281 test_loss:1608.661865234375\n",
      "183/3000 train_loss: 482.6244812011719 test_loss:1576.021728515625\n",
      "184/3000 train_loss: 473.79510498046875 test_loss:1565.488525390625\n",
      "185/3000 train_loss: 451.78857421875 test_loss:1571.91748046875\n",
      "186/3000 train_loss: 454.18157958984375 test_loss:1576.1533203125\n",
      "187/3000 train_loss: 459.27947998046875 test_loss:1576.151611328125\n",
      "188/3000 train_loss: 463.80859375 test_loss:1556.4173583984375\n",
      "189/3000 train_loss: 460.1520080566406 test_loss:1560.9168701171875\n",
      "190/3000 train_loss: 475.33514404296875 test_loss:1562.2720947265625\n",
      "191/3000 train_loss: 488.17388916015625 test_loss:1543.241455078125\n",
      "192/3000 train_loss: 468.8525695800781 test_loss:1554.1705322265625\n",
      "193/3000 train_loss: 450.4012756347656 test_loss:1550.6663818359375\n",
      "194/3000 train_loss: 474.6405029296875 test_loss:1608.4189453125\n",
      "195/3000 train_loss: 459.2445068359375 test_loss:1526.224609375\n",
      "196/3000 train_loss: 449.3033447265625 test_loss:1537.2279052734375\n",
      "197/3000 train_loss: 460.2964782714844 test_loss:1550.897216796875\n",
      "198/3000 train_loss: 440.3138122558594 test_loss:1533.4464111328125\n",
      "199/3000 train_loss: 432.8582763671875 test_loss:1571.42822265625\n",
      "200/3000 train_loss: 420.15252685546875 test_loss:1540.472900390625\n",
      "201/3000 train_loss: 427.1515808105469 test_loss:1565.9468994140625\n",
      "202/3000 train_loss: 437.7887268066406 test_loss:1548.990478515625\n",
      "203/3000 train_loss: 444.44293212890625 test_loss:1555.838623046875\n",
      "204/3000 train_loss: 419.8115539550781 test_loss:1584.5712890625\n",
      "205/3000 train_loss: 415.25872802734375 test_loss:1561.508544921875\n",
      "206/3000 train_loss: 449.14752197265625 test_loss:1534.977783203125\n",
      "207/3000 train_loss: 445.22821044921875 test_loss:1540.3275146484375\n",
      "208/3000 train_loss: 419.6710205078125 test_loss:1523.3529052734375\n",
      "209/3000 train_loss: 431.8852844238281 test_loss:1531.2724609375\n",
      "210/3000 train_loss: 416.8796081542969 test_loss:1549.51806640625\n",
      "211/3000 train_loss: 423.813720703125 test_loss:1541.3450927734375\n",
      "212/3000 train_loss: 422.71588134765625 test_loss:1559.04736328125\n",
      "213/3000 train_loss: 421.3321838378906 test_loss:1557.970947265625\n",
      "214/3000 train_loss: 424.6650695800781 test_loss:1552.157470703125\n",
      "215/3000 train_loss: 451.64923095703125 test_loss:1540.9039306640625\n",
      "216/3000 train_loss: 413.8038024902344 test_loss:1554.5467529296875\n",
      "217/3000 train_loss: 401.3656005859375 test_loss:1532.1480712890625\n",
      "218/3000 train_loss: 398.3469543457031 test_loss:1545.59033203125\n",
      "219/3000 train_loss: 394.58441162109375 test_loss:1547.25830078125\n",
      "220/3000 train_loss: 415.8371887207031 test_loss:1509.9290771484375\n",
      "221/3000 train_loss: 405.89404296875 test_loss:1508.8360595703125\n",
      "222/3000 train_loss: 393.35089111328125 test_loss:1506.156005859375\n",
      "223/3000 train_loss: 396.16558837890625 test_loss:1563.034912109375\n",
      "224/3000 train_loss: 412.6044921875 test_loss:1558.067138671875\n",
      "225/3000 train_loss: 392.32720947265625 test_loss:1541.0877685546875\n",
      "226/3000 train_loss: 401.8060302734375 test_loss:1513.2314453125\n",
      "227/3000 train_loss: 375.44830322265625 test_loss:1514.301513671875\n",
      "228/3000 train_loss: 389.6816101074219 test_loss:1505.650634765625\n",
      "229/3000 train_loss: 414.7200012207031 test_loss:1479.16455078125\n",
      "230/3000 train_loss: 405.4623107910156 test_loss:1492.7642822265625\n",
      "231/3000 train_loss: 387.48577880859375 test_loss:1507.6337890625\n",
      "232/3000 train_loss: 424.17926025390625 test_loss:1512.174072265625\n",
      "233/3000 train_loss: 372.8912658691406 test_loss:1542.330322265625\n",
      "234/3000 train_loss: 367.91180419921875 test_loss:1523.456787109375\n",
      "235/3000 train_loss: 388.9087219238281 test_loss:1517.169189453125\n",
      "236/3000 train_loss: 400.56646728515625 test_loss:1499.203857421875\n",
      "237/3000 train_loss: 388.623291015625 test_loss:1488.639404296875\n",
      "238/3000 train_loss: 400.745361328125 test_loss:1481.639892578125\n",
      "239/3000 train_loss: 394.4129333496094 test_loss:1512.77099609375\n",
      "240/3000 train_loss: 362.902099609375 test_loss:1512.3389892578125\n",
      "241/3000 train_loss: 415.44525146484375 test_loss:1507.45947265625\n",
      "242/3000 train_loss: 366.5498046875 test_loss:1510.2889404296875\n",
      "243/3000 train_loss: 359.16436767578125 test_loss:1493.779541015625\n",
      "244/3000 train_loss: 381.06939697265625 test_loss:1493.6219482421875\n",
      "245/3000 train_loss: 367.8685302734375 test_loss:1486.05908203125\n",
      "246/3000 train_loss: 372.4704895019531 test_loss:1489.566650390625\n",
      "247/3000 train_loss: 391.49822998046875 test_loss:1487.1419677734375\n",
      "248/3000 train_loss: 368.64227294921875 test_loss:1454.5322265625\n",
      "249/3000 train_loss: 363.0270690917969 test_loss:1444.003662109375\n",
      "250/3000 train_loss: 396.9420471191406 test_loss:1435.0546875\n",
      "251/3000 train_loss: 369.0108947753906 test_loss:1471.1578369140625\n",
      "252/3000 train_loss: 354.16717529296875 test_loss:1472.236572265625\n",
      "253/3000 train_loss: 344.863037109375 test_loss:1450.9810791015625\n",
      "254/3000 train_loss: 357.2124328613281 test_loss:1434.697021484375\n",
      "255/3000 train_loss: 369.5294494628906 test_loss:1457.6732177734375\n",
      "256/3000 train_loss: 360.9044189453125 test_loss:1456.6600341796875\n",
      "257/3000 train_loss: 361.420654296875 test_loss:1457.794921875\n",
      "258/3000 train_loss: 345.6451110839844 test_loss:1477.7088623046875\n",
      "259/3000 train_loss: 361.7682189941406 test_loss:1472.141845703125\n",
      "260/3000 train_loss: 345.8008117675781 test_loss:1452.091552734375\n",
      "261/3000 train_loss: 368.6553955078125 test_loss:1454.908203125\n",
      "262/3000 train_loss: 381.9230041503906 test_loss:1450.895751953125\n",
      "263/3000 train_loss: 368.6667785644531 test_loss:1456.1610107421875\n",
      "264/3000 train_loss: 342.43017578125 test_loss:1458.294189453125\n",
      "265/3000 train_loss: 341.1775817871094 test_loss:1456.9014892578125\n",
      "266/3000 train_loss: 333.87750244140625 test_loss:1458.78759765625\n",
      "267/3000 train_loss: 357.5067443847656 test_loss:1453.2843017578125\n",
      "268/3000 train_loss: 330.8411865234375 test_loss:1431.489013671875\n",
      "269/3000 train_loss: 355.712646484375 test_loss:1453.735107421875\n",
      "270/3000 train_loss: 366.8540954589844 test_loss:1471.8355712890625\n",
      "271/3000 train_loss: 346.0830383300781 test_loss:1424.919677734375\n",
      "272/3000 train_loss: 312.1898498535156 test_loss:1450.647705078125\n",
      "273/3000 train_loss: 335.1065979003906 test_loss:1426.02294921875\n",
      "274/3000 train_loss: 337.1598205566406 test_loss:1458.2255859375\n",
      "275/3000 train_loss: 344.96038818359375 test_loss:1413.70263671875\n",
      "276/3000 train_loss: 318.9375305175781 test_loss:1417.2940673828125\n",
      "277/3000 train_loss: 313.34649658203125 test_loss:1427.85595703125\n",
      "278/3000 train_loss: 318.5077209472656 test_loss:1412.7445068359375\n",
      "279/3000 train_loss: 334.06402587890625 test_loss:1401.46630859375\n",
      "280/3000 train_loss: 317.939453125 test_loss:1400.0850830078125\n",
      "281/3000 train_loss: 316.1289978027344 test_loss:1375.1552734375\n",
      "282/3000 train_loss: 304.5279235839844 test_loss:1369.958251953125\n",
      "283/3000 train_loss: 315.7475280761719 test_loss:1382.321044921875\n",
      "284/3000 train_loss: 298.93646240234375 test_loss:1389.1805419921875\n",
      "285/3000 train_loss: 301.01708984375 test_loss:1398.779541015625\n",
      "286/3000 train_loss: 333.98297119140625 test_loss:1400.123291015625\n",
      "287/3000 train_loss: 305.22991943359375 test_loss:1390.8946533203125\n",
      "288/3000 train_loss: 321.02178955078125 test_loss:1398.0513916015625\n",
      "289/3000 train_loss: 317.3353576660156 test_loss:1401.1864013671875\n",
      "290/3000 train_loss: 310.158447265625 test_loss:1395.06982421875\n",
      "291/3000 train_loss: 385.99365234375 test_loss:1416.9957275390625\n",
      "292/3000 train_loss: 331.1189880371094 test_loss:1422.681640625\n",
      "293/3000 train_loss: 305.62274169921875 test_loss:1400.915283203125\n",
      "294/3000 train_loss: 295.8556213378906 test_loss:1380.2763671875\n",
      "295/3000 train_loss: 297.2432556152344 test_loss:1410.4892578125\n",
      "296/3000 train_loss: 304.350341796875 test_loss:1392.55419921875\n",
      "297/3000 train_loss: 312.02056884765625 test_loss:1386.588623046875\n",
      "298/3000 train_loss: 324.9849853515625 test_loss:1412.4730224609375\n",
      "299/3000 train_loss: 305.78546142578125 test_loss:1395.1959228515625\n",
      "300/3000 train_loss: 316.3074035644531 test_loss:1411.4345703125\n",
      "301/3000 train_loss: 290.51824951171875 test_loss:1409.4000244140625\n",
      "302/3000 train_loss: 283.8343811035156 test_loss:1421.71435546875\n",
      "303/3000 train_loss: 316.99371337890625 test_loss:1387.8323974609375\n",
      "304/3000 train_loss: 299.8226013183594 test_loss:1359.4119873046875\n",
      "305/3000 train_loss: 316.02679443359375 test_loss:1394.332763671875\n",
      "306/3000 train_loss: 300.5430603027344 test_loss:1358.944580078125\n",
      "307/3000 train_loss: 283.6551513671875 test_loss:1365.6650390625\n",
      "308/3000 train_loss: 293.6224060058594 test_loss:1367.9444580078125\n",
      "309/3000 train_loss: 279.3240966796875 test_loss:1375.7974853515625\n",
      "310/3000 train_loss: 294.72625732421875 test_loss:1355.5433349609375\n",
      "311/3000 train_loss: 308.73895263671875 test_loss:1454.3114013671875\n",
      "312/3000 train_loss: 296.72174072265625 test_loss:1383.8411865234375\n",
      "313/3000 train_loss: 286.95318603515625 test_loss:1357.556396484375\n",
      "314/3000 train_loss: 275.8595886230469 test_loss:1359.0374755859375\n",
      "315/3000 train_loss: 272.1778564453125 test_loss:1378.04833984375\n",
      "316/3000 train_loss: 274.71759033203125 test_loss:1388.771728515625\n",
      "317/3000 train_loss: 276.0741882324219 test_loss:1360.0537109375\n",
      "318/3000 train_loss: 285.5482482910156 test_loss:1357.07763671875\n",
      "319/3000 train_loss: 279.6713562011719 test_loss:1372.017578125\n",
      "320/3000 train_loss: 267.1031188964844 test_loss:1393.924072265625\n",
      "321/3000 train_loss: 267.7022705078125 test_loss:1394.0064697265625\n",
      "322/3000 train_loss: 264.97784423828125 test_loss:1409.1307373046875\n",
      "323/3000 train_loss: 267.7972412109375 test_loss:1368.902099609375\n",
      "324/3000 train_loss: 263.5718688964844 test_loss:1374.9351806640625\n",
      "325/3000 train_loss: 264.0867919921875 test_loss:1380.6829833984375\n",
      "326/3000 train_loss: 268.82379150390625 test_loss:1391.9530029296875\n",
      "327/3000 train_loss: 263.4735412597656 test_loss:1379.264404296875\n",
      "328/3000 train_loss: 273.05499267578125 test_loss:1367.6658935546875\n",
      "329/3000 train_loss: 270.0565185546875 test_loss:1384.3587646484375\n",
      "330/3000 train_loss: 270.50738525390625 test_loss:1378.07080078125\n",
      "331/3000 train_loss: 283.7525634765625 test_loss:1369.1409912109375\n",
      "332/3000 train_loss: 279.48681640625 test_loss:1288.7301025390625\n",
      "333/3000 train_loss: 282.3930358886719 test_loss:1290.3267822265625\n",
      "334/3000 train_loss: 253.1278076171875 test_loss:1295.3292236328125\n",
      "335/3000 train_loss: 287.9963684082031 test_loss:1318.6173095703125\n",
      "336/3000 train_loss: 254.45484924316406 test_loss:1324.0045166015625\n",
      "337/3000 train_loss: 258.767822265625 test_loss:1334.3087158203125\n",
      "338/3000 train_loss: 256.84967041015625 test_loss:1314.2198486328125\n",
      "339/3000 train_loss: 253.1329803466797 test_loss:1323.019287109375\n",
      "340/3000 train_loss: 255.7640838623047 test_loss:1343.3900146484375\n",
      "341/3000 train_loss: 255.263916015625 test_loss:1338.5130615234375\n",
      "342/3000 train_loss: 246.57460021972656 test_loss:1322.1551513671875\n",
      "343/3000 train_loss: 264.6730651855469 test_loss:1308.158203125\n",
      "344/3000 train_loss: 292.1234436035156 test_loss:1345.0484619140625\n",
      "345/3000 train_loss: 267.7513732910156 test_loss:1312.5439453125\n",
      "346/3000 train_loss: 258.9300231933594 test_loss:1341.7818603515625\n",
      "347/3000 train_loss: 248.54835510253906 test_loss:1330.97900390625\n",
      "348/3000 train_loss: 252.65513610839844 test_loss:1304.042236328125\n",
      "349/3000 train_loss: 253.006103515625 test_loss:1272.90380859375\n",
      "350/3000 train_loss: 250.4843292236328 test_loss:1272.4693603515625\n",
      "351/3000 train_loss: 260.7731018066406 test_loss:1280.8494873046875\n",
      "352/3000 train_loss: 266.27691650390625 test_loss:1299.354248046875\n",
      "353/3000 train_loss: 243.72921752929688 test_loss:1265.2108154296875\n",
      "354/3000 train_loss: 242.37872314453125 test_loss:1275.6661376953125\n",
      "355/3000 train_loss: 233.30870056152344 test_loss:1266.6470947265625\n",
      "356/3000 train_loss: 233.30213928222656 test_loss:1258.66455078125\n",
      "357/3000 train_loss: 247.81817626953125 test_loss:1261.1513671875\n",
      "358/3000 train_loss: 243.21641540527344 test_loss:1297.4786376953125\n",
      "359/3000 train_loss: 267.1129455566406 test_loss:1290.1185302734375\n",
      "360/3000 train_loss: 256.6103820800781 test_loss:1254.50830078125\n",
      "361/3000 train_loss: 234.54989624023438 test_loss:1246.79931640625\n",
      "362/3000 train_loss: 235.2246856689453 test_loss:1276.900146484375\n",
      "363/3000 train_loss: 240.79327392578125 test_loss:1266.667724609375\n",
      "364/3000 train_loss: 225.75192260742188 test_loss:1255.8438720703125\n",
      "365/3000 train_loss: 240.9039306640625 test_loss:1271.9881591796875\n",
      "366/3000 train_loss: 231.64004516601562 test_loss:1267.7916259765625\n",
      "367/3000 train_loss: 219.3212432861328 test_loss:1266.67578125\n",
      "368/3000 train_loss: 252.65853881835938 test_loss:1281.428955078125\n",
      "369/3000 train_loss: 267.74951171875 test_loss:1276.4967041015625\n",
      "370/3000 train_loss: 230.0849609375 test_loss:1235.038818359375\n",
      "371/3000 train_loss: 224.74520874023438 test_loss:1240.5645751953125\n",
      "372/3000 train_loss: 254.21295166015625 test_loss:1243.548095703125\n",
      "373/3000 train_loss: 307.0972900390625 test_loss:1284.5467529296875\n",
      "374/3000 train_loss: 272.1764831542969 test_loss:1303.17822265625\n",
      "375/3000 train_loss: 257.6212158203125 test_loss:1300.4078369140625\n",
      "376/3000 train_loss: 249.96481323242188 test_loss:1294.414306640625\n",
      "377/3000 train_loss: 228.24807739257812 test_loss:1290.3665771484375\n",
      "378/3000 train_loss: 259.0927734375 test_loss:1262.4700927734375\n",
      "379/3000 train_loss: 227.09744262695312 test_loss:1301.88671875\n",
      "380/3000 train_loss: 221.93930053710938 test_loss:1297.13818359375\n",
      "381/3000 train_loss: 217.43637084960938 test_loss:1252.9464111328125\n",
      "382/3000 train_loss: 234.75289916992188 test_loss:1252.588134765625\n",
      "383/3000 train_loss: 240.7793731689453 test_loss:1242.0130615234375\n",
      "384/3000 train_loss: 237.0140380859375 test_loss:1273.5308837890625\n",
      "385/3000 train_loss: 220.38531494140625 test_loss:1251.73779296875\n",
      "386/3000 train_loss: 245.16705322265625 test_loss:1244.269775390625\n",
      "387/3000 train_loss: 211.51382446289062 test_loss:1287.685791015625\n",
      "388/3000 train_loss: 217.00750732421875 test_loss:1289.505615234375\n",
      "389/3000 train_loss: 221.63345336914062 test_loss:1286.6005859375\n",
      "390/3000 train_loss: 223.3626251220703 test_loss:1240.489990234375\n",
      "391/3000 train_loss: 344.2774353027344 test_loss:1225.912109375\n",
      "392/3000 train_loss: 352.3689880371094 test_loss:1204.3023681640625\n",
      "393/3000 train_loss: 271.5491027832031 test_loss:1273.3277587890625\n",
      "394/3000 train_loss: 239.03858947753906 test_loss:1253.8013916015625\n",
      "395/3000 train_loss: 242.34619140625 test_loss:1245.1690673828125\n",
      "396/3000 train_loss: 209.99771118164062 test_loss:1254.59716796875\n",
      "397/3000 train_loss: 207.47933959960938 test_loss:1253.2994384765625\n",
      "398/3000 train_loss: 218.3662872314453 test_loss:1229.647216796875\n",
      "399/3000 train_loss: 212.6774139404297 test_loss:1243.4029541015625\n",
      "400/3000 train_loss: 216.85748291015625 test_loss:1269.603515625\n",
      "401/3000 train_loss: 209.77447509765625 test_loss:1261.8939208984375\n",
      "402/3000 train_loss: 201.32174682617188 test_loss:1259.979248046875\n",
      "403/3000 train_loss: 200.55389404296875 test_loss:1237.181396484375\n",
      "404/3000 train_loss: 224.33717346191406 test_loss:1221.12939453125\n",
      "405/3000 train_loss: 286.7254638671875 test_loss:1265.7647705078125\n",
      "406/3000 train_loss: 223.6726837158203 test_loss:1273.020263671875\n",
      "407/3000 train_loss: 219.566650390625 test_loss:1268.3388671875\n",
      "408/3000 train_loss: 197.92544555664062 test_loss:1257.2119140625\n",
      "409/3000 train_loss: 220.5582733154297 test_loss:1232.953125\n",
      "410/3000 train_loss: 195.66139221191406 test_loss:1243.39111328125\n",
      "411/3000 train_loss: 200.4482879638672 test_loss:1261.6832275390625\n",
      "412/3000 train_loss: 203.3170623779297 test_loss:1242.196533203125\n",
      "413/3000 train_loss: 187.1390838623047 test_loss:1255.311279296875\n",
      "414/3000 train_loss: 187.1385040283203 test_loss:1236.0389404296875\n",
      "415/3000 train_loss: 203.63314819335938 test_loss:1244.896484375\n",
      "416/3000 train_loss: 205.05039978027344 test_loss:1258.233154296875\n",
      "417/3000 train_loss: 195.3240509033203 test_loss:1263.004638671875\n",
      "418/3000 train_loss: 197.6021270751953 test_loss:1230.277587890625\n",
      "419/3000 train_loss: 200.41253662109375 test_loss:1211.0372314453125\n",
      "420/3000 train_loss: 211.09532165527344 test_loss:1229.28466796875\n",
      "421/3000 train_loss: 245.03173828125 test_loss:1256.83740234375\n",
      "422/3000 train_loss: 195.20364379882812 test_loss:1250.8067626953125\n",
      "423/3000 train_loss: 198.00064086914062 test_loss:1246.1026611328125\n",
      "424/3000 train_loss: 197.83004760742188 test_loss:1248.48583984375\n",
      "425/3000 train_loss: 204.59552001953125 test_loss:1225.291259765625\n",
      "426/3000 train_loss: 193.28738403320312 test_loss:1240.8975830078125\n",
      "427/3000 train_loss: 196.1895751953125 test_loss:1235.2042236328125\n",
      "428/3000 train_loss: 187.21896362304688 test_loss:1233.4921875\n",
      "429/3000 train_loss: 196.45054626464844 test_loss:1215.1734619140625\n",
      "430/3000 train_loss: 183.5265350341797 test_loss:1228.1759033203125\n",
      "431/3000 train_loss: 185.0449981689453 test_loss:1222.335693359375\n",
      "432/3000 train_loss: 187.663330078125 test_loss:1231.546875\n",
      "433/3000 train_loss: 198.0628204345703 test_loss:1248.007080078125\n",
      "434/3000 train_loss: 207.96905517578125 test_loss:1224.698486328125\n",
      "435/3000 train_loss: 180.5005340576172 test_loss:1229.228759765625\n",
      "436/3000 train_loss: 201.85182189941406 test_loss:1225.37158203125\n",
      "437/3000 train_loss: 190.97210693359375 test_loss:1210.3206787109375\n",
      "438/3000 train_loss: 201.32891845703125 test_loss:1210.4957275390625\n",
      "439/3000 train_loss: 191.25823974609375 test_loss:1212.8516845703125\n",
      "440/3000 train_loss: 183.02395629882812 test_loss:1233.3262939453125\n",
      "441/3000 train_loss: 206.5006103515625 test_loss:1217.096923828125\n",
      "442/3000 train_loss: 177.73667907714844 test_loss:1254.6866455078125\n",
      "443/3000 train_loss: 179.47116088867188 test_loss:1235.4503173828125\n",
      "444/3000 train_loss: 187.63125610351562 test_loss:1192.4549560546875\n",
      "445/3000 train_loss: 193.73851013183594 test_loss:1189.8294677734375\n",
      "446/3000 train_loss: 185.4479522705078 test_loss:1218.852783203125\n",
      "447/3000 train_loss: 177.502197265625 test_loss:1223.2686767578125\n",
      "448/3000 train_loss: 167.066162109375 test_loss:1204.5660400390625\n",
      "449/3000 train_loss: 185.3717041015625 test_loss:1200.3486328125\n",
      "450/3000 train_loss: 196.3323516845703 test_loss:1181.640625\n",
      "451/3000 train_loss: 189.93980407714844 test_loss:1212.530029296875\n",
      "452/3000 train_loss: 191.9923858642578 test_loss:1203.74951171875\n",
      "453/3000 train_loss: 185.98187255859375 test_loss:1194.6793212890625\n",
      "454/3000 train_loss: 184.23252868652344 test_loss:1208.2215576171875\n",
      "455/3000 train_loss: 234.86961364746094 test_loss:1206.13671875\n",
      "456/3000 train_loss: 185.29159545898438 test_loss:1249.67041015625\n",
      "457/3000 train_loss: 192.45281982421875 test_loss:1268.6400146484375\n",
      "458/3000 train_loss: 181.1707305908203 test_loss:1285.4287109375\n",
      "459/3000 train_loss: 191.1210174560547 test_loss:1249.4217529296875\n",
      "460/3000 train_loss: 175.6759033203125 test_loss:1255.05810546875\n",
      "461/3000 train_loss: 184.7248992919922 test_loss:1242.0499267578125\n",
      "462/3000 train_loss: 196.1039581298828 test_loss:1185.050537109375\n",
      "463/3000 train_loss: 185.28521728515625 test_loss:1205.298828125\n",
      "464/3000 train_loss: 203.6123809814453 test_loss:1230.4715576171875\n",
      "465/3000 train_loss: 186.7869415283203 test_loss:1191.772705078125\n",
      "466/3000 train_loss: 174.19386291503906 test_loss:1197.6026611328125\n",
      "467/3000 train_loss: 247.37661743164062 test_loss:1239.4610595703125\n",
      "468/3000 train_loss: 187.034423828125 test_loss:1277.71435546875\n",
      "469/3000 train_loss: 178.34371948242188 test_loss:1254.2442626953125\n",
      "470/3000 train_loss: 175.48094177246094 test_loss:1240.38671875\n",
      "471/3000 train_loss: 182.489013671875 test_loss:1199.569580078125\n",
      "472/3000 train_loss: 194.0218505859375 test_loss:1240.9599609375\n",
      "473/3000 train_loss: 173.42701721191406 test_loss:1226.62109375\n",
      "474/3000 train_loss: 181.85552978515625 test_loss:1207.063232421875\n",
      "475/3000 train_loss: 168.50778198242188 test_loss:1201.9464111328125\n",
      "476/3000 train_loss: 190.6634979248047 test_loss:1226.9998779296875\n",
      "477/3000 train_loss: 177.90602111816406 test_loss:1235.1324462890625\n",
      "478/3000 train_loss: 164.78602600097656 test_loss:1235.39208984375\n",
      "479/3000 train_loss: 165.8304901123047 test_loss:1234.166748046875\n",
      "480/3000 train_loss: 179.302734375 test_loss:1227.8258056640625\n",
      "481/3000 train_loss: 177.29385375976562 test_loss:1234.0166015625\n",
      "482/3000 train_loss: 166.78567504882812 test_loss:1245.02197265625\n",
      "483/3000 train_loss: 160.95127868652344 test_loss:1230.473876953125\n",
      "484/3000 train_loss: 182.93438720703125 test_loss:1235.9927978515625\n",
      "485/3000 train_loss: 182.67271423339844 test_loss:1219.9715576171875\n",
      "486/3000 train_loss: 162.02255249023438 test_loss:1224.1019287109375\n",
      "487/3000 train_loss: 165.62423706054688 test_loss:1210.005126953125\n",
      "488/3000 train_loss: 178.97361755371094 test_loss:1219.6961669921875\n",
      "489/3000 train_loss: 171.68304443359375 test_loss:1202.4432373046875\n",
      "490/3000 train_loss: 161.29495239257812 test_loss:1225.1854248046875\n",
      "491/3000 train_loss: 163.60061645507812 test_loss:1223.029541015625\n",
      "492/3000 train_loss: 192.92649841308594 test_loss:1222.2305908203125\n",
      "493/3000 train_loss: 189.30487060546875 test_loss:1229.3797607421875\n",
      "494/3000 train_loss: 183.7489013671875 test_loss:1250.212646484375\n",
      "495/3000 train_loss: 152.95191955566406 test_loss:1220.6724853515625\n",
      "496/3000 train_loss: 158.60098266601562 test_loss:1207.361083984375\n",
      "497/3000 train_loss: 167.36459350585938 test_loss:1174.68359375\n",
      "498/3000 train_loss: 172.44921875 test_loss:1196.3309326171875\n",
      "499/3000 train_loss: 154.39715576171875 test_loss:1210.9713134765625\n",
      "500/3000 train_loss: 161.15017700195312 test_loss:1220.2967529296875\n",
      "501/3000 train_loss: 153.0729522705078 test_loss:1195.0247802734375\n",
      "502/3000 train_loss: 178.7825927734375 test_loss:1179.5850830078125\n",
      "503/3000 train_loss: 162.2633819580078 test_loss:1160.57421875\n",
      "504/3000 train_loss: 171.6217041015625 test_loss:1202.110595703125\n",
      "505/3000 train_loss: 168.79580688476562 test_loss:1217.15869140625\n",
      "506/3000 train_loss: 208.57073974609375 test_loss:1203.2750244140625\n",
      "507/3000 train_loss: 179.56121826171875 test_loss:1168.2659912109375\n",
      "508/3000 train_loss: 171.44500732421875 test_loss:1186.4541015625\n",
      "509/3000 train_loss: 172.10885620117188 test_loss:1167.2225341796875\n",
      "510/3000 train_loss: 181.89767456054688 test_loss:1213.2471923828125\n",
      "511/3000 train_loss: 209.62510681152344 test_loss:1217.5128173828125\n",
      "512/3000 train_loss: 179.7563934326172 test_loss:1165.02294921875\n",
      "513/3000 train_loss: 177.31182861328125 test_loss:1130.3193359375\n",
      "514/3000 train_loss: 164.79762268066406 test_loss:1098.50244140625\n",
      "515/3000 train_loss: 161.24949645996094 test_loss:1111.3055419921875\n",
      "516/3000 train_loss: 168.90426635742188 test_loss:1139.5745849609375\n",
      "517/3000 train_loss: 171.0153045654297 test_loss:1174.6109619140625\n",
      "518/3000 train_loss: 152.89935302734375 test_loss:1135.9591064453125\n",
      "519/3000 train_loss: 159.142578125 test_loss:1122.2080078125\n",
      "520/3000 train_loss: 150.11985778808594 test_loss:1149.4412841796875\n",
      "521/3000 train_loss: 150.15097045898438 test_loss:1112.18359375\n",
      "522/3000 train_loss: 155.20559692382812 test_loss:1140.464599609375\n",
      "523/3000 train_loss: 174.5717010498047 test_loss:1087.4312744140625\n",
      "524/3000 train_loss: 152.4476318359375 test_loss:1129.3486328125\n",
      "525/3000 train_loss: 244.61814880371094 test_loss:1138.599365234375\n",
      "526/3000 train_loss: 250.10052490234375 test_loss:1329.79541015625\n",
      "527/3000 train_loss: 251.66976928710938 test_loss:1272.3970947265625\n",
      "528/3000 train_loss: 193.7095184326172 test_loss:1199.402099609375\n",
      "529/3000 train_loss: 194.00961303710938 test_loss:1178.2889404296875\n",
      "530/3000 train_loss: 184.29010009765625 test_loss:1172.12109375\n",
      "531/3000 train_loss: 187.24517822265625 test_loss:1219.3125\n",
      "532/3000 train_loss: 179.13685607910156 test_loss:1235.2752685546875\n",
      "533/3000 train_loss: 164.69625854492188 test_loss:1216.2833251953125\n",
      "534/3000 train_loss: 167.26333618164062 test_loss:1187.6326904296875\n",
      "535/3000 train_loss: 181.3943328857422 test_loss:1217.881103515625\n",
      "536/3000 train_loss: 164.3025360107422 test_loss:1202.2138671875\n",
      "537/3000 train_loss: 160.5071563720703 test_loss:1184.442626953125\n",
      "538/3000 train_loss: 160.7991943359375 test_loss:1157.20751953125\n",
      "539/3000 train_loss: 184.9582061767578 test_loss:1174.6510009765625\n",
      "540/3000 train_loss: 165.48709106445312 test_loss:1188.410888671875\n",
      "541/3000 train_loss: 162.83152770996094 test_loss:1146.82470703125\n",
      "542/3000 train_loss: 163.41273498535156 test_loss:1171.823974609375\n",
      "543/3000 train_loss: 176.7561798095703 test_loss:1149.634033203125\n",
      "544/3000 train_loss: 157.9702606201172 test_loss:1142.65625\n",
      "545/3000 train_loss: 147.76548767089844 test_loss:1126.4796142578125\n",
      "546/3000 train_loss: 162.4278564453125 test_loss:1140.7720947265625\n",
      "547/3000 train_loss: 154.84716796875 test_loss:1146.3935546875\n",
      "548/3000 train_loss: 150.71646118164062 test_loss:1158.441650390625\n",
      "549/3000 train_loss: 184.0100555419922 test_loss:1183.4495849609375\n",
      "550/3000 train_loss: 159.15628051757812 test_loss:1156.924560546875\n",
      "551/3000 train_loss: 160.34539794921875 test_loss:1134.2498779296875\n",
      "552/3000 train_loss: 174.6427001953125 test_loss:1136.316650390625\n",
      "553/3000 train_loss: 156.00509643554688 test_loss:1165.794189453125\n",
      "554/3000 train_loss: 153.8236083984375 test_loss:1137.7374267578125\n",
      "555/3000 train_loss: 169.30116271972656 test_loss:1151.5455322265625\n",
      "556/3000 train_loss: 168.11376953125 test_loss:1142.6290283203125\n",
      "557/3000 train_loss: 161.62330627441406 test_loss:1152.621826171875\n",
      "558/3000 train_loss: 147.98324584960938 test_loss:1141.4481201171875\n",
      "559/3000 train_loss: 147.38970947265625 test_loss:1136.388427734375\n",
      "560/3000 train_loss: 164.11773681640625 test_loss:1131.9234619140625\n",
      "561/3000 train_loss: 149.5411834716797 test_loss:1123.460205078125\n",
      "562/3000 train_loss: 165.3862762451172 test_loss:1113.8529052734375\n",
      "563/3000 train_loss: 139.5016632080078 test_loss:1128.8382568359375\n",
      "564/3000 train_loss: 153.0532989501953 test_loss:1168.2740478515625\n",
      "565/3000 train_loss: 170.3468475341797 test_loss:1155.2515869140625\n",
      "566/3000 train_loss: 157.54844665527344 test_loss:1147.6558837890625\n",
      "567/3000 train_loss: 151.72422790527344 test_loss:1185.8294677734375\n",
      "568/3000 train_loss: 149.9689178466797 test_loss:1187.176025390625\n",
      "569/3000 train_loss: 166.08053588867188 test_loss:1156.583740234375\n",
      "570/3000 train_loss: 159.92901611328125 test_loss:1162.1783447265625\n",
      "571/3000 train_loss: 154.52456665039062 test_loss:1162.4609375\n",
      "572/3000 train_loss: 146.58639526367188 test_loss:1144.9852294921875\n",
      "573/3000 train_loss: 151.7160186767578 test_loss:1148.942626953125\n",
      "574/3000 train_loss: 157.28028869628906 test_loss:1132.01904296875\n",
      "575/3000 train_loss: 156.05538940429688 test_loss:1161.1710205078125\n",
      "576/3000 train_loss: 146.65682983398438 test_loss:1132.5623779296875\n",
      "577/3000 train_loss: 154.94692993164062 test_loss:1137.9283447265625\n",
      "578/3000 train_loss: 157.20547485351562 test_loss:1146.105224609375\n",
      "579/3000 train_loss: 181.34263610839844 test_loss:1180.3330078125\n",
      "580/3000 train_loss: 166.84921264648438 test_loss:1172.0469970703125\n",
      "581/3000 train_loss: 151.2869873046875 test_loss:1168.3662109375\n",
      "582/3000 train_loss: 155.2726593017578 test_loss:1165.7835693359375\n",
      "583/3000 train_loss: 150.60389709472656 test_loss:1136.5682373046875\n",
      "584/3000 train_loss: 149.53228759765625 test_loss:1134.49072265625\n",
      "585/3000 train_loss: 163.88522338867188 test_loss:1131.5421142578125\n",
      "586/3000 train_loss: 153.91253662109375 test_loss:1141.20361328125\n",
      "587/3000 train_loss: 166.5849151611328 test_loss:1138.045654296875\n",
      "588/3000 train_loss: 156.48011779785156 test_loss:1162.2833251953125\n",
      "589/3000 train_loss: 148.8755645751953 test_loss:1147.7882080078125\n",
      "590/3000 train_loss: 167.72958374023438 test_loss:1152.8681640625\n",
      "591/3000 train_loss: 161.18533325195312 test_loss:1146.005126953125\n",
      "592/3000 train_loss: 149.5615692138672 test_loss:1126.681396484375\n",
      "593/3000 train_loss: 152.29217529296875 test_loss:1122.3585205078125\n",
      "594/3000 train_loss: 159.27276611328125 test_loss:1136.566650390625\n",
      "595/3000 train_loss: 140.94874572753906 test_loss:1132.299560546875\n",
      "596/3000 train_loss: 157.47430419921875 test_loss:1148.671875\n",
      "597/3000 train_loss: 162.33482360839844 test_loss:1153.8287353515625\n",
      "598/3000 train_loss: 154.22589111328125 test_loss:1090.5126953125\n",
      "599/3000 train_loss: 139.6407470703125 test_loss:1143.468505859375\n",
      "600/3000 train_loss: 180.39524841308594 test_loss:1155.5126953125\n",
      "601/3000 train_loss: 151.2729034423828 test_loss:1176.5654296875\n",
      "602/3000 train_loss: 145.5975341796875 test_loss:1150.8671875\n",
      "603/3000 train_loss: 162.51028442382812 test_loss:1118.96484375\n",
      "604/3000 train_loss: 150.56655883789062 test_loss:1165.3836669921875\n",
      "605/3000 train_loss: 190.52467346191406 test_loss:1164.4833984375\n",
      "606/3000 train_loss: 152.7421112060547 test_loss:1164.6202392578125\n",
      "607/3000 train_loss: 149.16380310058594 test_loss:1127.04248046875\n",
      "608/3000 train_loss: 142.67074584960938 test_loss:1094.0098876953125\n",
      "609/3000 train_loss: 155.22877502441406 test_loss:1115.546875\n",
      "610/3000 train_loss: 155.69119262695312 test_loss:1106.239501953125\n",
      "611/3000 train_loss: 164.0229034423828 test_loss:1146.8389892578125\n",
      "612/3000 train_loss: 154.0552978515625 test_loss:1139.209228515625\n",
      "613/3000 train_loss: 225.10092163085938 test_loss:1133.7755126953125\n",
      "614/3000 train_loss: 147.3011474609375 test_loss:1178.43212890625\n",
      "615/3000 train_loss: 154.86695861816406 test_loss:1169.1424560546875\n",
      "616/3000 train_loss: 144.40737915039062 test_loss:1162.574951171875\n",
      "617/3000 train_loss: 145.67947387695312 test_loss:1170.6378173828125\n",
      "618/3000 train_loss: 164.88894653320312 test_loss:1166.9779052734375\n",
      "619/3000 train_loss: 144.71829223632812 test_loss:1168.00732421875\n",
      "620/3000 train_loss: 156.03135681152344 test_loss:1136.409423828125\n",
      "621/3000 train_loss: 143.121826171875 test_loss:1147.1129150390625\n",
      "622/3000 train_loss: 145.9852294921875 test_loss:1158.1280517578125\n",
      "623/3000 train_loss: 155.0305633544922 test_loss:1154.0577392578125\n",
      "624/3000 train_loss: 157.0449981689453 test_loss:1130.59814453125\n",
      "625/3000 train_loss: 155.77398681640625 test_loss:1113.8487548828125\n",
      "626/3000 train_loss: 147.38912963867188 test_loss:1146.92431640625\n",
      "627/3000 train_loss: 145.0673065185547 test_loss:1155.89306640625\n",
      "628/3000 train_loss: 142.06735229492188 test_loss:1150.5223388671875\n",
      "629/3000 train_loss: 140.71102905273438 test_loss:1148.9676513671875\n",
      "630/3000 train_loss: 137.44740295410156 test_loss:1148.5361328125\n",
      "631/3000 train_loss: 138.0165557861328 test_loss:1120.632080078125\n",
      "632/3000 train_loss: 137.4689483642578 test_loss:1155.0919189453125\n",
      "633/3000 train_loss: 132.79420471191406 test_loss:1152.6771240234375\n",
      "634/3000 train_loss: 150.66368103027344 test_loss:1154.3272705078125\n",
      "635/3000 train_loss: 145.23863220214844 test_loss:1170.838134765625\n",
      "636/3000 train_loss: 141.99630737304688 test_loss:1157.819580078125\n",
      "637/3000 train_loss: 156.5719757080078 test_loss:1140.4661865234375\n",
      "638/3000 train_loss: 147.096923828125 test_loss:1183.2960205078125\n",
      "639/3000 train_loss: 143.9622039794922 test_loss:1143.235107421875\n",
      "640/3000 train_loss: 145.72369384765625 test_loss:1161.2283935546875\n",
      "641/3000 train_loss: 145.94142150878906 test_loss:1128.1744384765625\n",
      "642/3000 train_loss: 197.38345336914062 test_loss:1100.7996826171875\n",
      "643/3000 train_loss: 150.34730529785156 test_loss:1175.431884765625\n",
      "644/3000 train_loss: 153.8757781982422 test_loss:1165.049072265625\n",
      "645/3000 train_loss: 141.85235595703125 test_loss:1146.808837890625\n",
      "646/3000 train_loss: 139.53208923339844 test_loss:1177.0224609375\n",
      "647/3000 train_loss: 143.77203369140625 test_loss:1162.211669921875\n",
      "648/3000 train_loss: 169.56375122070312 test_loss:1181.6060791015625\n",
      "649/3000 train_loss: 145.95901489257812 test_loss:1113.6917724609375\n",
      "650/3000 train_loss: 158.39686584472656 test_loss:1157.49365234375\n",
      "651/3000 train_loss: 148.38946533203125 test_loss:1154.948486328125\n",
      "652/3000 train_loss: 143.14633178710938 test_loss:1176.81640625\n",
      "653/3000 train_loss: 165.26197814941406 test_loss:1124.90380859375\n",
      "654/3000 train_loss: 146.89202880859375 test_loss:1099.9564208984375\n",
      "655/3000 train_loss: 146.57644653320312 test_loss:1112.6842041015625\n",
      "656/3000 train_loss: 151.1058807373047 test_loss:1114.6441650390625\n",
      "657/3000 train_loss: 156.2519073486328 test_loss:1130.7027587890625\n",
      "658/3000 train_loss: 134.209716796875 test_loss:1081.298828125\n",
      "659/3000 train_loss: 138.9009552001953 test_loss:1090.5050048828125\n",
      "660/3000 train_loss: 144.83062744140625 test_loss:1088.2867431640625\n",
      "661/3000 train_loss: 129.16477966308594 test_loss:1119.1265869140625\n",
      "662/3000 train_loss: 140.76998901367188 test_loss:1103.4102783203125\n",
      "663/3000 train_loss: 142.79342651367188 test_loss:1122.6331787109375\n",
      "664/3000 train_loss: 130.42091369628906 test_loss:1118.9461669921875\n",
      "665/3000 train_loss: 133.36770629882812 test_loss:1120.478515625\n",
      "666/3000 train_loss: 130.5919189453125 test_loss:1126.58642578125\n",
      "667/3000 train_loss: 169.7882843017578 test_loss:1096.73193359375\n",
      "668/3000 train_loss: 151.7362518310547 test_loss:1085.77783203125\n",
      "669/3000 train_loss: 146.85134887695312 test_loss:1091.541259765625\n",
      "670/3000 train_loss: 145.45675659179688 test_loss:1144.3006591796875\n",
      "671/3000 train_loss: 135.1161651611328 test_loss:1131.09619140625\n",
      "672/3000 train_loss: 132.73736572265625 test_loss:1091.72607421875\n",
      "673/3000 train_loss: 151.95542907714844 test_loss:1141.3416748046875\n",
      "674/3000 train_loss: 131.84841918945312 test_loss:1121.4095458984375\n",
      "675/3000 train_loss: 146.0059051513672 test_loss:1120.084228515625\n",
      "676/3000 train_loss: 150.52528381347656 test_loss:1172.0556640625\n",
      "677/3000 train_loss: 134.399169921875 test_loss:1138.07568359375\n",
      "678/3000 train_loss: 127.83392333984375 test_loss:1099.137451171875\n",
      "679/3000 train_loss: 134.1835479736328 test_loss:1102.2064208984375\n",
      "680/3000 train_loss: 137.7756805419922 test_loss:1097.7484130859375\n",
      "681/3000 train_loss: 141.9552764892578 test_loss:1117.6856689453125\n",
      "682/3000 train_loss: 139.48855590820312 test_loss:1119.98681640625\n",
      "683/3000 train_loss: 143.64328002929688 test_loss:1112.78173828125\n",
      "684/3000 train_loss: 144.2896270751953 test_loss:1111.617919921875\n",
      "685/3000 train_loss: 132.14706420898438 test_loss:1148.804931640625\n",
      "686/3000 train_loss: 136.93504333496094 test_loss:1121.7510986328125\n",
      "687/3000 train_loss: 137.33010864257812 test_loss:1133.66552734375\n",
      "688/3000 train_loss: 127.66438293457031 test_loss:1123.6397705078125\n",
      "689/3000 train_loss: 135.5762939453125 test_loss:1108.7921142578125\n",
      "690/3000 train_loss: 158.65298461914062 test_loss:1160.62744140625\n",
      "691/3000 train_loss: 140.401123046875 test_loss:1104.9527587890625\n",
      "692/3000 train_loss: 127.30061340332031 test_loss:1113.87255859375\n",
      "693/3000 train_loss: 141.57408142089844 test_loss:1123.067626953125\n",
      "694/3000 train_loss: 155.14334106445312 test_loss:1127.50341796875\n",
      "695/3000 train_loss: 145.84915161132812 test_loss:1071.599609375\n",
      "696/3000 train_loss: 147.5497283935547 test_loss:1075.4605712890625\n",
      "697/3000 train_loss: 165.15049743652344 test_loss:1078.1815185546875\n",
      "698/3000 train_loss: 135.44387817382812 test_loss:1147.4713134765625\n",
      "699/3000 train_loss: 138.9672393798828 test_loss:1100.715576171875\n",
      "700/3000 train_loss: 145.63758850097656 test_loss:1096.8944091796875\n",
      "701/3000 train_loss: 141.96604919433594 test_loss:1146.415283203125\n",
      "702/3000 train_loss: 141.5836944580078 test_loss:1123.9063720703125\n",
      "703/3000 train_loss: 139.1834716796875 test_loss:1113.5311279296875\n",
      "704/3000 train_loss: 170.19879150390625 test_loss:1110.3304443359375\n",
      "705/3000 train_loss: 127.59473419189453 test_loss:1075.112548828125\n",
      "706/3000 train_loss: 154.29710388183594 test_loss:1103.8065185546875\n",
      "707/3000 train_loss: 138.61883544921875 test_loss:1134.0970458984375\n",
      "708/3000 train_loss: 137.9257354736328 test_loss:1088.37158203125\n",
      "709/3000 train_loss: 155.64895629882812 test_loss:1097.26318359375\n",
      "710/3000 train_loss: 129.4635009765625 test_loss:1103.6417236328125\n",
      "711/3000 train_loss: 130.3213348388672 test_loss:1093.9844970703125\n",
      "712/3000 train_loss: 123.46722412109375 test_loss:1118.826904296875\n",
      "713/3000 train_loss: 141.63232421875 test_loss:1139.3214111328125\n",
      "714/3000 train_loss: 131.0929718017578 test_loss:1096.7628173828125\n",
      "715/3000 train_loss: 138.3099365234375 test_loss:1105.548828125\n",
      "716/3000 train_loss: 123.12594604492188 test_loss:1081.881591796875\n",
      "717/3000 train_loss: 128.61508178710938 test_loss:1108.82861328125\n",
      "718/3000 train_loss: 134.65072631835938 test_loss:1120.044189453125\n",
      "719/3000 train_loss: 128.4744873046875 test_loss:1086.507080078125\n",
      "720/3000 train_loss: 134.32821655273438 test_loss:1092.3885498046875\n",
      "721/3000 train_loss: 155.7730712890625 test_loss:1093.9083251953125\n",
      "722/3000 train_loss: 144.94000244140625 test_loss:1097.9871826171875\n",
      "723/3000 train_loss: 133.1929473876953 test_loss:1099.469482421875\n",
      "724/3000 train_loss: 125.6086196899414 test_loss:1105.276611328125\n",
      "725/3000 train_loss: 125.18782043457031 test_loss:1111.35107421875\n",
      "726/3000 train_loss: 142.76829528808594 test_loss:1137.724853515625\n",
      "727/3000 train_loss: 136.81297302246094 test_loss:1127.859375\n",
      "728/3000 train_loss: 129.645751953125 test_loss:1128.4420166015625\n",
      "729/3000 train_loss: 136.15826416015625 test_loss:1095.519775390625\n",
      "730/3000 train_loss: 122.5072250366211 test_loss:1115.072998046875\n",
      "731/3000 train_loss: 123.64794158935547 test_loss:1110.224853515625\n",
      "732/3000 train_loss: 145.0099334716797 test_loss:1129.8060302734375\n",
      "733/3000 train_loss: 132.92950439453125 test_loss:1153.5870361328125\n",
      "734/3000 train_loss: 136.7373504638672 test_loss:1131.47314453125\n",
      "735/3000 train_loss: 142.88803100585938 test_loss:1110.1888427734375\n",
      "736/3000 train_loss: 129.07284545898438 test_loss:1132.774658203125\n",
      "737/3000 train_loss: 174.22393798828125 test_loss:1112.601318359375\n",
      "738/3000 train_loss: 145.711181640625 test_loss:1129.929931640625\n",
      "739/3000 train_loss: 145.2741241455078 test_loss:1164.7220458984375\n",
      "740/3000 train_loss: 152.27883911132812 test_loss:1164.86767578125\n",
      "741/3000 train_loss: 139.49072265625 test_loss:1144.931396484375\n",
      "742/3000 train_loss: 139.07582092285156 test_loss:1144.54296875\n",
      "743/3000 train_loss: 130.77508544921875 test_loss:1139.955322265625\n",
      "744/3000 train_loss: 142.08958435058594 test_loss:1124.6595458984375\n",
      "745/3000 train_loss: 142.40257263183594 test_loss:1190.283447265625\n",
      "746/3000 train_loss: 125.46125030517578 test_loss:1119.1439208984375\n",
      "747/3000 train_loss: 138.04635620117188 test_loss:1096.977294921875\n",
      "748/3000 train_loss: 135.7849884033203 test_loss:1093.3975830078125\n",
      "749/3000 train_loss: 141.5511932373047 test_loss:1085.7498779296875\n",
      "750/3000 train_loss: 145.073974609375 test_loss:1085.042724609375\n",
      "751/3000 train_loss: 134.3808135986328 test_loss:1094.9295654296875\n",
      "752/3000 train_loss: 126.01618194580078 test_loss:1102.061279296875\n",
      "753/3000 train_loss: 143.7577362060547 test_loss:1091.3192138671875\n",
      "754/3000 train_loss: 128.9810791015625 test_loss:1108.8134765625\n",
      "755/3000 train_loss: 137.37844848632812 test_loss:1073.0323486328125\n",
      "756/3000 train_loss: 133.62997436523438 test_loss:1087.844970703125\n",
      "757/3000 train_loss: 138.22665405273438 test_loss:1082.8541259765625\n",
      "758/3000 train_loss: 130.31350708007812 test_loss:1098.286865234375\n",
      "759/3000 train_loss: 134.98939514160156 test_loss:1113.41015625\n",
      "760/3000 train_loss: 129.9327850341797 test_loss:1155.552490234375\n",
      "761/3000 train_loss: 130.447021484375 test_loss:1135.513671875\n",
      "762/3000 train_loss: 133.75282287597656 test_loss:1109.7821044921875\n",
      "763/3000 train_loss: 131.493408203125 test_loss:1097.1497802734375\n",
      "764/3000 train_loss: 131.37237548828125 test_loss:1128.2215576171875\n",
      "765/3000 train_loss: 135.36178588867188 test_loss:1115.3544921875\n",
      "766/3000 train_loss: 122.48704528808594 test_loss:1095.4000244140625\n",
      "767/3000 train_loss: 124.9781494140625 test_loss:1104.07421875\n",
      "768/3000 train_loss: 134.12451171875 test_loss:1086.368408203125\n",
      "769/3000 train_loss: 124.57707214355469 test_loss:1073.4847412109375\n",
      "770/3000 train_loss: 123.33668518066406 test_loss:1140.3875732421875\n",
      "771/3000 train_loss: 134.41976928710938 test_loss:1084.1136474609375\n",
      "772/3000 train_loss: 131.85020446777344 test_loss:1068.1964111328125\n",
      "773/3000 train_loss: 131.86380004882812 test_loss:1116.673583984375\n",
      "774/3000 train_loss: 142.7072296142578 test_loss:1077.9471435546875\n",
      "775/3000 train_loss: 121.67266082763672 test_loss:1090.8201904296875\n",
      "776/3000 train_loss: 129.53460693359375 test_loss:1088.753173828125\n",
      "777/3000 train_loss: 118.50440979003906 test_loss:1062.4822998046875\n",
      "778/3000 train_loss: 128.4467315673828 test_loss:1060.461181640625\n",
      "779/3000 train_loss: 117.53709411621094 test_loss:1107.9619140625\n",
      "780/3000 train_loss: 131.76589965820312 test_loss:1068.2261962890625\n",
      "781/3000 train_loss: 130.16107177734375 test_loss:1117.330810546875\n",
      "782/3000 train_loss: 182.4019317626953 test_loss:1122.0201416015625\n",
      "783/3000 train_loss: 129.64431762695312 test_loss:1143.467529296875\n",
      "784/3000 train_loss: 131.4098663330078 test_loss:1073.47802734375\n",
      "785/3000 train_loss: 119.13786315917969 test_loss:1081.480224609375\n",
      "786/3000 train_loss: 128.40603637695312 test_loss:1078.87939453125\n",
      "787/3000 train_loss: 138.4189453125 test_loss:1082.359130859375\n",
      "788/3000 train_loss: 116.64437103271484 test_loss:1103.607666015625\n",
      "789/3000 train_loss: 135.9358367919922 test_loss:1093.63427734375\n",
      "790/3000 train_loss: 112.5927963256836 test_loss:1120.3187255859375\n",
      "791/3000 train_loss: 133.31884765625 test_loss:1100.2474365234375\n",
      "792/3000 train_loss: 119.55708312988281 test_loss:1133.5538330078125\n",
      "793/3000 train_loss: 124.22676086425781 test_loss:1077.62548828125\n",
      "794/3000 train_loss: 121.862548828125 test_loss:1086.6351318359375\n",
      "795/3000 train_loss: 121.22648620605469 test_loss:1090.064453125\n",
      "796/3000 train_loss: 126.1193618774414 test_loss:1095.158935546875\n",
      "797/3000 train_loss: 131.925048828125 test_loss:1061.53564453125\n",
      "798/3000 train_loss: 126.876953125 test_loss:1046.97802734375\n",
      "799/3000 train_loss: 125.24730682373047 test_loss:1067.4979248046875\n",
      "800/3000 train_loss: 127.078857421875 test_loss:1103.75341796875\n",
      "801/3000 train_loss: 112.69707489013672 test_loss:1094.747314453125\n",
      "802/3000 train_loss: 110.50956726074219 test_loss:1097.5777587890625\n",
      "803/3000 train_loss: 118.40606689453125 test_loss:1046.440185546875\n",
      "804/3000 train_loss: 125.86951446533203 test_loss:1102.9388427734375\n",
      "805/3000 train_loss: 136.41537475585938 test_loss:1048.681884765625\n",
      "806/3000 train_loss: 122.25495910644531 test_loss:993.5349731445312\n",
      "807/3000 train_loss: 133.43136596679688 test_loss:1028.3927001953125\n",
      "808/3000 train_loss: 118.05329132080078 test_loss:1051.905517578125\n",
      "809/3000 train_loss: 125.4657974243164 test_loss:1043.2646484375\n",
      "810/3000 train_loss: 121.2701416015625 test_loss:1042.4371337890625\n",
      "811/3000 train_loss: 136.1436767578125 test_loss:1046.54736328125\n",
      "812/3000 train_loss: 123.16368865966797 test_loss:1075.6729736328125\n",
      "813/3000 train_loss: 116.69950866699219 test_loss:1092.33056640625\n",
      "814/3000 train_loss: 114.24383544921875 test_loss:1071.940185546875\n",
      "815/3000 train_loss: 119.57318115234375 test_loss:1092.360107421875\n",
      "816/3000 train_loss: 113.19818115234375 test_loss:1086.946533203125\n",
      "817/3000 train_loss: 137.2164306640625 test_loss:1115.087646484375\n",
      "818/3000 train_loss: 126.07671356201172 test_loss:1084.932861328125\n",
      "819/3000 train_loss: 115.81604766845703 test_loss:1060.8154296875\n",
      "820/3000 train_loss: 109.01625061035156 test_loss:1066.431396484375\n",
      "821/3000 train_loss: 118.59382629394531 test_loss:1061.551025390625\n",
      "822/3000 train_loss: 111.1524658203125 test_loss:1068.20947265625\n",
      "823/3000 train_loss: 125.19503784179688 test_loss:1114.171630859375\n",
      "824/3000 train_loss: 121.38328552246094 test_loss:1110.7567138671875\n",
      "825/3000 train_loss: 119.7726058959961 test_loss:1112.4169921875\n",
      "826/3000 train_loss: 117.80708312988281 test_loss:1107.43798828125\n",
      "827/3000 train_loss: 113.71939086914062 test_loss:1079.3175048828125\n",
      "828/3000 train_loss: 107.64035034179688 test_loss:1100.2095947265625\n",
      "829/3000 train_loss: 116.68115234375 test_loss:1060.5347900390625\n",
      "830/3000 train_loss: 128.22618103027344 test_loss:1031.8011474609375\n",
      "831/3000 train_loss: 131.54965209960938 test_loss:1053.6776123046875\n",
      "832/3000 train_loss: 150.64781188964844 test_loss:1036.8831787109375\n",
      "833/3000 train_loss: 117.01065063476562 test_loss:1120.9852294921875\n",
      "834/3000 train_loss: 113.16303253173828 test_loss:1120.027587890625\n",
      "835/3000 train_loss: 114.96685791015625 test_loss:1078.326171875\n",
      "836/3000 train_loss: 109.22718048095703 test_loss:1070.1358642578125\n",
      "837/3000 train_loss: 111.77751159667969 test_loss:1095.506103515625\n",
      "838/3000 train_loss: 107.37360382080078 test_loss:1080.954345703125\n",
      "839/3000 train_loss: 113.81108856201172 test_loss:1072.8660888671875\n",
      "840/3000 train_loss: 111.91381072998047 test_loss:1079.1298828125\n",
      "841/3000 train_loss: 110.16978454589844 test_loss:1082.7691650390625\n",
      "842/3000 train_loss: 140.96603393554688 test_loss:1065.11962890625\n",
      "843/3000 train_loss: 122.68093872070312 test_loss:1091.1448974609375\n",
      "844/3000 train_loss: 116.60548400878906 test_loss:1076.0401611328125\n",
      "845/3000 train_loss: 107.79164123535156 test_loss:1093.2061767578125\n",
      "846/3000 train_loss: 119.20477294921875 test_loss:1056.8392333984375\n",
      "847/3000 train_loss: 111.17359924316406 test_loss:1061.7633056640625\n",
      "848/3000 train_loss: 111.28681182861328 test_loss:1081.650390625\n",
      "849/3000 train_loss: 111.28927612304688 test_loss:1070.3634033203125\n",
      "850/3000 train_loss: 100.7618179321289 test_loss:1075.2647705078125\n",
      "851/3000 train_loss: 108.30286407470703 test_loss:1034.845947265625\n",
      "852/3000 train_loss: 108.28234100341797 test_loss:1015.086669921875\n",
      "853/3000 train_loss: 107.23418426513672 test_loss:1044.695556640625\n",
      "854/3000 train_loss: 131.58209228515625 test_loss:1057.216552734375\n",
      "855/3000 train_loss: 121.62760925292969 test_loss:1155.49853515625\n",
      "856/3000 train_loss: 139.6031951904297 test_loss:1121.1951904296875\n",
      "857/3000 train_loss: 121.87741088867188 test_loss:1106.7794189453125\n",
      "858/3000 train_loss: 115.98100280761719 test_loss:1066.453857421875\n",
      "859/3000 train_loss: 123.84069061279297 test_loss:1099.548828125\n",
      "860/3000 train_loss: 106.45408630371094 test_loss:1092.933349609375\n",
      "861/3000 train_loss: 124.243896484375 test_loss:1084.6258544921875\n",
      "862/3000 train_loss: 117.85355377197266 test_loss:1076.90087890625\n",
      "863/3000 train_loss: 104.11560821533203 test_loss:1074.2264404296875\n",
      "864/3000 train_loss: 117.24236297607422 test_loss:1054.198974609375\n",
      "865/3000 train_loss: 125.94921875 test_loss:1069.0531005859375\n",
      "866/3000 train_loss: 107.87332153320312 test_loss:1061.01220703125\n",
      "867/3000 train_loss: 108.19334411621094 test_loss:1041.331787109375\n",
      "868/3000 train_loss: 112.26653289794922 test_loss:1042.3057861328125\n",
      "869/3000 train_loss: 116.22850799560547 test_loss:1041.3809814453125\n",
      "870/3000 train_loss: 127.40605163574219 test_loss:1021.9365234375\n",
      "871/3000 train_loss: 144.2388153076172 test_loss:1049.87744140625\n",
      "872/3000 train_loss: 111.58860778808594 test_loss:1064.49755859375\n",
      "873/3000 train_loss: 113.01144409179688 test_loss:1068.826416015625\n",
      "874/3000 train_loss: 112.2050552368164 test_loss:1077.57080078125\n",
      "875/3000 train_loss: 117.75674438476562 test_loss:1103.2364501953125\n",
      "876/3000 train_loss: 111.87103271484375 test_loss:1071.123046875\n",
      "877/3000 train_loss: 110.9773178100586 test_loss:1075.58740234375\n",
      "878/3000 train_loss: 105.97343444824219 test_loss:1122.12548828125\n",
      "879/3000 train_loss: 114.21243286132812 test_loss:1114.64599609375\n",
      "880/3000 train_loss: 106.8819580078125 test_loss:1078.478515625\n",
      "881/3000 train_loss: 100.4110107421875 test_loss:1089.4124755859375\n",
      "882/3000 train_loss: 111.42778778076172 test_loss:1084.672119140625\n",
      "883/3000 train_loss: 116.08356475830078 test_loss:1091.4261474609375\n",
      "884/3000 train_loss: 109.68455505371094 test_loss:1084.2847900390625\n",
      "885/3000 train_loss: 114.54627227783203 test_loss:1080.7379150390625\n",
      "886/3000 train_loss: 98.93509674072266 test_loss:1041.7078857421875\n",
      "887/3000 train_loss: 102.53356170654297 test_loss:1034.92431640625\n",
      "888/3000 train_loss: 99.31916046142578 test_loss:1037.819091796875\n",
      "889/3000 train_loss: 106.33269500732422 test_loss:1044.7652587890625\n",
      "890/3000 train_loss: 107.48635864257812 test_loss:1047.1676025390625\n",
      "891/3000 train_loss: 109.47138214111328 test_loss:1058.934814453125\n",
      "892/3000 train_loss: 109.48983764648438 test_loss:998.341796875\n",
      "893/3000 train_loss: 101.7524185180664 test_loss:1015.3851318359375\n",
      "894/3000 train_loss: 110.96482849121094 test_loss:1027.7664794921875\n",
      "895/3000 train_loss: 100.660400390625 test_loss:1062.41455078125\n",
      "896/3000 train_loss: 100.20240020751953 test_loss:1066.720458984375\n",
      "897/3000 train_loss: 112.79780578613281 test_loss:1047.6756591796875\n",
      "898/3000 train_loss: 111.9692611694336 test_loss:1069.3802490234375\n",
      "899/3000 train_loss: 98.01460266113281 test_loss:1045.441650390625\n",
      "900/3000 train_loss: 110.16658782958984 test_loss:1061.8048095703125\n",
      "901/3000 train_loss: 104.21900939941406 test_loss:1032.567138671875\n",
      "902/3000 train_loss: 108.04144287109375 test_loss:1060.52197265625\n",
      "903/3000 train_loss: 102.98039245605469 test_loss:1038.01708984375\n",
      "904/3000 train_loss: 112.59037017822266 test_loss:1095.1810302734375\n",
      "905/3000 train_loss: 93.5240249633789 test_loss:1074.3802490234375\n",
      "906/3000 train_loss: 116.21099090576172 test_loss:1091.5108642578125\n",
      "907/3000 train_loss: 103.16938781738281 test_loss:1088.62158203125\n",
      "908/3000 train_loss: 101.64698028564453 test_loss:1067.381103515625\n",
      "909/3000 train_loss: 98.72216796875 test_loss:1060.423095703125\n",
      "910/3000 train_loss: 100.2022705078125 test_loss:1064.36181640625\n",
      "911/3000 train_loss: 107.78841400146484 test_loss:1056.1474609375\n",
      "912/3000 train_loss: 140.58010864257812 test_loss:1044.353271484375\n",
      "913/3000 train_loss: 103.3963851928711 test_loss:1043.143310546875\n",
      "914/3000 train_loss: 109.95130157470703 test_loss:1025.35693359375\n",
      "915/3000 train_loss: 95.6677017211914 test_loss:1057.4808349609375\n",
      "916/3000 train_loss: 106.80804443359375 test_loss:1021.9505004882812\n",
      "917/3000 train_loss: 104.41972351074219 test_loss:1053.321044921875\n",
      "918/3000 train_loss: 104.84324645996094 test_loss:1043.7039794921875\n",
      "919/3000 train_loss: 102.82892608642578 test_loss:1055.6925048828125\n",
      "920/3000 train_loss: 114.29852294921875 test_loss:1026.80712890625\n",
      "921/3000 train_loss: 105.04682922363281 test_loss:1030.6380615234375\n",
      "922/3000 train_loss: 106.01887512207031 test_loss:1089.514892578125\n",
      "923/3000 train_loss: 108.81959533691406 test_loss:1051.17578125\n",
      "924/3000 train_loss: 111.99404907226562 test_loss:1046.1085205078125\n",
      "925/3000 train_loss: 105.58984375 test_loss:1059.5091552734375\n",
      "926/3000 train_loss: 105.57527160644531 test_loss:1039.6485595703125\n",
      "927/3000 train_loss: 105.26354217529297 test_loss:1040.1380615234375\n",
      "928/3000 train_loss: 104.56905364990234 test_loss:1047.7979736328125\n",
      "929/3000 train_loss: 118.1003189086914 test_loss:1066.45458984375\n",
      "930/3000 train_loss: 138.8777313232422 test_loss:1042.4910888671875\n",
      "931/3000 train_loss: 115.68968200683594 test_loss:1074.0718994140625\n",
      "932/3000 train_loss: 106.09556579589844 test_loss:1058.62158203125\n",
      "933/3000 train_loss: 108.44535827636719 test_loss:1034.6563720703125\n",
      "934/3000 train_loss: 99.38532257080078 test_loss:1031.61083984375\n",
      "935/3000 train_loss: 106.86183166503906 test_loss:1024.015869140625\n",
      "936/3000 train_loss: 105.25202178955078 test_loss:1040.0352783203125\n",
      "937/3000 train_loss: 94.10699462890625 test_loss:1034.5555419921875\n",
      "938/3000 train_loss: 103.79017639160156 test_loss:1057.8541259765625\n",
      "939/3000 train_loss: 112.70905303955078 test_loss:1038.573974609375\n",
      "940/3000 train_loss: 110.18756103515625 test_loss:1018.0201416015625\n",
      "941/3000 train_loss: 103.9832534790039 test_loss:1079.21337890625\n",
      "942/3000 train_loss: 112.59724426269531 test_loss:1043.9461669921875\n",
      "943/3000 train_loss: 99.6561050415039 test_loss:1027.60546875\n",
      "944/3000 train_loss: 95.95703125 test_loss:1022.9891967773438\n",
      "945/3000 train_loss: 97.5555191040039 test_loss:1033.97021484375\n",
      "946/3000 train_loss: 109.99947357177734 test_loss:1027.9166259765625\n",
      "947/3000 train_loss: 101.59803009033203 test_loss:1029.7186279296875\n",
      "948/3000 train_loss: 109.95004272460938 test_loss:1066.4609375\n",
      "949/3000 train_loss: 102.76863098144531 test_loss:1066.7490234375\n",
      "950/3000 train_loss: 97.88330078125 test_loss:1045.9183349609375\n",
      "951/3000 train_loss: 100.60801696777344 test_loss:1032.81298828125\n",
      "952/3000 train_loss: 109.53562927246094 test_loss:1087.9573974609375\n",
      "953/3000 train_loss: 98.17359924316406 test_loss:1000.7276000976562\n",
      "954/3000 train_loss: 112.23053741455078 test_loss:989.2778930664062\n",
      "955/3000 train_loss: 94.5474624633789 test_loss:1019.5584716796875\n",
      "956/3000 train_loss: 95.3041000366211 test_loss:1010.559326171875\n",
      "957/3000 train_loss: 101.67153930664062 test_loss:1007.5632934570312\n",
      "958/3000 train_loss: 101.19956970214844 test_loss:1055.5576171875\n",
      "959/3000 train_loss: 96.42744445800781 test_loss:977.0528564453125\n",
      "960/3000 train_loss: 110.57444763183594 test_loss:989.6748046875\n",
      "961/3000 train_loss: 94.94642639160156 test_loss:1037.903564453125\n",
      "962/3000 train_loss: 97.19830322265625 test_loss:1027.23876953125\n",
      "963/3000 train_loss: 97.0996322631836 test_loss:995.0042724609375\n",
      "964/3000 train_loss: 106.39433288574219 test_loss:1002.7929077148438\n",
      "965/3000 train_loss: 104.26069641113281 test_loss:1012.6136474609375\n",
      "966/3000 train_loss: 95.12284088134766 test_loss:993.1484375\n",
      "967/3000 train_loss: 101.78491973876953 test_loss:1006.4109497070312\n",
      "968/3000 train_loss: 106.04714965820312 test_loss:1007.2393188476562\n",
      "969/3000 train_loss: 111.03636169433594 test_loss:1019.865966796875\n",
      "970/3000 train_loss: 222.10772705078125 test_loss:995.8663330078125\n",
      "971/3000 train_loss: 192.12158203125 test_loss:1056.2879638671875\n",
      "972/3000 train_loss: 139.5647430419922 test_loss:1088.2535400390625\n",
      "973/3000 train_loss: 127.63536071777344 test_loss:1082.72509765625\n",
      "974/3000 train_loss: 135.59994506835938 test_loss:1100.918212890625\n",
      "975/3000 train_loss: 106.11980438232422 test_loss:1066.9659423828125\n",
      "976/3000 train_loss: 111.35604095458984 test_loss:1065.4892578125\n",
      "977/3000 train_loss: 107.28836059570312 test_loss:1072.37744140625\n",
      "978/3000 train_loss: 99.58661651611328 test_loss:1054.9560546875\n",
      "979/3000 train_loss: 108.2637939453125 test_loss:1059.260986328125\n",
      "980/3000 train_loss: 103.4880142211914 test_loss:1050.1890869140625\n",
      "981/3000 train_loss: 149.5905303955078 test_loss:1071.22216796875\n",
      "982/3000 train_loss: 107.45317077636719 test_loss:1032.2685546875\n",
      "983/3000 train_loss: 112.03976440429688 test_loss:1065.699462890625\n",
      "984/3000 train_loss: 93.31021881103516 test_loss:1043.0928955078125\n",
      "985/3000 train_loss: 93.19063568115234 test_loss:1040.01513671875\n",
      "986/3000 train_loss: 96.7206039428711 test_loss:1050.7220458984375\n",
      "987/3000 train_loss: 101.49159240722656 test_loss:1041.57958984375\n",
      "988/3000 train_loss: 93.84104919433594 test_loss:1051.1197509765625\n",
      "989/3000 train_loss: 98.95697784423828 test_loss:1037.911865234375\n",
      "990/3000 train_loss: 98.63047790527344 test_loss:1019.7833251953125\n",
      "991/3000 train_loss: 103.84983825683594 test_loss:1058.33056640625\n",
      "992/3000 train_loss: 97.411376953125 test_loss:1072.5166015625\n",
      "993/3000 train_loss: 90.97797393798828 test_loss:1045.357666015625\n",
      "994/3000 train_loss: 90.7539291381836 test_loss:1067.861572265625\n",
      "995/3000 train_loss: 93.17211151123047 test_loss:1057.1944580078125\n",
      "996/3000 train_loss: 98.32220458984375 test_loss:1065.1309814453125\n",
      "997/3000 train_loss: 90.04845428466797 test_loss:1048.9010009765625\n",
      "998/3000 train_loss: 90.465087890625 test_loss:1047.3492431640625\n",
      "999/3000 train_loss: 86.73271179199219 test_loss:1053.8511962890625\n",
      "1000/3000 train_loss: 90.9021224975586 test_loss:1035.1898193359375\n",
      "1001/3000 train_loss: 101.3836441040039 test_loss:1027.483154296875\n",
      "1002/3000 train_loss: 84.9210433959961 test_loss:1012.5341186523438\n",
      "1003/3000 train_loss: 94.74140930175781 test_loss:1014.948974609375\n",
      "1004/3000 train_loss: 99.86881256103516 test_loss:1010.5062866210938\n",
      "1005/3000 train_loss: 95.27867126464844 test_loss:1005.6573486328125\n",
      "1006/3000 train_loss: 92.44828796386719 test_loss:995.2504272460938\n",
      "1007/3000 train_loss: 94.1900405883789 test_loss:977.8321533203125\n",
      "1008/3000 train_loss: 103.6152114868164 test_loss:973.6152954101562\n",
      "1009/3000 train_loss: 99.70325469970703 test_loss:992.283935546875\n",
      "1010/3000 train_loss: 92.5574951171875 test_loss:1015.6407470703125\n",
      "1011/3000 train_loss: 89.7320785522461 test_loss:978.8720092773438\n",
      "1012/3000 train_loss: 92.75945281982422 test_loss:1026.8206787109375\n",
      "1013/3000 train_loss: 122.69146728515625 test_loss:1016.2420043945312\n",
      "1014/3000 train_loss: 95.36504364013672 test_loss:1084.6708984375\n",
      "1015/3000 train_loss: 102.5845718383789 test_loss:1060.265869140625\n",
      "1016/3000 train_loss: 120.3624038696289 test_loss:1051.337646484375\n",
      "1017/3000 train_loss: 104.99760437011719 test_loss:1070.0009765625\n",
      "1018/3000 train_loss: 135.6611785888672 test_loss:1062.8714599609375\n",
      "1019/3000 train_loss: 94.71540069580078 test_loss:990.4450073242188\n",
      "1020/3000 train_loss: 98.65091705322266 test_loss:999.2399291992188\n",
      "1021/3000 train_loss: 97.90390014648438 test_loss:1012.7236328125\n",
      "1022/3000 train_loss: 106.16520690917969 test_loss:1013.196044921875\n",
      "1023/3000 train_loss: 99.60823059082031 test_loss:1014.6236572265625\n",
      "1024/3000 train_loss: 91.48229217529297 test_loss:1019.41650390625\n",
      "1025/3000 train_loss: 91.07006072998047 test_loss:1014.937744140625\n",
      "1026/3000 train_loss: 89.4286880493164 test_loss:1010.4329223632812\n",
      "1027/3000 train_loss: 97.63854217529297 test_loss:1021.2816162109375\n",
      "1028/3000 train_loss: 113.30631256103516 test_loss:975.4288330078125\n",
      "1029/3000 train_loss: 96.8248519897461 test_loss:964.4884643554688\n",
      "1030/3000 train_loss: 84.3010025024414 test_loss:967.1356201171875\n",
      "1031/3000 train_loss: 85.23667907714844 test_loss:935.5243530273438\n",
      "1032/3000 train_loss: 97.04981994628906 test_loss:972.093505859375\n",
      "1033/3000 train_loss: 88.32068634033203 test_loss:975.5050048828125\n",
      "1034/3000 train_loss: 87.12016296386719 test_loss:988.2302856445312\n",
      "1035/3000 train_loss: 87.51136016845703 test_loss:993.664306640625\n",
      "1036/3000 train_loss: 86.33363342285156 test_loss:985.6622314453125\n",
      "1037/3000 train_loss: 86.66618347167969 test_loss:974.1634521484375\n",
      "1038/3000 train_loss: 90.65400695800781 test_loss:956.0674438476562\n",
      "1039/3000 train_loss: 83.80823516845703 test_loss:969.5756225585938\n",
      "1040/3000 train_loss: 89.51614379882812 test_loss:979.3781127929688\n",
      "1041/3000 train_loss: 88.60946655273438 test_loss:976.3582763671875\n",
      "1042/3000 train_loss: 80.578369140625 test_loss:992.5482788085938\n",
      "1043/3000 train_loss: 86.21533203125 test_loss:989.782958984375\n",
      "1044/3000 train_loss: 88.23268127441406 test_loss:997.5581665039062\n",
      "1045/3000 train_loss: 90.5843734741211 test_loss:970.342529296875\n",
      "1046/3000 train_loss: 95.0048599243164 test_loss:935.19580078125\n",
      "1047/3000 train_loss: 88.0663070678711 test_loss:975.93994140625\n",
      "1048/3000 train_loss: 98.53358459472656 test_loss:928.5284423828125\n",
      "1049/3000 train_loss: 90.31787872314453 test_loss:926.7550659179688\n",
      "1050/3000 train_loss: 94.91605377197266 test_loss:953.8435668945312\n",
      "1051/3000 train_loss: 99.31510162353516 test_loss:932.06396484375\n",
      "1052/3000 train_loss: 93.10379791259766 test_loss:994.9988403320312\n",
      "1053/3000 train_loss: 85.17565155029297 test_loss:978.8848876953125\n",
      "1054/3000 train_loss: 104.19239807128906 test_loss:1001.7404174804688\n",
      "1055/3000 train_loss: 82.73341369628906 test_loss:953.9519653320312\n",
      "1056/3000 train_loss: 100.2460708618164 test_loss:981.1174926757812\n",
      "1057/3000 train_loss: 105.16255950927734 test_loss:980.5771484375\n",
      "1058/3000 train_loss: 104.9598617553711 test_loss:965.2696533203125\n",
      "1059/3000 train_loss: 100.54743194580078 test_loss:927.870361328125\n",
      "1060/3000 train_loss: 83.5704574584961 test_loss:918.427001953125\n",
      "1061/3000 train_loss: 84.78813171386719 test_loss:970.3974609375\n",
      "1062/3000 train_loss: 95.83968353271484 test_loss:988.34228515625\n",
      "1063/3000 train_loss: 91.76891326904297 test_loss:992.184814453125\n",
      "1064/3000 train_loss: 94.9925765991211 test_loss:1001.218505859375\n",
      "1065/3000 train_loss: 96.217529296875 test_loss:1001.5364379882812\n",
      "1066/3000 train_loss: 97.59979248046875 test_loss:974.3076171875\n",
      "1067/3000 train_loss: 90.21739959716797 test_loss:1039.942626953125\n",
      "1068/3000 train_loss: 88.64820861816406 test_loss:1048.131103515625\n",
      "1069/3000 train_loss: 93.51838684082031 test_loss:994.3616333007812\n",
      "1070/3000 train_loss: 90.986572265625 test_loss:952.4325561523438\n",
      "1071/3000 train_loss: 103.44837188720703 test_loss:980.2156372070312\n",
      "1072/3000 train_loss: 89.51469421386719 test_loss:1005.6195068359375\n",
      "1073/3000 train_loss: 92.32528686523438 test_loss:974.8771362304688\n",
      "1074/3000 train_loss: 85.64424896240234 test_loss:967.678466796875\n",
      "1075/3000 train_loss: 90.48705291748047 test_loss:964.1768188476562\n",
      "1076/3000 train_loss: 89.24925231933594 test_loss:951.9100952148438\n",
      "1077/3000 train_loss: 102.31631469726562 test_loss:942.5791625976562\n",
      "1078/3000 train_loss: 89.32787322998047 test_loss:960.3975219726562\n",
      "1079/3000 train_loss: 91.28363037109375 test_loss:967.14794921875\n",
      "1080/3000 train_loss: 80.17719268798828 test_loss:974.2487182617188\n",
      "1081/3000 train_loss: 82.0749282836914 test_loss:971.5244140625\n",
      "1082/3000 train_loss: 85.88867950439453 test_loss:989.1378173828125\n",
      "1083/3000 train_loss: 80.47142028808594 test_loss:969.5531616210938\n",
      "1084/3000 train_loss: 85.399169921875 test_loss:959.4429931640625\n",
      "1085/3000 train_loss: 87.41462707519531 test_loss:982.2734985351562\n",
      "1086/3000 train_loss: 89.23432922363281 test_loss:951.718994140625\n",
      "1087/3000 train_loss: 93.43077850341797 test_loss:930.031494140625\n",
      "1088/3000 train_loss: 94.33175659179688 test_loss:1025.4439697265625\n",
      "1089/3000 train_loss: 85.04389190673828 test_loss:958.1228637695312\n",
      "1090/3000 train_loss: 80.46961212158203 test_loss:941.6908569335938\n",
      "1091/3000 train_loss: 81.7437515258789 test_loss:937.251220703125\n",
      "1092/3000 train_loss: 82.26765441894531 test_loss:938.0430908203125\n",
      "1093/3000 train_loss: 85.43563079833984 test_loss:937.6616821289062\n",
      "1094/3000 train_loss: 88.10452270507812 test_loss:935.3367309570312\n",
      "1095/3000 train_loss: 84.72909545898438 test_loss:959.451416015625\n",
      "1096/3000 train_loss: 84.07219696044922 test_loss:982.3088989257812\n",
      "1097/3000 train_loss: 94.18072509765625 test_loss:965.6715698242188\n",
      "1098/3000 train_loss: 89.17604064941406 test_loss:1024.9326171875\n",
      "1099/3000 train_loss: 88.22164154052734 test_loss:982.5134887695312\n",
      "1100/3000 train_loss: 90.1761703491211 test_loss:975.6324462890625\n",
      "1101/3000 train_loss: 85.52428436279297 test_loss:969.1669921875\n",
      "1102/3000 train_loss: 90.70767211914062 test_loss:955.5277709960938\n",
      "1103/3000 train_loss: 91.38074493408203 test_loss:943.979736328125\n",
      "1104/3000 train_loss: 82.93305206298828 test_loss:929.791015625\n",
      "1105/3000 train_loss: 79.69486236572266 test_loss:934.316650390625\n",
      "1106/3000 train_loss: 83.58414459228516 test_loss:941.0612182617188\n",
      "1107/3000 train_loss: 91.45944213867188 test_loss:924.6485595703125\n",
      "1108/3000 train_loss: 84.06136322021484 test_loss:921.97509765625\n",
      "1109/3000 train_loss: 82.14656829833984 test_loss:913.0072021484375\n",
      "1110/3000 train_loss: 93.4390869140625 test_loss:932.335693359375\n",
      "1111/3000 train_loss: 93.70234680175781 test_loss:918.8846435546875\n",
      "1112/3000 train_loss: 84.34270477294922 test_loss:940.9696655273438\n",
      "1113/3000 train_loss: 86.51850891113281 test_loss:924.9097290039062\n",
      "1114/3000 train_loss: 84.71839141845703 test_loss:953.22900390625\n",
      "1115/3000 train_loss: 92.41473388671875 test_loss:907.4838256835938\n",
      "1116/3000 train_loss: 97.20354461669922 test_loss:878.1929931640625\n",
      "1117/3000 train_loss: 100.29063415527344 test_loss:985.6166381835938\n",
      "1118/3000 train_loss: 107.2668685913086 test_loss:915.6978149414062\n",
      "1119/3000 train_loss: 95.18573760986328 test_loss:937.5892333984375\n",
      "1120/3000 train_loss: 91.03174591064453 test_loss:1009.050048828125\n",
      "1121/3000 train_loss: 94.61431884765625 test_loss:1020.5142211914062\n",
      "1122/3000 train_loss: 83.93924713134766 test_loss:996.7378540039062\n",
      "1123/3000 train_loss: 85.81597137451172 test_loss:990.8234252929688\n",
      "1124/3000 train_loss: 104.67381286621094 test_loss:1013.230224609375\n",
      "1125/3000 train_loss: 85.24445343017578 test_loss:1029.181396484375\n",
      "1126/3000 train_loss: 95.15654754638672 test_loss:930.2877197265625\n",
      "1127/3000 train_loss: 89.03501892089844 test_loss:981.31396484375\n",
      "1128/3000 train_loss: 90.21666717529297 test_loss:958.684814453125\n",
      "1129/3000 train_loss: 82.04426574707031 test_loss:956.5952758789062\n",
      "1130/3000 train_loss: 88.96131896972656 test_loss:942.8428344726562\n",
      "1131/3000 train_loss: 80.91936492919922 test_loss:938.1286010742188\n",
      "1132/3000 train_loss: 85.9728775024414 test_loss:947.503662109375\n",
      "1133/3000 train_loss: 80.14236450195312 test_loss:966.00927734375\n",
      "1134/3000 train_loss: 96.24505615234375 test_loss:926.41455078125\n",
      "1135/3000 train_loss: 77.19099426269531 test_loss:928.0645141601562\n",
      "1136/3000 train_loss: 85.7166748046875 test_loss:937.411376953125\n",
      "1137/3000 train_loss: 76.77987670898438 test_loss:908.2225952148438\n",
      "1138/3000 train_loss: 89.96199035644531 test_loss:932.8226928710938\n",
      "1139/3000 train_loss: 90.71621704101562 test_loss:912.5938720703125\n",
      "1140/3000 train_loss: 83.55791473388672 test_loss:1023.2249145507812\n",
      "1141/3000 train_loss: 102.26029205322266 test_loss:911.9635620117188\n",
      "1142/3000 train_loss: 79.52422332763672 test_loss:949.7588500976562\n",
      "1143/3000 train_loss: 92.61390686035156 test_loss:944.8424682617188\n",
      "1144/3000 train_loss: 85.0882797241211 test_loss:931.023681640625\n",
      "1145/3000 train_loss: 90.491455078125 test_loss:926.9722900390625\n",
      "1146/3000 train_loss: 85.65625 test_loss:933.0333251953125\n",
      "1147/3000 train_loss: 86.5926742553711 test_loss:973.4031982421875\n",
      "1148/3000 train_loss: 72.882080078125 test_loss:930.6138305664062\n",
      "1149/3000 train_loss: 86.54940032958984 test_loss:930.5007934570312\n",
      "1150/3000 train_loss: 100.81761169433594 test_loss:956.8800048828125\n",
      "1151/3000 train_loss: 82.30903625488281 test_loss:991.3331909179688\n",
      "1152/3000 train_loss: 87.1822280883789 test_loss:969.5033569335938\n",
      "1153/3000 train_loss: 84.95304107666016 test_loss:936.8829956054688\n",
      "1154/3000 train_loss: 78.2564926147461 test_loss:943.7685546875\n",
      "1155/3000 train_loss: 77.58399963378906 test_loss:937.1375732421875\n",
      "1156/3000 train_loss: 102.90479278564453 test_loss:954.0369262695312\n",
      "1157/3000 train_loss: 90.66651153564453 test_loss:927.4003295898438\n",
      "1158/3000 train_loss: 77.4317626953125 test_loss:978.1740112304688\n",
      "1159/3000 train_loss: 82.09608459472656 test_loss:977.1953125\n",
      "1160/3000 train_loss: 82.83293151855469 test_loss:1003.1614990234375\n",
      "1161/3000 train_loss: 80.94096374511719 test_loss:967.5577392578125\n",
      "1162/3000 train_loss: 78.64542388916016 test_loss:958.3511352539062\n",
      "1163/3000 train_loss: 84.42330932617188 test_loss:965.6741943359375\n",
      "1164/3000 train_loss: 77.98358154296875 test_loss:947.8123779296875\n",
      "1165/3000 train_loss: 82.19844818115234 test_loss:976.3001708984375\n",
      "1166/3000 train_loss: 80.90083312988281 test_loss:993.7296142578125\n",
      "1167/3000 train_loss: 78.22090148925781 test_loss:977.5430297851562\n",
      "1168/3000 train_loss: 86.28529357910156 test_loss:983.2755737304688\n",
      "1169/3000 train_loss: 74.06782531738281 test_loss:956.555908203125\n",
      "1170/3000 train_loss: 73.52587127685547 test_loss:961.73828125\n",
      "1171/3000 train_loss: 83.69757843017578 test_loss:956.6492919921875\n",
      "1172/3000 train_loss: 72.81108093261719 test_loss:962.6954345703125\n",
      "1173/3000 train_loss: 77.01895141601562 test_loss:966.7891235351562\n",
      "1174/3000 train_loss: 92.4781723022461 test_loss:955.4195556640625\n",
      "1175/3000 train_loss: 83.71281433105469 test_loss:961.3687744140625\n",
      "1176/3000 train_loss: 76.31379699707031 test_loss:966.9601440429688\n",
      "1177/3000 train_loss: 78.5132827758789 test_loss:944.71435546875\n",
      "1178/3000 train_loss: 84.08081817626953 test_loss:957.783935546875\n",
      "1179/3000 train_loss: 79.5651626586914 test_loss:974.99755859375\n",
      "1180/3000 train_loss: 77.55529022216797 test_loss:957.9266357421875\n",
      "1181/3000 train_loss: 80.4457015991211 test_loss:973.1246948242188\n",
      "1182/3000 train_loss: 81.50261688232422 test_loss:990.6149291992188\n",
      "1183/3000 train_loss: 81.30158996582031 test_loss:957.1531982421875\n",
      "1184/3000 train_loss: 72.44771575927734 test_loss:988.8821411132812\n",
      "1185/3000 train_loss: 76.96682739257812 test_loss:965.7385864257812\n",
      "1186/3000 train_loss: 73.70138549804688 test_loss:1009.7110595703125\n",
      "1187/3000 train_loss: 80.62637329101562 test_loss:977.9119262695312\n",
      "1188/3000 train_loss: 77.57829284667969 test_loss:991.8017578125\n",
      "1189/3000 train_loss: 89.5381851196289 test_loss:1016.645263671875\n",
      "1190/3000 train_loss: 94.71280670166016 test_loss:949.55078125\n",
      "1191/3000 train_loss: 80.86848449707031 test_loss:981.506103515625\n",
      "1192/3000 train_loss: 71.09508514404297 test_loss:959.3035278320312\n",
      "1193/3000 train_loss: 70.78620910644531 test_loss:989.2421264648438\n",
      "1194/3000 train_loss: 68.61309814453125 test_loss:999.2554321289062\n",
      "1195/3000 train_loss: 86.9430160522461 test_loss:969.4478149414062\n",
      "1196/3000 train_loss: 73.93721008300781 test_loss:924.4296264648438\n",
      "1197/3000 train_loss: 79.2330551147461 test_loss:938.905517578125\n",
      "1198/3000 train_loss: 91.23330688476562 test_loss:966.6299438476562\n",
      "1199/3000 train_loss: 87.11674499511719 test_loss:958.2955932617188\n",
      "1200/3000 train_loss: 113.45301818847656 test_loss:953.7591552734375\n",
      "1201/3000 train_loss: 91.72624969482422 test_loss:922.4591064453125\n",
      "1202/3000 train_loss: 94.11282348632812 test_loss:1009.7616577148438\n",
      "1203/3000 train_loss: 77.65139770507812 test_loss:1011.1364135742188\n",
      "1204/3000 train_loss: 81.26679229736328 test_loss:1020.9179077148438\n",
      "1205/3000 train_loss: 80.66246032714844 test_loss:1035.3787841796875\n",
      "1206/3000 train_loss: 82.00519561767578 test_loss:995.595703125\n",
      "1207/3000 train_loss: 72.32717895507812 test_loss:979.5033569335938\n",
      "1208/3000 train_loss: 81.05335235595703 test_loss:1005.927978515625\n",
      "1209/3000 train_loss: 85.91661071777344 test_loss:976.5888061523438\n",
      "1210/3000 train_loss: 74.4122314453125 test_loss:978.217529296875\n",
      "1211/3000 train_loss: 83.04859924316406 test_loss:956.9071655273438\n",
      "1212/3000 train_loss: 89.45257568359375 test_loss:962.6165161132812\n",
      "1213/3000 train_loss: 81.91133880615234 test_loss:913.92333984375\n",
      "1214/3000 train_loss: 83.06456756591797 test_loss:929.5125122070312\n",
      "1215/3000 train_loss: 84.15605163574219 test_loss:928.72216796875\n",
      "1216/3000 train_loss: 102.49349975585938 test_loss:904.4050903320312\n",
      "1217/3000 train_loss: 75.32847595214844 test_loss:941.791748046875\n",
      "1218/3000 train_loss: 77.78395080566406 test_loss:937.8655395507812\n",
      "1219/3000 train_loss: 83.51905822753906 test_loss:941.886962890625\n",
      "1220/3000 train_loss: 87.1254653930664 test_loss:952.4402465820312\n",
      "1221/3000 train_loss: 81.09602355957031 test_loss:921.051513671875\n",
      "1222/3000 train_loss: 92.09172821044922 test_loss:934.94287109375\n",
      "1223/3000 train_loss: 83.58260345458984 test_loss:931.236572265625\n",
      "1224/3000 train_loss: 87.30783081054688 test_loss:963.1690673828125\n",
      "1225/3000 train_loss: 77.72249603271484 test_loss:973.2280883789062\n",
      "1226/3000 train_loss: 83.10591888427734 test_loss:973.1344604492188\n",
      "1227/3000 train_loss: 85.53764343261719 test_loss:971.284912109375\n",
      "1228/3000 train_loss: 76.8163833618164 test_loss:969.5316772460938\n",
      "1229/3000 train_loss: 86.24251556396484 test_loss:966.7352294921875\n",
      "1230/3000 train_loss: 95.86760711669922 test_loss:949.5299072265625\n",
      "1231/3000 train_loss: 85.14225006103516 test_loss:914.6846923828125\n",
      "1232/3000 train_loss: 73.00666809082031 test_loss:965.018798828125\n",
      "1233/3000 train_loss: 84.24250030517578 test_loss:897.1885375976562\n",
      "1234/3000 train_loss: 74.8321762084961 test_loss:916.55615234375\n",
      "1235/3000 train_loss: 83.2144546508789 test_loss:899.5503540039062\n",
      "1236/3000 train_loss: 89.21326446533203 test_loss:952.3555908203125\n",
      "1237/3000 train_loss: 77.74928283691406 test_loss:916.7373657226562\n",
      "1238/3000 train_loss: 77.15435028076172 test_loss:924.177978515625\n",
      "1239/3000 train_loss: 78.20736694335938 test_loss:948.5457763671875\n",
      "1240/3000 train_loss: 90.86095428466797 test_loss:902.094482421875\n",
      "1241/3000 train_loss: 86.18972778320312 test_loss:963.747802734375\n",
      "1242/3000 train_loss: 83.44718170166016 test_loss:1010.61376953125\n",
      "1243/3000 train_loss: 80.66708374023438 test_loss:955.024169921875\n",
      "1244/3000 train_loss: 78.10444641113281 test_loss:965.1934204101562\n",
      "1245/3000 train_loss: 100.11758422851562 test_loss:982.8502807617188\n",
      "1246/3000 train_loss: 79.51106262207031 test_loss:968.0402221679688\n",
      "1247/3000 train_loss: 81.35589599609375 test_loss:949.8379516601562\n",
      "1248/3000 train_loss: 76.08514404296875 test_loss:1008.7898559570312\n",
      "1249/3000 train_loss: 80.6939926147461 test_loss:979.8046264648438\n",
      "1250/3000 train_loss: 79.85282897949219 test_loss:981.1658935546875\n",
      "1251/3000 train_loss: 72.56682586669922 test_loss:945.4449462890625\n",
      "1252/3000 train_loss: 70.9220962524414 test_loss:964.8805541992188\n",
      "1253/3000 train_loss: 71.33406066894531 test_loss:943.1293334960938\n",
      "1254/3000 train_loss: 71.70840454101562 test_loss:938.5162963867188\n",
      "1255/3000 train_loss: 78.59477996826172 test_loss:965.0947875976562\n",
      "1256/3000 train_loss: 76.60929870605469 test_loss:958.4771118164062\n",
      "1257/3000 train_loss: 87.89009857177734 test_loss:942.33837890625\n",
      "1258/3000 train_loss: 74.49418640136719 test_loss:982.711181640625\n",
      "1259/3000 train_loss: 71.64460754394531 test_loss:979.770263671875\n",
      "1260/3000 train_loss: 77.57036590576172 test_loss:971.364990234375\n",
      "1261/3000 train_loss: 73.38713073730469 test_loss:993.9774169921875\n",
      "1262/3000 train_loss: 72.55956268310547 test_loss:1000.3206176757812\n",
      "1263/3000 train_loss: 76.55455017089844 test_loss:957.786865234375\n",
      "1264/3000 train_loss: 87.96283721923828 test_loss:954.1546020507812\n",
      "1265/3000 train_loss: 74.42090606689453 test_loss:935.3704833984375\n",
      "1266/3000 train_loss: 80.2654037475586 test_loss:962.2152709960938\n",
      "1267/3000 train_loss: 85.37840270996094 test_loss:961.0242309570312\n",
      "1268/3000 train_loss: 95.54013061523438 test_loss:925.5146484375\n",
      "1269/3000 train_loss: 81.57334899902344 test_loss:870.9827270507812\n",
      "1270/3000 train_loss: 80.70199584960938 test_loss:882.22412109375\n",
      "1271/3000 train_loss: 74.85757446289062 test_loss:935.84619140625\n",
      "1272/3000 train_loss: 76.50357818603516 test_loss:919.6631469726562\n",
      "1273/3000 train_loss: 72.11680603027344 test_loss:905.4060668945312\n",
      "1274/3000 train_loss: 75.99119567871094 test_loss:932.2199096679688\n",
      "1275/3000 train_loss: 69.3343734741211 test_loss:910.66845703125\n",
      "1276/3000 train_loss: 70.87628936767578 test_loss:923.7615356445312\n",
      "1277/3000 train_loss: 68.50580596923828 test_loss:920.4612426757812\n",
      "1278/3000 train_loss: 93.34185028076172 test_loss:904.5232543945312\n",
      "1279/3000 train_loss: 82.41580963134766 test_loss:879.5858154296875\n",
      "1280/3000 train_loss: 73.03541564941406 test_loss:938.2875366210938\n",
      "1281/3000 train_loss: 75.22381591796875 test_loss:876.185546875\n",
      "1282/3000 train_loss: 65.02401733398438 test_loss:885.9564208984375\n",
      "1283/3000 train_loss: 78.03617858886719 test_loss:889.9227905273438\n",
      "1284/3000 train_loss: 81.06636810302734 test_loss:955.5835571289062\n",
      "1285/3000 train_loss: 74.54631042480469 test_loss:905.4136962890625\n",
      "1286/3000 train_loss: 77.23149871826172 test_loss:947.8247680664062\n",
      "1287/3000 train_loss: 74.74143981933594 test_loss:928.7000732421875\n",
      "1288/3000 train_loss: 74.88389587402344 test_loss:944.262451171875\n",
      "1289/3000 train_loss: 74.88365173339844 test_loss:928.3292846679688\n",
      "1290/3000 train_loss: 87.832763671875 test_loss:928.1337280273438\n",
      "1291/3000 train_loss: 72.62857055664062 test_loss:946.9481201171875\n",
      "1292/3000 train_loss: 87.83258056640625 test_loss:974.9008178710938\n",
      "1293/3000 train_loss: 72.68310546875 test_loss:1000.4396362304688\n",
      "1294/3000 train_loss: 73.91719055175781 test_loss:964.7871704101562\n",
      "1295/3000 train_loss: 74.43569946289062 test_loss:973.0799560546875\n",
      "1296/3000 train_loss: 69.22885131835938 test_loss:918.6653442382812\n",
      "1297/3000 train_loss: 73.93467712402344 test_loss:923.141357421875\n",
      "1298/3000 train_loss: 72.67665100097656 test_loss:926.4241333007812\n",
      "1299/3000 train_loss: 72.68484497070312 test_loss:954.4654541015625\n",
      "1300/3000 train_loss: 69.25611114501953 test_loss:926.1576538085938\n",
      "1301/3000 train_loss: 71.34452819824219 test_loss:915.1339721679688\n",
      "1302/3000 train_loss: 67.04353332519531 test_loss:923.78173828125\n",
      "1303/3000 train_loss: 74.252197265625 test_loss:926.0776977539062\n",
      "1304/3000 train_loss: 70.34278869628906 test_loss:955.462890625\n",
      "1305/3000 train_loss: 65.81307983398438 test_loss:949.7244873046875\n",
      "1306/3000 train_loss: 75.19058990478516 test_loss:939.5908203125\n",
      "1307/3000 train_loss: 79.67793273925781 test_loss:923.61669921875\n",
      "1308/3000 train_loss: 87.26537322998047 test_loss:950.7667846679688\n",
      "1309/3000 train_loss: 98.20377349853516 test_loss:1033.343017578125\n",
      "1310/3000 train_loss: 81.54141235351562 test_loss:996.6066284179688\n",
      "1311/3000 train_loss: 80.32191467285156 test_loss:1014.9036865234375\n",
      "1312/3000 train_loss: 78.52699279785156 test_loss:987.2262573242188\n",
      "1313/3000 train_loss: 69.31246948242188 test_loss:928.4501342773438\n",
      "1314/3000 train_loss: 77.48284149169922 test_loss:941.6883544921875\n",
      "1315/3000 train_loss: 68.32402038574219 test_loss:904.9181518554688\n",
      "1316/3000 train_loss: 72.39209747314453 test_loss:911.5619506835938\n",
      "1317/3000 train_loss: 70.91114044189453 test_loss:915.2861938476562\n",
      "1318/3000 train_loss: 70.73739624023438 test_loss:893.359619140625\n",
      "1319/3000 train_loss: 71.06267547607422 test_loss:909.245849609375\n",
      "1320/3000 train_loss: 63.61449432373047 test_loss:920.0732421875\n",
      "1321/3000 train_loss: 68.54077911376953 test_loss:905.0296020507812\n",
      "1322/3000 train_loss: 85.80460357666016 test_loss:919.5692138671875\n",
      "1323/3000 train_loss: 73.8387451171875 test_loss:887.254150390625\n",
      "1324/3000 train_loss: 77.6300048828125 test_loss:894.1704711914062\n",
      "1325/3000 train_loss: 71.07241821289062 test_loss:907.081298828125\n",
      "1326/3000 train_loss: 81.00469970703125 test_loss:913.715576171875\n",
      "1327/3000 train_loss: 87.1530990600586 test_loss:1026.1439208984375\n",
      "1328/3000 train_loss: 76.04247283935547 test_loss:997.3560791015625\n",
      "1329/3000 train_loss: 75.19865417480469 test_loss:993.579833984375\n",
      "1330/3000 train_loss: 73.33639526367188 test_loss:963.5021362304688\n",
      "1331/3000 train_loss: 72.85737609863281 test_loss:976.35205078125\n",
      "1332/3000 train_loss: 77.21704864501953 test_loss:969.2893676757812\n",
      "1333/3000 train_loss: 74.02626037597656 test_loss:951.9661865234375\n",
      "1334/3000 train_loss: 74.4968490600586 test_loss:984.7232055664062\n",
      "1335/3000 train_loss: 68.11729431152344 test_loss:1000.9856567382812\n",
      "1336/3000 train_loss: 71.58773803710938 test_loss:976.370361328125\n",
      "1337/3000 train_loss: 71.87975311279297 test_loss:966.0641479492188\n",
      "1338/3000 train_loss: 70.6613998413086 test_loss:949.423828125\n",
      "1339/3000 train_loss: 67.04165649414062 test_loss:926.673095703125\n",
      "1340/3000 train_loss: 74.61016845703125 test_loss:928.3861083984375\n",
      "1341/3000 train_loss: 71.30857849121094 test_loss:956.6076049804688\n",
      "1342/3000 train_loss: 70.159423828125 test_loss:935.8367919921875\n",
      "1343/3000 train_loss: 71.53435516357422 test_loss:932.2740478515625\n",
      "1344/3000 train_loss: 67.30175018310547 test_loss:937.8840942382812\n",
      "1345/3000 train_loss: 69.93730163574219 test_loss:927.6505737304688\n",
      "1346/3000 train_loss: 63.048221588134766 test_loss:928.5403442382812\n",
      "1347/3000 train_loss: 71.00175476074219 test_loss:926.4482421875\n",
      "1348/3000 train_loss: 55.88410949707031 test_loss:927.1452026367188\n",
      "1349/3000 train_loss: 79.5313491821289 test_loss:927.2964477539062\n",
      "1350/3000 train_loss: 59.800479888916016 test_loss:945.12548828125\n",
      "1351/3000 train_loss: 65.63880920410156 test_loss:945.8328857421875\n",
      "1352/3000 train_loss: 79.28343963623047 test_loss:926.2534790039062\n",
      "1353/3000 train_loss: 73.41228485107422 test_loss:967.247314453125\n",
      "1354/3000 train_loss: 69.81355285644531 test_loss:961.86767578125\n",
      "1355/3000 train_loss: 70.46758270263672 test_loss:988.1296997070312\n",
      "1356/3000 train_loss: 80.35276794433594 test_loss:936.6194458007812\n",
      "1357/3000 train_loss: 62.672401428222656 test_loss:929.8033447265625\n",
      "1358/3000 train_loss: 75.36250305175781 test_loss:928.810302734375\n",
      "1359/3000 train_loss: 63.620574951171875 test_loss:911.1871948242188\n",
      "1360/3000 train_loss: 65.68152618408203 test_loss:918.6921997070312\n",
      "1361/3000 train_loss: 64.6971664428711 test_loss:933.6474609375\n",
      "1362/3000 train_loss: 83.43543243408203 test_loss:991.18115234375\n",
      "1363/3000 train_loss: 66.16378021240234 test_loss:976.890869140625\n",
      "1364/3000 train_loss: 73.56202697753906 test_loss:969.072509765625\n",
      "1365/3000 train_loss: 72.82510375976562 test_loss:969.419189453125\n",
      "1366/3000 train_loss: 67.65205383300781 test_loss:970.888671875\n",
      "1367/3000 train_loss: 63.94499969482422 test_loss:964.6284790039062\n",
      "1368/3000 train_loss: 64.42015838623047 test_loss:955.508056640625\n",
      "1369/3000 train_loss: 60.379512786865234 test_loss:959.293701171875\n",
      "1370/3000 train_loss: 76.82046508789062 test_loss:939.8770751953125\n",
      "1371/3000 train_loss: 64.26860046386719 test_loss:965.5687255859375\n",
      "1372/3000 train_loss: 64.09698486328125 test_loss:983.8701171875\n",
      "1373/3000 train_loss: 93.44911193847656 test_loss:981.10546875\n",
      "1374/3000 train_loss: 77.05062103271484 test_loss:1070.0286865234375\n",
      "1375/3000 train_loss: 74.05096435546875 test_loss:1040.365966796875\n",
      "1376/3000 train_loss: 75.76205444335938 test_loss:1019.4542236328125\n",
      "1377/3000 train_loss: 70.12384796142578 test_loss:1024.24267578125\n",
      "1378/3000 train_loss: 75.46894836425781 test_loss:1018.1002807617188\n",
      "1379/3000 train_loss: 93.94876861572266 test_loss:1063.066650390625\n",
      "1380/3000 train_loss: 102.20096588134766 test_loss:1099.5\n",
      "1381/3000 train_loss: 115.44417572021484 test_loss:1009.2138671875\n",
      "1382/3000 train_loss: 90.11736297607422 test_loss:974.9580078125\n",
      "1383/3000 train_loss: 72.72471618652344 test_loss:966.97998046875\n",
      "1384/3000 train_loss: 77.06344604492188 test_loss:950.0082397460938\n",
      "1385/3000 train_loss: 79.29913330078125 test_loss:943.5740966796875\n",
      "1386/3000 train_loss: 85.44914245605469 test_loss:938.4873657226562\n",
      "1387/3000 train_loss: 65.74848937988281 test_loss:912.7407836914062\n",
      "1388/3000 train_loss: 74.57218933105469 test_loss:934.4285888671875\n",
      "1389/3000 train_loss: 106.85992431640625 test_loss:918.52490234375\n",
      "1390/3000 train_loss: 72.61914825439453 test_loss:959.736083984375\n",
      "1391/3000 train_loss: 71.78791046142578 test_loss:964.1220703125\n",
      "1392/3000 train_loss: 64.7139892578125 test_loss:942.0740356445312\n",
      "1393/3000 train_loss: 72.25403594970703 test_loss:952.3136596679688\n",
      "1394/3000 train_loss: 76.07093048095703 test_loss:887.1574096679688\n",
      "1395/3000 train_loss: 68.58265686035156 test_loss:894.3595581054688\n",
      "1396/3000 train_loss: 66.21212768554688 test_loss:906.78173828125\n",
      "1397/3000 train_loss: 66.81843566894531 test_loss:906.2456665039062\n",
      "1398/3000 train_loss: 66.93734741210938 test_loss:915.6016845703125\n",
      "1399/3000 train_loss: 63.00114440917969 test_loss:933.6112060546875\n",
      "1400/3000 train_loss: 79.96489715576172 test_loss:919.0081176757812\n",
      "1401/3000 train_loss: 75.63065338134766 test_loss:888.4620971679688\n",
      "1402/3000 train_loss: 63.471588134765625 test_loss:885.166748046875\n",
      "1403/3000 train_loss: 67.57984161376953 test_loss:882.6710815429688\n",
      "1404/3000 train_loss: 71.60920715332031 test_loss:885.8921508789062\n",
      "1405/3000 train_loss: 63.13809585571289 test_loss:853.9110717773438\n",
      "1406/3000 train_loss: 66.26331329345703 test_loss:896.895751953125\n",
      "1407/3000 train_loss: 66.25175476074219 test_loss:890.1196899414062\n",
      "1408/3000 train_loss: 57.43287658691406 test_loss:905.3320922851562\n",
      "1409/3000 train_loss: 59.25068283081055 test_loss:906.2798461914062\n",
      "1410/3000 train_loss: 65.59603881835938 test_loss:913.3592529296875\n",
      "1411/3000 train_loss: 87.37882995605469 test_loss:923.4427490234375\n",
      "1412/3000 train_loss: 84.2015609741211 test_loss:959.353515625\n",
      "1413/3000 train_loss: 68.05785369873047 test_loss:919.9489135742188\n",
      "1414/3000 train_loss: 67.52809143066406 test_loss:903.6046752929688\n",
      "1415/3000 train_loss: 73.23947143554688 test_loss:896.7691650390625\n",
      "1416/3000 train_loss: 109.37361145019531 test_loss:881.533447265625\n",
      "1417/3000 train_loss: 60.001319885253906 test_loss:908.03759765625\n",
      "1418/3000 train_loss: 71.64643096923828 test_loss:896.5670776367188\n",
      "1419/3000 train_loss: 67.19406127929688 test_loss:903.1211547851562\n",
      "1420/3000 train_loss: 59.55254364013672 test_loss:887.0778198242188\n",
      "1421/3000 train_loss: 65.88053131103516 test_loss:914.2844848632812\n",
      "1422/3000 train_loss: 71.46332550048828 test_loss:933.4175415039062\n",
      "1423/3000 train_loss: 61.677833557128906 test_loss:963.0509643554688\n",
      "1424/3000 train_loss: 72.54520416259766 test_loss:931.8661499023438\n",
      "1425/3000 train_loss: 58.05250549316406 test_loss:922.1882934570312\n",
      "1426/3000 train_loss: 75.33223724365234 test_loss:937.2913818359375\n",
      "1427/3000 train_loss: 64.27845764160156 test_loss:993.9841918945312\n",
      "1428/3000 train_loss: 62.35198974609375 test_loss:945.4327392578125\n",
      "1429/3000 train_loss: 63.45006561279297 test_loss:936.74951171875\n",
      "1430/3000 train_loss: 75.888916015625 test_loss:890.6216430664062\n",
      "1431/3000 train_loss: 62.217002868652344 test_loss:877.8318481445312\n",
      "1432/3000 train_loss: 64.5381851196289 test_loss:871.3117065429688\n",
      "1433/3000 train_loss: 71.13705444335938 test_loss:889.228515625\n",
      "1434/3000 train_loss: 56.8631591796875 test_loss:873.6353759765625\n",
      "1435/3000 train_loss: 59.66741180419922 test_loss:923.4611206054688\n",
      "1436/3000 train_loss: 67.41403198242188 test_loss:910.5953369140625\n",
      "1437/3000 train_loss: 71.01547241210938 test_loss:929.465576171875\n",
      "1438/3000 train_loss: 69.94184875488281 test_loss:944.50341796875\n",
      "1439/3000 train_loss: 54.3492431640625 test_loss:900.7499389648438\n",
      "1440/3000 train_loss: 60.18277359008789 test_loss:924.7615966796875\n",
      "1441/3000 train_loss: 55.5087776184082 test_loss:909.3056640625\n",
      "1442/3000 train_loss: 55.38743591308594 test_loss:929.1791381835938\n",
      "1443/3000 train_loss: 64.69361114501953 test_loss:910.5848999023438\n",
      "1444/3000 train_loss: 61.102909088134766 test_loss:935.9207763671875\n",
      "1445/3000 train_loss: 57.0357551574707 test_loss:930.7709350585938\n",
      "1446/3000 train_loss: 70.41350555419922 test_loss:921.0703735351562\n",
      "1447/3000 train_loss: 65.3853988647461 test_loss:902.7774047851562\n",
      "1448/3000 train_loss: 64.4793930053711 test_loss:872.0062255859375\n",
      "1449/3000 train_loss: 68.0872573852539 test_loss:882.1644897460938\n",
      "1450/3000 train_loss: 66.72312927246094 test_loss:927.439208984375\n",
      "1451/3000 train_loss: 59.881690979003906 test_loss:895.17236328125\n",
      "1452/3000 train_loss: 59.6759033203125 test_loss:916.3536987304688\n",
      "1453/3000 train_loss: 60.28959655761719 test_loss:876.4138793945312\n",
      "1454/3000 train_loss: 69.09446716308594 test_loss:878.884033203125\n",
      "1455/3000 train_loss: 65.99036407470703 test_loss:864.6429443359375\n",
      "1456/3000 train_loss: 61.67921829223633 test_loss:873.9811401367188\n",
      "1457/3000 train_loss: 60.292686462402344 test_loss:883.0618286132812\n",
      "1458/3000 train_loss: 63.86433410644531 test_loss:892.575927734375\n",
      "1459/3000 train_loss: 69.19766998291016 test_loss:902.007568359375\n",
      "1460/3000 train_loss: 60.18218231201172 test_loss:874.4144897460938\n",
      "1461/3000 train_loss: 62.54669952392578 test_loss:896.6317138671875\n",
      "1462/3000 train_loss: 76.87861633300781 test_loss:894.398193359375\n",
      "1463/3000 train_loss: 57.07795333862305 test_loss:922.61474609375\n",
      "1464/3000 train_loss: 66.77873229980469 test_loss:863.1577758789062\n",
      "1465/3000 train_loss: 74.50387573242188 test_loss:907.2760009765625\n",
      "1466/3000 train_loss: 58.248313903808594 test_loss:871.1536254882812\n",
      "1467/3000 train_loss: 63.98601150512695 test_loss:890.039306640625\n",
      "1468/3000 train_loss: 57.531883239746094 test_loss:908.7760009765625\n",
      "1469/3000 train_loss: 63.43029022216797 test_loss:889.47021484375\n",
      "1470/3000 train_loss: 57.512691497802734 test_loss:889.9051513671875\n",
      "1471/3000 train_loss: 70.443115234375 test_loss:891.4175415039062\n",
      "1472/3000 train_loss: 62.8235969543457 test_loss:883.8250122070312\n",
      "1473/3000 train_loss: 70.6614990234375 test_loss:866.3296508789062\n",
      "1474/3000 train_loss: 60.69439697265625 test_loss:921.469482421875\n",
      "1475/3000 train_loss: 66.21723175048828 test_loss:910.3589477539062\n",
      "1476/3000 train_loss: 66.48439025878906 test_loss:876.1409301757812\n",
      "1477/3000 train_loss: 66.01126098632812 test_loss:895.5845336914062\n",
      "1478/3000 train_loss: 60.976470947265625 test_loss:923.2582397460938\n",
      "1479/3000 train_loss: 71.29578399658203 test_loss:905.7391357421875\n",
      "1480/3000 train_loss: 58.19145584106445 test_loss:867.1896362304688\n",
      "1481/3000 train_loss: 58.302486419677734 test_loss:900.1325073242188\n",
      "1482/3000 train_loss: 56.5893669128418 test_loss:872.4642944335938\n",
      "1483/3000 train_loss: 62.95188522338867 test_loss:923.4475708007812\n",
      "1484/3000 train_loss: 55.47673416137695 test_loss:892.6072998046875\n",
      "1485/3000 train_loss: 79.85173034667969 test_loss:902.3794555664062\n",
      "1486/3000 train_loss: 55.27762985229492 test_loss:890.2023315429688\n",
      "1487/3000 train_loss: 58.596561431884766 test_loss:898.8108520507812\n",
      "1488/3000 train_loss: 62.02833557128906 test_loss:910.7742919921875\n",
      "1489/3000 train_loss: 64.79489135742188 test_loss:905.7044067382812\n",
      "1490/3000 train_loss: 57.72840118408203 test_loss:898.3468017578125\n",
      "1491/3000 train_loss: 51.836280822753906 test_loss:905.4076538085938\n",
      "1492/3000 train_loss: 64.20277404785156 test_loss:896.2572631835938\n",
      "1493/3000 train_loss: 58.993431091308594 test_loss:927.072265625\n",
      "1494/3000 train_loss: 65.19584655761719 test_loss:894.6351928710938\n",
      "1495/3000 train_loss: 65.88412475585938 test_loss:897.0739135742188\n",
      "1496/3000 train_loss: 63.57917022705078 test_loss:910.7216796875\n",
      "1497/3000 train_loss: 70.84201049804688 test_loss:879.8775634765625\n",
      "1498/3000 train_loss: 54.88434982299805 test_loss:880.1358642578125\n",
      "1499/3000 train_loss: 64.11813354492188 test_loss:876.0704345703125\n",
      "1500/3000 train_loss: 53.633018493652344 test_loss:871.9166259765625\n",
      "1501/3000 train_loss: 62.580108642578125 test_loss:892.0202026367188\n",
      "1502/3000 train_loss: 60.669734954833984 test_loss:878.1660766601562\n",
      "1503/3000 train_loss: 67.35499572753906 test_loss:856.0941162109375\n",
      "1504/3000 train_loss: 62.255245208740234 test_loss:877.597412109375\n",
      "1505/3000 train_loss: 69.11339569091797 test_loss:884.9930419921875\n",
      "1506/3000 train_loss: 60.68030548095703 test_loss:906.408447265625\n",
      "1507/3000 train_loss: 65.08973693847656 test_loss:885.907470703125\n",
      "1508/3000 train_loss: 62.788455963134766 test_loss:893.9773559570312\n",
      "1509/3000 train_loss: 67.4063491821289 test_loss:865.46337890625\n",
      "1510/3000 train_loss: 75.91012573242188 test_loss:910.2767333984375\n",
      "1511/3000 train_loss: 63.32862854003906 test_loss:915.6494140625\n",
      "1512/3000 train_loss: 67.48479461669922 test_loss:923.198974609375\n",
      "1513/3000 train_loss: 65.0145492553711 test_loss:924.5399169921875\n",
      "1514/3000 train_loss: 55.25801467895508 test_loss:931.8624267578125\n",
      "1515/3000 train_loss: 60.71855163574219 test_loss:905.0676879882812\n",
      "1516/3000 train_loss: 65.9365234375 test_loss:936.5623168945312\n",
      "1517/3000 train_loss: 62.81475830078125 test_loss:954.5811157226562\n",
      "1518/3000 train_loss: 60.14627456665039 test_loss:944.6292114257812\n",
      "1519/3000 train_loss: 59.68621063232422 test_loss:940.94921875\n",
      "1520/3000 train_loss: 62.35377502441406 test_loss:907.2861938476562\n",
      "1521/3000 train_loss: 56.12528991699219 test_loss:884.796630859375\n",
      "1522/3000 train_loss: 59.53913497924805 test_loss:920.6184692382812\n",
      "1523/3000 train_loss: 65.98876190185547 test_loss:901.9835205078125\n",
      "1524/3000 train_loss: 63.249385833740234 test_loss:871.697509765625\n",
      "1525/3000 train_loss: 59.49335479736328 test_loss:856.6954345703125\n",
      "1526/3000 train_loss: 65.4563980102539 test_loss:894.8917236328125\n",
      "1527/3000 train_loss: 56.15542221069336 test_loss:881.2655639648438\n",
      "1528/3000 train_loss: 63.78126525878906 test_loss:917.7142944335938\n",
      "1529/3000 train_loss: 66.89452362060547 test_loss:902.9768676757812\n",
      "1530/3000 train_loss: 65.5217056274414 test_loss:895.4471435546875\n",
      "1531/3000 train_loss: 63.38043975830078 test_loss:860.0501708984375\n",
      "1532/3000 train_loss: 59.52950668334961 test_loss:885.1973266601562\n",
      "1533/3000 train_loss: 63.26118469238281 test_loss:900.8668823242188\n",
      "1534/3000 train_loss: 56.02863693237305 test_loss:937.9525146484375\n",
      "1535/3000 train_loss: 61.99412536621094 test_loss:929.4228515625\n",
      "1536/3000 train_loss: 58.278175354003906 test_loss:912.8741455078125\n",
      "1537/3000 train_loss: 70.50713348388672 test_loss:922.9362182617188\n",
      "1538/3000 train_loss: 63.26218795776367 test_loss:886.673095703125\n",
      "1539/3000 train_loss: 63.85585021972656 test_loss:982.1212768554688\n",
      "1540/3000 train_loss: 66.13020324707031 test_loss:915.3941040039062\n",
      "1541/3000 train_loss: 57.56875991821289 test_loss:940.9202270507812\n",
      "1542/3000 train_loss: 61.01976776123047 test_loss:946.4056396484375\n",
      "1543/3000 train_loss: 64.53064727783203 test_loss:899.4452514648438\n",
      "1544/3000 train_loss: 57.72146987915039 test_loss:915.6242065429688\n",
      "1545/3000 train_loss: 68.80889892578125 test_loss:896.4214477539062\n",
      "1546/3000 train_loss: 63.43149185180664 test_loss:885.1680297851562\n",
      "1547/3000 train_loss: 60.829288482666016 test_loss:936.2000122070312\n",
      "1548/3000 train_loss: 72.51727294921875 test_loss:833.9700317382812\n",
      "1549/3000 train_loss: 57.93251037597656 test_loss:881.4229736328125\n",
      "1550/3000 train_loss: 59.85896301269531 test_loss:881.4127807617188\n",
      "1551/3000 train_loss: 57.08348846435547 test_loss:867.2068481445312\n",
      "1552/3000 train_loss: 62.7498893737793 test_loss:881.2294311523438\n",
      "1553/3000 train_loss: 56.87490463256836 test_loss:898.1829223632812\n",
      "1554/3000 train_loss: 63.25712585449219 test_loss:898.9332275390625\n",
      "1555/3000 train_loss: 58.51811599731445 test_loss:890.0484619140625\n",
      "1556/3000 train_loss: 48.9511833190918 test_loss:902.0512084960938\n",
      "1557/3000 train_loss: 59.698246002197266 test_loss:889.93115234375\n",
      "1558/3000 train_loss: 66.97114562988281 test_loss:884.7330932617188\n",
      "1559/3000 train_loss: 55.78994369506836 test_loss:917.739013671875\n",
      "1560/3000 train_loss: 62.10480499267578 test_loss:876.5036010742188\n",
      "1561/3000 train_loss: 59.74378967285156 test_loss:865.0514526367188\n",
      "1562/3000 train_loss: 64.26930236816406 test_loss:899.017822265625\n",
      "1563/3000 train_loss: 54.04319763183594 test_loss:872.7715454101562\n",
      "1564/3000 train_loss: 71.80349731445312 test_loss:906.5596923828125\n",
      "1565/3000 train_loss: 59.6021728515625 test_loss:864.0731811523438\n",
      "1566/3000 train_loss: 56.97715759277344 test_loss:855.6887817382812\n",
      "1567/3000 train_loss: 67.94783020019531 test_loss:894.8321533203125\n",
      "1568/3000 train_loss: 67.52460479736328 test_loss:914.4398193359375\n",
      "1569/3000 train_loss: 62.70574188232422 test_loss:899.6431884765625\n",
      "1570/3000 train_loss: 57.336971282958984 test_loss:878.371337890625\n",
      "1571/3000 train_loss: 64.90637969970703 test_loss:903.5283813476562\n",
      "1572/3000 train_loss: 61.31278991699219 test_loss:860.7042846679688\n",
      "1573/3000 train_loss: 50.64448928833008 test_loss:899.9203491210938\n",
      "1574/3000 train_loss: 60.35857391357422 test_loss:887.8536987304688\n",
      "1575/3000 train_loss: 54.7711181640625 test_loss:882.0810546875\n",
      "1576/3000 train_loss: 51.05146789550781 test_loss:882.413330078125\n",
      "1577/3000 train_loss: 57.46455764770508 test_loss:877.1995239257812\n",
      "1578/3000 train_loss: 58.52769088745117 test_loss:884.3778076171875\n",
      "1579/3000 train_loss: 50.325523376464844 test_loss:900.4654541015625\n",
      "1580/3000 train_loss: 55.44303894042969 test_loss:899.1400756835938\n",
      "1581/3000 train_loss: 54.34208297729492 test_loss:925.3345336914062\n",
      "1582/3000 train_loss: 59.142005920410156 test_loss:928.7217407226562\n",
      "1583/3000 train_loss: 56.9375 test_loss:981.5533447265625\n",
      "1584/3000 train_loss: 71.98889923095703 test_loss:932.714599609375\n",
      "1585/3000 train_loss: 64.21299743652344 test_loss:905.19970703125\n",
      "1586/3000 train_loss: 58.989097595214844 test_loss:933.4829711914062\n",
      "1587/3000 train_loss: 66.42596435546875 test_loss:910.2333374023438\n",
      "1588/3000 train_loss: 61.361244201660156 test_loss:965.2157592773438\n",
      "1589/3000 train_loss: 52.9748649597168 test_loss:969.2215576171875\n",
      "1590/3000 train_loss: 57.5413703918457 test_loss:940.69189453125\n",
      "1591/3000 train_loss: 52.90523147583008 test_loss:958.50732421875\n",
      "1592/3000 train_loss: 59.33384704589844 test_loss:927.621337890625\n",
      "1593/3000 train_loss: 58.01385498046875 test_loss:984.0305786132812\n",
      "1594/3000 train_loss: 67.47441864013672 test_loss:973.7218627929688\n",
      "1595/3000 train_loss: 61.09613800048828 test_loss:930.4218139648438\n",
      "1596/3000 train_loss: 55.99864959716797 test_loss:948.6061401367188\n",
      "1597/3000 train_loss: 60.06428527832031 test_loss:894.2175903320312\n",
      "1598/3000 train_loss: 59.82542037963867 test_loss:896.4684448242188\n",
      "1599/3000 train_loss: 62.56354904174805 test_loss:881.1237182617188\n",
      "1600/3000 train_loss: 58.3597526550293 test_loss:932.3050537109375\n",
      "1601/3000 train_loss: 56.60789489746094 test_loss:895.4428100585938\n",
      "1602/3000 train_loss: 55.67552185058594 test_loss:926.7434692382812\n",
      "1603/3000 train_loss: 60.898929595947266 test_loss:915.9483642578125\n",
      "1604/3000 train_loss: 65.59693908691406 test_loss:880.5993041992188\n",
      "1605/3000 train_loss: 58.61388397216797 test_loss:907.5821533203125\n",
      "1606/3000 train_loss: 51.53477478027344 test_loss:915.844970703125\n",
      "1607/3000 train_loss: 56.11262130737305 test_loss:904.4981079101562\n",
      "1608/3000 train_loss: 57.48820877075195 test_loss:915.6469116210938\n",
      "1609/3000 train_loss: 52.33454895019531 test_loss:888.205322265625\n",
      "1610/3000 train_loss: 53.142513275146484 test_loss:900.5275268554688\n",
      "1611/3000 train_loss: 58.762664794921875 test_loss:907.0767211914062\n",
      "1612/3000 train_loss: 70.50438690185547 test_loss:874.4129028320312\n",
      "1613/3000 train_loss: 61.8742561340332 test_loss:846.1317138671875\n",
      "1614/3000 train_loss: 59.02423858642578 test_loss:890.742431640625\n",
      "1615/3000 train_loss: 58.02013397216797 test_loss:823.8804321289062\n",
      "1616/3000 train_loss: 50.919960021972656 test_loss:884.3720092773438\n",
      "1617/3000 train_loss: 55.63624572753906 test_loss:876.7464599609375\n",
      "1618/3000 train_loss: 56.77955627441406 test_loss:857.1846923828125\n",
      "1619/3000 train_loss: 56.848995208740234 test_loss:869.0228271484375\n",
      "1620/3000 train_loss: 51.89528274536133 test_loss:927.1808471679688\n",
      "1621/3000 train_loss: 55.333251953125 test_loss:911.3576049804688\n",
      "1622/3000 train_loss: 64.03526306152344 test_loss:896.5071411132812\n",
      "1623/3000 train_loss: 56.19062805175781 test_loss:845.4625854492188\n",
      "1624/3000 train_loss: 62.034339904785156 test_loss:857.1767578125\n",
      "1625/3000 train_loss: 59.41074752807617 test_loss:855.732177734375\n",
      "1626/3000 train_loss: 59.72673416137695 test_loss:900.0787353515625\n",
      "1627/3000 train_loss: 58.212890625 test_loss:885.777587890625\n",
      "1628/3000 train_loss: 74.25464630126953 test_loss:864.1168212890625\n",
      "1629/3000 train_loss: 89.8946304321289 test_loss:878.108154296875\n",
      "1630/3000 train_loss: 62.1488151550293 test_loss:951.31005859375\n",
      "1631/3000 train_loss: 53.08367919921875 test_loss:925.3648071289062\n",
      "1632/3000 train_loss: 55.114620208740234 test_loss:882.026123046875\n",
      "1633/3000 train_loss: 61.727989196777344 test_loss:911.282470703125\n",
      "1634/3000 train_loss: 64.9288330078125 test_loss:853.58642578125\n",
      "1635/3000 train_loss: 52.34245681762695 test_loss:879.498779296875\n",
      "1636/3000 train_loss: 58.58580017089844 test_loss:884.6267700195312\n",
      "1637/3000 train_loss: 50.48191833496094 test_loss:879.7313232421875\n",
      "1638/3000 train_loss: 55.076072692871094 test_loss:905.0056762695312\n",
      "1639/3000 train_loss: 65.98231506347656 test_loss:876.1689453125\n",
      "1640/3000 train_loss: 64.2607421875 test_loss:930.0399169921875\n",
      "1641/3000 train_loss: 61.8537712097168 test_loss:913.2887573242188\n",
      "1642/3000 train_loss: 64.33269500732422 test_loss:923.013916015625\n",
      "1643/3000 train_loss: 58.22265625 test_loss:915.5038452148438\n",
      "1644/3000 train_loss: 67.0195083618164 test_loss:907.557861328125\n",
      "1645/3000 train_loss: 54.96735382080078 test_loss:943.8029174804688\n",
      "1646/3000 train_loss: 58.93101501464844 test_loss:931.9349365234375\n",
      "1647/3000 train_loss: 49.20088195800781 test_loss:911.7310791015625\n",
      "1648/3000 train_loss: 49.21500015258789 test_loss:893.6906127929688\n",
      "1649/3000 train_loss: 64.55572509765625 test_loss:889.914794921875\n",
      "1650/3000 train_loss: 56.639408111572266 test_loss:869.7354736328125\n",
      "1651/3000 train_loss: 59.711917877197266 test_loss:905.3468627929688\n",
      "1652/3000 train_loss: 60.485084533691406 test_loss:886.8306884765625\n",
      "1653/3000 train_loss: 72.35411071777344 test_loss:959.4129028320312\n",
      "1654/3000 train_loss: 69.18618774414062 test_loss:973.25732421875\n",
      "1655/3000 train_loss: 68.5860824584961 test_loss:899.1502075195312\n",
      "1656/3000 train_loss: 59.318603515625 test_loss:898.9443969726562\n",
      "1657/3000 train_loss: 54.752418518066406 test_loss:909.2948608398438\n",
      "1658/3000 train_loss: 60.55113220214844 test_loss:892.3659057617188\n",
      "1659/3000 train_loss: 58.447181701660156 test_loss:932.5553588867188\n",
      "1660/3000 train_loss: 55.133384704589844 test_loss:898.7871704101562\n",
      "1661/3000 train_loss: 58.083805084228516 test_loss:886.3445434570312\n",
      "1662/3000 train_loss: 56.666961669921875 test_loss:868.1546630859375\n",
      "1663/3000 train_loss: 76.01268005371094 test_loss:893.205322265625\n",
      "1664/3000 train_loss: 82.22666931152344 test_loss:1028.3399658203125\n",
      "1665/3000 train_loss: 63.39891815185547 test_loss:1008.5242309570312\n",
      "1666/3000 train_loss: 65.74836730957031 test_loss:1032.484375\n",
      "1667/3000 train_loss: 60.201602935791016 test_loss:981.8245239257812\n",
      "1668/3000 train_loss: 61.325923919677734 test_loss:996.9301147460938\n",
      "1669/3000 train_loss: 53.11663055419922 test_loss:975.0694580078125\n",
      "1670/3000 train_loss: 54.35035705566406 test_loss:977.7132568359375\n",
      "1671/3000 train_loss: 57.962074279785156 test_loss:977.3582153320312\n",
      "1672/3000 train_loss: 53.241615295410156 test_loss:986.4801025390625\n",
      "1673/3000 train_loss: 59.40138626098633 test_loss:962.7767944335938\n",
      "1674/3000 train_loss: 58.48884582519531 test_loss:978.5034790039062\n",
      "1675/3000 train_loss: 54.6656608581543 test_loss:1005.3281860351562\n",
      "1676/3000 train_loss: 54.72234344482422 test_loss:967.666015625\n",
      "1677/3000 train_loss: 54.473270416259766 test_loss:951.0115966796875\n",
      "1678/3000 train_loss: 61.824546813964844 test_loss:949.56884765625\n",
      "1679/3000 train_loss: 81.21942901611328 test_loss:961.1776733398438\n",
      "1680/3000 train_loss: 68.9183349609375 test_loss:966.680908203125\n",
      "1681/3000 train_loss: 62.281612396240234 test_loss:1020.5062866210938\n",
      "1682/3000 train_loss: 70.10404968261719 test_loss:983.413330078125\n",
      "1683/3000 train_loss: 53.34553909301758 test_loss:975.6641235351562\n",
      "1684/3000 train_loss: 55.52540969848633 test_loss:958.0498657226562\n",
      "1685/3000 train_loss: 55.354454040527344 test_loss:920.4752197265625\n",
      "1686/3000 train_loss: 62.73578643798828 test_loss:946.3341674804688\n",
      "1687/3000 train_loss: 62.81071472167969 test_loss:924.5125732421875\n",
      "1688/3000 train_loss: 58.49890899658203 test_loss:970.5126342773438\n",
      "1689/3000 train_loss: 67.54603576660156 test_loss:909.37353515625\n",
      "1690/3000 train_loss: 53.118927001953125 test_loss:904.7771606445312\n",
      "1691/3000 train_loss: 58.21641540527344 test_loss:927.0662231445312\n",
      "1692/3000 train_loss: 56.67342758178711 test_loss:922.1649169921875\n",
      "1693/3000 train_loss: 48.771629333496094 test_loss:918.0225219726562\n",
      "1694/3000 train_loss: 58.61792755126953 test_loss:922.2084350585938\n",
      "1695/3000 train_loss: 55.79932403564453 test_loss:946.2835083007812\n",
      "1696/3000 train_loss: 58.74473571777344 test_loss:947.5623779296875\n",
      "1697/3000 train_loss: 58.698020935058594 test_loss:954.9774169921875\n",
      "1698/3000 train_loss: 64.90818786621094 test_loss:946.3255615234375\n",
      "1699/3000 train_loss: 68.387939453125 test_loss:910.9470825195312\n",
      "1700/3000 train_loss: 67.13288879394531 test_loss:947.9766235351562\n",
      "1701/3000 train_loss: 64.16773223876953 test_loss:940.2686157226562\n",
      "1702/3000 train_loss: 54.968441009521484 test_loss:922.5941162109375\n",
      "1703/3000 train_loss: 56.38678741455078 test_loss:936.6785278320312\n",
      "1704/3000 train_loss: 57.72024154663086 test_loss:944.5160522460938\n",
      "1705/3000 train_loss: 59.891578674316406 test_loss:907.7244873046875\n",
      "1706/3000 train_loss: 50.388694763183594 test_loss:906.1389770507812\n",
      "1707/3000 train_loss: 52.497589111328125 test_loss:916.8339233398438\n",
      "1708/3000 train_loss: 47.59724807739258 test_loss:903.8544921875\n",
      "1709/3000 train_loss: 51.363826751708984 test_loss:903.1138916015625\n",
      "1710/3000 train_loss: 60.674842834472656 test_loss:882.8331909179688\n",
      "1711/3000 train_loss: 51.203758239746094 test_loss:907.5118408203125\n",
      "1712/3000 train_loss: 58.055877685546875 test_loss:917.6531982421875\n",
      "1713/3000 train_loss: 58.940452575683594 test_loss:948.5712280273438\n",
      "1714/3000 train_loss: 62.46028518676758 test_loss:963.8519897460938\n",
      "1715/3000 train_loss: 61.56864929199219 test_loss:951.7822875976562\n",
      "1716/3000 train_loss: 64.72535705566406 test_loss:958.7395629882812\n",
      "1717/3000 train_loss: 59.58477783203125 test_loss:970.0885620117188\n",
      "1718/3000 train_loss: 43.30989456176758 test_loss:927.640380859375\n",
      "1719/3000 train_loss: 55.904178619384766 test_loss:904.7479858398438\n",
      "1720/3000 train_loss: 48.671295166015625 test_loss:896.5805053710938\n",
      "1721/3000 train_loss: 57.014976501464844 test_loss:912.8463134765625\n",
      "1722/3000 train_loss: 60.32114791870117 test_loss:912.510986328125\n",
      "1723/3000 train_loss: 54.612701416015625 test_loss:911.9132690429688\n",
      "1724/3000 train_loss: 54.831565856933594 test_loss:898.8947143554688\n",
      "1725/3000 train_loss: 59.44804763793945 test_loss:909.2127685546875\n",
      "1726/3000 train_loss: 56.7625846862793 test_loss:949.6348266601562\n",
      "1727/3000 train_loss: 53.712039947509766 test_loss:938.492919921875\n",
      "1728/3000 train_loss: 45.96560287475586 test_loss:913.9127807617188\n",
      "1729/3000 train_loss: 50.48577880859375 test_loss:920.0557861328125\n",
      "1730/3000 train_loss: 56.6046142578125 test_loss:885.0285034179688\n",
      "1731/3000 train_loss: 51.460208892822266 test_loss:906.75732421875\n",
      "1732/3000 train_loss: 55.58800506591797 test_loss:897.065185546875\n",
      "1733/3000 train_loss: 57.023681640625 test_loss:940.19287109375\n",
      "1734/3000 train_loss: 58.28987121582031 test_loss:907.7156372070312\n",
      "1735/3000 train_loss: 54.53541564941406 test_loss:929.87158203125\n",
      "1736/3000 train_loss: 50.92511749267578 test_loss:923.2011108398438\n",
      "1737/3000 train_loss: 48.84254837036133 test_loss:900.2994384765625\n",
      "1738/3000 train_loss: 50.615638732910156 test_loss:927.7008056640625\n",
      "1739/3000 train_loss: 49.4145393371582 test_loss:915.7291870117188\n",
      "1740/3000 train_loss: 59.089080810546875 test_loss:903.0479736328125\n",
      "1741/3000 train_loss: 57.10106658935547 test_loss:908.2017822265625\n",
      "1742/3000 train_loss: 53.40220642089844 test_loss:887.3255615234375\n",
      "1743/3000 train_loss: 57.139686584472656 test_loss:900.0157470703125\n",
      "1744/3000 train_loss: 56.00881576538086 test_loss:894.4716796875\n",
      "1745/3000 train_loss: 71.64473724365234 test_loss:886.1080932617188\n",
      "1746/3000 train_loss: 70.84954833984375 test_loss:918.5753173828125\n",
      "1747/3000 train_loss: 68.86201477050781 test_loss:894.2288208007812\n",
      "1748/3000 train_loss: 66.3221435546875 test_loss:897.2478637695312\n",
      "1749/3000 train_loss: 60.69975280761719 test_loss:910.9612426757812\n",
      "1750/3000 train_loss: 61.62671661376953 test_loss:894.2051391601562\n",
      "1751/3000 train_loss: 54.56754684448242 test_loss:847.9976806640625\n",
      "1752/3000 train_loss: 63.24900817871094 test_loss:894.859619140625\n",
      "1753/3000 train_loss: 53.01528549194336 test_loss:925.5084838867188\n",
      "1754/3000 train_loss: 51.94484329223633 test_loss:892.5078735351562\n",
      "1755/3000 train_loss: 54.11103057861328 test_loss:936.5419921875\n",
      "1756/3000 train_loss: 57.038414001464844 test_loss:908.7034912109375\n",
      "1757/3000 train_loss: 59.007293701171875 test_loss:908.2952880859375\n",
      "1758/3000 train_loss: 50.24239730834961 test_loss:883.59228515625\n",
      "1759/3000 train_loss: 54.919761657714844 test_loss:910.63427734375\n",
      "1760/3000 train_loss: 49.92243194580078 test_loss:902.9923095703125\n",
      "1761/3000 train_loss: 59.36853790283203 test_loss:931.2754516601562\n",
      "1762/3000 train_loss: 56.35192108154297 test_loss:911.784912109375\n",
      "1763/3000 train_loss: 51.01701736450195 test_loss:917.6572265625\n",
      "1764/3000 train_loss: 60.77545928955078 test_loss:937.7193603515625\n",
      "1765/3000 train_loss: 72.89495086669922 test_loss:916.843994140625\n",
      "1766/3000 train_loss: 58.24750518798828 test_loss:971.3118286132812\n",
      "1767/3000 train_loss: 57.56174850463867 test_loss:923.7420654296875\n",
      "1768/3000 train_loss: 54.912681579589844 test_loss:908.5286254882812\n",
      "1769/3000 train_loss: 55.792999267578125 test_loss:927.9421997070312\n",
      "1770/3000 train_loss: 52.8866081237793 test_loss:889.0722045898438\n",
      "1771/3000 train_loss: 51.44877624511719 test_loss:929.6505126953125\n",
      "1772/3000 train_loss: 59.30276870727539 test_loss:868.6975708007812\n",
      "1773/3000 train_loss: 62.01388931274414 test_loss:930.1343994140625\n",
      "1774/3000 train_loss: 47.08969497680664 test_loss:877.3474731445312\n",
      "1775/3000 train_loss: 49.28192138671875 test_loss:923.3729858398438\n",
      "1776/3000 train_loss: 54.74528503417969 test_loss:896.2723388671875\n",
      "1777/3000 train_loss: 53.76845169067383 test_loss:908.4729614257812\n",
      "1778/3000 train_loss: 48.596492767333984 test_loss:891.5137329101562\n",
      "1779/3000 train_loss: 51.110416412353516 test_loss:899.2515869140625\n",
      "1780/3000 train_loss: 59.01980972290039 test_loss:896.7637329101562\n",
      "1781/3000 train_loss: 51.53186798095703 test_loss:882.0618286132812\n",
      "1782/3000 train_loss: 49.231407165527344 test_loss:887.0656127929688\n",
      "1783/3000 train_loss: 55.06580352783203 test_loss:893.9107666015625\n",
      "1784/3000 train_loss: 49.20705032348633 test_loss:894.791748046875\n",
      "1785/3000 train_loss: 61.87260437011719 test_loss:892.01513671875\n",
      "1786/3000 train_loss: 55.38865661621094 test_loss:957.515625\n",
      "1787/3000 train_loss: 48.924659729003906 test_loss:959.9957885742188\n",
      "1788/3000 train_loss: 53.11489486694336 test_loss:924.6076049804688\n",
      "1789/3000 train_loss: 48.714996337890625 test_loss:939.861328125\n",
      "1790/3000 train_loss: 49.09513854980469 test_loss:970.6323852539062\n",
      "1791/3000 train_loss: 52.1427116394043 test_loss:952.4586181640625\n",
      "1792/3000 train_loss: 49.689205169677734 test_loss:925.560546875\n",
      "1793/3000 train_loss: 54.16471862792969 test_loss:921.8367919921875\n",
      "1794/3000 train_loss: 62.25764465332031 test_loss:918.4542846679688\n",
      "1795/3000 train_loss: 56.098724365234375 test_loss:953.3941040039062\n",
      "1796/3000 train_loss: 57.159324645996094 test_loss:931.5899047851562\n",
      "1797/3000 train_loss: 58.97142791748047 test_loss:966.9041748046875\n",
      "1798/3000 train_loss: 70.3186264038086 test_loss:955.904052734375\n",
      "1799/3000 train_loss: 50.893516540527344 test_loss:883.4835205078125\n",
      "1800/3000 train_loss: 48.69371795654297 test_loss:901.4259033203125\n",
      "1801/3000 train_loss: 55.09108352661133 test_loss:920.4067993164062\n",
      "1802/3000 train_loss: 58.2940673828125 test_loss:894.8739013671875\n",
      "1803/3000 train_loss: 47.54835891723633 test_loss:931.3283081054688\n",
      "1804/3000 train_loss: 48.82368087768555 test_loss:911.2418823242188\n",
      "1805/3000 train_loss: 57.843833923339844 test_loss:917.990966796875\n",
      "1806/3000 train_loss: 58.21335983276367 test_loss:889.3014526367188\n",
      "1807/3000 train_loss: 48.564117431640625 test_loss:916.6986083984375\n",
      "1808/3000 train_loss: 54.05428695678711 test_loss:909.2799072265625\n",
      "1809/3000 train_loss: 55.257564544677734 test_loss:987.5807495117188\n",
      "1810/3000 train_loss: 55.019287109375 test_loss:948.6915893554688\n",
      "1811/3000 train_loss: 52.189056396484375 test_loss:953.2338256835938\n",
      "1812/3000 train_loss: 59.6939811706543 test_loss:955.6812133789062\n",
      "1813/3000 train_loss: 62.659828186035156 test_loss:979.6014404296875\n",
      "1814/3000 train_loss: 48.77627182006836 test_loss:976.147705078125\n",
      "1815/3000 train_loss: 57.958457946777344 test_loss:945.51806640625\n",
      "1816/3000 train_loss: 47.71865463256836 test_loss:964.3029174804688\n",
      "1817/3000 train_loss: 53.331687927246094 test_loss:915.8920288085938\n",
      "1818/3000 train_loss: 81.19019317626953 test_loss:907.1990356445312\n",
      "1819/3000 train_loss: 56.26729965209961 test_loss:869.74169921875\n",
      "1820/3000 train_loss: 62.153099060058594 test_loss:936.3721923828125\n",
      "1821/3000 train_loss: 59.19319534301758 test_loss:912.7520751953125\n",
      "1822/3000 train_loss: 50.75468444824219 test_loss:934.7865600585938\n",
      "1823/3000 train_loss: 56.13877487182617 test_loss:890.0530395507812\n",
      "1824/3000 train_loss: 56.72736740112305 test_loss:941.345458984375\n",
      "1825/3000 train_loss: 47.16754150390625 test_loss:897.6094360351562\n",
      "1826/3000 train_loss: 60.00235366821289 test_loss:908.9364013671875\n",
      "1827/3000 train_loss: 58.444679260253906 test_loss:920.6179809570312\n",
      "1828/3000 train_loss: 63.929908752441406 test_loss:932.4279174804688\n",
      "1829/3000 train_loss: 50.896671295166016 test_loss:890.4932250976562\n",
      "1830/3000 train_loss: 64.7514877319336 test_loss:907.2685546875\n",
      "1831/3000 train_loss: 49.935482025146484 test_loss:939.1851196289062\n",
      "1832/3000 train_loss: 59.02226257324219 test_loss:941.3707885742188\n",
      "1833/3000 train_loss: 55.97535705566406 test_loss:971.640380859375\n",
      "1834/3000 train_loss: 46.1098518371582 test_loss:955.252197265625\n",
      "1835/3000 train_loss: 51.350975036621094 test_loss:927.4984130859375\n",
      "1836/3000 train_loss: 56.08393859863281 test_loss:925.0322875976562\n",
      "1837/3000 train_loss: 53.53063201904297 test_loss:902.4404907226562\n",
      "1838/3000 train_loss: 49.918121337890625 test_loss:916.2303466796875\n",
      "1839/3000 train_loss: 52.82259750366211 test_loss:916.4223022460938\n",
      "1840/3000 train_loss: 55.34429931640625 test_loss:896.5313110351562\n",
      "1841/3000 train_loss: 56.229766845703125 test_loss:926.0095825195312\n",
      "1842/3000 train_loss: 58.10588455200195 test_loss:887.3169555664062\n",
      "1843/3000 train_loss: 62.46278381347656 test_loss:919.6741333007812\n",
      "1844/3000 train_loss: 55.880393981933594 test_loss:922.209228515625\n",
      "1845/3000 train_loss: 49.93553924560547 test_loss:915.9194946289062\n",
      "1846/3000 train_loss: 51.855445861816406 test_loss:923.8251342773438\n",
      "1847/3000 train_loss: 54.698524475097656 test_loss:914.9257202148438\n",
      "1848/3000 train_loss: 55.88494110107422 test_loss:939.4172973632812\n",
      "1849/3000 train_loss: 58.887054443359375 test_loss:906.2897338867188\n",
      "1850/3000 train_loss: 52.21737289428711 test_loss:895.3546752929688\n",
      "1851/3000 train_loss: 55.36355972290039 test_loss:917.595458984375\n",
      "1852/3000 train_loss: 46.94952392578125 test_loss:914.9779052734375\n",
      "1853/3000 train_loss: 50.04476547241211 test_loss:893.1231689453125\n",
      "1854/3000 train_loss: 53.16966247558594 test_loss:916.8840942382812\n",
      "1855/3000 train_loss: 60.39885711669922 test_loss:904.8983764648438\n",
      "1856/3000 train_loss: 49.66740036010742 test_loss:919.2523193359375\n",
      "1857/3000 train_loss: 47.14622497558594 test_loss:902.6420288085938\n",
      "1858/3000 train_loss: 49.80042266845703 test_loss:902.9956665039062\n",
      "1859/3000 train_loss: 50.78636932373047 test_loss:942.9393310546875\n",
      "1860/3000 train_loss: 53.351985931396484 test_loss:895.6199340820312\n",
      "1861/3000 train_loss: 53.805599212646484 test_loss:917.7175903320312\n",
      "1862/3000 train_loss: 49.419525146484375 test_loss:940.5730590820312\n",
      "1863/3000 train_loss: 49.299617767333984 test_loss:913.6555786132812\n",
      "1864/3000 train_loss: 50.60408020019531 test_loss:933.4060668945312\n",
      "1865/3000 train_loss: 48.40238952636719 test_loss:939.2996826171875\n",
      "1866/3000 train_loss: 52.54534149169922 test_loss:914.21484375\n",
      "1867/3000 train_loss: 49.22481918334961 test_loss:905.0708618164062\n",
      "1868/3000 train_loss: 45.48841094970703 test_loss:940.89013671875\n",
      "1869/3000 train_loss: 55.79450988769531 test_loss:888.79443359375\n",
      "1870/3000 train_loss: 62.775760650634766 test_loss:919.4842529296875\n",
      "1871/3000 train_loss: 52.287776947021484 test_loss:901.4599609375\n",
      "1872/3000 train_loss: 47.72812271118164 test_loss:911.5391845703125\n",
      "1873/3000 train_loss: 50.466373443603516 test_loss:911.3429565429688\n",
      "1874/3000 train_loss: 49.7509765625 test_loss:918.5595703125\n",
      "1875/3000 train_loss: 49.2242317199707 test_loss:915.2059936523438\n",
      "1876/3000 train_loss: 46.96971130371094 test_loss:913.6029052734375\n",
      "1877/3000 train_loss: 64.02506256103516 test_loss:913.7315673828125\n",
      "1878/3000 train_loss: 52.62537384033203 test_loss:946.5036010742188\n",
      "1879/3000 train_loss: 64.11917114257812 test_loss:917.7208251953125\n",
      "1880/3000 train_loss: 46.54521560668945 test_loss:907.5223999023438\n",
      "1881/3000 train_loss: 49.06448745727539 test_loss:904.9378662109375\n",
      "1882/3000 train_loss: 47.236881256103516 test_loss:928.42919921875\n",
      "1883/3000 train_loss: 52.56870651245117 test_loss:878.27734375\n",
      "1884/3000 train_loss: 58.704715728759766 test_loss:936.7332153320312\n",
      "1885/3000 train_loss: 45.55141830444336 test_loss:916.0907592773438\n",
      "1886/3000 train_loss: 101.32080078125 test_loss:914.138427734375\n",
      "1887/3000 train_loss: 48.75991439819336 test_loss:854.37744140625\n",
      "1888/3000 train_loss: 49.06785202026367 test_loss:837.1986083984375\n",
      "1889/3000 train_loss: 51.23187255859375 test_loss:878.087158203125\n",
      "1890/3000 train_loss: 47.04631042480469 test_loss:889.577392578125\n",
      "1891/3000 train_loss: 51.63127899169922 test_loss:931.69921875\n",
      "1892/3000 train_loss: 83.4373779296875 test_loss:838.42822265625\n",
      "1893/3000 train_loss: 57.567588806152344 test_loss:946.72607421875\n",
      "1894/3000 train_loss: 59.420509338378906 test_loss:919.3378295898438\n",
      "1895/3000 train_loss: 43.74325180053711 test_loss:880.3673095703125\n",
      "1896/3000 train_loss: 51.11244201660156 test_loss:947.016845703125\n",
      "1897/3000 train_loss: 43.627601623535156 test_loss:886.0267944335938\n",
      "1898/3000 train_loss: 46.81639099121094 test_loss:913.058349609375\n",
      "1899/3000 train_loss: 49.63764190673828 test_loss:850.9512939453125\n",
      "1900/3000 train_loss: 48.951229095458984 test_loss:930.6089477539062\n",
      "1901/3000 train_loss: 52.526023864746094 test_loss:857.5338134765625\n",
      "1902/3000 train_loss: 60.99501037597656 test_loss:942.2056274414062\n",
      "1903/3000 train_loss: 53.498600006103516 test_loss:871.9838256835938\n",
      "1904/3000 train_loss: 53.580909729003906 test_loss:933.6156616210938\n",
      "1905/3000 train_loss: 50.84821319580078 test_loss:896.772216796875\n",
      "1906/3000 train_loss: 51.228485107421875 test_loss:940.4528198242188\n",
      "1907/3000 train_loss: 51.457027435302734 test_loss:905.697509765625\n",
      "1908/3000 train_loss: 47.96121597290039 test_loss:892.6287231445312\n",
      "1909/3000 train_loss: 49.92342758178711 test_loss:873.3383178710938\n",
      "1910/3000 train_loss: 47.99759292602539 test_loss:932.4169311523438\n",
      "1911/3000 train_loss: 47.37580108642578 test_loss:880.742919921875\n",
      "1912/3000 train_loss: 54.76288986206055 test_loss:902.3388061523438\n",
      "1913/3000 train_loss: 49.17856979370117 test_loss:923.942138671875\n",
      "1914/3000 train_loss: 49.7053108215332 test_loss:927.5543212890625\n",
      "1915/3000 train_loss: 56.184383392333984 test_loss:985.7552490234375\n",
      "1916/3000 train_loss: 51.586280822753906 test_loss:911.2474365234375\n",
      "1917/3000 train_loss: 50.37992477416992 test_loss:943.9108276367188\n",
      "1918/3000 train_loss: 56.08767318725586 test_loss:908.1417846679688\n",
      "1919/3000 train_loss: 51.031837463378906 test_loss:926.5468139648438\n",
      "1920/3000 train_loss: 55.05280685424805 test_loss:889.2904663085938\n",
      "1921/3000 train_loss: 49.82482147216797 test_loss:865.5628051757812\n",
      "1922/3000 train_loss: 46.590511322021484 test_loss:912.9304809570312\n",
      "1923/3000 train_loss: 56.810462951660156 test_loss:876.591796875\n",
      "1924/3000 train_loss: 61.62090301513672 test_loss:906.1776733398438\n",
      "1925/3000 train_loss: 52.255653381347656 test_loss:918.0833129882812\n",
      "1926/3000 train_loss: 55.135013580322266 test_loss:928.4918212890625\n",
      "1927/3000 train_loss: 48.755409240722656 test_loss:923.1124877929688\n",
      "1928/3000 train_loss: 54.52738571166992 test_loss:924.7094116210938\n",
      "1929/3000 train_loss: 58.64220428466797 test_loss:886.1820068359375\n",
      "1930/3000 train_loss: 63.963836669921875 test_loss:888.442138671875\n",
      "1931/3000 train_loss: 71.84125518798828 test_loss:881.3938598632812\n",
      "1932/3000 train_loss: 47.18799591064453 test_loss:888.7537231445312\n",
      "1933/3000 train_loss: 47.59156036376953 test_loss:876.5382080078125\n",
      "1934/3000 train_loss: 56.98835754394531 test_loss:886.169921875\n",
      "1935/3000 train_loss: 44.92863082885742 test_loss:897.5408325195312\n",
      "1936/3000 train_loss: 52.88651657104492 test_loss:881.8594970703125\n",
      "1937/3000 train_loss: 50.778526306152344 test_loss:863.6700439453125\n",
      "1938/3000 train_loss: 47.26766586303711 test_loss:889.746826171875\n",
      "1939/3000 train_loss: 53.746803283691406 test_loss:855.7543334960938\n",
      "1940/3000 train_loss: 49.794044494628906 test_loss:894.1181030273438\n",
      "1941/3000 train_loss: 42.00057601928711 test_loss:872.9725341796875\n",
      "1942/3000 train_loss: 59.8139762878418 test_loss:857.7186279296875\n",
      "1943/3000 train_loss: 57.56532669067383 test_loss:868.3241577148438\n",
      "1944/3000 train_loss: 43.4578971862793 test_loss:873.5084838867188\n",
      "1945/3000 train_loss: 45.099586486816406 test_loss:868.2455444335938\n",
      "1946/3000 train_loss: 46.384849548339844 test_loss:902.5162353515625\n",
      "1947/3000 train_loss: 43.366939544677734 test_loss:856.3345947265625\n",
      "1948/3000 train_loss: 59.02833557128906 test_loss:906.8103637695312\n",
      "1949/3000 train_loss: 50.01203155517578 test_loss:890.94921875\n",
      "1950/3000 train_loss: 51.07130813598633 test_loss:908.6726684570312\n",
      "1951/3000 train_loss: 44.6730842590332 test_loss:904.6295166015625\n",
      "1952/3000 train_loss: 47.83335876464844 test_loss:911.1951293945312\n",
      "1953/3000 train_loss: 53.94780731201172 test_loss:887.6881103515625\n",
      "1954/3000 train_loss: 48.2199592590332 test_loss:892.370361328125\n",
      "1955/3000 train_loss: 41.003196716308594 test_loss:880.8942260742188\n",
      "1956/3000 train_loss: 48.86450958251953 test_loss:893.4876708984375\n",
      "1957/3000 train_loss: 54.248992919921875 test_loss:909.7431030273438\n",
      "1958/3000 train_loss: 44.87725830078125 test_loss:863.1049194335938\n",
      "1959/3000 train_loss: 48.15856170654297 test_loss:889.3323364257812\n",
      "1960/3000 train_loss: 43.99984359741211 test_loss:867.520751953125\n",
      "1961/3000 train_loss: 48.497013092041016 test_loss:934.0213012695312\n",
      "1962/3000 train_loss: 49.895896911621094 test_loss:895.5735473632812\n",
      "1963/3000 train_loss: 52.38892364501953 test_loss:918.0137329101562\n",
      "1964/3000 train_loss: 42.026973724365234 test_loss:890.5244140625\n",
      "1965/3000 train_loss: 55.4144287109375 test_loss:897.9711303710938\n",
      "1966/3000 train_loss: 48.24626159667969 test_loss:892.428955078125\n",
      "1967/3000 train_loss: 45.44646072387695 test_loss:823.55908203125\n",
      "1968/3000 train_loss: 40.724517822265625 test_loss:894.9346923828125\n",
      "1969/3000 train_loss: 48.6722412109375 test_loss:855.4052734375\n",
      "1970/3000 train_loss: 58.54377365112305 test_loss:881.4669189453125\n",
      "1971/3000 train_loss: 84.8218002319336 test_loss:867.2687377929688\n",
      "1972/3000 train_loss: 64.68878173828125 test_loss:929.7583618164062\n",
      "1973/3000 train_loss: 59.604000091552734 test_loss:938.0601806640625\n",
      "1974/3000 train_loss: 55.03962326049805 test_loss:933.267578125\n",
      "1975/3000 train_loss: 51.41481399536133 test_loss:935.7399291992188\n",
      "1976/3000 train_loss: 48.58936309814453 test_loss:888.8378295898438\n",
      "1977/3000 train_loss: 49.1517448425293 test_loss:892.6212768554688\n",
      "1978/3000 train_loss: 45.45088577270508 test_loss:894.8530883789062\n",
      "1979/3000 train_loss: 42.968868255615234 test_loss:912.6974487304688\n",
      "1980/3000 train_loss: 45.29344177246094 test_loss:907.9970703125\n",
      "1981/3000 train_loss: 42.74436569213867 test_loss:888.1729736328125\n",
      "1982/3000 train_loss: 48.10505676269531 test_loss:900.8558959960938\n",
      "1983/3000 train_loss: 60.024932861328125 test_loss:902.472900390625\n",
      "1984/3000 train_loss: 48.78327178955078 test_loss:914.6857299804688\n",
      "1985/3000 train_loss: 52.292850494384766 test_loss:915.8189086914062\n",
      "1986/3000 train_loss: 48.68689727783203 test_loss:920.9323120117188\n",
      "1987/3000 train_loss: 48.13698196411133 test_loss:943.0304565429688\n",
      "1988/3000 train_loss: 42.91413879394531 test_loss:942.4995727539062\n",
      "1989/3000 train_loss: 47.92564392089844 test_loss:918.4017333984375\n",
      "1990/3000 train_loss: 42.38267517089844 test_loss:910.4362182617188\n",
      "1991/3000 train_loss: 48.93443298339844 test_loss:924.017333984375\n",
      "1992/3000 train_loss: 56.342716217041016 test_loss:904.5672607421875\n",
      "1993/3000 train_loss: 52.179386138916016 test_loss:902.737060546875\n",
      "1994/3000 train_loss: 50.05842590332031 test_loss:921.4694213867188\n",
      "1995/3000 train_loss: 44.19363021850586 test_loss:940.3457641601562\n",
      "1996/3000 train_loss: 57.944976806640625 test_loss:919.2369995117188\n",
      "1997/3000 train_loss: 50.5898551940918 test_loss:897.5253295898438\n",
      "1998/3000 train_loss: 54.482566833496094 test_loss:920.5369873046875\n",
      "1999/3000 train_loss: 48.70214080810547 test_loss:951.489013671875\n",
      "2000/3000 train_loss: 48.77930450439453 test_loss:914.3984375\n",
      "2001/3000 train_loss: 45.51966094970703 test_loss:894.9918212890625\n",
      "2002/3000 train_loss: 42.24317169189453 test_loss:926.59228515625\n",
      "2003/3000 train_loss: 43.900787353515625 test_loss:933.10107421875\n",
      "2004/3000 train_loss: 43.15312957763672 test_loss:923.6267700195312\n",
      "2005/3000 train_loss: 59.564903259277344 test_loss:889.3532104492188\n",
      "2006/3000 train_loss: 46.792598724365234 test_loss:852.4727783203125\n",
      "2007/3000 train_loss: 53.63746643066406 test_loss:860.6192626953125\n",
      "2008/3000 train_loss: 47.27359390258789 test_loss:909.908203125\n",
      "2009/3000 train_loss: 41.18736267089844 test_loss:879.0228271484375\n",
      "2010/3000 train_loss: 44.47019577026367 test_loss:883.2859497070312\n",
      "2011/3000 train_loss: 48.6094856262207 test_loss:905.0770874023438\n",
      "2012/3000 train_loss: 58.22330093383789 test_loss:889.765380859375\n",
      "2013/3000 train_loss: 48.19646453857422 test_loss:861.4822387695312\n",
      "2014/3000 train_loss: 51.131134033203125 test_loss:915.467529296875\n",
      "2015/3000 train_loss: 45.93290328979492 test_loss:887.4791259765625\n",
      "2016/3000 train_loss: 50.28976821899414 test_loss:875.6592407226562\n",
      "2017/3000 train_loss: 47.31620788574219 test_loss:887.4947509765625\n",
      "2018/3000 train_loss: 50.16625213623047 test_loss:870.3284301757812\n",
      "2019/3000 train_loss: 62.4183235168457 test_loss:837.5694580078125\n",
      "2020/3000 train_loss: 58.04993438720703 test_loss:840.9660034179688\n",
      "2021/3000 train_loss: 56.97251510620117 test_loss:869.560546875\n",
      "2022/3000 train_loss: 55.22800064086914 test_loss:889.9487915039062\n",
      "2023/3000 train_loss: 49.59697341918945 test_loss:908.9710693359375\n",
      "2024/3000 train_loss: 51.5057373046875 test_loss:917.6369018554688\n",
      "2025/3000 train_loss: 51.88081359863281 test_loss:872.3546142578125\n",
      "2026/3000 train_loss: 56.92969512939453 test_loss:956.4978637695312\n",
      "2027/3000 train_loss: 55.5521354675293 test_loss:851.8894653320312\n",
      "2028/3000 train_loss: 64.23003387451172 test_loss:890.265625\n",
      "2029/3000 train_loss: 47.72278594970703 test_loss:850.6082763671875\n",
      "2030/3000 train_loss: 67.94808959960938 test_loss:850.7656860351562\n",
      "2031/3000 train_loss: 54.16849136352539 test_loss:837.4515991210938\n",
      "2032/3000 train_loss: 57.55852508544922 test_loss:847.2371826171875\n",
      "2033/3000 train_loss: 57.32004165649414 test_loss:867.9177856445312\n",
      "2034/3000 train_loss: 57.67426300048828 test_loss:926.4848022460938\n",
      "2035/3000 train_loss: 51.39665985107422 test_loss:900.043212890625\n",
      "2036/3000 train_loss: 56.31631851196289 test_loss:916.3267211914062\n",
      "2037/3000 train_loss: 46.51304244995117 test_loss:877.3236694335938\n",
      "2038/3000 train_loss: 49.83580780029297 test_loss:918.8014526367188\n",
      "2039/3000 train_loss: 60.66550827026367 test_loss:933.5202026367188\n",
      "2040/3000 train_loss: 46.3553352355957 test_loss:903.31494140625\n",
      "2041/3000 train_loss: 43.37449264526367 test_loss:896.3980102539062\n",
      "2042/3000 train_loss: 40.67387771606445 test_loss:903.8070678710938\n",
      "2043/3000 train_loss: 46.75039291381836 test_loss:936.2391357421875\n",
      "2044/3000 train_loss: 45.42210388183594 test_loss:888.9952392578125\n",
      "2045/3000 train_loss: 42.67133712768555 test_loss:891.9420166015625\n",
      "2046/3000 train_loss: 44.73807144165039 test_loss:880.6985473632812\n",
      "2047/3000 train_loss: 47.173370361328125 test_loss:909.9991455078125\n",
      "2048/3000 train_loss: 55.836631774902344 test_loss:891.9547729492188\n",
      "2049/3000 train_loss: 59.68308639526367 test_loss:909.7951049804688\n",
      "2050/3000 train_loss: 54.22734832763672 test_loss:858.6494140625\n",
      "2051/3000 train_loss: 60.13837432861328 test_loss:915.1527709960938\n",
      "2052/3000 train_loss: 45.05170822143555 test_loss:916.4741821289062\n",
      "2053/3000 train_loss: 47.47058868408203 test_loss:976.200927734375\n",
      "2054/3000 train_loss: 51.188899993896484 test_loss:966.3417358398438\n",
      "2055/3000 train_loss: 41.900146484375 test_loss:969.540283203125\n",
      "2056/3000 train_loss: 49.92048645019531 test_loss:928.7568359375\n",
      "2057/3000 train_loss: 47.132774353027344 test_loss:957.0286254882812\n",
      "2058/3000 train_loss: 59.957271575927734 test_loss:908.7833251953125\n",
      "2059/3000 train_loss: 49.71333312988281 test_loss:936.8407592773438\n",
      "2060/3000 train_loss: 43.85361862182617 test_loss:924.7474975585938\n",
      "2061/3000 train_loss: 43.44541931152344 test_loss:898.0711669921875\n",
      "2062/3000 train_loss: 43.65945816040039 test_loss:917.46142578125\n",
      "2063/3000 train_loss: 49.657901763916016 test_loss:923.42529296875\n",
      "2064/3000 train_loss: 49.60085678100586 test_loss:905.7141723632812\n",
      "2065/3000 train_loss: 44.321617126464844 test_loss:945.782958984375\n",
      "2066/3000 train_loss: 52.41188430786133 test_loss:938.3409423828125\n",
      "2067/3000 train_loss: 48.22281265258789 test_loss:938.746826171875\n",
      "2068/3000 train_loss: 53.612403869628906 test_loss:925.571533203125\n",
      "2069/3000 train_loss: 47.7014274597168 test_loss:928.6246337890625\n",
      "2070/3000 train_loss: 48.98384475708008 test_loss:857.9654541015625\n",
      "2071/3000 train_loss: 44.10685729980469 test_loss:874.0111083984375\n",
      "2072/3000 train_loss: 44.91718673706055 test_loss:837.4017333984375\n",
      "2073/3000 train_loss: 45.54854202270508 test_loss:881.8720703125\n",
      "2074/3000 train_loss: 48.330238342285156 test_loss:853.5687255859375\n",
      "2075/3000 train_loss: 57.78055191040039 test_loss:888.2396240234375\n",
      "2076/3000 train_loss: 50.305335998535156 test_loss:886.2439575195312\n",
      "2077/3000 train_loss: 49.25410842895508 test_loss:876.31640625\n",
      "2078/3000 train_loss: 42.6523323059082 test_loss:909.1345825195312\n",
      "2079/3000 train_loss: 45.42478561401367 test_loss:903.2481079101562\n",
      "2080/3000 train_loss: 40.677677154541016 test_loss:916.1314697265625\n",
      "2081/3000 train_loss: 46.099361419677734 test_loss:910.35791015625\n",
      "2082/3000 train_loss: 47.61357116699219 test_loss:920.3510131835938\n",
      "2083/3000 train_loss: 42.72893142700195 test_loss:920.5469970703125\n",
      "2084/3000 train_loss: 42.2602653503418 test_loss:896.6205444335938\n",
      "2085/3000 train_loss: 59.144622802734375 test_loss:894.7843017578125\n",
      "2086/3000 train_loss: 50.657127380371094 test_loss:892.6903076171875\n",
      "2087/3000 train_loss: 47.216670989990234 test_loss:925.4901123046875\n",
      "2088/3000 train_loss: 49.369163513183594 test_loss:940.9049682617188\n",
      "2089/3000 train_loss: 45.4874153137207 test_loss:885.35791015625\n",
      "2090/3000 train_loss: 71.94734191894531 test_loss:916.8031616210938\n",
      "2091/3000 train_loss: 49.42570114135742 test_loss:793.6306762695312\n",
      "2092/3000 train_loss: 55.394771575927734 test_loss:889.7510986328125\n",
      "2093/3000 train_loss: 51.156883239746094 test_loss:854.3795776367188\n",
      "2094/3000 train_loss: 51.633548736572266 test_loss:860.5907592773438\n",
      "2095/3000 train_loss: 38.5300178527832 test_loss:888.3653564453125\n",
      "2096/3000 train_loss: 52.72235870361328 test_loss:891.0258178710938\n",
      "2097/3000 train_loss: 43.51276779174805 test_loss:885.4700927734375\n",
      "2098/3000 train_loss: 42.206233978271484 test_loss:882.4448852539062\n",
      "2099/3000 train_loss: 46.555931091308594 test_loss:893.2595825195312\n",
      "2100/3000 train_loss: 44.14448547363281 test_loss:889.198974609375\n",
      "2101/3000 train_loss: 50.71186065673828 test_loss:909.0895385742188\n",
      "2102/3000 train_loss: 47.35090637207031 test_loss:920.9951782226562\n",
      "2103/3000 train_loss: 37.77663803100586 test_loss:916.3919067382812\n",
      "2104/3000 train_loss: 44.726036071777344 test_loss:891.4662475585938\n",
      "2105/3000 train_loss: 41.58921813964844 test_loss:887.3688354492188\n",
      "2106/3000 train_loss: 47.71445846557617 test_loss:883.6679077148438\n",
      "2107/3000 train_loss: 49.503257751464844 test_loss:918.93798828125\n",
      "2108/3000 train_loss: 50.492366790771484 test_loss:910.7466430664062\n",
      "2109/3000 train_loss: 51.38593673706055 test_loss:976.12353515625\n",
      "2110/3000 train_loss: 45.16075897216797 test_loss:891.1493530273438\n",
      "2111/3000 train_loss: 47.10525894165039 test_loss:904.291015625\n",
      "2112/3000 train_loss: 47.1751823425293 test_loss:838.5639038085938\n",
      "2113/3000 train_loss: 63.28783416748047 test_loss:877.9561767578125\n",
      "2114/3000 train_loss: 46.687049865722656 test_loss:857.8589477539062\n",
      "2115/3000 train_loss: 50.62286376953125 test_loss:838.8963012695312\n",
      "2116/3000 train_loss: 44.8318977355957 test_loss:850.9706420898438\n",
      "2117/3000 train_loss: 47.928123474121094 test_loss:866.8118286132812\n",
      "2118/3000 train_loss: 49.39331817626953 test_loss:874.3665161132812\n",
      "2119/3000 train_loss: 43.62845993041992 test_loss:913.3184204101562\n",
      "2120/3000 train_loss: 46.28398895263672 test_loss:876.1791381835938\n",
      "2121/3000 train_loss: 49.550994873046875 test_loss:877.23828125\n",
      "2122/3000 train_loss: 46.64574432373047 test_loss:889.8412475585938\n",
      "2123/3000 train_loss: 49.15746307373047 test_loss:898.0264282226562\n",
      "2124/3000 train_loss: 50.72929763793945 test_loss:852.8406982421875\n",
      "2125/3000 train_loss: 50.76171875 test_loss:902.5844116210938\n",
      "2126/3000 train_loss: 49.282474517822266 test_loss:930.5904541015625\n",
      "2127/3000 train_loss: 48.986412048339844 test_loss:882.6537475585938\n",
      "2128/3000 train_loss: 42.093414306640625 test_loss:905.9274291992188\n",
      "2129/3000 train_loss: 38.858238220214844 test_loss:887.491943359375\n",
      "2130/3000 train_loss: 42.2385139465332 test_loss:881.4474487304688\n",
      "2131/3000 train_loss: 46.46620178222656 test_loss:891.446533203125\n",
      "2132/3000 train_loss: 55.12958526611328 test_loss:893.962646484375\n",
      "2133/3000 train_loss: 48.55614471435547 test_loss:892.633544921875\n",
      "2134/3000 train_loss: 45.50191879272461 test_loss:870.5101318359375\n",
      "2135/3000 train_loss: 49.239566802978516 test_loss:883.443359375\n",
      "2136/3000 train_loss: 42.261165618896484 test_loss:882.4227294921875\n",
      "2137/3000 train_loss: 40.264007568359375 test_loss:897.3416748046875\n",
      "2138/3000 train_loss: 38.05084228515625 test_loss:913.9862060546875\n",
      "2139/3000 train_loss: 49.71986770629883 test_loss:883.5498657226562\n",
      "2140/3000 train_loss: 46.34844970703125 test_loss:932.1253662109375\n",
      "2141/3000 train_loss: 53.33351516723633 test_loss:914.2015380859375\n",
      "2142/3000 train_loss: 51.129425048828125 test_loss:888.36181640625\n",
      "2143/3000 train_loss: 43.92307662963867 test_loss:882.1409301757812\n",
      "2144/3000 train_loss: 49.68834686279297 test_loss:867.985595703125\n",
      "2145/3000 train_loss: 50.97251892089844 test_loss:896.5654907226562\n",
      "2146/3000 train_loss: 44.5523681640625 test_loss:890.27734375\n",
      "2147/3000 train_loss: 49.33070373535156 test_loss:908.9027099609375\n",
      "2148/3000 train_loss: 50.00748062133789 test_loss:843.409912109375\n",
      "2149/3000 train_loss: 48.94248962402344 test_loss:862.9520263671875\n",
      "2150/3000 train_loss: 44.97595977783203 test_loss:909.3768920898438\n",
      "2151/3000 train_loss: 45.2984504699707 test_loss:909.1900634765625\n",
      "2152/3000 train_loss: 46.83156967163086 test_loss:923.938232421875\n",
      "2153/3000 train_loss: 44.01506042480469 test_loss:875.8109741210938\n",
      "2154/3000 train_loss: 44.021400451660156 test_loss:860.2584228515625\n",
      "2155/3000 train_loss: 44.72334289550781 test_loss:866.7814331054688\n",
      "2156/3000 train_loss: 40.555240631103516 test_loss:859.531982421875\n",
      "2157/3000 train_loss: 38.791507720947266 test_loss:858.0012817382812\n",
      "2158/3000 train_loss: 38.95594024658203 test_loss:867.43408203125\n",
      "2159/3000 train_loss: 57.376800537109375 test_loss:853.6669311523438\n",
      "2160/3000 train_loss: 45.76524353027344 test_loss:838.9930419921875\n",
      "2161/3000 train_loss: 46.832115173339844 test_loss:851.2078247070312\n",
      "2162/3000 train_loss: 42.709983825683594 test_loss:848.7648315429688\n",
      "2163/3000 train_loss: 48.218074798583984 test_loss:894.7583618164062\n",
      "2164/3000 train_loss: 49.33393859863281 test_loss:834.9241333007812\n",
      "2165/3000 train_loss: 39.419273376464844 test_loss:858.056884765625\n",
      "2166/3000 train_loss: 39.799774169921875 test_loss:865.9819946289062\n",
      "2167/3000 train_loss: 52.95583724975586 test_loss:840.4464721679688\n",
      "2168/3000 train_loss: 42.30112838745117 test_loss:883.530029296875\n",
      "2169/3000 train_loss: 45.140892028808594 test_loss:884.662841796875\n",
      "2170/3000 train_loss: 50.80337142944336 test_loss:874.325927734375\n",
      "2171/3000 train_loss: 40.98616409301758 test_loss:874.67626953125\n",
      "2172/3000 train_loss: 46.528602600097656 test_loss:899.75927734375\n",
      "2173/3000 train_loss: 47.54934310913086 test_loss:900.7516479492188\n",
      "2174/3000 train_loss: 55.20867919921875 test_loss:958.8124389648438\n",
      "2175/3000 train_loss: 49.175594329833984 test_loss:955.1629638671875\n",
      "2176/3000 train_loss: 56.06578826904297 test_loss:911.9716796875\n",
      "2177/3000 train_loss: 42.74232864379883 test_loss:953.4647216796875\n",
      "2178/3000 train_loss: 46.525291442871094 test_loss:923.3995361328125\n",
      "2179/3000 train_loss: 47.709529876708984 test_loss:896.4384765625\n",
      "2180/3000 train_loss: 47.326934814453125 test_loss:853.2487182617188\n",
      "2181/3000 train_loss: 48.8011360168457 test_loss:852.0838623046875\n",
      "2182/3000 train_loss: 43.2451057434082 test_loss:875.1936645507812\n",
      "2183/3000 train_loss: 41.447288513183594 test_loss:882.1348876953125\n",
      "2184/3000 train_loss: 46.24083709716797 test_loss:873.455078125\n",
      "2185/3000 train_loss: 44.71601867675781 test_loss:890.3865966796875\n",
      "2186/3000 train_loss: 42.53858184814453 test_loss:910.1980590820312\n",
      "2187/3000 train_loss: 50.06407928466797 test_loss:877.2685546875\n",
      "2188/3000 train_loss: 43.10284423828125 test_loss:886.9334716796875\n",
      "2189/3000 train_loss: 59.248714447021484 test_loss:891.5484008789062\n",
      "2190/3000 train_loss: 48.12236785888672 test_loss:859.6209716796875\n",
      "2191/3000 train_loss: 43.81825637817383 test_loss:902.6054077148438\n",
      "2192/3000 train_loss: 48.484886169433594 test_loss:886.7924194335938\n",
      "2193/3000 train_loss: 43.135520935058594 test_loss:931.7053833007812\n",
      "2194/3000 train_loss: 49.244293212890625 test_loss:931.4293212890625\n",
      "2195/3000 train_loss: 51.13746643066406 test_loss:878.5494995117188\n",
      "2196/3000 train_loss: 42.36313247680664 test_loss:944.7745361328125\n",
      "2197/3000 train_loss: 47.9237060546875 test_loss:882.140380859375\n",
      "2198/3000 train_loss: 51.588645935058594 test_loss:890.1012573242188\n",
      "2199/3000 train_loss: 43.539695739746094 test_loss:865.3424072265625\n",
      "2200/3000 train_loss: 41.730045318603516 test_loss:851.5752563476562\n",
      "2201/3000 train_loss: 44.22750473022461 test_loss:886.1180419921875\n",
      "2202/3000 train_loss: 56.44340133666992 test_loss:873.1787109375\n",
      "2203/3000 train_loss: 44.8066520690918 test_loss:874.619140625\n",
      "2204/3000 train_loss: 36.14085388183594 test_loss:846.5060424804688\n",
      "2205/3000 train_loss: 45.051536560058594 test_loss:865.6441040039062\n",
      "2206/3000 train_loss: 37.395233154296875 test_loss:866.7505493164062\n",
      "2207/3000 train_loss: 44.28011703491211 test_loss:875.6407470703125\n",
      "2208/3000 train_loss: 37.69964599609375 test_loss:913.4083862304688\n",
      "2209/3000 train_loss: 45.78647994995117 test_loss:900.3489379882812\n",
      "2210/3000 train_loss: 43.53617858886719 test_loss:913.3289184570312\n",
      "2211/3000 train_loss: 49.27559280395508 test_loss:861.8799438476562\n",
      "2212/3000 train_loss: 46.118412017822266 test_loss:853.367431640625\n",
      "2213/3000 train_loss: 48.18745040893555 test_loss:877.5470581054688\n",
      "2214/3000 train_loss: 40.708927154541016 test_loss:846.9793701171875\n",
      "2215/3000 train_loss: 39.68964385986328 test_loss:857.4927368164062\n",
      "2216/3000 train_loss: 45.54947280883789 test_loss:877.66552734375\n",
      "2217/3000 train_loss: 53.00584411621094 test_loss:872.2426147460938\n",
      "2218/3000 train_loss: 64.70733642578125 test_loss:811.7300415039062\n",
      "2219/3000 train_loss: 57.9028434753418 test_loss:894.658203125\n",
      "2220/3000 train_loss: 47.36765670776367 test_loss:909.6937255859375\n",
      "2221/3000 train_loss: 39.2263298034668 test_loss:904.2847290039062\n",
      "2222/3000 train_loss: 46.959129333496094 test_loss:868.1167602539062\n",
      "2223/3000 train_loss: 40.7501106262207 test_loss:899.2883911132812\n",
      "2224/3000 train_loss: 42.20243453979492 test_loss:888.0823974609375\n",
      "2225/3000 train_loss: 39.192054748535156 test_loss:901.7367553710938\n",
      "2226/3000 train_loss: 40.82041549682617 test_loss:909.1569213867188\n",
      "2227/3000 train_loss: 55.90768051147461 test_loss:883.3074951171875\n",
      "2228/3000 train_loss: 39.89619064331055 test_loss:868.6063232421875\n",
      "2229/3000 train_loss: 40.066322326660156 test_loss:867.0736694335938\n",
      "2230/3000 train_loss: 44.12759017944336 test_loss:888.4478149414062\n",
      "2231/3000 train_loss: 45.93252182006836 test_loss:908.0653076171875\n",
      "2232/3000 train_loss: 48.32731628417969 test_loss:942.2698364257812\n",
      "2233/3000 train_loss: 44.590232849121094 test_loss:827.6773071289062\n",
      "2234/3000 train_loss: 41.619224548339844 test_loss:846.1701049804688\n",
      "2235/3000 train_loss: 51.59877395629883 test_loss:856.1321411132812\n",
      "2236/3000 train_loss: 40.773250579833984 test_loss:835.2840576171875\n",
      "2237/3000 train_loss: 45.219688415527344 test_loss:873.56640625\n",
      "2238/3000 train_loss: 39.62062072753906 test_loss:906.2637939453125\n",
      "2239/3000 train_loss: 38.97462844848633 test_loss:865.6124267578125\n",
      "2240/3000 train_loss: 35.825653076171875 test_loss:900.9252319335938\n",
      "2241/3000 train_loss: 39.449764251708984 test_loss:875.4732666015625\n",
      "2242/3000 train_loss: 39.105018615722656 test_loss:882.6023559570312\n",
      "2243/3000 train_loss: 41.816932678222656 test_loss:874.360107421875\n",
      "2244/3000 train_loss: 46.47605895996094 test_loss:866.0787353515625\n",
      "2245/3000 train_loss: 56.95542907714844 test_loss:909.8164672851562\n",
      "2246/3000 train_loss: 55.80227279663086 test_loss:888.113037109375\n",
      "2247/3000 train_loss: 51.14045333862305 test_loss:881.5684204101562\n",
      "2248/3000 train_loss: 47.98358917236328 test_loss:868.8477172851562\n",
      "2249/3000 train_loss: 42.542152404785156 test_loss:853.662109375\n",
      "2250/3000 train_loss: 48.10791015625 test_loss:855.0466918945312\n",
      "2251/3000 train_loss: 47.812461853027344 test_loss:858.4442138671875\n",
      "2252/3000 train_loss: 43.101505279541016 test_loss:877.1878662109375\n",
      "2253/3000 train_loss: 49.47453689575195 test_loss:859.471435546875\n",
      "2254/3000 train_loss: 40.86663818359375 test_loss:829.526611328125\n",
      "2255/3000 train_loss: 46.36424255371094 test_loss:820.103515625\n",
      "2256/3000 train_loss: 48.88386917114258 test_loss:841.3435668945312\n",
      "2257/3000 train_loss: 44.144737243652344 test_loss:854.4554443359375\n",
      "2258/3000 train_loss: 52.22062683105469 test_loss:874.1531372070312\n",
      "2259/3000 train_loss: 47.493492126464844 test_loss:870.3927612304688\n",
      "2260/3000 train_loss: 40.1323356628418 test_loss:909.1019897460938\n",
      "2261/3000 train_loss: 38.2430419921875 test_loss:883.78564453125\n",
      "2262/3000 train_loss: 42.14755630493164 test_loss:907.68212890625\n",
      "2263/3000 train_loss: 49.324363708496094 test_loss:901.2642822265625\n",
      "2264/3000 train_loss: 73.3140869140625 test_loss:906.0681762695312\n",
      "2265/3000 train_loss: 47.972198486328125 test_loss:965.4580078125\n",
      "2266/3000 train_loss: 42.158485412597656 test_loss:946.6237182617188\n",
      "2267/3000 train_loss: 50.80261993408203 test_loss:880.9498291015625\n",
      "2268/3000 train_loss: 45.242923736572266 test_loss:921.1242065429688\n",
      "2269/3000 train_loss: 45.41497039794922 test_loss:932.5861206054688\n",
      "2270/3000 train_loss: 48.38999557495117 test_loss:885.0161743164062\n",
      "2271/3000 train_loss: 47.125877380371094 test_loss:862.1939697265625\n",
      "2272/3000 train_loss: 44.41507339477539 test_loss:854.777587890625\n",
      "2273/3000 train_loss: 48.71873092651367 test_loss:855.5216064453125\n",
      "2274/3000 train_loss: 44.88140106201172 test_loss:866.4899291992188\n",
      "2275/3000 train_loss: 40.59864044189453 test_loss:886.3984375\n",
      "2276/3000 train_loss: 45.570919036865234 test_loss:866.320556640625\n",
      "2277/3000 train_loss: 47.319889068603516 test_loss:868.2722778320312\n",
      "2278/3000 train_loss: 45.566917419433594 test_loss:900.4570922851562\n",
      "2279/3000 train_loss: 39.409481048583984 test_loss:888.048583984375\n",
      "2280/3000 train_loss: 43.63597106933594 test_loss:926.51904296875\n",
      "2281/3000 train_loss: 41.40966796875 test_loss:911.1862182617188\n",
      "2282/3000 train_loss: 35.930076599121094 test_loss:888.3003540039062\n",
      "2283/3000 train_loss: 39.32172393798828 test_loss:905.5418090820312\n",
      "2284/3000 train_loss: 52.07041931152344 test_loss:875.38427734375\n",
      "2285/3000 train_loss: 50.027099609375 test_loss:873.7356567382812\n",
      "2286/3000 train_loss: 46.20832824707031 test_loss:913.1827392578125\n",
      "2287/3000 train_loss: 60.67913055419922 test_loss:933.4937133789062\n",
      "2288/3000 train_loss: 43.98055648803711 test_loss:958.9542846679688\n",
      "2289/3000 train_loss: 43.299560546875 test_loss:941.8550415039062\n",
      "2290/3000 train_loss: 47.29655075073242 test_loss:906.7867431640625\n",
      "2291/3000 train_loss: 48.97868347167969 test_loss:909.9744873046875\n",
      "2292/3000 train_loss: 49.10814666748047 test_loss:921.6676025390625\n",
      "2293/3000 train_loss: 45.748687744140625 test_loss:890.8794555664062\n",
      "2294/3000 train_loss: 42.64816665649414 test_loss:935.6399536132812\n",
      "2295/3000 train_loss: 39.28467559814453 test_loss:928.1091918945312\n",
      "2296/3000 train_loss: 42.43752670288086 test_loss:911.5613403320312\n",
      "2297/3000 train_loss: 45.13064956665039 test_loss:889.1965942382812\n",
      "2298/3000 train_loss: 36.10336685180664 test_loss:865.3466186523438\n",
      "2299/3000 train_loss: 46.405418395996094 test_loss:873.18115234375\n",
      "2300/3000 train_loss: 42.446685791015625 test_loss:840.9313354492188\n",
      "2301/3000 train_loss: 52.6320915222168 test_loss:867.0980834960938\n",
      "2302/3000 train_loss: 40.1992073059082 test_loss:866.5883178710938\n",
      "2303/3000 train_loss: 41.17654037475586 test_loss:865.5528564453125\n",
      "2304/3000 train_loss: 38.599308013916016 test_loss:860.2655029296875\n",
      "2305/3000 train_loss: 43.042659759521484 test_loss:862.3599853515625\n",
      "2306/3000 train_loss: 49.2004508972168 test_loss:861.595947265625\n",
      "2307/3000 train_loss: 46.43708801269531 test_loss:882.0438232421875\n",
      "2308/3000 train_loss: 40.49728775024414 test_loss:848.50390625\n",
      "2309/3000 train_loss: 54.54199981689453 test_loss:856.7451171875\n",
      "2310/3000 train_loss: 43.10712432861328 test_loss:850.6671142578125\n",
      "2311/3000 train_loss: 46.188045501708984 test_loss:863.5773315429688\n",
      "2312/3000 train_loss: 44.07809829711914 test_loss:834.00146484375\n",
      "2313/3000 train_loss: 40.61472702026367 test_loss:864.104736328125\n",
      "2314/3000 train_loss: 37.897640228271484 test_loss:871.4235229492188\n",
      "2315/3000 train_loss: 41.38416290283203 test_loss:868.2280883789062\n",
      "2316/3000 train_loss: 59.483680725097656 test_loss:884.968505859375\n",
      "2317/3000 train_loss: 44.4990234375 test_loss:864.7844848632812\n",
      "2318/3000 train_loss: 41.45133972167969 test_loss:867.0166625976562\n",
      "2319/3000 train_loss: 45.93250274658203 test_loss:866.82958984375\n",
      "2320/3000 train_loss: 44.233970642089844 test_loss:882.9813842773438\n",
      "2321/3000 train_loss: 41.10247039794922 test_loss:842.1994018554688\n",
      "2322/3000 train_loss: 52.766029357910156 test_loss:867.142333984375\n",
      "2323/3000 train_loss: 46.4199333190918 test_loss:850.0402221679688\n",
      "2324/3000 train_loss: 43.35549545288086 test_loss:870.975341796875\n",
      "2325/3000 train_loss: 41.821292877197266 test_loss:907.5357666015625\n",
      "2326/3000 train_loss: 36.65445327758789 test_loss:910.6609497070312\n",
      "2327/3000 train_loss: 35.59395217895508 test_loss:880.59521484375\n",
      "2328/3000 train_loss: 38.560237884521484 test_loss:879.8335571289062\n",
      "2329/3000 train_loss: 39.161277770996094 test_loss:913.4736328125\n",
      "2330/3000 train_loss: 48.11885452270508 test_loss:873.5203857421875\n",
      "2331/3000 train_loss: 38.57408905029297 test_loss:902.0817260742188\n",
      "2332/3000 train_loss: 38.90644073486328 test_loss:871.2759399414062\n",
      "2333/3000 train_loss: 37.20355987548828 test_loss:887.3822631835938\n",
      "2334/3000 train_loss: 42.257286071777344 test_loss:895.50390625\n",
      "2335/3000 train_loss: 41.36104965209961 test_loss:916.9176025390625\n",
      "2336/3000 train_loss: 47.44353485107422 test_loss:917.5740356445312\n",
      "2337/3000 train_loss: 48.871490478515625 test_loss:914.3137817382812\n",
      "2338/3000 train_loss: 41.77561950683594 test_loss:996.7786254882812\n",
      "2339/3000 train_loss: 53.012325286865234 test_loss:942.398193359375\n",
      "2340/3000 train_loss: 48.247013092041016 test_loss:902.882568359375\n",
      "2341/3000 train_loss: 40.25934982299805 test_loss:858.7168579101562\n",
      "2342/3000 train_loss: 48.348304748535156 test_loss:901.4454956054688\n",
      "2343/3000 train_loss: 43.31425857543945 test_loss:877.8359985351562\n",
      "2344/3000 train_loss: 38.4287223815918 test_loss:891.4774780273438\n",
      "2345/3000 train_loss: 45.20737838745117 test_loss:848.8442993164062\n",
      "2346/3000 train_loss: 39.625648498535156 test_loss:886.251953125\n",
      "2347/3000 train_loss: 47.40168380737305 test_loss:884.868896484375\n",
      "2348/3000 train_loss: 39.26322555541992 test_loss:871.3382568359375\n",
      "2349/3000 train_loss: 39.616424560546875 test_loss:872.2227783203125\n",
      "2350/3000 train_loss: 62.10076904296875 test_loss:884.9288940429688\n",
      "2351/3000 train_loss: 46.842681884765625 test_loss:882.1215209960938\n",
      "2352/3000 train_loss: 46.098793029785156 test_loss:883.3079833984375\n",
      "2353/3000 train_loss: 40.35695266723633 test_loss:848.398193359375\n",
      "2354/3000 train_loss: 47.224239349365234 test_loss:849.105712890625\n",
      "2355/3000 train_loss: 44.639198303222656 test_loss:923.733154296875\n",
      "2356/3000 train_loss: 38.856689453125 test_loss:908.5568237304688\n",
      "2357/3000 train_loss: 43.51076126098633 test_loss:920.9315185546875\n",
      "2358/3000 train_loss: 49.98048400878906 test_loss:865.6485595703125\n",
      "2359/3000 train_loss: 54.729759216308594 test_loss:887.0072631835938\n",
      "2360/3000 train_loss: 45.91824722290039 test_loss:901.752685546875\n",
      "2361/3000 train_loss: 38.74111557006836 test_loss:930.97314453125\n",
      "2362/3000 train_loss: 44.32939910888672 test_loss:895.7667236328125\n",
      "2363/3000 train_loss: 43.42415237426758 test_loss:897.2648315429688\n",
      "2364/3000 train_loss: 48.51057052612305 test_loss:851.0321655273438\n",
      "2365/3000 train_loss: 43.312137603759766 test_loss:915.0576171875\n",
      "2366/3000 train_loss: 37.42207717895508 test_loss:902.1422729492188\n",
      "2367/3000 train_loss: 42.48099136352539 test_loss:871.9739990234375\n",
      "2368/3000 train_loss: 43.964229583740234 test_loss:881.171630859375\n",
      "2369/3000 train_loss: 42.38890075683594 test_loss:890.9110717773438\n",
      "2370/3000 train_loss: 48.41423034667969 test_loss:870.1618041992188\n",
      "2371/3000 train_loss: 40.64472198486328 test_loss:859.3053588867188\n",
      "2372/3000 train_loss: 41.910987854003906 test_loss:849.15576171875\n",
      "2373/3000 train_loss: 48.30998992919922 test_loss:829.5410766601562\n",
      "2374/3000 train_loss: 57.27640914916992 test_loss:847.999267578125\n",
      "2375/3000 train_loss: 41.73772430419922 test_loss:872.8599853515625\n",
      "2376/3000 train_loss: 40.895416259765625 test_loss:874.5504150390625\n",
      "2377/3000 train_loss: 38.54695129394531 test_loss:881.630126953125\n",
      "2378/3000 train_loss: 44.45059585571289 test_loss:889.3613891601562\n",
      "2379/3000 train_loss: 40.345794677734375 test_loss:865.7160034179688\n",
      "2380/3000 train_loss: 53.23957824707031 test_loss:875.568115234375\n",
      "2381/3000 train_loss: 45.64967346191406 test_loss:886.7172241210938\n",
      "2382/3000 train_loss: 43.23101806640625 test_loss:858.829345703125\n",
      "2383/3000 train_loss: 47.30652618408203 test_loss:838.2124633789062\n",
      "2384/3000 train_loss: 43.170711517333984 test_loss:880.11669921875\n",
      "2385/3000 train_loss: 41.22309875488281 test_loss:863.2891235351562\n",
      "2386/3000 train_loss: 59.41997528076172 test_loss:964.8798217773438\n",
      "2387/3000 train_loss: 51.214385986328125 test_loss:937.723876953125\n",
      "2388/3000 train_loss: 49.02949142456055 test_loss:953.7086791992188\n",
      "2389/3000 train_loss: 39.27153396606445 test_loss:924.8392944335938\n",
      "2390/3000 train_loss: 43.483253479003906 test_loss:942.4273071289062\n",
      "2391/3000 train_loss: 41.97200012207031 test_loss:913.01611328125\n",
      "2392/3000 train_loss: 39.26066970825195 test_loss:914.5731201171875\n",
      "2393/3000 train_loss: 51.61747741699219 test_loss:868.4794921875\n",
      "2394/3000 train_loss: 43.900787353515625 test_loss:907.2630004882812\n",
      "2395/3000 train_loss: 45.23197937011719 test_loss:903.8902587890625\n",
      "2396/3000 train_loss: 49.74272918701172 test_loss:928.6434936523438\n",
      "2397/3000 train_loss: 37.05282211303711 test_loss:902.5012817382812\n",
      "2398/3000 train_loss: 37.812015533447266 test_loss:892.8270874023438\n",
      "2399/3000 train_loss: 45.61835479736328 test_loss:830.1019287109375\n",
      "2400/3000 train_loss: 40.824317932128906 test_loss:867.350341796875\n",
      "2401/3000 train_loss: 37.853275299072266 test_loss:863.3831176757812\n",
      "2402/3000 train_loss: 39.627262115478516 test_loss:857.869873046875\n",
      "2403/3000 train_loss: 47.99134063720703 test_loss:889.2029418945312\n",
      "2404/3000 train_loss: 44.725486755371094 test_loss:873.2982788085938\n",
      "2405/3000 train_loss: 43.08484649658203 test_loss:871.6847534179688\n",
      "2406/3000 train_loss: 37.427310943603516 test_loss:891.202392578125\n",
      "2407/3000 train_loss: 47.13932418823242 test_loss:882.4422607421875\n",
      "2408/3000 train_loss: 40.93621063232422 test_loss:869.6574096679688\n",
      "2409/3000 train_loss: 47.185394287109375 test_loss:875.2229614257812\n",
      "2410/3000 train_loss: 37.099063873291016 test_loss:883.8155517578125\n",
      "2411/3000 train_loss: 39.48574447631836 test_loss:899.9450073242188\n",
      "2412/3000 train_loss: 40.860382080078125 test_loss:878.9865112304688\n",
      "2413/3000 train_loss: 62.772132873535156 test_loss:876.7177734375\n",
      "2414/3000 train_loss: 44.49324035644531 test_loss:864.8338623046875\n",
      "2415/3000 train_loss: 45.7069091796875 test_loss:844.0247192382812\n",
      "2416/3000 train_loss: 43.915122985839844 test_loss:847.8568115234375\n",
      "2417/3000 train_loss: 45.685890197753906 test_loss:884.4193725585938\n",
      "2418/3000 train_loss: 42.96847152709961 test_loss:858.607421875\n",
      "2419/3000 train_loss: 38.735904693603516 test_loss:913.0206909179688\n",
      "2420/3000 train_loss: 42.19474792480469 test_loss:863.3260498046875\n",
      "2421/3000 train_loss: 41.60049057006836 test_loss:887.1488647460938\n",
      "2422/3000 train_loss: 38.70810317993164 test_loss:871.1847534179688\n",
      "2423/3000 train_loss: 44.86717224121094 test_loss:869.5278930664062\n",
      "2424/3000 train_loss: 53.7033576965332 test_loss:872.8232421875\n",
      "2425/3000 train_loss: 38.879051208496094 test_loss:887.387451171875\n",
      "2426/3000 train_loss: 40.77168655395508 test_loss:870.8168334960938\n",
      "2427/3000 train_loss: 48.09921646118164 test_loss:948.066162109375\n",
      "2428/3000 train_loss: 62.932029724121094 test_loss:886.3473510742188\n",
      "2429/3000 train_loss: 39.7090950012207 test_loss:868.4261474609375\n",
      "2430/3000 train_loss: 36.93452453613281 test_loss:845.15185546875\n",
      "2431/3000 train_loss: 45.8743896484375 test_loss:825.5193481445312\n",
      "2432/3000 train_loss: 39.35872268676758 test_loss:854.4305419921875\n",
      "2433/3000 train_loss: 43.458396911621094 test_loss:846.138916015625\n",
      "2434/3000 train_loss: 37.75184631347656 test_loss:836.2421264648438\n",
      "2435/3000 train_loss: 40.9539794921875 test_loss:835.3754272460938\n",
      "2436/3000 train_loss: 57.31181335449219 test_loss:863.4094848632812\n",
      "2437/3000 train_loss: 46.305301666259766 test_loss:868.7522583007812\n",
      "2438/3000 train_loss: 45.874290466308594 test_loss:889.8027954101562\n",
      "2439/3000 train_loss: 38.85713577270508 test_loss:887.0465698242188\n",
      "2440/3000 train_loss: 41.355838775634766 test_loss:901.9418334960938\n",
      "2441/3000 train_loss: 47.668312072753906 test_loss:931.1406860351562\n",
      "2442/3000 train_loss: 39.433536529541016 test_loss:916.941650390625\n",
      "2443/3000 train_loss: 39.53638458251953 test_loss:906.7761840820312\n",
      "2444/3000 train_loss: 50.278621673583984 test_loss:903.9814453125\n",
      "2445/3000 train_loss: 44.47391128540039 test_loss:886.262451171875\n",
      "2446/3000 train_loss: 39.63030242919922 test_loss:881.3272705078125\n",
      "2447/3000 train_loss: 46.7177734375 test_loss:895.1110229492188\n",
      "2448/3000 train_loss: 43.55045700073242 test_loss:912.423583984375\n",
      "2449/3000 train_loss: 46.126625061035156 test_loss:885.0289916992188\n",
      "2450/3000 train_loss: 46.00762176513672 test_loss:883.775634765625\n",
      "2451/3000 train_loss: 44.11906814575195 test_loss:890.13818359375\n",
      "2452/3000 train_loss: 39.4289436340332 test_loss:862.5962524414062\n",
      "2453/3000 train_loss: 40.09404754638672 test_loss:894.6712036132812\n",
      "2454/3000 train_loss: 52.963523864746094 test_loss:889.3436889648438\n",
      "2455/3000 train_loss: 50.2034912109375 test_loss:907.2075805664062\n",
      "2456/3000 train_loss: 57.277076721191406 test_loss:991.9817504882812\n",
      "2457/3000 train_loss: 57.93362045288086 test_loss:932.0607299804688\n",
      "2458/3000 train_loss: 45.1882209777832 test_loss:914.6292724609375\n",
      "2459/3000 train_loss: 50.084510803222656 test_loss:886.292236328125\n",
      "2460/3000 train_loss: 37.97333908081055 test_loss:948.2164306640625\n",
      "2461/3000 train_loss: 46.432273864746094 test_loss:907.5928955078125\n",
      "2462/3000 train_loss: 39.43303298950195 test_loss:919.1320190429688\n",
      "2463/3000 train_loss: 40.35292053222656 test_loss:898.6689453125\n",
      "2464/3000 train_loss: 49.83259201049805 test_loss:965.994384765625\n",
      "2465/3000 train_loss: 42.76309585571289 test_loss:944.7031860351562\n",
      "2466/3000 train_loss: 40.48917007446289 test_loss:946.256103515625\n",
      "2467/3000 train_loss: 43.585670471191406 test_loss:934.6180419921875\n",
      "2468/3000 train_loss: 52.116302490234375 test_loss:974.0718383789062\n",
      "2469/3000 train_loss: 40.422874450683594 test_loss:941.62548828125\n",
      "2470/3000 train_loss: 41.407188415527344 test_loss:922.6014404296875\n",
      "2471/3000 train_loss: 41.574256896972656 test_loss:924.8908081054688\n",
      "2472/3000 train_loss: 48.166263580322266 test_loss:951.926513671875\n",
      "2473/3000 train_loss: 44.678131103515625 test_loss:959.9131469726562\n",
      "2474/3000 train_loss: 41.86834716796875 test_loss:918.4702758789062\n",
      "2475/3000 train_loss: 39.011417388916016 test_loss:906.107177734375\n",
      "2476/3000 train_loss: 43.619598388671875 test_loss:916.8700561523438\n",
      "2477/3000 train_loss: 46.441261291503906 test_loss:943.2413330078125\n",
      "2478/3000 train_loss: 36.377906799316406 test_loss:967.5270385742188\n",
      "2479/3000 train_loss: 46.396610260009766 test_loss:956.5980834960938\n",
      "2480/3000 train_loss: 38.982784271240234 test_loss:914.93896484375\n",
      "2481/3000 train_loss: 42.02095413208008 test_loss:968.1642456054688\n",
      "2482/3000 train_loss: 44.90929412841797 test_loss:914.0694580078125\n",
      "2483/3000 train_loss: 39.65753173828125 test_loss:940.194580078125\n",
      "2484/3000 train_loss: 44.31791305541992 test_loss:910.1695556640625\n",
      "2485/3000 train_loss: 42.66279983520508 test_loss:902.3939208984375\n",
      "2486/3000 train_loss: 41.761634826660156 test_loss:940.12255859375\n",
      "2487/3000 train_loss: 45.67308807373047 test_loss:913.4322509765625\n",
      "2488/3000 train_loss: 35.435420989990234 test_loss:955.57958984375\n",
      "2489/3000 train_loss: 39.20797348022461 test_loss:957.673095703125\n",
      "2490/3000 train_loss: 43.95419692993164 test_loss:942.94287109375\n",
      "2491/3000 train_loss: 36.40446472167969 test_loss:915.234375\n",
      "2492/3000 train_loss: 41.94105529785156 test_loss:942.53076171875\n",
      "2493/3000 train_loss: 40.241031646728516 test_loss:920.1539306640625\n",
      "2494/3000 train_loss: 42.20746612548828 test_loss:942.4227905273438\n",
      "2495/3000 train_loss: 37.842498779296875 test_loss:891.7853393554688\n",
      "2496/3000 train_loss: 45.11836242675781 test_loss:917.2269287109375\n",
      "2497/3000 train_loss: 44.82935333251953 test_loss:876.4332275390625\n",
      "2498/3000 train_loss: 46.922725677490234 test_loss:856.6209716796875\n",
      "2499/3000 train_loss: 47.17681884765625 test_loss:928.6922607421875\n",
      "2500/3000 train_loss: 43.74513244628906 test_loss:916.0440673828125\n",
      "2501/3000 train_loss: 48.236236572265625 test_loss:954.0143432617188\n",
      "2502/3000 train_loss: 39.7184944152832 test_loss:923.9734497070312\n",
      "2503/3000 train_loss: 39.87113952636719 test_loss:964.5404663085938\n",
      "2504/3000 train_loss: 44.45125961303711 test_loss:917.0873413085938\n",
      "2505/3000 train_loss: 44.00309753417969 test_loss:951.7432250976562\n",
      "2506/3000 train_loss: 43.157691955566406 test_loss:914.1710205078125\n",
      "2507/3000 train_loss: 38.28664779663086 test_loss:951.2614135742188\n",
      "2508/3000 train_loss: 54.96331787109375 test_loss:913.812255859375\n",
      "2509/3000 train_loss: 43.5267219543457 test_loss:883.7119750976562\n",
      "2510/3000 train_loss: 49.304443359375 test_loss:840.9226684570312\n",
      "2511/3000 train_loss: 45.49853515625 test_loss:889.5685424804688\n",
      "2512/3000 train_loss: 44.412960052490234 test_loss:912.1976928710938\n",
      "2513/3000 train_loss: 44.29222869873047 test_loss:927.7032470703125\n",
      "2514/3000 train_loss: 46.178810119628906 test_loss:911.3233032226562\n",
      "2515/3000 train_loss: 38.23271560668945 test_loss:924.4358520507812\n",
      "2516/3000 train_loss: 41.19169616699219 test_loss:927.79296875\n",
      "2517/3000 train_loss: 42.47079086303711 test_loss:911.8982543945312\n",
      "2518/3000 train_loss: 36.130184173583984 test_loss:915.6312866210938\n",
      "2519/3000 train_loss: 39.153141021728516 test_loss:923.3751220703125\n",
      "2520/3000 train_loss: 41.54239273071289 test_loss:905.8177490234375\n",
      "2521/3000 train_loss: 50.26419448852539 test_loss:942.3775634765625\n",
      "2522/3000 train_loss: 44.78294372558594 test_loss:938.0385131835938\n",
      "2523/3000 train_loss: 38.00248718261719 test_loss:907.225341796875\n",
      "2524/3000 train_loss: 36.751502990722656 test_loss:922.4745483398438\n",
      "2525/3000 train_loss: 40.2686653137207 test_loss:895.532958984375\n",
      "2526/3000 train_loss: 38.46855926513672 test_loss:907.8580322265625\n",
      "2527/3000 train_loss: 37.93968963623047 test_loss:902.5838623046875\n",
      "2528/3000 train_loss: 42.840457916259766 test_loss:905.038818359375\n",
      "2529/3000 train_loss: 37.50410079956055 test_loss:883.759033203125\n",
      "2530/3000 train_loss: 55.62683868408203 test_loss:934.068359375\n",
      "2531/3000 train_loss: 55.58804702758789 test_loss:971.2421264648438\n",
      "2532/3000 train_loss: 53.667396545410156 test_loss:911.5755004882812\n",
      "2533/3000 train_loss: 40.048980712890625 test_loss:895.1065673828125\n",
      "2534/3000 train_loss: 41.20814514160156 test_loss:930.9322509765625\n",
      "2535/3000 train_loss: 36.504608154296875 test_loss:883.19970703125\n",
      "2536/3000 train_loss: 40.87303161621094 test_loss:929.8527221679688\n",
      "2537/3000 train_loss: 40.52885818481445 test_loss:923.8898315429688\n",
      "2538/3000 train_loss: 39.674434661865234 test_loss:929.8565063476562\n",
      "2539/3000 train_loss: 49.98088836669922 test_loss:905.0701293945312\n",
      "2540/3000 train_loss: 40.61897277832031 test_loss:930.5994262695312\n",
      "2541/3000 train_loss: 47.01544952392578 test_loss:941.8333129882812\n",
      "2542/3000 train_loss: 36.77389907836914 test_loss:932.5379028320312\n",
      "2543/3000 train_loss: 37.97694396972656 test_loss:956.7340698242188\n",
      "2544/3000 train_loss: 48.4029426574707 test_loss:909.8878173828125\n",
      "2545/3000 train_loss: 41.43157958984375 test_loss:936.7842407226562\n",
      "2546/3000 train_loss: 43.21592712402344 test_loss:929.2015991210938\n",
      "2547/3000 train_loss: 38.50038528442383 test_loss:905.8659057617188\n",
      "2548/3000 train_loss: 39.63942337036133 test_loss:904.4031372070312\n",
      "2549/3000 train_loss: 36.69693374633789 test_loss:913.22119140625\n",
      "2550/3000 train_loss: 34.912200927734375 test_loss:925.3779907226562\n",
      "2551/3000 train_loss: 38.19804382324219 test_loss:917.1456909179688\n",
      "2552/3000 train_loss: 33.25128173828125 test_loss:943.1619873046875\n",
      "2553/3000 train_loss: 42.3836555480957 test_loss:953.2130737304688\n",
      "2554/3000 train_loss: 40.69742202758789 test_loss:912.0672607421875\n",
      "2555/3000 train_loss: 45.64851379394531 test_loss:884.128662109375\n",
      "2556/3000 train_loss: 40.50480270385742 test_loss:888.2815551757812\n",
      "2557/3000 train_loss: 39.824058532714844 test_loss:911.5892944335938\n",
      "2558/3000 train_loss: 51.34776306152344 test_loss:908.2481689453125\n",
      "2559/3000 train_loss: 47.428863525390625 test_loss:950.42138671875\n",
      "2560/3000 train_loss: 40.52861022949219 test_loss:844.2142333984375\n",
      "2561/3000 train_loss: 36.76283264160156 test_loss:869.5797729492188\n",
      "2562/3000 train_loss: 37.39958190917969 test_loss:865.654296875\n",
      "2563/3000 train_loss: 42.367156982421875 test_loss:891.2467651367188\n",
      "2564/3000 train_loss: 42.67930603027344 test_loss:858.2149658203125\n",
      "2565/3000 train_loss: 34.60943603515625 test_loss:892.34130859375\n",
      "2566/3000 train_loss: 37.2646598815918 test_loss:858.207763671875\n",
      "2567/3000 train_loss: 37.88168716430664 test_loss:904.2011108398438\n",
      "2568/3000 train_loss: 48.12346649169922 test_loss:829.7161254882812\n",
      "2569/3000 train_loss: 37.0977897644043 test_loss:840.0718383789062\n",
      "2570/3000 train_loss: 36.13582992553711 test_loss:876.4539184570312\n",
      "2571/3000 train_loss: 39.2286376953125 test_loss:861.1472778320312\n",
      "2572/3000 train_loss: 36.348297119140625 test_loss:870.2921142578125\n",
      "2573/3000 train_loss: 39.039066314697266 test_loss:897.6626586914062\n",
      "2574/3000 train_loss: 43.9122314453125 test_loss:855.8954467773438\n",
      "2575/3000 train_loss: 39.73269271850586 test_loss:861.2088623046875\n",
      "2576/3000 train_loss: 45.9163932800293 test_loss:875.038330078125\n",
      "2577/3000 train_loss: 46.9455451965332 test_loss:872.0670776367188\n",
      "2578/3000 train_loss: 44.43730163574219 test_loss:917.3101806640625\n",
      "2579/3000 train_loss: 41.61270523071289 test_loss:885.4456176757812\n",
      "2580/3000 train_loss: 36.46828079223633 test_loss:831.6142578125\n",
      "2581/3000 train_loss: 52.709781646728516 test_loss:882.8521118164062\n",
      "2582/3000 train_loss: 41.1351318359375 test_loss:825.2821044921875\n",
      "2583/3000 train_loss: 49.45248794555664 test_loss:878.030029296875\n",
      "2584/3000 train_loss: 43.98794937133789 test_loss:908.0629272460938\n",
      "2585/3000 train_loss: 34.75235366821289 test_loss:910.1214599609375\n",
      "2586/3000 train_loss: 36.09020233154297 test_loss:899.320556640625\n",
      "2587/3000 train_loss: 36.132041931152344 test_loss:903.2254028320312\n",
      "2588/3000 train_loss: 38.802268981933594 test_loss:927.2990112304688\n",
      "2589/3000 train_loss: 40.439910888671875 test_loss:901.7017211914062\n",
      "2590/3000 train_loss: 62.75806427001953 test_loss:909.7167358398438\n",
      "2591/3000 train_loss: 54.65732192993164 test_loss:882.5709228515625\n",
      "2592/3000 train_loss: 41.328155517578125 test_loss:866.1517333984375\n",
      "2593/3000 train_loss: 38.7764778137207 test_loss:859.1917724609375\n",
      "2594/3000 train_loss: 47.166473388671875 test_loss:887.253662109375\n",
      "2595/3000 train_loss: 47.943580627441406 test_loss:908.9227905273438\n",
      "2596/3000 train_loss: 48.813682556152344 test_loss:847.6712036132812\n",
      "2597/3000 train_loss: 42.978668212890625 test_loss:889.803955078125\n",
      "2598/3000 train_loss: 41.50566482543945 test_loss:877.8322143554688\n",
      "2599/3000 train_loss: 43.889041900634766 test_loss:884.627197265625\n",
      "2600/3000 train_loss: 38.75944137573242 test_loss:858.2605590820312\n",
      "2601/3000 train_loss: 44.14268493652344 test_loss:859.8676147460938\n",
      "2602/3000 train_loss: 40.45695877075195 test_loss:881.2978515625\n",
      "2603/3000 train_loss: 43.53500747680664 test_loss:865.9287719726562\n",
      "2604/3000 train_loss: 54.043113708496094 test_loss:871.3480834960938\n",
      "2605/3000 train_loss: 42.97119903564453 test_loss:836.197509765625\n",
      "2606/3000 train_loss: 34.66332244873047 test_loss:859.067138671875\n",
      "2607/3000 train_loss: 43.22372055053711 test_loss:846.2986450195312\n",
      "2608/3000 train_loss: 47.78939437866211 test_loss:875.9257202148438\n",
      "2609/3000 train_loss: 42.52490997314453 test_loss:911.6082153320312\n",
      "2610/3000 train_loss: 42.76405715942383 test_loss:913.095947265625\n",
      "2611/3000 train_loss: 53.438331604003906 test_loss:933.528564453125\n",
      "2612/3000 train_loss: 42.78477478027344 test_loss:908.62744140625\n",
      "2613/3000 train_loss: 41.120643615722656 test_loss:898.2648315429688\n",
      "2614/3000 train_loss: 38.365806579589844 test_loss:907.133544921875\n",
      "2615/3000 train_loss: 41.22855758666992 test_loss:885.2466430664062\n",
      "2616/3000 train_loss: 39.36526870727539 test_loss:888.9276123046875\n",
      "2617/3000 train_loss: 36.638301849365234 test_loss:887.6737670898438\n",
      "2618/3000 train_loss: 43.52325439453125 test_loss:875.6996459960938\n",
      "2619/3000 train_loss: 44.311790466308594 test_loss:849.1762084960938\n",
      "2620/3000 train_loss: 41.617149353027344 test_loss:870.7431030273438\n",
      "2621/3000 train_loss: 34.60174560546875 test_loss:888.6940307617188\n",
      "2622/3000 train_loss: 46.364498138427734 test_loss:886.0263061523438\n",
      "2623/3000 train_loss: 37.16427230834961 test_loss:880.2222290039062\n",
      "2624/3000 train_loss: 39.76153564453125 test_loss:849.4183959960938\n",
      "2625/3000 train_loss: 44.98598098754883 test_loss:879.785888671875\n",
      "2626/3000 train_loss: 40.2733268737793 test_loss:846.1539306640625\n",
      "2627/3000 train_loss: 38.49518585205078 test_loss:887.7991943359375\n",
      "2628/3000 train_loss: 41.28324890136719 test_loss:869.3609619140625\n",
      "2629/3000 train_loss: 40.14809799194336 test_loss:886.718017578125\n",
      "2630/3000 train_loss: 40.00605392456055 test_loss:886.62841796875\n",
      "2631/3000 train_loss: 34.81751251220703 test_loss:866.1797485351562\n",
      "2632/3000 train_loss: 38.80946731567383 test_loss:884.4407958984375\n",
      "2633/3000 train_loss: 46.455326080322266 test_loss:845.5833740234375\n",
      "2634/3000 train_loss: 42.8089599609375 test_loss:888.2096557617188\n",
      "2635/3000 train_loss: 49.78015899658203 test_loss:849.1500854492188\n",
      "2636/3000 train_loss: 44.28278350830078 test_loss:878.0624389648438\n",
      "2637/3000 train_loss: 38.005760192871094 test_loss:883.6715087890625\n",
      "2638/3000 train_loss: 34.96563720703125 test_loss:883.2837524414062\n",
      "2639/3000 train_loss: 36.77674865722656 test_loss:888.1986083984375\n",
      "2640/3000 train_loss: 39.72309875488281 test_loss:870.506103515625\n",
      "2641/3000 train_loss: 41.81422424316406 test_loss:869.0889892578125\n",
      "2642/3000 train_loss: 33.86854553222656 test_loss:854.8524169921875\n",
      "2643/3000 train_loss: 42.03801727294922 test_loss:841.6226196289062\n",
      "2644/3000 train_loss: 43.142662048339844 test_loss:944.5978393554688\n",
      "2645/3000 train_loss: 46.08824157714844 test_loss:865.52978515625\n",
      "2646/3000 train_loss: 40.59569549560547 test_loss:980.6427001953125\n",
      "2647/3000 train_loss: 46.84833908081055 test_loss:937.0419921875\n",
      "2648/3000 train_loss: 40.26020050048828 test_loss:943.9382934570312\n",
      "2649/3000 train_loss: 42.089664459228516 test_loss:895.4356079101562\n",
      "2650/3000 train_loss: 44.573890686035156 test_loss:920.130126953125\n",
      "2651/3000 train_loss: 52.922401428222656 test_loss:868.1376342773438\n",
      "2652/3000 train_loss: 43.70243835449219 test_loss:936.284423828125\n",
      "2653/3000 train_loss: 39.06358337402344 test_loss:902.24365234375\n",
      "2654/3000 train_loss: 37.97129440307617 test_loss:928.1279296875\n",
      "2655/3000 train_loss: 43.659767150878906 test_loss:932.988037109375\n",
      "2656/3000 train_loss: 49.80364990234375 test_loss:913.9346313476562\n",
      "2657/3000 train_loss: 36.50728988647461 test_loss:928.9977416992188\n",
      "2658/3000 train_loss: 37.346214294433594 test_loss:897.3832397460938\n",
      "2659/3000 train_loss: 38.75028991699219 test_loss:931.13623046875\n",
      "2660/3000 train_loss: 37.04380798339844 test_loss:884.6908569335938\n",
      "2661/3000 train_loss: 43.11833953857422 test_loss:918.63623046875\n",
      "2662/3000 train_loss: 41.40516662597656 test_loss:891.1431884765625\n",
      "2663/3000 train_loss: 38.674644470214844 test_loss:886.2854614257812\n",
      "2664/3000 train_loss: 38.9154052734375 test_loss:883.7648315429688\n",
      "2665/3000 train_loss: 37.257835388183594 test_loss:896.3765869140625\n",
      "2666/3000 train_loss: 37.71364974975586 test_loss:872.3167724609375\n",
      "2667/3000 train_loss: 38.85237121582031 test_loss:935.0571899414062\n",
      "2668/3000 train_loss: 42.43356704711914 test_loss:865.324951171875\n",
      "2669/3000 train_loss: 33.78487777709961 test_loss:893.436279296875\n",
      "2670/3000 train_loss: 34.70323181152344 test_loss:871.5086669921875\n",
      "2671/3000 train_loss: 37.79425811767578 test_loss:877.5790405273438\n",
      "2672/3000 train_loss: 38.92449951171875 test_loss:878.7638549804688\n",
      "2673/3000 train_loss: 33.45622253417969 test_loss:874.6431274414062\n",
      "2674/3000 train_loss: 31.45065689086914 test_loss:853.2232666015625\n",
      "2675/3000 train_loss: 35.18781280517578 test_loss:891.7008056640625\n",
      "2676/3000 train_loss: 69.48677062988281 test_loss:865.9634399414062\n",
      "2677/3000 train_loss: 32.86058807373047 test_loss:863.020263671875\n",
      "2678/3000 train_loss: 42.33393096923828 test_loss:857.7975463867188\n",
      "2679/3000 train_loss: 52.06338882446289 test_loss:858.7271728515625\n",
      "2680/3000 train_loss: 43.8122673034668 test_loss:912.3746948242188\n",
      "2681/3000 train_loss: 40.7086067199707 test_loss:909.8741455078125\n",
      "2682/3000 train_loss: 36.23270797729492 test_loss:959.1007080078125\n",
      "2683/3000 train_loss: 41.35500717163086 test_loss:931.0640869140625\n",
      "2684/3000 train_loss: 36.65652084350586 test_loss:901.7939453125\n",
      "2685/3000 train_loss: 43.611106872558594 test_loss:880.5178833007812\n",
      "2686/3000 train_loss: 38.113563537597656 test_loss:898.38330078125\n",
      "2687/3000 train_loss: 36.77154541015625 test_loss:903.3231201171875\n",
      "2688/3000 train_loss: 45.545318603515625 test_loss:897.4945678710938\n",
      "2689/3000 train_loss: 43.70390701293945 test_loss:858.5496826171875\n",
      "2690/3000 train_loss: 39.64310073852539 test_loss:943.4754028320312\n",
      "2691/3000 train_loss: 38.04475784301758 test_loss:885.8573608398438\n",
      "2692/3000 train_loss: 39.170021057128906 test_loss:911.0430908203125\n",
      "2693/3000 train_loss: 40.53046798706055 test_loss:868.1547241210938\n",
      "2694/3000 train_loss: 37.00663757324219 test_loss:915.4697265625\n",
      "2695/3000 train_loss: 45.46357727050781 test_loss:901.6047973632812\n",
      "2696/3000 train_loss: 49.3271598815918 test_loss:915.3444213867188\n",
      "2697/3000 train_loss: 39.842716217041016 test_loss:932.817138671875\n",
      "2698/3000 train_loss: 39.241981506347656 test_loss:880.569091796875\n",
      "2699/3000 train_loss: 39.3458137512207 test_loss:943.1190795898438\n",
      "2700/3000 train_loss: 35.524497985839844 test_loss:887.9635009765625\n",
      "2701/3000 train_loss: 47.670162200927734 test_loss:902.91064453125\n",
      "2702/3000 train_loss: 36.972808837890625 test_loss:890.3291625976562\n",
      "2703/3000 train_loss: 38.82646942138672 test_loss:855.294189453125\n",
      "2704/3000 train_loss: 39.746673583984375 test_loss:909.0200805664062\n",
      "2705/3000 train_loss: 46.29574966430664 test_loss:876.3568115234375\n",
      "2706/3000 train_loss: 49.11191177368164 test_loss:923.7948608398438\n",
      "2707/3000 train_loss: 40.856597900390625 test_loss:896.554931640625\n",
      "2708/3000 train_loss: 34.94465637207031 test_loss:891.2677612304688\n",
      "2709/3000 train_loss: 46.86885452270508 test_loss:937.3390502929688\n",
      "2710/3000 train_loss: 42.998329162597656 test_loss:924.4283447265625\n",
      "2711/3000 train_loss: 53.2844123840332 test_loss:887.5923461914062\n",
      "2712/3000 train_loss: 35.646018981933594 test_loss:923.4739990234375\n",
      "2713/3000 train_loss: 46.690406799316406 test_loss:899.3695678710938\n",
      "2714/3000 train_loss: 48.440330505371094 test_loss:931.6366577148438\n",
      "2715/3000 train_loss: 42.770904541015625 test_loss:901.29150390625\n",
      "2716/3000 train_loss: 46.88496780395508 test_loss:891.439697265625\n",
      "2717/3000 train_loss: 37.228904724121094 test_loss:916.541748046875\n",
      "2718/3000 train_loss: 38.99663162231445 test_loss:876.755615234375\n",
      "2719/3000 train_loss: 36.39434051513672 test_loss:897.7194213867188\n",
      "2720/3000 train_loss: 37.82706832885742 test_loss:891.0846557617188\n",
      "2721/3000 train_loss: 39.78256607055664 test_loss:896.4373779296875\n",
      "2722/3000 train_loss: 41.72251510620117 test_loss:887.9345703125\n",
      "2723/3000 train_loss: 41.433860778808594 test_loss:913.5096435546875\n",
      "2724/3000 train_loss: 39.692813873291016 test_loss:922.2302856445312\n",
      "2725/3000 train_loss: 45.10047912597656 test_loss:911.6904296875\n",
      "2726/3000 train_loss: 31.777057647705078 test_loss:908.214111328125\n",
      "2727/3000 train_loss: 36.45835876464844 test_loss:921.7421264648438\n",
      "2728/3000 train_loss: 31.336997985839844 test_loss:946.6602172851562\n",
      "2729/3000 train_loss: 41.0228385925293 test_loss:894.2533569335938\n",
      "2730/3000 train_loss: 39.4160041809082 test_loss:923.3138427734375\n",
      "2731/3000 train_loss: 38.85414505004883 test_loss:888.773681640625\n",
      "2732/3000 train_loss: 40.64829635620117 test_loss:877.0596313476562\n",
      "2733/3000 train_loss: 34.961936950683594 test_loss:887.3355712890625\n",
      "2734/3000 train_loss: 42.116695404052734 test_loss:904.9076538085938\n",
      "2735/3000 train_loss: 44.55342483520508 test_loss:882.1482543945312\n",
      "2736/3000 train_loss: 43.11748504638672 test_loss:897.9878540039062\n",
      "2737/3000 train_loss: 37.35022735595703 test_loss:872.6072387695312\n",
      "2738/3000 train_loss: 36.665771484375 test_loss:896.3500366210938\n",
      "2739/3000 train_loss: 33.04570770263672 test_loss:887.5430908203125\n",
      "2740/3000 train_loss: 38.321510314941406 test_loss:924.3816528320312\n",
      "2741/3000 train_loss: 33.33301544189453 test_loss:856.6614379882812\n",
      "2742/3000 train_loss: 41.756622314453125 test_loss:894.7067260742188\n",
      "2743/3000 train_loss: 38.017250061035156 test_loss:885.1188354492188\n",
      "2744/3000 train_loss: 37.29738235473633 test_loss:888.42724609375\n",
      "2745/3000 train_loss: 37.03888702392578 test_loss:854.6932983398438\n",
      "2746/3000 train_loss: 44.743080139160156 test_loss:860.3363037109375\n",
      "2747/3000 train_loss: 42.1364631652832 test_loss:928.8609008789062\n",
      "2748/3000 train_loss: 32.9819221496582 test_loss:885.8255004882812\n",
      "2749/3000 train_loss: 39.0565185546875 test_loss:892.848388671875\n",
      "2750/3000 train_loss: 46.35515213012695 test_loss:893.4590454101562\n",
      "2751/3000 train_loss: 38.277076721191406 test_loss:889.8245239257812\n",
      "2752/3000 train_loss: 32.135009765625 test_loss:861.0731811523438\n",
      "2753/3000 train_loss: 35.55875015258789 test_loss:877.7213134765625\n",
      "2754/3000 train_loss: 43.23440170288086 test_loss:909.1664428710938\n",
      "2755/3000 train_loss: 36.95448303222656 test_loss:907.81982421875\n",
      "2756/3000 train_loss: 39.78239440917969 test_loss:897.5189819335938\n",
      "2757/3000 train_loss: 37.51288986206055 test_loss:911.0409545898438\n",
      "2758/3000 train_loss: 37.52479553222656 test_loss:924.5052490234375\n",
      "2759/3000 train_loss: 38.17364501953125 test_loss:885.2593383789062\n",
      "2760/3000 train_loss: 33.38554763793945 test_loss:899.6163330078125\n",
      "2761/3000 train_loss: 35.449371337890625 test_loss:903.4241333007812\n",
      "2762/3000 train_loss: 37.355255126953125 test_loss:906.6842651367188\n",
      "2763/3000 train_loss: 35.365509033203125 test_loss:878.5799560546875\n",
      "2764/3000 train_loss: 42.26496124267578 test_loss:928.2886352539062\n",
      "2765/3000 train_loss: 40.40013122558594 test_loss:906.8092041015625\n",
      "2766/3000 train_loss: 39.405433654785156 test_loss:930.6304931640625\n",
      "2767/3000 train_loss: 42.89006423950195 test_loss:882.1854858398438\n",
      "2768/3000 train_loss: 47.0500602722168 test_loss:904.7897338867188\n",
      "2769/3000 train_loss: 42.06252670288086 test_loss:895.0816040039062\n",
      "2770/3000 train_loss: 40.43682861328125 test_loss:914.5956420898438\n",
      "2771/3000 train_loss: 46.51481246948242 test_loss:920.780029296875\n",
      "2772/3000 train_loss: 37.92510223388672 test_loss:948.1007080078125\n",
      "2773/3000 train_loss: 38.18574142456055 test_loss:969.1738891601562\n",
      "2774/3000 train_loss: 39.177284240722656 test_loss:892.2757568359375\n",
      "2775/3000 train_loss: 39.766117095947266 test_loss:930.9324951171875\n",
      "2776/3000 train_loss: 48.99656677246094 test_loss:936.8447875976562\n",
      "2777/3000 train_loss: 36.36288833618164 test_loss:912.9253540039062\n",
      "2778/3000 train_loss: 37.35102844238281 test_loss:917.0472412109375\n",
      "2779/3000 train_loss: 36.879638671875 test_loss:907.5506591796875\n",
      "2780/3000 train_loss: 37.42481994628906 test_loss:895.7503662109375\n",
      "2781/3000 train_loss: 37.65350341796875 test_loss:920.5025634765625\n",
      "2782/3000 train_loss: 48.05704116821289 test_loss:924.5001831054688\n",
      "2783/3000 train_loss: 38.92875289916992 test_loss:969.2415771484375\n",
      "2784/3000 train_loss: 48.69358444213867 test_loss:934.4454345703125\n",
      "2785/3000 train_loss: 38.50188446044922 test_loss:917.4572143554688\n",
      "2786/3000 train_loss: 35.619895935058594 test_loss:917.1799926757812\n",
      "2787/3000 train_loss: 36.610015869140625 test_loss:888.920654296875\n",
      "2788/3000 train_loss: 34.686912536621094 test_loss:937.4174194335938\n",
      "2789/3000 train_loss: 38.38010787963867 test_loss:902.4061279296875\n",
      "2790/3000 train_loss: 38.51133728027344 test_loss:927.6204223632812\n",
      "2791/3000 train_loss: 52.803619384765625 test_loss:919.1347045898438\n",
      "2792/3000 train_loss: 38.18845748901367 test_loss:890.8541870117188\n",
      "2793/3000 train_loss: 36.844932556152344 test_loss:920.2916870117188\n",
      "2794/3000 train_loss: 41.92026901245117 test_loss:931.0247192382812\n",
      "2795/3000 train_loss: 39.17713165283203 test_loss:950.51416015625\n",
      "2796/3000 train_loss: 50.90283203125 test_loss:890.1547241210938\n",
      "2797/3000 train_loss: 40.89045333862305 test_loss:931.5651245117188\n",
      "2798/3000 train_loss: 43.15821075439453 test_loss:935.55126953125\n",
      "2799/3000 train_loss: 39.54554748535156 test_loss:903.55224609375\n",
      "2800/3000 train_loss: 41.35173797607422 test_loss:947.0012817382812\n",
      "2801/3000 train_loss: 39.97036361694336 test_loss:909.3809814453125\n",
      "2802/3000 train_loss: 35.44429016113281 test_loss:900.6332397460938\n",
      "2803/3000 train_loss: 38.48339080810547 test_loss:904.8906860351562\n",
      "2804/3000 train_loss: 42.31821823120117 test_loss:927.112060546875\n",
      "2805/3000 train_loss: 37.30257034301758 test_loss:907.809814453125\n",
      "2806/3000 train_loss: 32.09164047241211 test_loss:931.7383422851562\n",
      "2807/3000 train_loss: 36.014686584472656 test_loss:892.6746215820312\n",
      "2808/3000 train_loss: 34.890384674072266 test_loss:936.96826171875\n",
      "2809/3000 train_loss: 30.93560791015625 test_loss:889.4424438476562\n",
      "2810/3000 train_loss: 32.555999755859375 test_loss:939.0165405273438\n",
      "2811/3000 train_loss: 39.596641540527344 test_loss:903.7999267578125\n",
      "2812/3000 train_loss: 36.15904998779297 test_loss:975.3848876953125\n",
      "2813/3000 train_loss: 39.0328483581543 test_loss:948.1429443359375\n",
      "2814/3000 train_loss: 38.246826171875 test_loss:917.1973266601562\n",
      "2815/3000 train_loss: 41.95158767700195 test_loss:924.2059936523438\n",
      "2816/3000 train_loss: 36.636810302734375 test_loss:909.9044189453125\n",
      "2817/3000 train_loss: 41.82133865356445 test_loss:913.4140625\n",
      "2818/3000 train_loss: 36.70275115966797 test_loss:885.541748046875\n",
      "2819/3000 train_loss: 38.59822463989258 test_loss:970.5830688476562\n",
      "2820/3000 train_loss: 35.10618591308594 test_loss:964.4588012695312\n",
      "2821/3000 train_loss: 39.030548095703125 test_loss:975.5953369140625\n",
      "2822/3000 train_loss: 36.39583206176758 test_loss:906.1426391601562\n",
      "2823/3000 train_loss: 42.42965316772461 test_loss:930.0374755859375\n",
      "2824/3000 train_loss: 39.32796096801758 test_loss:947.8247680664062\n",
      "2825/3000 train_loss: 41.721038818359375 test_loss:957.9779052734375\n",
      "2826/3000 train_loss: 36.425514221191406 test_loss:942.4917602539062\n",
      "2827/3000 train_loss: 44.705718994140625 test_loss:951.5509643554688\n",
      "2828/3000 train_loss: 36.376121520996094 test_loss:942.4771728515625\n",
      "2829/3000 train_loss: 35.484893798828125 test_loss:942.7789306640625\n",
      "2830/3000 train_loss: 41.60804748535156 test_loss:947.3112182617188\n",
      "2831/3000 train_loss: 39.1104736328125 test_loss:902.5664672851562\n",
      "2832/3000 train_loss: 35.749916076660156 test_loss:939.4833984375\n",
      "2833/3000 train_loss: 36.663692474365234 test_loss:922.3934936523438\n",
      "2834/3000 train_loss: 52.4841194152832 test_loss:948.052978515625\n",
      "2835/3000 train_loss: 35.04119110107422 test_loss:905.2965698242188\n",
      "2836/3000 train_loss: 40.62009048461914 test_loss:959.4295654296875\n",
      "2837/3000 train_loss: 64.09231567382812 test_loss:951.674072265625\n",
      "2838/3000 train_loss: 39.380306243896484 test_loss:975.0466918945312\n",
      "2839/3000 train_loss: 38.67681884765625 test_loss:949.3876342773438\n",
      "2840/3000 train_loss: 43.52657699584961 test_loss:975.4437866210938\n",
      "2841/3000 train_loss: 33.50696563720703 test_loss:949.0271606445312\n",
      "2842/3000 train_loss: 41.71321105957031 test_loss:923.1240844726562\n",
      "2843/3000 train_loss: 39.59190368652344 test_loss:990.65234375\n",
      "2844/3000 train_loss: 35.85820770263672 test_loss:935.0723876953125\n",
      "2845/3000 train_loss: 41.098575592041016 test_loss:963.26025390625\n",
      "2846/3000 train_loss: 39.85224914550781 test_loss:963.5305786132812\n",
      "2847/3000 train_loss: 36.03368377685547 test_loss:977.214599609375\n",
      "2848/3000 train_loss: 40.65797805786133 test_loss:1001.2141723632812\n",
      "2849/3000 train_loss: 40.84320068359375 test_loss:928.88134765625\n",
      "2850/3000 train_loss: 34.799381256103516 test_loss:963.9625244140625\n",
      "2851/3000 train_loss: 42.10416030883789 test_loss:955.04052734375\n",
      "2852/3000 train_loss: 45.06278610229492 test_loss:966.52490234375\n",
      "2853/3000 train_loss: 38.7668342590332 test_loss:938.3178100585938\n",
      "2854/3000 train_loss: 43.23960876464844 test_loss:965.939453125\n",
      "2855/3000 train_loss: 40.651641845703125 test_loss:919.6868286132812\n",
      "2856/3000 train_loss: 49.113609313964844 test_loss:901.4701538085938\n",
      "2857/3000 train_loss: 35.20473098754883 test_loss:917.2843627929688\n",
      "2858/3000 train_loss: 42.69954299926758 test_loss:892.1024780273438\n",
      "2859/3000 train_loss: 43.486961364746094 test_loss:903.8406372070312\n",
      "2860/3000 train_loss: 42.793460845947266 test_loss:866.1574096679688\n",
      "2861/3000 train_loss: 36.2359733581543 test_loss:911.5350341796875\n",
      "2862/3000 train_loss: 43.070213317871094 test_loss:901.5180053710938\n",
      "2863/3000 train_loss: 35.83391571044922 test_loss:897.6435546875\n",
      "2864/3000 train_loss: 33.51615905761719 test_loss:899.5148315429688\n",
      "2865/3000 train_loss: 35.5977668762207 test_loss:896.05517578125\n",
      "2866/3000 train_loss: 38.409725189208984 test_loss:917.5911254882812\n",
      "2867/3000 train_loss: 49.30943298339844 test_loss:865.8659057617188\n",
      "2868/3000 train_loss: 31.993892669677734 test_loss:893.2599487304688\n",
      "2869/3000 train_loss: 39.971435546875 test_loss:888.8818359375\n",
      "2870/3000 train_loss: 43.09019470214844 test_loss:901.8351440429688\n",
      "2871/3000 train_loss: 36.75531768798828 test_loss:875.7969360351562\n",
      "2872/3000 train_loss: 34.02239990234375 test_loss:907.2618408203125\n",
      "2873/3000 train_loss: 36.234039306640625 test_loss:888.4635009765625\n",
      "2874/3000 train_loss: 35.635337829589844 test_loss:923.1185913085938\n",
      "2875/3000 train_loss: 40.31708908081055 test_loss:915.9620361328125\n",
      "2876/3000 train_loss: 36.02482604980469 test_loss:889.6784057617188\n",
      "2877/3000 train_loss: 36.981719970703125 test_loss:895.08154296875\n",
      "2878/3000 train_loss: 32.48433303833008 test_loss:893.8685913085938\n",
      "2879/3000 train_loss: 47.595481872558594 test_loss:890.8650512695312\n",
      "2880/3000 train_loss: 35.46931457519531 test_loss:899.7947998046875\n",
      "2881/3000 train_loss: 40.89569091796875 test_loss:968.5225219726562\n",
      "2882/3000 train_loss: 42.22462844848633 test_loss:895.206298828125\n",
      "2883/3000 train_loss: 42.557777404785156 test_loss:846.7611083984375\n",
      "2884/3000 train_loss: 39.562828063964844 test_loss:843.1923828125\n",
      "2885/3000 train_loss: 39.26568603515625 test_loss:877.2017211914062\n",
      "2886/3000 train_loss: 63.769962310791016 test_loss:872.8399047851562\n",
      "2887/3000 train_loss: 94.26268005371094 test_loss:894.0336303710938\n",
      "2888/3000 train_loss: 40.94032669067383 test_loss:897.3994140625\n",
      "2889/3000 train_loss: 50.9225959777832 test_loss:873.1974487304688\n",
      "2890/3000 train_loss: 41.029232025146484 test_loss:885.250244140625\n",
      "2891/3000 train_loss: 35.672393798828125 test_loss:899.1963500976562\n",
      "2892/3000 train_loss: 42.314537048339844 test_loss:894.44873046875\n",
      "2893/3000 train_loss: 36.31790542602539 test_loss:892.3898315429688\n",
      "2894/3000 train_loss: 38.612220764160156 test_loss:869.0443115234375\n",
      "2895/3000 train_loss: 45.987449645996094 test_loss:863.7638549804688\n",
      "2896/3000 train_loss: 39.44735336303711 test_loss:896.572998046875\n",
      "2897/3000 train_loss: 38.54029846191406 test_loss:886.3433227539062\n",
      "2898/3000 train_loss: 40.43898010253906 test_loss:873.7679443359375\n",
      "2899/3000 train_loss: 42.72832107543945 test_loss:903.4259033203125\n",
      "2900/3000 train_loss: 43.35480499267578 test_loss:913.6742553710938\n",
      "2901/3000 train_loss: 41.64791488647461 test_loss:895.9222412109375\n",
      "2902/3000 train_loss: 36.66499328613281 test_loss:938.8721923828125\n",
      "2903/3000 train_loss: 46.60041427612305 test_loss:863.7818603515625\n",
      "2904/3000 train_loss: 34.08785629272461 test_loss:892.1475830078125\n",
      "2905/3000 train_loss: 30.948209762573242 test_loss:871.3402709960938\n",
      "2906/3000 train_loss: 36.97407531738281 test_loss:902.71337890625\n",
      "2907/3000 train_loss: 38.83649826049805 test_loss:864.7295532226562\n",
      "2908/3000 train_loss: 36.4329833984375 test_loss:895.5216674804688\n",
      "2909/3000 train_loss: 34.892723083496094 test_loss:867.3270263671875\n",
      "2910/3000 train_loss: 34.0135612487793 test_loss:879.3847045898438\n",
      "2911/3000 train_loss: 35.21134567260742 test_loss:859.67626953125\n",
      "2912/3000 train_loss: 34.86198806762695 test_loss:900.1719360351562\n",
      "2913/3000 train_loss: 36.71352005004883 test_loss:875.27880859375\n",
      "2914/3000 train_loss: 39.2320556640625 test_loss:935.328125\n",
      "2915/3000 train_loss: 38.518619537353516 test_loss:892.0563354492188\n",
      "2916/3000 train_loss: 66.104736328125 test_loss:861.1447143554688\n",
      "2917/3000 train_loss: 39.59061050415039 test_loss:897.7969970703125\n",
      "2918/3000 train_loss: 36.43233871459961 test_loss:834.697265625\n",
      "2919/3000 train_loss: 36.46078109741211 test_loss:873.3331909179688\n",
      "2920/3000 train_loss: 37.02041244506836 test_loss:865.560546875\n",
      "2921/3000 train_loss: 40.0606803894043 test_loss:874.69482421875\n",
      "2922/3000 train_loss: 40.37355041503906 test_loss:884.5079345703125\n",
      "2923/3000 train_loss: 40.43071746826172 test_loss:883.9224243164062\n",
      "2924/3000 train_loss: 38.19089126586914 test_loss:866.7977294921875\n",
      "2925/3000 train_loss: 36.69913864135742 test_loss:853.2925415039062\n",
      "2926/3000 train_loss: 35.87138366699219 test_loss:860.106201171875\n",
      "2927/3000 train_loss: 33.34140396118164 test_loss:844.4220581054688\n",
      "2928/3000 train_loss: 37.29703140258789 test_loss:893.5140380859375\n",
      "2929/3000 train_loss: 96.23263549804688 test_loss:915.775390625\n",
      "2930/3000 train_loss: 62.99271011352539 test_loss:995.8103637695312\n",
      "2931/3000 train_loss: 86.31716918945312 test_loss:966.802734375\n",
      "2932/3000 train_loss: 53.155372619628906 test_loss:989.6383056640625\n",
      "2933/3000 train_loss: 43.811336517333984 test_loss:965.9346923828125\n",
      "2934/3000 train_loss: 40.0018310546875 test_loss:949.2097778320312\n",
      "2935/3000 train_loss: 58.54357147216797 test_loss:942.4070434570312\n",
      "2936/3000 train_loss: 36.8650016784668 test_loss:919.2763671875\n",
      "2937/3000 train_loss: 40.92717742919922 test_loss:909.8359375\n",
      "2938/3000 train_loss: 38.00417709350586 test_loss:909.9268188476562\n",
      "2939/3000 train_loss: 37.41969299316406 test_loss:929.9340209960938\n",
      "2940/3000 train_loss: 35.39455032348633 test_loss:937.6701049804688\n",
      "2941/3000 train_loss: 41.81665802001953 test_loss:909.7653198242188\n",
      "2942/3000 train_loss: 57.75948715209961 test_loss:909.8849487304688\n",
      "2943/3000 train_loss: 42.6414680480957 test_loss:907.4522705078125\n",
      "2944/3000 train_loss: 39.1766242980957 test_loss:948.890625\n",
      "2945/3000 train_loss: 36.14243698120117 test_loss:919.4531860351562\n",
      "2946/3000 train_loss: 42.00144958496094 test_loss:915.7039184570312\n",
      "2947/3000 train_loss: 37.16246795654297 test_loss:897.2002563476562\n",
      "2948/3000 train_loss: 38.6456413269043 test_loss:919.0156860351562\n",
      "2949/3000 train_loss: 44.530029296875 test_loss:939.8056640625\n",
      "2950/3000 train_loss: 41.55601119995117 test_loss:897.3950805664062\n",
      "2951/3000 train_loss: 51.56401062011719 test_loss:914.5670166015625\n",
      "2952/3000 train_loss: 37.921165466308594 test_loss:924.9727172851562\n",
      "2953/3000 train_loss: 34.990848541259766 test_loss:942.561279296875\n",
      "2954/3000 train_loss: 39.33574676513672 test_loss:919.1867065429688\n",
      "2955/3000 train_loss: 40.37966537475586 test_loss:929.9559326171875\n",
      "2956/3000 train_loss: 33.63191604614258 test_loss:924.4712524414062\n",
      "2957/3000 train_loss: 38.384464263916016 test_loss:914.237060546875\n",
      "2958/3000 train_loss: 31.897533416748047 test_loss:922.5680541992188\n",
      "2959/3000 train_loss: 46.98354721069336 test_loss:955.2969360351562\n",
      "2960/3000 train_loss: 40.17685317993164 test_loss:920.1874389648438\n",
      "2961/3000 train_loss: 34.41691589355469 test_loss:932.4971923828125\n",
      "2962/3000 train_loss: 38.01445007324219 test_loss:907.4764404296875\n",
      "2963/3000 train_loss: 45.41416549682617 test_loss:920.5502319335938\n",
      "2964/3000 train_loss: 37.533267974853516 test_loss:919.7232666015625\n",
      "2965/3000 train_loss: 30.551105499267578 test_loss:901.4229125976562\n",
      "2966/3000 train_loss: 37.926822662353516 test_loss:901.6079711914062\n",
      "2967/3000 train_loss: 33.03697204589844 test_loss:918.957763671875\n",
      "2968/3000 train_loss: 29.86642074584961 test_loss:903.7752685546875\n",
      "2969/3000 train_loss: 34.058441162109375 test_loss:893.7037353515625\n",
      "2970/3000 train_loss: 31.190168380737305 test_loss:886.5496215820312\n",
      "2971/3000 train_loss: 38.37509536743164 test_loss:930.1915283203125\n",
      "2972/3000 train_loss: 40.546993255615234 test_loss:921.0123901367188\n",
      "2973/3000 train_loss: 43.27321243286133 test_loss:906.834716796875\n",
      "2974/3000 train_loss: 37.769412994384766 test_loss:940.5553588867188\n",
      "2975/3000 train_loss: 31.398744583129883 test_loss:922.1882934570312\n",
      "2976/3000 train_loss: 41.01969528198242 test_loss:959.206787109375\n",
      "2977/3000 train_loss: 32.45996856689453 test_loss:948.9227294921875\n",
      "2978/3000 train_loss: 45.858802795410156 test_loss:942.8424682617188\n",
      "2979/3000 train_loss: 29.90485382080078 test_loss:967.0771484375\n",
      "2980/3000 train_loss: 32.328514099121094 test_loss:926.81640625\n",
      "2981/3000 train_loss: 40.31248092651367 test_loss:950.1964721679688\n",
      "2982/3000 train_loss: 30.340925216674805 test_loss:947.4300537109375\n",
      "2983/3000 train_loss: 38.7425422668457 test_loss:944.93505859375\n",
      "2984/3000 train_loss: 34.05305480957031 test_loss:948.0654296875\n",
      "2985/3000 train_loss: 32.13773727416992 test_loss:938.068603515625\n",
      "2986/3000 train_loss: 35.802574157714844 test_loss:952.082763671875\n",
      "2987/3000 train_loss: 35.03056716918945 test_loss:935.0518798828125\n",
      "2988/3000 train_loss: 36.122528076171875 test_loss:938.0765991210938\n",
      "2989/3000 train_loss: 37.60618209838867 test_loss:900.3353881835938\n",
      "2990/3000 train_loss: 38.69271469116211 test_loss:970.604248046875\n",
      "2991/3000 train_loss: 37.20355987548828 test_loss:948.276611328125\n",
      "2992/3000 train_loss: 33.9388427734375 test_loss:959.9130249023438\n",
      "2993/3000 train_loss: 36.96916198730469 test_loss:922.9446411132812\n",
      "2994/3000 train_loss: 34.37358474731445 test_loss:931.2504272460938\n",
      "2995/3000 train_loss: 31.57431411743164 test_loss:929.7433471679688\n",
      "2996/3000 train_loss: 30.47016716003418 test_loss:905.53564453125\n",
      "2997/3000 train_loss: 33.91484832763672 test_loss:920.07958984375\n",
      "2998/3000 train_loss: 38.27170944213867 test_loss:925.3687744140625\n",
      "2999/3000 train_loss: 40.31098937988281 test_loss:914.3692626953125\n",
      "3000/3000 train_loss: 36.8389892578125 test_loss:927.528564453125\n"
     ]
    }
   ],
   "source": [
    "losses = train(model = unet, optimizer = optimizer, criterion=criterion, data_tr=train_data,\n",
    "               data_val = test_data, scheduler = scheduler,device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "6Ew7_F0-q7aL",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "6757e7d5-5d83-4e65-880b-39038e897176"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(927.5286)"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "avg_loss = 0\n",
    "# unet.eval()\n",
    "# unet.train()\n",
    "preds = []\n",
    "i = 0\n",
    "test_anomaly_losses = []\n",
    "test_normal_losses = []\n",
    "test_losses = []\n",
    "test_real = y_test.tolist()\n",
    "for batch in test_data:\n",
    "  with torch.no_grad():\n",
    "    # unet.train()\n",
    "    predictions = unet(batch.to(device)).cpu()\n",
    "    preds.append(predictions)\n",
    "  loss = criterion(predictions, batch.cpu())\n",
    "  for j in range(len(predictions)):\n",
    "    if int(y_test[i]) == 1:\n",
    "      test_anomaly_losses.append(float(criterion(predictions[j], batch[j])))\n",
    "    else:\n",
    "      test_normal_losses.append(float(criterion(predictions[j], batch[j])))\n",
    "    i += 1\n",
    "    test_losses.append(criterion(predictions[j], batch[j]))\n",
    "  # print(loss)\n",
    "  # print(loss)\n",
    "  avg_loss += loss / len(test_data)\n",
    "# avg_loss\n",
    "\n",
    "avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "VpDKorrRso9o",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "e5392288-6b87-4f12-bebe-2ce3105c1042"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1266.3805607427819, 82.82782974243165)"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "sum(test_anomaly_losses)/len(test_anomaly_losses) , sum(test_normal_losses)/len(test_normal_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5LEvbZKYuh7J",
    "outputId": "0eff3cc8-b5e6-4b9a-a913-ef37fde23e90"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.9721824991487913\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "vals = np.arange(40, 900, 0.5).tolist()\n",
    "for threshold in vals:\n",
    "  preds = []\n",
    "  for j in range(len(test_losses)):\n",
    "    if test_losses[j] > threshold:\n",
    "      preds.append(1)\n",
    "    else:\n",
    "      preds.append(0)\n",
    "  \n",
    "  results.append(roc_auc_score(test_real,preds))\n",
    "\n",
    "print(max(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QaSSqG8SbAw2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
