{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "9SStKf4G0V5H"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "import torchaudio\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from skimage.util import img_as_ubyte\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import io\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_XtxbKLZq5KX",
    "outputId": "0c6de58c-f736-440b-85ce-a6ad2ae77c29"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CYxHegIM0Z4i",
    "outputId": "8b35e1ba-3d36-4aca-c569-e94fbad47a14"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive/\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "h9DATQwS0ivD"
   },
   "outputs": [],
   "source": [
    "class MimiiDataset(Dataset):\n",
    "    def __init__(self,audio_dir, n_fft = 1024, win_length = 1024,\n",
    "                 hop_length = 512,power = 2,n_mels = 128,pad_mode = 'reflect',\n",
    "                 sr = 16000,center = True,norm = None):\n",
    "      \n",
    "        super(MimiiDataset, self).__init__()\n",
    "        self.audio_dir = audio_dir\n",
    "        self.n_mels = n_mels\n",
    "        self.n_fft = n_fft\n",
    "        self.win_length = win_length\n",
    "        self.hop_length = hop_length\n",
    "        self.power = power\n",
    "        self.pad_mode = pad_mode\n",
    "        self.sr = sr\n",
    "        self.center = center\n",
    "        self.norm = norm\n",
    "\n",
    "    def get_files(self):\n",
    "       return self.train_files, self.test_files\n",
    "    \n",
    "    def get_data(self,device, id):\n",
    "        \n",
    "        self.train_files, self.train_labels = self._train_file_list(device, id)\n",
    "        self.test_files, self.test_labels = self._test_file_list(device, id)\n",
    "        \n",
    "        self.train_data = self.get_audios(self.train_files)\n",
    "        self.test_data = self.get_audios(self.test_files)\n",
    "        \n",
    "        return self.train_data, self.test_data, self.train_labels, self.test_labels\n",
    "    \n",
    "    def _train_file_list(self, device, id):\n",
    "        query = os.path.abspath(\n",
    "            f\"{self.audio_dir}/{device}/train/normal_id_0{id}*.wav\"\n",
    "        )\n",
    "        train_normal_files = sorted(glob.glob(query))\n",
    "        train_normal_labels = np.zeros(len(train_normal_files))\n",
    "        \n",
    "        query = os.path.abspath(\n",
    "                f\"{self.audio_dir}/{device}/train/anomaly_id_0{id}*.wav\"\n",
    "            )\n",
    "        train_anomaly_files = sorted(glob.glob(query))\n",
    "        train_anomaly_labels = np.ones(len(train_anomaly_files))\n",
    "        \n",
    "        train_file_list = np.concatenate((train_normal_files, train_anomaly_files), axis=0)\n",
    "        train_labels = np.concatenate((train_normal_labels, train_anomaly_labels), axis=0)\n",
    "        \n",
    "        return train_file_list, train_labels\n",
    "    \n",
    "    def _test_file_list(self, device, id):     \n",
    "        query = os.path.abspath(\n",
    "            f\"{self.audio_dir}/{device}/test/normal_id_0{id}*.wav\"\n",
    "            )\n",
    "        test_normal_files = sorted(glob.glob(query))\n",
    "        test_normal_labels = np.zeros(len(test_normal_files))\n",
    "        \n",
    "        query = os.path.abspath(\n",
    "            f\"{self.audio_dir}/{device}/test/anomaly_id_0{id}*.wav\"\n",
    "            )\n",
    "        test_anomaly_files = sorted(glob.glob(query))\n",
    "        test_anomaly_labels = np.ones(len(test_anomaly_files))\n",
    "        \n",
    "        test_file_list = np.concatenate((test_normal_files, \n",
    "                                          test_anomaly_files), axis=0)\n",
    "        test_labels = np.concatenate((test_normal_labels,\n",
    "                                      test_anomaly_labels), axis=0)\n",
    "          \n",
    "        return test_file_list, test_labels\n",
    "\n",
    "    def normalize(self,tensor):\n",
    "        tensor_minusmean = tensor - tensor.mean()\n",
    "        return tensor_minusmean/np.absolute(tensor_minusmean).max()\n",
    "\n",
    "    def make0min(self,tensornd):\n",
    "        tensor = tensornd.numpy()\n",
    "        res = np.where(tensor == 0, 1E-19 , tensor)\n",
    "        return torch.from_numpy(res)\n",
    "\n",
    "    def spectrogrameToImage(self,specgram):\n",
    "        # specgram = torchaudio.transforms.MelSpectrogram(n_fft=1024, win_length=1024, \n",
    "        #                                                 hop_length=512, power=2, \n",
    "        #                                                 normalized=True, n_mels=128)(waveform )\n",
    "        specgram= self.make0min(specgram)\n",
    "        specgram = specgram.log2()[0,:,:].numpy()\n",
    "        \n",
    "        tr2image = transforms.Compose([transforms.ToPILImage()])\n",
    "\n",
    "        specgram= self.normalize(specgram)\n",
    "        # specgram = img_as_ubyte(specgram)\n",
    "        specgramImage = tr2image(specgram)\n",
    "        return specgramImage\n",
    "\n",
    "    def get_logmelspectrogram(self, waveform):\n",
    "        melspec = librosa.feature.melspectrogram(\n",
    "          n_fft=self.n_fft, win_length=self.win_length, hop_length=self.hop_length,\n",
    "          power=self.power,n_mels=self.n_mels,pad_mode=self.pad_mode,sr=self.sr,\n",
    "          center=self.center,norm=self.norm,htk=True,\n",
    "          y = waveform.numpy()\n",
    "        )\n",
    "\n",
    "        logmelspec = librosa.power_to_db(melspec)\n",
    "\n",
    "        return logmelspec\n",
    "\n",
    "    def get_melspectrogram(self,waveform):\n",
    "        melspec = librosa.feature.melspectrogram(\n",
    "            n_fft=self.n_fft, win_length=self.win_length, hop_length=self.hop_length,\n",
    "            power=self.power,n_mels=self.n_mels,pad_mode=self.pad_mode,sr=self.sr,\n",
    "            center=self.center,norm=self.norm,htk=True,\n",
    "            y = waveform.numpy()\n",
    "        )\n",
    "\n",
    "        return melspec\n",
    "    \n",
    "    def get_mfcc(self,waveform):\n",
    "        mfcc = librosa.feature.mfcc(    \n",
    "            n_fft=self.n_fft, win_length=self.win_length, \n",
    "            hop_length=self.hop_length,pad_mode=self.pad_mode,sr=self.sr,\n",
    "            center=self.center,norm=self.norm,n_mfcc=40,\n",
    "            y = waveform.numpy()\n",
    "        )\n",
    "\n",
    "        return mfcc\n",
    "\n",
    "    def get_chroma_stft(self,waveform):\n",
    "        stft = librosa.feature.chroma_stft(\n",
    "            n_fft=self.n_fft, win_length=self.win_length, \n",
    "            hop_length=self.hop_length,pad_mode=self.pad_mode,sr=self.sr,\n",
    "            center=self.center,norm=self.norm,n_chroma=12,\n",
    "            y=waveform.numpy()\n",
    "        )\n",
    "\n",
    "        return stft\n",
    "\n",
    "    def get_spectral_contrast(self,waveform):\n",
    "        spec_contrast = librosa.feature.spectral_contrast(    \n",
    "            n_fft=self.n_fft, win_length=self.win_length,center=self.center,\n",
    "            hop_length=self.hop_length,pad_mode=self.pad_mode,sr=self.sr,\n",
    "            y = waveform.numpy()\n",
    "        )\n",
    "\n",
    "        return spec_contrast\n",
    "    \n",
    "    def get_tonnetz(self,waveform):\n",
    "        harmonic = librosa.effects.harmonic(waveform.numpy())\n",
    "        tonnetz = librosa.feature.tonnetz(y=harmonic,sr=self.sr)\n",
    "\n",
    "        return tonnetz\n",
    "\n",
    "    def get_audios(self, file_list):\n",
    "        data = []\n",
    "        for i in range(len(file_list)):\n",
    "          y, sr = torchaudio.load(file_list[i])  \n",
    "          data.append(y)\n",
    "\n",
    "        return data\n",
    "    def _derive_data(self, file_list):\n",
    "        train_data = []\n",
    "        test_data = []\n",
    "        train_mode = True\n",
    "        for file_list in [self.train_files, self.test_files]:\n",
    "          tr2tensor = transforms.Compose([transforms.PILToTensor()])\n",
    "          data = []\n",
    "          for j in range(len(file_list)):\n",
    "            y, sr = torchaudio.load(file_list[j])  \n",
    "            spec = self.get_melspectrogram(y)\n",
    "            spec = self.spectrogrameToImage(spec)\n",
    "            spec = spec.convert('RGB')\n",
    "            vectors = tr2tensor(spec)\n",
    "            if train_mode:     \n",
    "              train_data.append(vectors)\n",
    "            else:\n",
    "              test_data.append(vectors)\n",
    "            \n",
    "          train_mode = False\n",
    "                \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "S96soeIc0o13"
   },
   "outputs": [],
   "source": [
    "dataset = MimiiDataset('/content/drive/MyDrive/mimii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "Gn2zdn92doi1"
   },
   "outputs": [],
   "source": [
    "_, _, y_train, y_test = dataset.get_data('slider', 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "SgjpeWy_RV1C"
   },
   "outputs": [],
   "source": [
    "train_mixed_f = torch.load('/content/drive/MyDrive/mixed_features/train_mf_slider6.pt')\n",
    "test_mixed_f = torch.load('/content/drive/MyDrive/mixed_features/test_mf_slider6.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "jWMPVGu1qiEq"
   },
   "outputs": [],
   "source": [
    "train_data = DataLoader(train_mixed_f, batch_size=64, shuffle = True)\n",
    "test_data = DataLoader(test_mixed_f, batch_size = 64, shuffle= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "vNTBTRe6qnBq"
   },
   "outputs": [],
   "source": [
    "class UNet_FC(nn.Module):\n",
    "\n",
    "  def __init__(self, in_features):\n",
    "    super().__init__()\n",
    "    \n",
    "    self.bn = nn.BatchNorm1d(128)\n",
    "    self.relu = nn.ReLU()\n",
    "\n",
    "    self.fc0 = nn.Linear(in_features=in_features,out_features=in_features)\n",
    "\n",
    "    # encoder\n",
    "    self.fc1 = nn.Linear(in_features=in_features, out_features=128)\n",
    "    self.fc2 = nn.Linear(in_features=128,out_features=128)\n",
    "    self.fc3 = nn.Linear(in_features=128, out_features=128)\n",
    "    self.fc4 = nn.Linear(in_features=128, out_features=128)\n",
    "    self.fc5 = nn.Linear(in_features=128, out_features=8)\n",
    "\n",
    "    # decoder\n",
    "    self.fc6 = nn.Linear(in_features=8, out_features=128)\n",
    "    self.fc7 = nn.Linear(in_features=128*2, out_features=128)\n",
    "    self.fc8 = nn.Linear(in_features=128*2, out_features=128)\n",
    "    self.fc9 = nn.Linear(in_features=128*2, out_features=128)\n",
    "\n",
    "    self.out = nn.Linear(in_features=128*2, out_features=in_features)\n",
    "\n",
    "  def forward(self, x):\n",
    "    input = self.fc0(x)\n",
    "\n",
    "    x1 = self.relu(self.bn(self.fc1(input)))\n",
    "    x2 = self.relu(self.bn(self.fc2(x1)))\n",
    "    x3 = self.relu(self.bn(self.fc3(x2)))\n",
    "    x4 = self.relu(self.bn(self.fc4(x3)))\n",
    "    x5 = self.relu(self.fc5(x4))\n",
    "\n",
    "    xy = [x5, x4, x3, x2, x1]\n",
    "\n",
    "    x6 = self.relu(self.fc6(xy[0]))\n",
    "    con1 = torch.cat((x6,xy[1]), 1) \n",
    "    x7 = self.relu(self.bn(self.fc7(con1)))\n",
    "    con2 = torch.cat((x7,xy[2]), 1)\n",
    "    x8 = self.relu(self.bn(self.fc8(con2)))\n",
    "    con3 = torch.cat((x8,xy[3]), 1)\n",
    "    x9 = self.relu(self.bn(self.fc9(con3)))\n",
    "    con4 = torch.cat((x9,xy[4]), 1)\n",
    "\n",
    "    x10 = self.out(con4)\n",
    "\n",
    "    return x10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "ZfgcBtQ3qn5l"
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, data_tr, data_val, scheduler = None,\n",
    "          epochs = 3000, device = 'cpu'):\n",
    "    # X_val, Y_val = next(iter(data_val))\n",
    "    losses = []\n",
    "    prev_avg_loss = 100000\n",
    "    for epoch in range(epochs):\n",
    "        train_avg_loss = 0\n",
    "        test_avg_loss = 0\n",
    "        # model.train()  # train mode\n",
    "        for batch in data_tr:\n",
    "          # data to device\n",
    "          batch = batch.to(device)\n",
    "          # set parameter gradients to zero\n",
    "          optimizer.zero_grad()\n",
    "          # forward\n",
    "          # print(Y_batch.shape)\n",
    "          predictions = model(batch)\n",
    "          loss = criterion(predictions, batch)\n",
    "          loss.backward() # backward-pass\n",
    "          optimizer.step()  # update weights\n",
    "          # calculate loss to show the user\n",
    "          if scheduler:\n",
    "            scheduler.step(loss)\n",
    "          train_avg_loss += loss / len(data_tr)\n",
    "\n",
    "        # model.eval()\n",
    "        for batch in data_val:\n",
    "          with torch.no_grad():\n",
    "            preds = model(batch.to(device)).cpu()\n",
    "            loss = criterion(preds,batch)\n",
    "            test_avg_loss += loss / len(data_val)\n",
    "                    \n",
    "        losses.append(train_avg_loss.item())\n",
    "        # if (epoch+1)%50 == 0:\n",
    "        print(\"{}/{} train_loss: {} test_loss:{}\".format(epoch+1, epochs, train_avg_loss, test_avg_loss))\n",
    "        # if test_avg_loss < 70:\n",
    "        #   break\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "ptkVTF55quOL"
   },
   "outputs": [],
   "source": [
    "unet = UNet_FC(in_features=193).to(device)\n",
    "optimizer = Adam(params = unet.parameters(), lr = 10e-3)\n",
    "# optimizer = Adam(params = unet.parameters())\n",
    "criterion = nn.MSELoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, \n",
    "                                                       min_lr=10e-4, mode = 'min',\n",
    "                                                       patience = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QkfmYl9oXhcB",
    "outputId": "ce963fe8-699b-4b01-fb6d-959737e458cc"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1/3000 train_loss: 318796.34375 test_loss:317723.53125\n",
      "2/3000 train_loss: 317837.0625 test_loss:316493.96875\n",
      "3/3000 train_loss: 316280.625 test_loss:314979.34375\n",
      "4/3000 train_loss: 314789.28125 test_loss:313099.1875\n",
      "5/3000 train_loss: 312735.53125 test_loss:310815.46875\n",
      "6/3000 train_loss: 310369.15625 test_loss:308095.4375\n",
      "7/3000 train_loss: 307364.21875 test_loss:304900.59375\n",
      "8/3000 train_loss: 303891.9375 test_loss:301295.53125\n",
      "9/3000 train_loss: 300266.71875 test_loss:297181.8125\n",
      "10/3000 train_loss: 295441.78125 test_loss:292418.9375\n",
      "11/3000 train_loss: 290595.0625 test_loss:287003.9375\n",
      "12/3000 train_loss: 284922.84375 test_loss:280991.28125\n",
      "13/3000 train_loss: 278717.8125 test_loss:274774.4375\n",
      "14/3000 train_loss: 272331.6875 test_loss:268119.4375\n",
      "15/3000 train_loss: 265376.46875 test_loss:260940.40625\n",
      "16/3000 train_loss: 258245.59375 test_loss:253609.9375\n",
      "17/3000 train_loss: 250798.515625 test_loss:245802.40625\n",
      "18/3000 train_loss: 242642.390625 test_loss:238174.4375\n",
      "19/3000 train_loss: 234404.640625 test_loss:229687.96875\n",
      "20/3000 train_loss: 226428.609375 test_loss:221498.25\n",
      "21/3000 train_loss: 217993.078125 test_loss:212786.9375\n",
      "22/3000 train_loss: 209482.015625 test_loss:203981.9375\n",
      "23/3000 train_loss: 200389.265625 test_loss:195236.6875\n",
      "24/3000 train_loss: 191380.15625 test_loss:185928.421875\n",
      "25/3000 train_loss: 182348.8125 test_loss:177198.84375\n",
      "26/3000 train_loss: 173873.390625 test_loss:169106.9375\n",
      "27/3000 train_loss: 165054.515625 test_loss:159673.21875\n",
      "28/3000 train_loss: 155721.84375 test_loss:151538.75\n",
      "29/3000 train_loss: 147186.265625 test_loss:142028.90625\n",
      "30/3000 train_loss: 137668.734375 test_loss:132670.890625\n",
      "31/3000 train_loss: 128701.5625 test_loss:123752.1796875\n",
      "32/3000 train_loss: 119589.59375 test_loss:114791.46875\n",
      "33/3000 train_loss: 110613.484375 test_loss:106065.421875\n",
      "34/3000 train_loss: 102303.0234375 test_loss:97664.109375\n",
      "35/3000 train_loss: 94812.984375 test_loss:90572.2890625\n",
      "36/3000 train_loss: 86743.828125 test_loss:82639.6484375\n",
      "37/3000 train_loss: 78848.4453125 test_loss:75015.5859375\n",
      "38/3000 train_loss: 73245.0859375 test_loss:68950.046875\n",
      "39/3000 train_loss: 66091.2578125 test_loss:62867.37109375\n",
      "40/3000 train_loss: 59433.70703125 test_loss:56391.375\n",
      "41/3000 train_loss: 53262.56640625 test_loss:50305.1328125\n",
      "42/3000 train_loss: 47365.5 test_loss:44352.34765625\n",
      "43/3000 train_loss: 41875.1796875 test_loss:39490.99609375\n",
      "44/3000 train_loss: 37249.93359375 test_loss:34931.42578125\n",
      "45/3000 train_loss: 33310.16015625 test_loss:31689.361328125\n",
      "46/3000 train_loss: 29855.51171875 test_loss:28196.8046875\n",
      "47/3000 train_loss: 26170.818359375 test_loss:24390.0078125\n",
      "48/3000 train_loss: 23364.365234375 test_loss:21263.140625\n",
      "49/3000 train_loss: 20186.869140625 test_loss:18369.345703125\n",
      "50/3000 train_loss: 16998.060546875 test_loss:15796.3955078125\n",
      "51/3000 train_loss: 15610.7890625 test_loss:14519.783203125\n",
      "52/3000 train_loss: 13069.3037109375 test_loss:11876.2890625\n",
      "53/3000 train_loss: 11302.505859375 test_loss:10267.869140625\n",
      "54/3000 train_loss: 9306.353515625 test_loss:8592.6494140625\n",
      "55/3000 train_loss: 7922.0556640625 test_loss:7347.5654296875\n",
      "56/3000 train_loss: 6722.09716796875 test_loss:6229.63671875\n",
      "57/3000 train_loss: 5953.38623046875 test_loss:4996.3603515625\n",
      "58/3000 train_loss: 4820.1953125 test_loss:4231.796875\n",
      "59/3000 train_loss: 4169.2158203125 test_loss:3611.6181640625\n",
      "60/3000 train_loss: 3512.068359375 test_loss:2996.932861328125\n",
      "61/3000 train_loss: 2839.5771484375 test_loss:2619.6220703125\n",
      "62/3000 train_loss: 2495.75927734375 test_loss:2507.69091796875\n",
      "63/3000 train_loss: 2293.357177734375 test_loss:2048.142822265625\n",
      "64/3000 train_loss: 1878.285888671875 test_loss:1890.54443359375\n",
      "65/3000 train_loss: 1719.990966796875 test_loss:1605.605712890625\n",
      "66/3000 train_loss: 1494.17529296875 test_loss:1372.1051025390625\n",
      "67/3000 train_loss: 1315.9847412109375 test_loss:1338.780029296875\n",
      "68/3000 train_loss: 1168.571533203125 test_loss:1162.72119140625\n",
      "69/3000 train_loss: 1043.76904296875 test_loss:942.7791748046875\n",
      "70/3000 train_loss: 944.5840454101562 test_loss:946.5284423828125\n",
      "71/3000 train_loss: 974.51708984375 test_loss:1002.275146484375\n",
      "72/3000 train_loss: 1013.7607421875 test_loss:1051.4891357421875\n",
      "73/3000 train_loss: 1338.52001953125 test_loss:1037.0635986328125\n",
      "74/3000 train_loss: 1032.52685546875 test_loss:899.106201171875\n",
      "75/3000 train_loss: 859.6107788085938 test_loss:768.2349243164062\n",
      "76/3000 train_loss: 753.6800537109375 test_loss:661.8199462890625\n",
      "77/3000 train_loss: 702.5166015625 test_loss:670.92333984375\n",
      "78/3000 train_loss: 721.4630737304688 test_loss:773.3837890625\n",
      "79/3000 train_loss: 698.980712890625 test_loss:672.068359375\n",
      "80/3000 train_loss: 662.3518676757812 test_loss:720.5675659179688\n",
      "81/3000 train_loss: 634.8634033203125 test_loss:624.3811645507812\n",
      "82/3000 train_loss: 669.9220581054688 test_loss:638.63525390625\n",
      "83/3000 train_loss: 595.4891357421875 test_loss:662.078369140625\n",
      "84/3000 train_loss: 577.7952880859375 test_loss:585.0762939453125\n",
      "85/3000 train_loss: 540.315185546875 test_loss:640.7877807617188\n",
      "86/3000 train_loss: 544.2368774414062 test_loss:566.61572265625\n",
      "87/3000 train_loss: 507.3446044921875 test_loss:573.2037963867188\n",
      "88/3000 train_loss: 517.8233032226562 test_loss:533.750732421875\n",
      "89/3000 train_loss: 565.0438842773438 test_loss:631.826171875\n",
      "90/3000 train_loss: 617.9027709960938 test_loss:547.6325073242188\n",
      "91/3000 train_loss: 521.5738525390625 test_loss:544.6298217773438\n",
      "92/3000 train_loss: 538.5958251953125 test_loss:493.19207763671875\n",
      "93/3000 train_loss: 511.2901916503906 test_loss:480.2106628417969\n",
      "94/3000 train_loss: 475.7716979980469 test_loss:473.7680969238281\n",
      "95/3000 train_loss: 465.0826416015625 test_loss:473.31890869140625\n",
      "96/3000 train_loss: 460.90087890625 test_loss:474.907470703125\n",
      "97/3000 train_loss: 457.6254577636719 test_loss:459.91741943359375\n",
      "98/3000 train_loss: 421.06854248046875 test_loss:470.302734375\n",
      "99/3000 train_loss: 440.0561218261719 test_loss:456.1295166015625\n",
      "100/3000 train_loss: 454.7936096191406 test_loss:459.8409423828125\n",
      "101/3000 train_loss: 411.65679931640625 test_loss:449.2688293457031\n",
      "102/3000 train_loss: 425.32196044921875 test_loss:447.4040832519531\n",
      "103/3000 train_loss: 430.0265808105469 test_loss:430.18865966796875\n",
      "104/3000 train_loss: 426.0249938964844 test_loss:433.66259765625\n",
      "105/3000 train_loss: 425.07794189453125 test_loss:425.4910583496094\n",
      "106/3000 train_loss: 403.2622375488281 test_loss:423.1227722167969\n",
      "107/3000 train_loss: 421.99224853515625 test_loss:428.28228759765625\n",
      "108/3000 train_loss: 393.7000427246094 test_loss:425.2222900390625\n",
      "109/3000 train_loss: 407.20233154296875 test_loss:424.028564453125\n",
      "110/3000 train_loss: 390.3656005859375 test_loss:420.11614990234375\n",
      "111/3000 train_loss: 394.8670654296875 test_loss:419.885986328125\n",
      "112/3000 train_loss: 385.3861389160156 test_loss:417.3345947265625\n",
      "113/3000 train_loss: 389.88592529296875 test_loss:419.2117919921875\n",
      "114/3000 train_loss: 408.19696044921875 test_loss:426.8136291503906\n",
      "115/3000 train_loss: 377.68438720703125 test_loss:427.9194030761719\n",
      "116/3000 train_loss: 394.7615051269531 test_loss:430.28582763671875\n",
      "117/3000 train_loss: 394.8938903808594 test_loss:429.1209411621094\n",
      "118/3000 train_loss: 390.51849365234375 test_loss:429.70196533203125\n",
      "119/3000 train_loss: 387.4852600097656 test_loss:425.09637451171875\n",
      "120/3000 train_loss: 386.0056457519531 test_loss:429.059326171875\n",
      "121/3000 train_loss: 369.926513671875 test_loss:421.6706848144531\n",
      "122/3000 train_loss: 450.5438537597656 test_loss:418.6336975097656\n",
      "123/3000 train_loss: 383.8147277832031 test_loss:417.619140625\n",
      "124/3000 train_loss: 385.66717529296875 test_loss:421.69708251953125\n",
      "125/3000 train_loss: 397.1502380371094 test_loss:417.7330322265625\n",
      "126/3000 train_loss: 369.62884521484375 test_loss:419.2669982910156\n",
      "127/3000 train_loss: 363.8478088378906 test_loss:425.90191650390625\n",
      "128/3000 train_loss: 392.4281921386719 test_loss:425.6590576171875\n",
      "129/3000 train_loss: 392.25982666015625 test_loss:427.26220703125\n",
      "130/3000 train_loss: 379.2586364746094 test_loss:416.79193115234375\n",
      "131/3000 train_loss: 385.1340637207031 test_loss:413.83184814453125\n",
      "132/3000 train_loss: 377.0324401855469 test_loss:411.38128662109375\n",
      "133/3000 train_loss: 402.60113525390625 test_loss:412.05712890625\n",
      "134/3000 train_loss: 385.0147399902344 test_loss:425.791259765625\n",
      "135/3000 train_loss: 361.4803161621094 test_loss:422.79705810546875\n",
      "136/3000 train_loss: 368.958740234375 test_loss:423.153564453125\n",
      "137/3000 train_loss: 370.5735168457031 test_loss:420.63507080078125\n",
      "138/3000 train_loss: 415.6701965332031 test_loss:417.7896423339844\n",
      "139/3000 train_loss: 410.7818603515625 test_loss:422.2884521484375\n",
      "140/3000 train_loss: 371.8661804199219 test_loss:417.6869812011719\n",
      "141/3000 train_loss: 384.5983581542969 test_loss:414.27178955078125\n",
      "142/3000 train_loss: 382.4207458496094 test_loss:409.2145690917969\n",
      "143/3000 train_loss: 409.57208251953125 test_loss:407.29132080078125\n",
      "144/3000 train_loss: 379.5777282714844 test_loss:410.6756286621094\n",
      "145/3000 train_loss: 370.38323974609375 test_loss:411.29248046875\n",
      "146/3000 train_loss: 404.2301940917969 test_loss:410.7369689941406\n",
      "147/3000 train_loss: 410.5750427246094 test_loss:411.4327087402344\n",
      "148/3000 train_loss: 364.5229797363281 test_loss:412.2142639160156\n",
      "149/3000 train_loss: 372.9041748046875 test_loss:406.2330017089844\n",
      "150/3000 train_loss: 358.4833679199219 test_loss:404.4206237792969\n",
      "151/3000 train_loss: 360.1540832519531 test_loss:403.6082763671875\n",
      "152/3000 train_loss: 375.854248046875 test_loss:401.52264404296875\n",
      "153/3000 train_loss: 381.1972961425781 test_loss:402.52679443359375\n",
      "154/3000 train_loss: 351.7419128417969 test_loss:407.1876525878906\n",
      "155/3000 train_loss: 347.66455078125 test_loss:411.7104797363281\n",
      "156/3000 train_loss: 364.0146179199219 test_loss:405.4497985839844\n",
      "157/3000 train_loss: 387.89996337890625 test_loss:403.2279968261719\n",
      "158/3000 train_loss: 376.2676086425781 test_loss:397.07763671875\n",
      "159/3000 train_loss: 372.4137268066406 test_loss:398.1134338378906\n",
      "160/3000 train_loss: 352.4752197265625 test_loss:397.7475280761719\n",
      "161/3000 train_loss: 360.3983154296875 test_loss:403.13336181640625\n",
      "162/3000 train_loss: 366.1054382324219 test_loss:399.85455322265625\n",
      "163/3000 train_loss: 352.5981140136719 test_loss:395.68804931640625\n",
      "164/3000 train_loss: 343.480712890625 test_loss:397.1751403808594\n",
      "165/3000 train_loss: 368.1113586425781 test_loss:401.99822998046875\n",
      "166/3000 train_loss: 375.2716979980469 test_loss:401.32867431640625\n",
      "167/3000 train_loss: 345.92242431640625 test_loss:399.9019775390625\n",
      "168/3000 train_loss: 339.95025634765625 test_loss:398.4857177734375\n",
      "169/3000 train_loss: 345.50732421875 test_loss:398.65350341796875\n",
      "170/3000 train_loss: 343.78564453125 test_loss:395.9931335449219\n",
      "171/3000 train_loss: 335.05780029296875 test_loss:396.7856140136719\n",
      "172/3000 train_loss: 338.1188049316406 test_loss:395.6034240722656\n",
      "173/3000 train_loss: 343.36822509765625 test_loss:391.9610900878906\n",
      "174/3000 train_loss: 361.0246887207031 test_loss:385.2406311035156\n",
      "175/3000 train_loss: 344.0572814941406 test_loss:384.4562683105469\n",
      "176/3000 train_loss: 347.4799499511719 test_loss:389.0090026855469\n",
      "177/3000 train_loss: 368.571044921875 test_loss:386.40582275390625\n",
      "178/3000 train_loss: 336.6225891113281 test_loss:383.48858642578125\n",
      "179/3000 train_loss: 346.23260498046875 test_loss:379.9582214355469\n",
      "180/3000 train_loss: 355.656005859375 test_loss:378.59112548828125\n",
      "181/3000 train_loss: 353.57220458984375 test_loss:379.5030517578125\n",
      "182/3000 train_loss: 352.5096130371094 test_loss:379.5948486328125\n",
      "183/3000 train_loss: 348.40380859375 test_loss:383.6512451171875\n",
      "184/3000 train_loss: 349.29083251953125 test_loss:382.28863525390625\n",
      "185/3000 train_loss: 353.53570556640625 test_loss:382.16064453125\n",
      "186/3000 train_loss: 339.7620544433594 test_loss:386.0810241699219\n",
      "187/3000 train_loss: 374.3182373046875 test_loss:385.50067138671875\n",
      "188/3000 train_loss: 365.6853332519531 test_loss:380.9206237792969\n",
      "189/3000 train_loss: 343.3931579589844 test_loss:382.26397705078125\n",
      "190/3000 train_loss: 345.9997253417969 test_loss:376.4106750488281\n",
      "191/3000 train_loss: 330.4485778808594 test_loss:380.6650390625\n",
      "192/3000 train_loss: 356.2809753417969 test_loss:379.89227294921875\n",
      "193/3000 train_loss: 333.8135681152344 test_loss:376.8326110839844\n",
      "194/3000 train_loss: 335.4508972167969 test_loss:379.92156982421875\n",
      "195/3000 train_loss: 345.8338317871094 test_loss:376.4320983886719\n",
      "196/3000 train_loss: 332.7798767089844 test_loss:371.73309326171875\n",
      "197/3000 train_loss: 329.7740478515625 test_loss:373.3629150390625\n",
      "198/3000 train_loss: 346.6456298828125 test_loss:369.94866943359375\n",
      "199/3000 train_loss: 346.8554992675781 test_loss:367.7013854980469\n",
      "200/3000 train_loss: 334.91705322265625 test_loss:368.4466552734375\n",
      "201/3000 train_loss: 334.9044494628906 test_loss:367.38970947265625\n",
      "202/3000 train_loss: 366.18121337890625 test_loss:370.9251403808594\n",
      "203/3000 train_loss: 344.8275146484375 test_loss:373.0085144042969\n",
      "204/3000 train_loss: 364.1678466796875 test_loss:368.49835205078125\n",
      "205/3000 train_loss: 331.5598449707031 test_loss:366.8707275390625\n",
      "206/3000 train_loss: 323.9788513183594 test_loss:368.6541442871094\n",
      "207/3000 train_loss: 325.1038513183594 test_loss:366.8983154296875\n",
      "208/3000 train_loss: 313.5422668457031 test_loss:366.5312194824219\n",
      "209/3000 train_loss: 325.87091064453125 test_loss:366.640380859375\n",
      "210/3000 train_loss: 372.84307861328125 test_loss:364.38262939453125\n",
      "211/3000 train_loss: 326.1470947265625 test_loss:364.1092834472656\n",
      "212/3000 train_loss: 340.3462829589844 test_loss:365.5776062011719\n",
      "213/3000 train_loss: 349.3257751464844 test_loss:367.91168212890625\n",
      "214/3000 train_loss: 312.2371826171875 test_loss:362.41302490234375\n",
      "215/3000 train_loss: 358.0057067871094 test_loss:360.57623291015625\n",
      "216/3000 train_loss: 318.9532165527344 test_loss:364.6138916015625\n",
      "217/3000 train_loss: 325.09716796875 test_loss:359.1222839355469\n",
      "218/3000 train_loss: 357.6261291503906 test_loss:359.6654968261719\n",
      "219/3000 train_loss: 346.80792236328125 test_loss:363.1902160644531\n",
      "220/3000 train_loss: 316.447509765625 test_loss:363.262451171875\n",
      "221/3000 train_loss: 331.54901123046875 test_loss:364.02337646484375\n",
      "222/3000 train_loss: 363.0440979003906 test_loss:359.9874267578125\n",
      "223/3000 train_loss: 347.05755615234375 test_loss:361.2520751953125\n",
      "224/3000 train_loss: 312.046142578125 test_loss:356.85821533203125\n",
      "225/3000 train_loss: 321.1789855957031 test_loss:356.2735595703125\n",
      "226/3000 train_loss: 344.59722900390625 test_loss:361.6910705566406\n",
      "227/3000 train_loss: 336.6191101074219 test_loss:363.6480712890625\n",
      "228/3000 train_loss: 332.74224853515625 test_loss:357.77325439453125\n",
      "229/3000 train_loss: 316.245849609375 test_loss:357.0970458984375\n",
      "230/3000 train_loss: 305.7770080566406 test_loss:357.2016296386719\n",
      "231/3000 train_loss: 338.895751953125 test_loss:357.7571716308594\n",
      "232/3000 train_loss: 328.97265625 test_loss:354.3085632324219\n",
      "233/3000 train_loss: 322.7571105957031 test_loss:354.6050720214844\n",
      "234/3000 train_loss: 329.18157958984375 test_loss:356.0875244140625\n",
      "235/3000 train_loss: 329.6300048828125 test_loss:353.4371032714844\n",
      "236/3000 train_loss: 308.6340026855469 test_loss:354.1622619628906\n",
      "237/3000 train_loss: 299.1761779785156 test_loss:352.2801208496094\n",
      "238/3000 train_loss: 310.4126281738281 test_loss:353.014404296875\n",
      "239/3000 train_loss: 314.9463195800781 test_loss:352.2640075683594\n",
      "240/3000 train_loss: 315.2797546386719 test_loss:356.32806396484375\n",
      "241/3000 train_loss: 315.0726318359375 test_loss:351.36724853515625\n",
      "242/3000 train_loss: 310.6812744140625 test_loss:349.89886474609375\n",
      "243/3000 train_loss: 339.0718078613281 test_loss:348.81170654296875\n",
      "244/3000 train_loss: 335.638671875 test_loss:352.2216491699219\n",
      "245/3000 train_loss: 324.6162109375 test_loss:347.9903564453125\n",
      "246/3000 train_loss: 338.5119323730469 test_loss:347.1618957519531\n",
      "247/3000 train_loss: 335.5850830078125 test_loss:349.3890075683594\n",
      "248/3000 train_loss: 333.8673400878906 test_loss:346.5951843261719\n",
      "249/3000 train_loss: 338.6607666015625 test_loss:348.6466064453125\n",
      "250/3000 train_loss: 319.49114990234375 test_loss:346.01318359375\n",
      "251/3000 train_loss: 303.8485412597656 test_loss:348.537109375\n",
      "252/3000 train_loss: 318.5000915527344 test_loss:346.98504638671875\n",
      "253/3000 train_loss: 325.12994384765625 test_loss:348.56024169921875\n",
      "254/3000 train_loss: 313.6319274902344 test_loss:349.4013366699219\n",
      "255/3000 train_loss: 303.9479675292969 test_loss:345.070556640625\n",
      "256/3000 train_loss: 299.3810119628906 test_loss:345.0892028808594\n",
      "257/3000 train_loss: 310.793701171875 test_loss:343.4869079589844\n",
      "258/3000 train_loss: 307.2525939941406 test_loss:342.5132141113281\n",
      "259/3000 train_loss: 314.3509826660156 test_loss:343.1528015136719\n",
      "260/3000 train_loss: 300.9839172363281 test_loss:343.87451171875\n",
      "261/3000 train_loss: 332.54644775390625 test_loss:340.3356018066406\n",
      "262/3000 train_loss: 350.6960144042969 test_loss:342.920654296875\n",
      "263/3000 train_loss: 308.5267639160156 test_loss:342.58770751953125\n",
      "264/3000 train_loss: 320.2836608886719 test_loss:342.9695739746094\n",
      "265/3000 train_loss: 323.6647644042969 test_loss:342.7113952636719\n",
      "266/3000 train_loss: 314.14154052734375 test_loss:344.039794921875\n",
      "267/3000 train_loss: 357.7946472167969 test_loss:342.927490234375\n",
      "268/3000 train_loss: 320.954833984375 test_loss:339.9425964355469\n",
      "269/3000 train_loss: 300.12359619140625 test_loss:340.2790222167969\n",
      "270/3000 train_loss: 329.99493408203125 test_loss:342.56500244140625\n",
      "271/3000 train_loss: 319.36444091796875 test_loss:342.6667175292969\n",
      "272/3000 train_loss: 321.3839416503906 test_loss:341.4254455566406\n",
      "273/3000 train_loss: 301.21051025390625 test_loss:338.0381164550781\n",
      "274/3000 train_loss: 356.33856201171875 test_loss:337.9809875488281\n",
      "275/3000 train_loss: 294.2759704589844 test_loss:339.3899841308594\n",
      "276/3000 train_loss: 315.4071960449219 test_loss:337.41796875\n",
      "277/3000 train_loss: 290.6041259765625 test_loss:342.19134521484375\n",
      "278/3000 train_loss: 304.5008239746094 test_loss:337.1430358886719\n",
      "279/3000 train_loss: 281.6668395996094 test_loss:335.15283203125\n",
      "280/3000 train_loss: 323.6983337402344 test_loss:339.53131103515625\n",
      "281/3000 train_loss: 311.160888671875 test_loss:336.04620361328125\n",
      "282/3000 train_loss: 289.4175109863281 test_loss:333.8830871582031\n",
      "283/3000 train_loss: 292.5506286621094 test_loss:340.67730712890625\n",
      "284/3000 train_loss: 303.0257263183594 test_loss:335.04754638671875\n",
      "285/3000 train_loss: 285.9183044433594 test_loss:337.5403137207031\n",
      "286/3000 train_loss: 288.4826354980469 test_loss:336.925537109375\n",
      "287/3000 train_loss: 312.48846435546875 test_loss:337.2397766113281\n",
      "288/3000 train_loss: 306.3926086425781 test_loss:337.8086853027344\n",
      "289/3000 train_loss: 288.037353515625 test_loss:333.93353271484375\n",
      "290/3000 train_loss: 306.1614685058594 test_loss:337.2870788574219\n",
      "291/3000 train_loss: 313.4195251464844 test_loss:335.5131530761719\n",
      "292/3000 train_loss: 282.8826904296875 test_loss:335.25543212890625\n",
      "293/3000 train_loss: 319.35382080078125 test_loss:333.0074768066406\n",
      "294/3000 train_loss: 300.7575988769531 test_loss:329.63482666015625\n",
      "295/3000 train_loss: 315.5583190917969 test_loss:332.4633483886719\n",
      "296/3000 train_loss: 320.5810852050781 test_loss:330.9532165527344\n",
      "297/3000 train_loss: 327.8804931640625 test_loss:335.5224304199219\n",
      "298/3000 train_loss: 292.76300048828125 test_loss:332.010498046875\n",
      "299/3000 train_loss: 308.8674011230469 test_loss:328.64666748046875\n",
      "300/3000 train_loss: 312.7313232421875 test_loss:331.48748779296875\n",
      "301/3000 train_loss: 306.2027587890625 test_loss:328.5451965332031\n",
      "302/3000 train_loss: 322.17279052734375 test_loss:327.96954345703125\n",
      "303/3000 train_loss: 307.88677978515625 test_loss:325.37091064453125\n",
      "304/3000 train_loss: 288.2214050292969 test_loss:328.2031555175781\n",
      "305/3000 train_loss: 289.5035400390625 test_loss:333.8854064941406\n",
      "306/3000 train_loss: 295.503662109375 test_loss:324.899658203125\n",
      "307/3000 train_loss: 318.0383605957031 test_loss:327.7647399902344\n",
      "308/3000 train_loss: 291.8999938964844 test_loss:326.26788330078125\n",
      "309/3000 train_loss: 302.1137390136719 test_loss:326.73516845703125\n",
      "310/3000 train_loss: 310.8304748535156 test_loss:327.31585693359375\n",
      "311/3000 train_loss: 277.8621826171875 test_loss:328.1658935546875\n",
      "312/3000 train_loss: 296.8888244628906 test_loss:325.30426025390625\n",
      "313/3000 train_loss: 322.35308837890625 test_loss:323.23992919921875\n",
      "314/3000 train_loss: 296.6801452636719 test_loss:322.6986389160156\n",
      "315/3000 train_loss: 293.0813903808594 test_loss:329.31640625\n",
      "316/3000 train_loss: 281.3744201660156 test_loss:325.5257873535156\n",
      "317/3000 train_loss: 295.3153076171875 test_loss:324.5372314453125\n",
      "318/3000 train_loss: 294.97900390625 test_loss:323.5922546386719\n",
      "319/3000 train_loss: 287.87994384765625 test_loss:321.04278564453125\n",
      "320/3000 train_loss: 276.8283386230469 test_loss:322.7476501464844\n",
      "321/3000 train_loss: 290.0442199707031 test_loss:320.6422119140625\n",
      "322/3000 train_loss: 301.29986572265625 test_loss:321.81378173828125\n",
      "323/3000 train_loss: 271.24847412109375 test_loss:317.2758483886719\n",
      "324/3000 train_loss: 295.8780212402344 test_loss:318.55035400390625\n",
      "325/3000 train_loss: 304.18548583984375 test_loss:320.54241943359375\n",
      "326/3000 train_loss: 307.15130615234375 test_loss:316.9095764160156\n",
      "327/3000 train_loss: 274.27276611328125 test_loss:321.14923095703125\n",
      "328/3000 train_loss: 293.8782043457031 test_loss:316.601318359375\n",
      "329/3000 train_loss: 264.9335021972656 test_loss:317.88311767578125\n",
      "330/3000 train_loss: 305.69122314453125 test_loss:313.75634765625\n",
      "331/3000 train_loss: 267.8444519042969 test_loss:316.9560546875\n",
      "332/3000 train_loss: 269.9764404296875 test_loss:314.7859191894531\n",
      "333/3000 train_loss: 272.89569091796875 test_loss:316.4582214355469\n",
      "334/3000 train_loss: 290.4900817871094 test_loss:314.44403076171875\n",
      "335/3000 train_loss: 267.72369384765625 test_loss:316.2281188964844\n",
      "336/3000 train_loss: 275.04461669921875 test_loss:315.91473388671875\n",
      "337/3000 train_loss: 274.5342102050781 test_loss:315.78887939453125\n",
      "338/3000 train_loss: 284.08587646484375 test_loss:316.1134948730469\n",
      "339/3000 train_loss: 316.29290771484375 test_loss:316.52777099609375\n",
      "340/3000 train_loss: 288.8724365234375 test_loss:314.4232177734375\n",
      "341/3000 train_loss: 302.8628845214844 test_loss:313.25836181640625\n",
      "342/3000 train_loss: 304.9517822265625 test_loss:310.7073059082031\n",
      "343/3000 train_loss: 296.5097961425781 test_loss:312.1600036621094\n",
      "344/3000 train_loss: 269.43780517578125 test_loss:312.2867431640625\n",
      "345/3000 train_loss: 271.2507019042969 test_loss:315.490966796875\n",
      "346/3000 train_loss: 279.2925720214844 test_loss:315.4353942871094\n",
      "347/3000 train_loss: 278.6370544433594 test_loss:311.4671630859375\n",
      "348/3000 train_loss: 293.0662536621094 test_loss:316.58197021484375\n",
      "349/3000 train_loss: 264.7408447265625 test_loss:310.1280517578125\n",
      "350/3000 train_loss: 303.7355041503906 test_loss:308.66680908203125\n",
      "351/3000 train_loss: 302.8829040527344 test_loss:313.4451599121094\n",
      "352/3000 train_loss: 290.44793701171875 test_loss:315.798828125\n",
      "353/3000 train_loss: 280.3006591796875 test_loss:311.90802001953125\n",
      "354/3000 train_loss: 259.2679138183594 test_loss:320.0804138183594\n",
      "355/3000 train_loss: 295.5621643066406 test_loss:315.2552185058594\n",
      "356/3000 train_loss: 288.361328125 test_loss:311.538330078125\n",
      "357/3000 train_loss: 287.786376953125 test_loss:306.944580078125\n",
      "358/3000 train_loss: 288.6188659667969 test_loss:306.1529235839844\n",
      "359/3000 train_loss: 274.3612976074219 test_loss:308.2887268066406\n",
      "360/3000 train_loss: 286.4565124511719 test_loss:304.0709533691406\n",
      "361/3000 train_loss: 270.1291198730469 test_loss:306.6835021972656\n",
      "362/3000 train_loss: 287.16607666015625 test_loss:306.11456298828125\n",
      "363/3000 train_loss: 262.6020812988281 test_loss:307.49969482421875\n",
      "364/3000 train_loss: 251.6167755126953 test_loss:306.4042053222656\n",
      "365/3000 train_loss: 292.7911376953125 test_loss:307.09893798828125\n",
      "366/3000 train_loss: 280.86016845703125 test_loss:307.6554260253906\n",
      "367/3000 train_loss: 292.19830322265625 test_loss:303.7808532714844\n",
      "368/3000 train_loss: 265.1561584472656 test_loss:305.8750915527344\n",
      "369/3000 train_loss: 262.9532165527344 test_loss:303.4932861328125\n",
      "370/3000 train_loss: 269.3699035644531 test_loss:304.44818115234375\n",
      "371/3000 train_loss: 266.5049133300781 test_loss:302.1777648925781\n",
      "372/3000 train_loss: 252.0213165283203 test_loss:302.7301330566406\n",
      "373/3000 train_loss: 292.82159423828125 test_loss:301.13287353515625\n",
      "374/3000 train_loss: 292.5620422363281 test_loss:303.2746276855469\n",
      "375/3000 train_loss: 293.5072021484375 test_loss:305.4420471191406\n",
      "376/3000 train_loss: 300.15966796875 test_loss:303.0380554199219\n",
      "377/3000 train_loss: 286.9422302246094 test_loss:301.9995422363281\n",
      "378/3000 train_loss: 252.10043334960938 test_loss:301.81268310546875\n",
      "379/3000 train_loss: 269.1806945800781 test_loss:302.69549560546875\n",
      "380/3000 train_loss: 261.2698974609375 test_loss:304.04339599609375\n",
      "381/3000 train_loss: 305.34765625 test_loss:302.1087646484375\n",
      "382/3000 train_loss: 284.395751953125 test_loss:306.1638488769531\n",
      "383/3000 train_loss: 269.95660400390625 test_loss:301.4938049316406\n",
      "384/3000 train_loss: 293.8851013183594 test_loss:303.42474365234375\n",
      "385/3000 train_loss: 275.49169921875 test_loss:302.068359375\n",
      "386/3000 train_loss: 265.7121887207031 test_loss:301.6672058105469\n",
      "387/3000 train_loss: 256.3597412109375 test_loss:299.5181884765625\n",
      "388/3000 train_loss: 259.0972595214844 test_loss:297.2374267578125\n",
      "389/3000 train_loss: 272.1039123535156 test_loss:295.5420227050781\n",
      "390/3000 train_loss: 246.42803955078125 test_loss:300.64044189453125\n",
      "391/3000 train_loss: 274.2823486328125 test_loss:298.350830078125\n",
      "392/3000 train_loss: 259.5550537109375 test_loss:302.816650390625\n",
      "393/3000 train_loss: 259.6726989746094 test_loss:295.9913330078125\n",
      "394/3000 train_loss: 264.78472900390625 test_loss:302.2179870605469\n",
      "395/3000 train_loss: 252.16232299804688 test_loss:298.09857177734375\n",
      "396/3000 train_loss: 271.74493408203125 test_loss:295.95745849609375\n",
      "397/3000 train_loss: 272.9517822265625 test_loss:296.6288146972656\n",
      "398/3000 train_loss: 249.4842987060547 test_loss:297.0979919433594\n",
      "399/3000 train_loss: 267.10791015625 test_loss:295.9849548339844\n",
      "400/3000 train_loss: 260.3179626464844 test_loss:297.3917541503906\n",
      "401/3000 train_loss: 257.6697998046875 test_loss:294.9880065917969\n",
      "402/3000 train_loss: 272.1475830078125 test_loss:292.8223571777344\n",
      "403/3000 train_loss: 275.0882263183594 test_loss:292.5011291503906\n",
      "404/3000 train_loss: 248.24459838867188 test_loss:296.3507385253906\n",
      "405/3000 train_loss: 266.0786437988281 test_loss:295.1008605957031\n",
      "406/3000 train_loss: 271.9838562011719 test_loss:294.7467956542969\n",
      "407/3000 train_loss: 253.61886596679688 test_loss:293.9786071777344\n",
      "408/3000 train_loss: 258.4209899902344 test_loss:295.1454772949219\n",
      "409/3000 train_loss: 277.23126220703125 test_loss:295.05596923828125\n",
      "410/3000 train_loss: 261.7579345703125 test_loss:293.626220703125\n",
      "411/3000 train_loss: 248.97691345214844 test_loss:292.8359680175781\n",
      "412/3000 train_loss: 255.31072998046875 test_loss:292.4764404296875\n",
      "413/3000 train_loss: 257.43170166015625 test_loss:293.2267761230469\n",
      "414/3000 train_loss: 253.29849243164062 test_loss:289.5096435546875\n",
      "415/3000 train_loss: 269.7792663574219 test_loss:291.7674560546875\n",
      "416/3000 train_loss: 291.52490234375 test_loss:292.94677734375\n",
      "417/3000 train_loss: 265.42877197265625 test_loss:293.42474365234375\n",
      "418/3000 train_loss: 273.1150207519531 test_loss:291.0135803222656\n",
      "419/3000 train_loss: 253.655029296875 test_loss:289.7003479003906\n",
      "420/3000 train_loss: 243.67391967773438 test_loss:293.8553161621094\n",
      "421/3000 train_loss: 249.6789093017578 test_loss:292.1710205078125\n",
      "422/3000 train_loss: 256.09539794921875 test_loss:289.85015869140625\n",
      "423/3000 train_loss: 259.31878662109375 test_loss:294.957275390625\n",
      "424/3000 train_loss: 253.33975219726562 test_loss:296.0746765136719\n",
      "425/3000 train_loss: 260.4862365722656 test_loss:294.8982849121094\n",
      "426/3000 train_loss: 254.26580810546875 test_loss:292.5243835449219\n",
      "427/3000 train_loss: 244.06777954101562 test_loss:287.8839111328125\n",
      "428/3000 train_loss: 264.1312255859375 test_loss:288.5019836425781\n",
      "429/3000 train_loss: 263.9136657714844 test_loss:291.539306640625\n",
      "430/3000 train_loss: 241.51455688476562 test_loss:286.1019287109375\n",
      "431/3000 train_loss: 249.38043212890625 test_loss:289.596435546875\n",
      "432/3000 train_loss: 234.61099243164062 test_loss:286.2206726074219\n",
      "433/3000 train_loss: 255.70614624023438 test_loss:283.3574523925781\n",
      "434/3000 train_loss: 244.85836791992188 test_loss:283.73834228515625\n",
      "435/3000 train_loss: 242.70545959472656 test_loss:285.8526916503906\n",
      "436/3000 train_loss: 242.51553344726562 test_loss:287.8534240722656\n",
      "437/3000 train_loss: 239.9386444091797 test_loss:285.72039794921875\n",
      "438/3000 train_loss: 261.24639892578125 test_loss:285.09197998046875\n",
      "439/3000 train_loss: 233.7448272705078 test_loss:287.9097900390625\n",
      "440/3000 train_loss: 241.4791259765625 test_loss:286.48333740234375\n",
      "441/3000 train_loss: 244.4230499267578 test_loss:282.1282043457031\n",
      "442/3000 train_loss: 252.86886596679688 test_loss:283.69244384765625\n",
      "443/3000 train_loss: 253.455078125 test_loss:284.80279541015625\n",
      "444/3000 train_loss: 266.1275329589844 test_loss:283.095947265625\n",
      "445/3000 train_loss: 239.05857849121094 test_loss:286.2366638183594\n",
      "446/3000 train_loss: 256.49639892578125 test_loss:282.4137268066406\n",
      "447/3000 train_loss: 244.4544677734375 test_loss:282.50115966796875\n",
      "448/3000 train_loss: 269.5299072265625 test_loss:281.3406982421875\n",
      "449/3000 train_loss: 249.6442413330078 test_loss:283.1216735839844\n",
      "450/3000 train_loss: 269.20892333984375 test_loss:281.62567138671875\n",
      "451/3000 train_loss: 241.2376251220703 test_loss:280.2367858886719\n",
      "452/3000 train_loss: 259.2315979003906 test_loss:281.4980773925781\n",
      "453/3000 train_loss: 251.09616088867188 test_loss:280.24920654296875\n",
      "454/3000 train_loss: 252.71131896972656 test_loss:279.8934631347656\n",
      "455/3000 train_loss: 239.09500122070312 test_loss:281.3794860839844\n",
      "456/3000 train_loss: 286.1019592285156 test_loss:279.6532897949219\n",
      "457/3000 train_loss: 234.00735473632812 test_loss:280.6846618652344\n",
      "458/3000 train_loss: 246.99111938476562 test_loss:278.88824462890625\n",
      "459/3000 train_loss: 239.21173095703125 test_loss:278.59228515625\n",
      "460/3000 train_loss: 227.27099609375 test_loss:279.9400329589844\n",
      "461/3000 train_loss: 259.65869140625 test_loss:279.4658508300781\n",
      "462/3000 train_loss: 232.11415100097656 test_loss:285.59759521484375\n",
      "463/3000 train_loss: 277.50787353515625 test_loss:288.77496337890625\n",
      "464/3000 train_loss: 255.97686767578125 test_loss:285.1273498535156\n",
      "465/3000 train_loss: 247.43775939941406 test_loss:282.2400207519531\n",
      "466/3000 train_loss: 240.98033142089844 test_loss:288.0820007324219\n",
      "467/3000 train_loss: 255.0867156982422 test_loss:286.796630859375\n",
      "468/3000 train_loss: 231.56639099121094 test_loss:287.12261962890625\n",
      "469/3000 train_loss: 241.46463012695312 test_loss:287.8368835449219\n",
      "470/3000 train_loss: 249.95260620117188 test_loss:285.1253967285156\n",
      "471/3000 train_loss: 251.07174682617188 test_loss:277.9077453613281\n",
      "472/3000 train_loss: 235.767333984375 test_loss:277.3065185546875\n",
      "473/3000 train_loss: 262.85247802734375 test_loss:276.274169921875\n",
      "474/3000 train_loss: 236.62313842773438 test_loss:277.0120849609375\n",
      "475/3000 train_loss: 227.18673706054688 test_loss:276.3060607910156\n",
      "476/3000 train_loss: 248.54408264160156 test_loss:276.3568115234375\n",
      "477/3000 train_loss: 223.80511474609375 test_loss:273.49267578125\n",
      "478/3000 train_loss: 253.1385498046875 test_loss:276.0192565917969\n",
      "479/3000 train_loss: 223.7877960205078 test_loss:272.072509765625\n",
      "480/3000 train_loss: 247.64407348632812 test_loss:273.27130126953125\n",
      "481/3000 train_loss: 239.91635131835938 test_loss:271.4360046386719\n",
      "482/3000 train_loss: 229.1027374267578 test_loss:271.56402587890625\n",
      "483/3000 train_loss: 227.87294006347656 test_loss:270.05609130859375\n",
      "484/3000 train_loss: 223.95770263671875 test_loss:271.99444580078125\n",
      "485/3000 train_loss: 217.9277801513672 test_loss:269.74359130859375\n",
      "486/3000 train_loss: 239.2188720703125 test_loss:268.2280578613281\n",
      "487/3000 train_loss: 227.07350158691406 test_loss:272.48785400390625\n",
      "488/3000 train_loss: 214.4576873779297 test_loss:268.696044921875\n",
      "489/3000 train_loss: 229.71466064453125 test_loss:269.30694580078125\n",
      "490/3000 train_loss: 251.89236450195312 test_loss:269.62872314453125\n",
      "491/3000 train_loss: 226.25082397460938 test_loss:269.3199157714844\n",
      "492/3000 train_loss: 245.765869140625 test_loss:268.1066589355469\n",
      "493/3000 train_loss: 224.22177124023438 test_loss:271.40765380859375\n",
      "494/3000 train_loss: 238.6143035888672 test_loss:270.70623779296875\n",
      "495/3000 train_loss: 228.76890563964844 test_loss:270.8818359375\n",
      "496/3000 train_loss: 210.65504455566406 test_loss:269.2946472167969\n",
      "497/3000 train_loss: 220.785888671875 test_loss:271.9176025390625\n",
      "498/3000 train_loss: 244.40542602539062 test_loss:268.10504150390625\n",
      "499/3000 train_loss: 239.6846160888672 test_loss:267.1419982910156\n",
      "500/3000 train_loss: 244.40289306640625 test_loss:268.4747619628906\n",
      "501/3000 train_loss: 227.04153442382812 test_loss:266.51873779296875\n",
      "502/3000 train_loss: 226.6464385986328 test_loss:266.9974365234375\n",
      "503/3000 train_loss: 240.17349243164062 test_loss:266.55999755859375\n",
      "504/3000 train_loss: 215.48190307617188 test_loss:268.2573547363281\n",
      "505/3000 train_loss: 222.54071044921875 test_loss:268.3135986328125\n",
      "506/3000 train_loss: 240.49179077148438 test_loss:268.80816650390625\n",
      "507/3000 train_loss: 257.4205627441406 test_loss:266.58819580078125\n",
      "508/3000 train_loss: 220.46522521972656 test_loss:266.5185852050781\n",
      "509/3000 train_loss: 219.9376220703125 test_loss:266.3224182128906\n",
      "510/3000 train_loss: 241.7296600341797 test_loss:268.68206787109375\n",
      "511/3000 train_loss: 231.73892211914062 test_loss:266.77325439453125\n",
      "512/3000 train_loss: 228.07675170898438 test_loss:265.010009765625\n",
      "513/3000 train_loss: 220.415771484375 test_loss:266.37042236328125\n",
      "514/3000 train_loss: 215.97409057617188 test_loss:264.2733154296875\n",
      "515/3000 train_loss: 220.9503936767578 test_loss:263.6208190917969\n",
      "516/3000 train_loss: 239.26522827148438 test_loss:263.587890625\n",
      "517/3000 train_loss: 217.41542053222656 test_loss:265.9129943847656\n",
      "518/3000 train_loss: 223.65025329589844 test_loss:266.6324157714844\n",
      "519/3000 train_loss: 233.16921997070312 test_loss:263.2348327636719\n",
      "520/3000 train_loss: 226.49073791503906 test_loss:265.22052001953125\n",
      "521/3000 train_loss: 217.66302490234375 test_loss:260.8003234863281\n",
      "522/3000 train_loss: 225.74227905273438 test_loss:261.7016906738281\n",
      "523/3000 train_loss: 221.20346069335938 test_loss:261.4310607910156\n",
      "524/3000 train_loss: 217.21044921875 test_loss:261.8547058105469\n",
      "525/3000 train_loss: 212.0631103515625 test_loss:263.9207763671875\n",
      "526/3000 train_loss: 247.5026397705078 test_loss:262.5400085449219\n",
      "527/3000 train_loss: 215.42098999023438 test_loss:262.4399719238281\n",
      "528/3000 train_loss: 223.03387451171875 test_loss:260.8570861816406\n",
      "529/3000 train_loss: 220.37899780273438 test_loss:263.61968994140625\n",
      "530/3000 train_loss: 232.79885864257812 test_loss:261.0802917480469\n",
      "531/3000 train_loss: 236.2265625 test_loss:261.2212829589844\n",
      "532/3000 train_loss: 209.95516967773438 test_loss:260.68115234375\n",
      "533/3000 train_loss: 210.29696655273438 test_loss:259.29266357421875\n",
      "534/3000 train_loss: 241.10287475585938 test_loss:264.0677185058594\n",
      "535/3000 train_loss: 228.14227294921875 test_loss:260.5623779296875\n",
      "536/3000 train_loss: 232.01980590820312 test_loss:259.9717712402344\n",
      "537/3000 train_loss: 224.75669860839844 test_loss:262.212158203125\n",
      "538/3000 train_loss: 220.9209442138672 test_loss:259.147705078125\n",
      "539/3000 train_loss: 231.5380096435547 test_loss:260.214599609375\n",
      "540/3000 train_loss: 203.81492614746094 test_loss:257.8453674316406\n",
      "541/3000 train_loss: 210.14077758789062 test_loss:259.6966552734375\n",
      "542/3000 train_loss: 223.64549255371094 test_loss:256.86822509765625\n",
      "543/3000 train_loss: 217.87228393554688 test_loss:258.6737976074219\n",
      "544/3000 train_loss: 215.30490112304688 test_loss:258.5663757324219\n",
      "545/3000 train_loss: 209.89259338378906 test_loss:256.4190979003906\n",
      "546/3000 train_loss: 215.91778564453125 test_loss:259.3855895996094\n",
      "547/3000 train_loss: 207.46055603027344 test_loss:256.80035400390625\n",
      "548/3000 train_loss: 265.41595458984375 test_loss:259.3963928222656\n",
      "549/3000 train_loss: 208.6575164794922 test_loss:259.75592041015625\n",
      "550/3000 train_loss: 217.0691375732422 test_loss:256.9635009765625\n",
      "551/3000 train_loss: 210.43984985351562 test_loss:257.6947937011719\n",
      "552/3000 train_loss: 226.39939880371094 test_loss:256.0550842285156\n",
      "553/3000 train_loss: 213.77651977539062 test_loss:253.82662963867188\n",
      "554/3000 train_loss: 215.24600219726562 test_loss:253.66482543945312\n",
      "555/3000 train_loss: 229.98019409179688 test_loss:253.79861450195312\n",
      "556/3000 train_loss: 224.00436401367188 test_loss:255.44140625\n",
      "557/3000 train_loss: 235.0416717529297 test_loss:254.85794067382812\n",
      "558/3000 train_loss: 206.1651611328125 test_loss:255.1763153076172\n",
      "559/3000 train_loss: 225.66192626953125 test_loss:253.03199768066406\n",
      "560/3000 train_loss: 226.05613708496094 test_loss:256.738525390625\n",
      "561/3000 train_loss: 211.51483154296875 test_loss:253.1350860595703\n",
      "562/3000 train_loss: 205.5794677734375 test_loss:252.9198760986328\n",
      "563/3000 train_loss: 246.40908813476562 test_loss:253.88232421875\n",
      "564/3000 train_loss: 199.6173553466797 test_loss:252.68472290039062\n",
      "565/3000 train_loss: 218.33035278320312 test_loss:252.59439086914062\n",
      "566/3000 train_loss: 207.4307098388672 test_loss:257.0843505859375\n",
      "567/3000 train_loss: 212.5072479248047 test_loss:252.44998168945312\n",
      "568/3000 train_loss: 229.9468536376953 test_loss:252.02734375\n",
      "569/3000 train_loss: 208.98509216308594 test_loss:253.4052734375\n",
      "570/3000 train_loss: 215.92056274414062 test_loss:250.8540496826172\n",
      "571/3000 train_loss: 199.58221435546875 test_loss:252.42974853515625\n",
      "572/3000 train_loss: 211.85130310058594 test_loss:252.19625854492188\n",
      "573/3000 train_loss: 205.40533447265625 test_loss:250.61032104492188\n",
      "574/3000 train_loss: 213.17311096191406 test_loss:252.34298706054688\n",
      "575/3000 train_loss: 207.9248046875 test_loss:252.1251220703125\n",
      "576/3000 train_loss: 219.1845703125 test_loss:249.05780029296875\n",
      "577/3000 train_loss: 223.08135986328125 test_loss:247.54489135742188\n",
      "578/3000 train_loss: 229.4351348876953 test_loss:252.33380126953125\n",
      "579/3000 train_loss: 236.68081665039062 test_loss:252.15330505371094\n",
      "580/3000 train_loss: 210.87355041503906 test_loss:251.05978393554688\n",
      "581/3000 train_loss: 216.84005737304688 test_loss:250.98135375976562\n",
      "582/3000 train_loss: 218.07379150390625 test_loss:250.03781127929688\n",
      "583/3000 train_loss: 234.2518310546875 test_loss:250.8665771484375\n",
      "584/3000 train_loss: 217.37896728515625 test_loss:250.34173583984375\n",
      "585/3000 train_loss: 229.40496826171875 test_loss:251.9889678955078\n",
      "586/3000 train_loss: 234.89007568359375 test_loss:249.36953735351562\n",
      "587/3000 train_loss: 215.58042907714844 test_loss:250.41488647460938\n",
      "588/3000 train_loss: 240.36410522460938 test_loss:246.6319580078125\n",
      "589/3000 train_loss: 198.40228271484375 test_loss:246.39727783203125\n",
      "590/3000 train_loss: 205.68968200683594 test_loss:246.69677734375\n",
      "591/3000 train_loss: 222.20022583007812 test_loss:249.51461791992188\n",
      "592/3000 train_loss: 212.14820861816406 test_loss:246.16384887695312\n",
      "593/3000 train_loss: 225.23800659179688 test_loss:245.24990844726562\n",
      "594/3000 train_loss: 233.30914306640625 test_loss:251.06976318359375\n",
      "595/3000 train_loss: 215.16497802734375 test_loss:247.7559814453125\n",
      "596/3000 train_loss: 203.27687072753906 test_loss:246.75357055664062\n",
      "597/3000 train_loss: 222.0954132080078 test_loss:246.93475341796875\n",
      "598/3000 train_loss: 205.65594482421875 test_loss:251.43939208984375\n",
      "599/3000 train_loss: 213.6842041015625 test_loss:250.39541625976562\n",
      "600/3000 train_loss: 208.5263671875 test_loss:253.3682861328125\n",
      "601/3000 train_loss: 201.70687866210938 test_loss:246.08229064941406\n",
      "602/3000 train_loss: 198.41725158691406 test_loss:245.95172119140625\n",
      "603/3000 train_loss: 200.25843811035156 test_loss:245.50790405273438\n",
      "604/3000 train_loss: 214.54336547851562 test_loss:245.86898803710938\n",
      "605/3000 train_loss: 215.31378173828125 test_loss:246.568603515625\n",
      "606/3000 train_loss: 212.62030029296875 test_loss:244.89739990234375\n",
      "607/3000 train_loss: 206.83761596679688 test_loss:244.18243408203125\n",
      "608/3000 train_loss: 226.9091339111328 test_loss:246.85597229003906\n",
      "609/3000 train_loss: 194.97308349609375 test_loss:245.13690185546875\n",
      "610/3000 train_loss: 219.45834350585938 test_loss:242.20529174804688\n",
      "611/3000 train_loss: 211.9921417236328 test_loss:241.65045166015625\n",
      "612/3000 train_loss: 215.31915283203125 test_loss:243.0128173828125\n",
      "613/3000 train_loss: 215.1068572998047 test_loss:241.82443237304688\n",
      "614/3000 train_loss: 209.60377502441406 test_loss:243.73019409179688\n",
      "615/3000 train_loss: 211.61117553710938 test_loss:241.54681396484375\n",
      "616/3000 train_loss: 212.15548706054688 test_loss:241.64071655273438\n",
      "617/3000 train_loss: 205.3233184814453 test_loss:240.59051513671875\n",
      "618/3000 train_loss: 187.69174194335938 test_loss:243.41844177246094\n",
      "619/3000 train_loss: 212.603515625 test_loss:240.96987915039062\n",
      "620/3000 train_loss: 196.61163330078125 test_loss:241.54302978515625\n",
      "621/3000 train_loss: 205.25674438476562 test_loss:242.42172241210938\n",
      "622/3000 train_loss: 209.568115234375 test_loss:240.7357177734375\n",
      "623/3000 train_loss: 190.30319213867188 test_loss:247.51516723632812\n",
      "624/3000 train_loss: 230.56861877441406 test_loss:241.8046875\n",
      "625/3000 train_loss: 212.30792236328125 test_loss:242.5357666015625\n",
      "626/3000 train_loss: 202.57357788085938 test_loss:241.43234252929688\n",
      "627/3000 train_loss: 227.50936889648438 test_loss:240.869384765625\n",
      "628/3000 train_loss: 214.7230224609375 test_loss:244.20169067382812\n",
      "629/3000 train_loss: 205.44432067871094 test_loss:241.74609375\n",
      "630/3000 train_loss: 201.37831115722656 test_loss:241.54881286621094\n",
      "631/3000 train_loss: 192.9476318359375 test_loss:239.94061279296875\n",
      "632/3000 train_loss: 208.52810668945312 test_loss:240.86697387695312\n",
      "633/3000 train_loss: 203.06773376464844 test_loss:238.32350158691406\n",
      "634/3000 train_loss: 195.6174774169922 test_loss:240.18084716796875\n",
      "635/3000 train_loss: 207.1377716064453 test_loss:243.1694793701172\n",
      "636/3000 train_loss: 191.5572509765625 test_loss:241.714111328125\n",
      "637/3000 train_loss: 193.9588165283203 test_loss:240.57028198242188\n",
      "638/3000 train_loss: 205.8110809326172 test_loss:244.24456787109375\n",
      "639/3000 train_loss: 198.2565155029297 test_loss:239.93743896484375\n",
      "640/3000 train_loss: 190.10589599609375 test_loss:241.65866088867188\n",
      "641/3000 train_loss: 209.7061004638672 test_loss:240.23883056640625\n",
      "642/3000 train_loss: 203.48541259765625 test_loss:239.13565063476562\n",
      "643/3000 train_loss: 209.218994140625 test_loss:237.50259399414062\n",
      "644/3000 train_loss: 217.9038848876953 test_loss:238.90020751953125\n",
      "645/3000 train_loss: 199.61483764648438 test_loss:237.85589599609375\n",
      "646/3000 train_loss: 194.988525390625 test_loss:238.9888916015625\n",
      "647/3000 train_loss: 191.05712890625 test_loss:238.47496032714844\n",
      "648/3000 train_loss: 197.93490600585938 test_loss:237.48773193359375\n",
      "649/3000 train_loss: 186.02516174316406 test_loss:237.22930908203125\n",
      "650/3000 train_loss: 203.52297973632812 test_loss:236.0331268310547\n",
      "651/3000 train_loss: 197.76319885253906 test_loss:238.03585815429688\n",
      "652/3000 train_loss: 184.3052520751953 test_loss:236.82162475585938\n",
      "653/3000 train_loss: 212.1993408203125 test_loss:237.49288940429688\n",
      "654/3000 train_loss: 203.8083038330078 test_loss:237.14810180664062\n",
      "655/3000 train_loss: 217.8072967529297 test_loss:235.63522338867188\n",
      "656/3000 train_loss: 196.2619171142578 test_loss:235.72531127929688\n",
      "657/3000 train_loss: 183.91488647460938 test_loss:235.54946899414062\n",
      "658/3000 train_loss: 194.58277893066406 test_loss:235.795166015625\n",
      "659/3000 train_loss: 229.5670166015625 test_loss:238.21876525878906\n",
      "660/3000 train_loss: 185.2794952392578 test_loss:236.76300048828125\n",
      "661/3000 train_loss: 190.88815307617188 test_loss:238.26104736328125\n",
      "662/3000 train_loss: 206.68951416015625 test_loss:239.9795684814453\n",
      "663/3000 train_loss: 206.39190673828125 test_loss:234.6046142578125\n",
      "664/3000 train_loss: 211.7792205810547 test_loss:237.9700927734375\n",
      "665/3000 train_loss: 206.45309448242188 test_loss:235.52566528320312\n",
      "666/3000 train_loss: 192.20176696777344 test_loss:234.2692413330078\n",
      "667/3000 train_loss: 193.69589233398438 test_loss:234.77615356445312\n",
      "668/3000 train_loss: 213.98875427246094 test_loss:233.12600708007812\n",
      "669/3000 train_loss: 190.24057006835938 test_loss:236.3574676513672\n",
      "670/3000 train_loss: 206.0283203125 test_loss:237.02078247070312\n",
      "671/3000 train_loss: 201.44195556640625 test_loss:237.31890869140625\n",
      "672/3000 train_loss: 191.9473419189453 test_loss:238.13140869140625\n",
      "673/3000 train_loss: 230.2648468017578 test_loss:236.5438690185547\n",
      "674/3000 train_loss: 204.51393127441406 test_loss:233.78460693359375\n",
      "675/3000 train_loss: 198.58644104003906 test_loss:235.78985595703125\n",
      "676/3000 train_loss: 196.3707275390625 test_loss:235.75726318359375\n",
      "677/3000 train_loss: 186.7433319091797 test_loss:234.08863830566406\n",
      "678/3000 train_loss: 199.0606231689453 test_loss:238.02255249023438\n",
      "679/3000 train_loss: 214.72633361816406 test_loss:234.7401123046875\n",
      "680/3000 train_loss: 200.84893798828125 test_loss:239.84759521484375\n",
      "681/3000 train_loss: 197.23960876464844 test_loss:237.34292602539062\n",
      "682/3000 train_loss: 194.85110473632812 test_loss:236.67967224121094\n",
      "683/3000 train_loss: 193.82940673828125 test_loss:234.53363037109375\n",
      "684/3000 train_loss: 186.6739044189453 test_loss:232.42306518554688\n",
      "685/3000 train_loss: 203.30674743652344 test_loss:232.4434356689453\n",
      "686/3000 train_loss: 207.20787048339844 test_loss:236.04141235351562\n",
      "687/3000 train_loss: 199.1951904296875 test_loss:233.98123168945312\n",
      "688/3000 train_loss: 181.3417205810547 test_loss:232.73033142089844\n",
      "689/3000 train_loss: 192.0913848876953 test_loss:230.75601196289062\n",
      "690/3000 train_loss: 191.1545867919922 test_loss:230.90728759765625\n",
      "691/3000 train_loss: 208.36593627929688 test_loss:230.97723388671875\n",
      "692/3000 train_loss: 186.06240844726562 test_loss:230.84889221191406\n",
      "693/3000 train_loss: 194.73300170898438 test_loss:231.88916015625\n",
      "694/3000 train_loss: 211.39622497558594 test_loss:230.9772186279297\n",
      "695/3000 train_loss: 210.1781463623047 test_loss:230.67242431640625\n",
      "696/3000 train_loss: 208.26287841796875 test_loss:229.51602172851562\n",
      "697/3000 train_loss: 198.23387145996094 test_loss:229.7267303466797\n",
      "698/3000 train_loss: 181.509765625 test_loss:231.77200317382812\n",
      "699/3000 train_loss: 188.60812377929688 test_loss:229.0833740234375\n",
      "700/3000 train_loss: 193.7394256591797 test_loss:226.6204833984375\n",
      "701/3000 train_loss: 189.38189697265625 test_loss:225.8490753173828\n",
      "702/3000 train_loss: 178.80848693847656 test_loss:228.77627563476562\n",
      "703/3000 train_loss: 182.14877319335938 test_loss:228.95050048828125\n",
      "704/3000 train_loss: 186.3031768798828 test_loss:228.27810668945312\n",
      "705/3000 train_loss: 181.6073760986328 test_loss:231.6930389404297\n",
      "706/3000 train_loss: 194.71519470214844 test_loss:226.52398681640625\n",
      "707/3000 train_loss: 208.56976318359375 test_loss:229.2235107421875\n",
      "708/3000 train_loss: 203.87847900390625 test_loss:230.8763427734375\n",
      "709/3000 train_loss: 197.88987731933594 test_loss:230.72256469726562\n",
      "710/3000 train_loss: 189.62240600585938 test_loss:228.85690307617188\n",
      "711/3000 train_loss: 191.3612060546875 test_loss:229.82510375976562\n",
      "712/3000 train_loss: 199.37391662597656 test_loss:226.99229431152344\n",
      "713/3000 train_loss: 182.5443572998047 test_loss:231.4197998046875\n",
      "714/3000 train_loss: 199.39794921875 test_loss:226.04525756835938\n",
      "715/3000 train_loss: 185.58421325683594 test_loss:227.44708251953125\n",
      "716/3000 train_loss: 187.4788055419922 test_loss:226.5318603515625\n",
      "717/3000 train_loss: 185.27342224121094 test_loss:225.08984375\n",
      "718/3000 train_loss: 219.30068969726562 test_loss:228.24758911132812\n",
      "719/3000 train_loss: 198.87548828125 test_loss:226.32522583007812\n",
      "720/3000 train_loss: 195.55247497558594 test_loss:226.97329711914062\n",
      "721/3000 train_loss: 208.83563232421875 test_loss:228.65731811523438\n",
      "722/3000 train_loss: 189.3291015625 test_loss:227.498291015625\n",
      "723/3000 train_loss: 186.41688537597656 test_loss:228.68292236328125\n",
      "724/3000 train_loss: 204.37355041503906 test_loss:230.165283203125\n",
      "725/3000 train_loss: 176.10443115234375 test_loss:228.9224853515625\n",
      "726/3000 train_loss: 194.40451049804688 test_loss:232.87570190429688\n",
      "727/3000 train_loss: 178.2876739501953 test_loss:227.80691528320312\n",
      "728/3000 train_loss: 186.65977478027344 test_loss:227.638671875\n",
      "729/3000 train_loss: 188.24569702148438 test_loss:226.17318725585938\n",
      "730/3000 train_loss: 190.11378479003906 test_loss:226.46633911132812\n",
      "731/3000 train_loss: 182.15802001953125 test_loss:229.22244262695312\n",
      "732/3000 train_loss: 185.28074645996094 test_loss:229.8642578125\n",
      "733/3000 train_loss: 183.023681640625 test_loss:235.25991821289062\n",
      "734/3000 train_loss: 203.3622283935547 test_loss:227.1440887451172\n",
      "735/3000 train_loss: 184.1852569580078 test_loss:228.25990295410156\n",
      "736/3000 train_loss: 199.80833435058594 test_loss:227.67755126953125\n",
      "737/3000 train_loss: 170.0838623046875 test_loss:226.957275390625\n",
      "738/3000 train_loss: 181.996337890625 test_loss:228.05361938476562\n",
      "739/3000 train_loss: 171.39105224609375 test_loss:229.43417358398438\n",
      "740/3000 train_loss: 185.17041015625 test_loss:228.35516357421875\n",
      "741/3000 train_loss: 204.96136474609375 test_loss:228.8626708984375\n",
      "742/3000 train_loss: 173.32550048828125 test_loss:228.4876708984375\n",
      "743/3000 train_loss: 195.8516387939453 test_loss:222.81787109375\n",
      "744/3000 train_loss: 178.57882690429688 test_loss:223.50469970703125\n",
      "745/3000 train_loss: 179.4842987060547 test_loss:225.00473022460938\n",
      "746/3000 train_loss: 162.28041076660156 test_loss:222.65966796875\n",
      "747/3000 train_loss: 182.78990173339844 test_loss:221.35269165039062\n",
      "748/3000 train_loss: 177.76467895507812 test_loss:220.82510375976562\n",
      "749/3000 train_loss: 196.7220001220703 test_loss:221.9210968017578\n",
      "750/3000 train_loss: 175.9222869873047 test_loss:222.449951171875\n",
      "751/3000 train_loss: 195.90382385253906 test_loss:224.7052001953125\n",
      "752/3000 train_loss: 194.39646911621094 test_loss:222.01937866210938\n",
      "753/3000 train_loss: 162.975830078125 test_loss:223.40097045898438\n",
      "754/3000 train_loss: 228.38558959960938 test_loss:221.87557983398438\n",
      "755/3000 train_loss: 188.59593200683594 test_loss:222.6254119873047\n",
      "756/3000 train_loss: 170.7849578857422 test_loss:223.9783935546875\n",
      "757/3000 train_loss: 202.7075653076172 test_loss:222.05955505371094\n",
      "758/3000 train_loss: 173.9212646484375 test_loss:223.69973754882812\n",
      "759/3000 train_loss: 167.35755920410156 test_loss:221.03189086914062\n",
      "760/3000 train_loss: 183.30287170410156 test_loss:225.20849609375\n",
      "761/3000 train_loss: 183.91476440429688 test_loss:223.29083251953125\n",
      "762/3000 train_loss: 174.81190490722656 test_loss:222.05410766601562\n",
      "763/3000 train_loss: 181.13002014160156 test_loss:222.208740234375\n",
      "764/3000 train_loss: 176.6119384765625 test_loss:220.18106079101562\n",
      "765/3000 train_loss: 161.07960510253906 test_loss:218.5084228515625\n",
      "766/3000 train_loss: 198.030517578125 test_loss:217.1305694580078\n",
      "767/3000 train_loss: 175.46022033691406 test_loss:217.18434143066406\n",
      "768/3000 train_loss: 182.32923889160156 test_loss:216.5075225830078\n",
      "769/3000 train_loss: 175.194091796875 test_loss:216.32296752929688\n",
      "770/3000 train_loss: 188.33706665039062 test_loss:215.8209991455078\n",
      "771/3000 train_loss: 190.6978759765625 test_loss:215.42864990234375\n",
      "772/3000 train_loss: 178.94287109375 test_loss:215.92391967773438\n",
      "773/3000 train_loss: 175.33444213867188 test_loss:217.99896240234375\n",
      "774/3000 train_loss: 169.4995880126953 test_loss:217.04833984375\n",
      "775/3000 train_loss: 175.4430389404297 test_loss:217.4874267578125\n",
      "776/3000 train_loss: 185.51031494140625 test_loss:214.9599609375\n",
      "777/3000 train_loss: 191.85533142089844 test_loss:213.9449462890625\n",
      "778/3000 train_loss: 175.10362243652344 test_loss:216.5474395751953\n",
      "779/3000 train_loss: 191.50375366210938 test_loss:214.10064697265625\n",
      "780/3000 train_loss: 169.1820831298828 test_loss:214.78302001953125\n",
      "781/3000 train_loss: 185.221435546875 test_loss:215.9581298828125\n",
      "782/3000 train_loss: 191.1194305419922 test_loss:218.06024169921875\n",
      "783/3000 train_loss: 170.30319213867188 test_loss:213.324462890625\n",
      "784/3000 train_loss: 191.24960327148438 test_loss:214.64193725585938\n",
      "785/3000 train_loss: 189.2517852783203 test_loss:217.26565551757812\n",
      "786/3000 train_loss: 163.4154510498047 test_loss:220.12158203125\n",
      "787/3000 train_loss: 170.75877380371094 test_loss:217.24252319335938\n",
      "788/3000 train_loss: 172.39462280273438 test_loss:218.380126953125\n",
      "789/3000 train_loss: 164.55429077148438 test_loss:217.61309814453125\n",
      "790/3000 train_loss: 177.09432983398438 test_loss:216.39163208007812\n",
      "791/3000 train_loss: 161.18797302246094 test_loss:216.33282470703125\n",
      "792/3000 train_loss: 174.12841796875 test_loss:214.74777221679688\n",
      "793/3000 train_loss: 176.76246643066406 test_loss:213.880126953125\n",
      "794/3000 train_loss: 177.37075805664062 test_loss:213.1907958984375\n",
      "795/3000 train_loss: 191.3633575439453 test_loss:214.14486694335938\n",
      "796/3000 train_loss: 195.34986877441406 test_loss:217.4422607421875\n",
      "797/3000 train_loss: 165.98899841308594 test_loss:217.30886840820312\n",
      "798/3000 train_loss: 174.4689178466797 test_loss:218.63339233398438\n",
      "799/3000 train_loss: 191.9557647705078 test_loss:217.05172729492188\n",
      "800/3000 train_loss: 189.7132568359375 test_loss:216.2073974609375\n",
      "801/3000 train_loss: 168.65133666992188 test_loss:214.82379150390625\n",
      "802/3000 train_loss: 162.5867156982422 test_loss:215.01670837402344\n",
      "803/3000 train_loss: 174.76600646972656 test_loss:215.8845672607422\n",
      "804/3000 train_loss: 164.04029846191406 test_loss:212.65673828125\n",
      "805/3000 train_loss: 153.02011108398438 test_loss:211.39834594726562\n",
      "806/3000 train_loss: 181.39797973632812 test_loss:217.0023193359375\n",
      "807/3000 train_loss: 161.40760803222656 test_loss:212.74758911132812\n",
      "808/3000 train_loss: 164.89178466796875 test_loss:215.9798583984375\n",
      "809/3000 train_loss: 184.13522338867188 test_loss:214.34408569335938\n",
      "810/3000 train_loss: 173.6154327392578 test_loss:214.0186767578125\n",
      "811/3000 train_loss: 187.07945251464844 test_loss:212.83123779296875\n",
      "812/3000 train_loss: 162.23046875 test_loss:214.62982177734375\n",
      "813/3000 train_loss: 170.50851440429688 test_loss:212.46136474609375\n",
      "814/3000 train_loss: 169.64552307128906 test_loss:212.33868408203125\n",
      "815/3000 train_loss: 162.07986450195312 test_loss:211.58895874023438\n",
      "816/3000 train_loss: 193.4442901611328 test_loss:213.03070068359375\n",
      "817/3000 train_loss: 180.22698974609375 test_loss:213.4664306640625\n",
      "818/3000 train_loss: 162.2335662841797 test_loss:213.34249877929688\n",
      "819/3000 train_loss: 156.8727264404297 test_loss:210.70941162109375\n",
      "820/3000 train_loss: 170.999267578125 test_loss:207.68960571289062\n",
      "821/3000 train_loss: 168.31002807617188 test_loss:210.6483154296875\n",
      "822/3000 train_loss: 162.5614013671875 test_loss:209.95123291015625\n",
      "823/3000 train_loss: 163.1959991455078 test_loss:209.1284942626953\n",
      "824/3000 train_loss: 161.18841552734375 test_loss:209.71044921875\n",
      "825/3000 train_loss: 161.03977966308594 test_loss:209.64471435546875\n",
      "826/3000 train_loss: 158.98805236816406 test_loss:210.13156127929688\n",
      "827/3000 train_loss: 169.3462677001953 test_loss:209.3529052734375\n",
      "828/3000 train_loss: 158.7368621826172 test_loss:207.45257568359375\n",
      "829/3000 train_loss: 165.20443725585938 test_loss:209.11734008789062\n",
      "830/3000 train_loss: 163.39503479003906 test_loss:207.27740478515625\n",
      "831/3000 train_loss: 172.28482055664062 test_loss:209.4525146484375\n",
      "832/3000 train_loss: 172.3349151611328 test_loss:212.85963439941406\n",
      "833/3000 train_loss: 161.22364807128906 test_loss:211.23446655273438\n",
      "834/3000 train_loss: 167.8563690185547 test_loss:208.4273681640625\n",
      "835/3000 train_loss: 151.04058837890625 test_loss:208.40658569335938\n",
      "836/3000 train_loss: 185.70965576171875 test_loss:207.32627868652344\n",
      "837/3000 train_loss: 159.15744018554688 test_loss:207.92767333984375\n",
      "838/3000 train_loss: 171.13690185546875 test_loss:206.79702758789062\n",
      "839/3000 train_loss: 165.43487548828125 test_loss:206.580078125\n",
      "840/3000 train_loss: 162.88645935058594 test_loss:207.36492919921875\n",
      "841/3000 train_loss: 166.54177856445312 test_loss:207.2200927734375\n",
      "842/3000 train_loss: 154.36790466308594 test_loss:207.51910400390625\n",
      "843/3000 train_loss: 164.86375427246094 test_loss:208.29150390625\n",
      "844/3000 train_loss: 167.21075439453125 test_loss:204.4533233642578\n",
      "845/3000 train_loss: 180.72201538085938 test_loss:205.321533203125\n",
      "846/3000 train_loss: 162.0370635986328 test_loss:204.11956787109375\n",
      "847/3000 train_loss: 162.9560546875 test_loss:203.84877014160156\n",
      "848/3000 train_loss: 171.01968383789062 test_loss:205.69638061523438\n",
      "849/3000 train_loss: 159.73887634277344 test_loss:203.7434844970703\n",
      "850/3000 train_loss: 148.09901428222656 test_loss:202.16641235351562\n",
      "851/3000 train_loss: 154.96566772460938 test_loss:205.92007446289062\n",
      "852/3000 train_loss: 151.02182006835938 test_loss:201.2406005859375\n",
      "853/3000 train_loss: 182.56480407714844 test_loss:203.47518920898438\n",
      "854/3000 train_loss: 154.27626037597656 test_loss:202.08428955078125\n",
      "855/3000 train_loss: 159.15614318847656 test_loss:202.50473022460938\n",
      "856/3000 train_loss: 168.98611450195312 test_loss:200.99017333984375\n",
      "857/3000 train_loss: 151.5218048095703 test_loss:203.81546020507812\n",
      "858/3000 train_loss: 154.73678588867188 test_loss:201.18380737304688\n",
      "859/3000 train_loss: 164.588134765625 test_loss:203.8944549560547\n",
      "860/3000 train_loss: 151.1695556640625 test_loss:202.043701171875\n",
      "861/3000 train_loss: 164.74539184570312 test_loss:204.47439575195312\n",
      "862/3000 train_loss: 159.0521240234375 test_loss:203.12969970703125\n",
      "863/3000 train_loss: 163.497314453125 test_loss:201.97842407226562\n",
      "864/3000 train_loss: 159.30517578125 test_loss:201.9754638671875\n",
      "865/3000 train_loss: 155.01419067382812 test_loss:202.78683471679688\n",
      "866/3000 train_loss: 169.77207946777344 test_loss:202.10147094726562\n",
      "867/3000 train_loss: 145.85714721679688 test_loss:200.13385009765625\n",
      "868/3000 train_loss: 154.028564453125 test_loss:200.6564178466797\n",
      "869/3000 train_loss: 153.4235076904297 test_loss:198.30972290039062\n",
      "870/3000 train_loss: 171.85446166992188 test_loss:199.73745727539062\n",
      "871/3000 train_loss: 167.6927947998047 test_loss:200.49420166015625\n",
      "872/3000 train_loss: 169.69456481933594 test_loss:196.26670837402344\n",
      "873/3000 train_loss: 157.76101684570312 test_loss:196.95558166503906\n",
      "874/3000 train_loss: 153.80125427246094 test_loss:196.55032348632812\n",
      "875/3000 train_loss: 169.50631713867188 test_loss:198.27120971679688\n",
      "876/3000 train_loss: 156.3592987060547 test_loss:196.92893981933594\n",
      "877/3000 train_loss: 144.72991943359375 test_loss:193.50384521484375\n",
      "878/3000 train_loss: 150.04144287109375 test_loss:194.72535705566406\n",
      "879/3000 train_loss: 154.51734924316406 test_loss:193.1945343017578\n",
      "880/3000 train_loss: 141.64413452148438 test_loss:194.4298095703125\n",
      "881/3000 train_loss: 153.60467529296875 test_loss:195.74398803710938\n",
      "882/3000 train_loss: 144.8217010498047 test_loss:194.9080810546875\n",
      "883/3000 train_loss: 157.125732421875 test_loss:195.39035034179688\n",
      "884/3000 train_loss: 153.60916137695312 test_loss:195.07693481445312\n",
      "885/3000 train_loss: 146.0759735107422 test_loss:193.93472290039062\n",
      "886/3000 train_loss: 145.91717529296875 test_loss:194.06072998046875\n",
      "887/3000 train_loss: 163.11671447753906 test_loss:195.28863525390625\n",
      "888/3000 train_loss: 153.5297393798828 test_loss:191.7007293701172\n",
      "889/3000 train_loss: 142.8805389404297 test_loss:194.74058532714844\n",
      "890/3000 train_loss: 171.20123291015625 test_loss:193.11993408203125\n",
      "891/3000 train_loss: 149.4033660888672 test_loss:195.71331787109375\n",
      "892/3000 train_loss: 149.0879364013672 test_loss:194.8365478515625\n",
      "893/3000 train_loss: 159.86343383789062 test_loss:200.56805419921875\n",
      "894/3000 train_loss: 152.18191528320312 test_loss:191.6772003173828\n",
      "895/3000 train_loss: 159.35499572753906 test_loss:190.3990936279297\n",
      "896/3000 train_loss: 140.68728637695312 test_loss:193.43524169921875\n",
      "897/3000 train_loss: 149.65719604492188 test_loss:190.89122009277344\n",
      "898/3000 train_loss: 152.16973876953125 test_loss:191.83265686035156\n",
      "899/3000 train_loss: 160.87583923339844 test_loss:191.37110900878906\n",
      "900/3000 train_loss: 165.79306030273438 test_loss:195.00332641601562\n",
      "901/3000 train_loss: 160.54457092285156 test_loss:189.99130249023438\n",
      "902/3000 train_loss: 152.10499572753906 test_loss:191.5069122314453\n",
      "903/3000 train_loss: 146.89759826660156 test_loss:189.20620727539062\n",
      "904/3000 train_loss: 156.14491271972656 test_loss:190.93374633789062\n",
      "905/3000 train_loss: 159.47401428222656 test_loss:190.37693786621094\n",
      "906/3000 train_loss: 150.5477752685547 test_loss:188.9649200439453\n",
      "907/3000 train_loss: 152.14601135253906 test_loss:190.6990203857422\n",
      "908/3000 train_loss: 156.49472045898438 test_loss:191.658935546875\n",
      "909/3000 train_loss: 160.18565368652344 test_loss:188.8460235595703\n",
      "910/3000 train_loss: 136.78720092773438 test_loss:193.7113494873047\n",
      "911/3000 train_loss: 158.0598907470703 test_loss:186.58895874023438\n",
      "912/3000 train_loss: 164.59930419921875 test_loss:187.3040313720703\n",
      "913/3000 train_loss: 138.49545288085938 test_loss:187.6371612548828\n",
      "914/3000 train_loss: 150.01687622070312 test_loss:189.61077880859375\n",
      "915/3000 train_loss: 153.096435546875 test_loss:188.86102294921875\n",
      "916/3000 train_loss: 160.47991943359375 test_loss:190.94908142089844\n",
      "917/3000 train_loss: 157.77230834960938 test_loss:187.28196716308594\n",
      "918/3000 train_loss: 140.4761962890625 test_loss:190.18612670898438\n",
      "919/3000 train_loss: 138.72682189941406 test_loss:191.72755432128906\n",
      "920/3000 train_loss: 140.6288604736328 test_loss:190.1485137939453\n",
      "921/3000 train_loss: 165.16683959960938 test_loss:194.67739868164062\n",
      "922/3000 train_loss: 145.931640625 test_loss:190.68853759765625\n",
      "923/3000 train_loss: 158.24461364746094 test_loss:191.43997192382812\n",
      "924/3000 train_loss: 153.13780212402344 test_loss:191.21099853515625\n",
      "925/3000 train_loss: 136.1434326171875 test_loss:189.38958740234375\n",
      "926/3000 train_loss: 160.0316619873047 test_loss:196.10971069335938\n",
      "927/3000 train_loss: 138.13525390625 test_loss:192.15086364746094\n",
      "928/3000 train_loss: 157.35403442382812 test_loss:188.65853881835938\n",
      "929/3000 train_loss: 145.0400390625 test_loss:196.84860229492188\n",
      "930/3000 train_loss: 143.99984741210938 test_loss:187.8707275390625\n",
      "931/3000 train_loss: 133.60028076171875 test_loss:193.65052795410156\n",
      "932/3000 train_loss: 146.66464233398438 test_loss:188.00320434570312\n",
      "933/3000 train_loss: 136.88157653808594 test_loss:196.79974365234375\n",
      "934/3000 train_loss: 136.72393798828125 test_loss:192.625244140625\n",
      "935/3000 train_loss: 169.90121459960938 test_loss:189.0958251953125\n",
      "936/3000 train_loss: 145.73291015625 test_loss:187.0433349609375\n",
      "937/3000 train_loss: 152.84060668945312 test_loss:186.75917053222656\n",
      "938/3000 train_loss: 160.8430633544922 test_loss:188.4647216796875\n",
      "939/3000 train_loss: 146.29156494140625 test_loss:184.6966552734375\n",
      "940/3000 train_loss: 147.27296447753906 test_loss:188.29307556152344\n",
      "941/3000 train_loss: 137.6517333984375 test_loss:186.3677215576172\n",
      "942/3000 train_loss: 146.1389617919922 test_loss:189.07843017578125\n",
      "943/3000 train_loss: 141.8474578857422 test_loss:186.81312561035156\n",
      "944/3000 train_loss: 151.5714111328125 test_loss:186.69198608398438\n",
      "945/3000 train_loss: 147.64761352539062 test_loss:188.2307891845703\n",
      "946/3000 train_loss: 149.84396362304688 test_loss:194.16351318359375\n",
      "947/3000 train_loss: 151.6936798095703 test_loss:184.71815490722656\n",
      "948/3000 train_loss: 148.7737579345703 test_loss:193.7215576171875\n",
      "949/3000 train_loss: 154.51089477539062 test_loss:189.89837646484375\n",
      "950/3000 train_loss: 139.161865234375 test_loss:194.86944580078125\n",
      "951/3000 train_loss: 142.55252075195312 test_loss:189.50363159179688\n",
      "952/3000 train_loss: 164.98545837402344 test_loss:193.85104370117188\n",
      "953/3000 train_loss: 140.35989379882812 test_loss:188.01800537109375\n",
      "954/3000 train_loss: 136.01617431640625 test_loss:192.8069305419922\n",
      "955/3000 train_loss: 134.35513305664062 test_loss:188.36578369140625\n",
      "956/3000 train_loss: 135.6118621826172 test_loss:187.16690063476562\n",
      "957/3000 train_loss: 134.8132781982422 test_loss:187.34950256347656\n",
      "958/3000 train_loss: 137.04855346679688 test_loss:183.0362548828125\n",
      "959/3000 train_loss: 139.16615295410156 test_loss:183.99058532714844\n",
      "960/3000 train_loss: 132.95399475097656 test_loss:190.81204223632812\n",
      "961/3000 train_loss: 131.93841552734375 test_loss:184.81646728515625\n",
      "962/3000 train_loss: 132.28567504882812 test_loss:184.14627075195312\n",
      "963/3000 train_loss: 148.000732421875 test_loss:186.8489227294922\n",
      "964/3000 train_loss: 139.71400451660156 test_loss:182.90402221679688\n",
      "965/3000 train_loss: 137.64154052734375 test_loss:183.7834014892578\n",
      "966/3000 train_loss: 148.16717529296875 test_loss:185.25115966796875\n",
      "967/3000 train_loss: 152.2241668701172 test_loss:184.89698791503906\n",
      "968/3000 train_loss: 142.496826171875 test_loss:195.6732177734375\n",
      "969/3000 train_loss: 161.84767150878906 test_loss:184.636474609375\n",
      "970/3000 train_loss: 131.7661590576172 test_loss:196.17987060546875\n",
      "971/3000 train_loss: 142.68357849121094 test_loss:186.23678588867188\n",
      "972/3000 train_loss: 144.2410888671875 test_loss:181.6210479736328\n",
      "973/3000 train_loss: 135.3321533203125 test_loss:185.885498046875\n",
      "974/3000 train_loss: 136.94342041015625 test_loss:180.7438507080078\n",
      "975/3000 train_loss: 148.54190063476562 test_loss:180.60397338867188\n",
      "976/3000 train_loss: 138.74183654785156 test_loss:182.88760375976562\n",
      "977/3000 train_loss: 139.2550506591797 test_loss:179.91578674316406\n",
      "978/3000 train_loss: 141.69647216796875 test_loss:189.0196990966797\n",
      "979/3000 train_loss: 135.57077026367188 test_loss:178.12139892578125\n",
      "980/3000 train_loss: 138.83702087402344 test_loss:180.415283203125\n",
      "981/3000 train_loss: 139.04183959960938 test_loss:180.28298950195312\n",
      "982/3000 train_loss: 144.54347229003906 test_loss:176.68016052246094\n",
      "983/3000 train_loss: 141.29061889648438 test_loss:184.67947387695312\n",
      "984/3000 train_loss: 130.97738647460938 test_loss:178.61270141601562\n",
      "985/3000 train_loss: 138.708740234375 test_loss:179.4291534423828\n",
      "986/3000 train_loss: 143.4971466064453 test_loss:178.2179412841797\n",
      "987/3000 train_loss: 133.88465881347656 test_loss:179.529052734375\n",
      "988/3000 train_loss: 124.99759674072266 test_loss:180.38490295410156\n",
      "989/3000 train_loss: 127.25325775146484 test_loss:176.87916564941406\n",
      "990/3000 train_loss: 143.8486328125 test_loss:178.7750244140625\n",
      "991/3000 train_loss: 128.89991760253906 test_loss:178.3843536376953\n",
      "992/3000 train_loss: 127.61875915527344 test_loss:180.98171997070312\n",
      "993/3000 train_loss: 133.04611206054688 test_loss:175.600830078125\n",
      "994/3000 train_loss: 140.35633850097656 test_loss:177.42086791992188\n",
      "995/3000 train_loss: 128.44451904296875 test_loss:178.828857421875\n",
      "996/3000 train_loss: 129.41534423828125 test_loss:177.7572479248047\n",
      "997/3000 train_loss: 139.48377990722656 test_loss:180.9459228515625\n",
      "998/3000 train_loss: 134.2568817138672 test_loss:177.64285278320312\n",
      "999/3000 train_loss: 130.6435089111328 test_loss:180.55917358398438\n",
      "1000/3000 train_loss: 156.56874084472656 test_loss:183.76092529296875\n",
      "1001/3000 train_loss: 142.56558227539062 test_loss:184.96597290039062\n",
      "1002/3000 train_loss: 151.29470825195312 test_loss:180.39361572265625\n",
      "1003/3000 train_loss: 140.2225341796875 test_loss:182.40377807617188\n",
      "1004/3000 train_loss: 129.3643341064453 test_loss:181.4483642578125\n",
      "1005/3000 train_loss: 142.97695922851562 test_loss:183.47991943359375\n",
      "1006/3000 train_loss: 144.71575927734375 test_loss:180.2210693359375\n",
      "1007/3000 train_loss: 127.46790313720703 test_loss:182.13096618652344\n",
      "1008/3000 train_loss: 125.87274169921875 test_loss:180.85292053222656\n",
      "1009/3000 train_loss: 132.72535705566406 test_loss:178.4596710205078\n",
      "1010/3000 train_loss: 132.1228485107422 test_loss:176.99855041503906\n",
      "1011/3000 train_loss: 122.15072631835938 test_loss:180.01107788085938\n",
      "1012/3000 train_loss: 131.1326141357422 test_loss:175.4145965576172\n",
      "1013/3000 train_loss: 132.0833282470703 test_loss:182.93447875976562\n",
      "1014/3000 train_loss: 133.9662628173828 test_loss:176.4891357421875\n",
      "1015/3000 train_loss: 129.93740844726562 test_loss:177.00164794921875\n",
      "1016/3000 train_loss: 137.00396728515625 test_loss:175.66455078125\n",
      "1017/3000 train_loss: 126.50816345214844 test_loss:179.08843994140625\n",
      "1018/3000 train_loss: 128.39381408691406 test_loss:176.85711669921875\n",
      "1019/3000 train_loss: 127.39059448242188 test_loss:179.86180114746094\n",
      "1020/3000 train_loss: 128.610107421875 test_loss:177.13168334960938\n",
      "1021/3000 train_loss: 129.02804565429688 test_loss:179.20770263671875\n",
      "1022/3000 train_loss: 123.48514556884766 test_loss:174.1067352294922\n",
      "1023/3000 train_loss: 124.93489074707031 test_loss:179.63427734375\n",
      "1024/3000 train_loss: 148.7208251953125 test_loss:174.23594665527344\n",
      "1025/3000 train_loss: 125.50550842285156 test_loss:173.61402893066406\n",
      "1026/3000 train_loss: 119.85543823242188 test_loss:177.03591918945312\n",
      "1027/3000 train_loss: 127.80702209472656 test_loss:174.1809539794922\n",
      "1028/3000 train_loss: 138.5040283203125 test_loss:177.89244079589844\n",
      "1029/3000 train_loss: 126.30972290039062 test_loss:172.1004638671875\n",
      "1030/3000 train_loss: 131.0392608642578 test_loss:171.3743896484375\n",
      "1031/3000 train_loss: 114.6891860961914 test_loss:174.34425354003906\n",
      "1032/3000 train_loss: 128.45440673828125 test_loss:172.1622314453125\n",
      "1033/3000 train_loss: 146.22909545898438 test_loss:175.3385009765625\n",
      "1034/3000 train_loss: 136.21051025390625 test_loss:172.82057189941406\n",
      "1035/3000 train_loss: 117.52842712402344 test_loss:179.34686279296875\n",
      "1036/3000 train_loss: 128.850830078125 test_loss:172.2982635498047\n",
      "1037/3000 train_loss: 139.02378845214844 test_loss:177.34503173828125\n",
      "1038/3000 train_loss: 158.50048828125 test_loss:175.71792602539062\n",
      "1039/3000 train_loss: 122.3403091430664 test_loss:182.31333923339844\n",
      "1040/3000 train_loss: 135.21739196777344 test_loss:178.55630493164062\n",
      "1041/3000 train_loss: 144.09671020507812 test_loss:178.75894165039062\n",
      "1042/3000 train_loss: 138.30377197265625 test_loss:176.439697265625\n",
      "1043/3000 train_loss: 137.42701721191406 test_loss:172.5654296875\n",
      "1044/3000 train_loss: 129.62916564941406 test_loss:179.21539306640625\n",
      "1045/3000 train_loss: 115.26397705078125 test_loss:172.89694213867188\n",
      "1046/3000 train_loss: 129.26637268066406 test_loss:174.72702026367188\n",
      "1047/3000 train_loss: 124.77189636230469 test_loss:178.42547607421875\n",
      "1048/3000 train_loss: 124.96002960205078 test_loss:174.4019317626953\n",
      "1049/3000 train_loss: 122.91053009033203 test_loss:178.58981323242188\n",
      "1050/3000 train_loss: 127.31869506835938 test_loss:178.63648986816406\n",
      "1051/3000 train_loss: 124.79888916015625 test_loss:175.52999877929688\n",
      "1052/3000 train_loss: 141.17019653320312 test_loss:185.67037963867188\n",
      "1053/3000 train_loss: 121.70198059082031 test_loss:177.22894287109375\n",
      "1054/3000 train_loss: 124.38308715820312 test_loss:176.22021484375\n",
      "1055/3000 train_loss: 135.515625 test_loss:181.10830688476562\n",
      "1056/3000 train_loss: 121.46083068847656 test_loss:175.74072265625\n",
      "1057/3000 train_loss: 126.07460021972656 test_loss:176.720703125\n",
      "1058/3000 train_loss: 113.4048843383789 test_loss:174.72976684570312\n",
      "1059/3000 train_loss: 132.7242431640625 test_loss:172.52536010742188\n",
      "1060/3000 train_loss: 140.8125 test_loss:173.85342407226562\n",
      "1061/3000 train_loss: 116.09124755859375 test_loss:174.587890625\n",
      "1062/3000 train_loss: 122.99395751953125 test_loss:171.7617950439453\n",
      "1063/3000 train_loss: 125.45394134521484 test_loss:173.2427520751953\n",
      "1064/3000 train_loss: 115.62521362304688 test_loss:170.1931610107422\n",
      "1065/3000 train_loss: 131.5908966064453 test_loss:168.15081787109375\n",
      "1066/3000 train_loss: 122.06275939941406 test_loss:174.42385864257812\n",
      "1067/3000 train_loss: 147.77000427246094 test_loss:171.20086669921875\n",
      "1068/3000 train_loss: 130.23007202148438 test_loss:172.70584106445312\n",
      "1069/3000 train_loss: 120.86317443847656 test_loss:172.58055114746094\n",
      "1070/3000 train_loss: 121.5777816772461 test_loss:171.91455078125\n",
      "1071/3000 train_loss: 125.30604553222656 test_loss:170.30718994140625\n",
      "1072/3000 train_loss: 134.4033203125 test_loss:169.3547821044922\n",
      "1073/3000 train_loss: 126.96459197998047 test_loss:170.9391326904297\n",
      "1074/3000 train_loss: 125.34215545654297 test_loss:169.3995361328125\n",
      "1075/3000 train_loss: 169.02442932128906 test_loss:167.67515563964844\n",
      "1076/3000 train_loss: 122.4357681274414 test_loss:175.58860778808594\n",
      "1077/3000 train_loss: 115.76300811767578 test_loss:170.02455139160156\n",
      "1078/3000 train_loss: 129.79421997070312 test_loss:171.58465576171875\n",
      "1079/3000 train_loss: 107.57239532470703 test_loss:167.832763671875\n",
      "1080/3000 train_loss: 119.00589752197266 test_loss:174.6385955810547\n",
      "1081/3000 train_loss: 119.7544174194336 test_loss:168.05209350585938\n",
      "1082/3000 train_loss: 129.35787963867188 test_loss:169.957763671875\n",
      "1083/3000 train_loss: 119.52151489257812 test_loss:168.39254760742188\n",
      "1084/3000 train_loss: 131.0013885498047 test_loss:166.9052734375\n",
      "1085/3000 train_loss: 119.46651458740234 test_loss:174.80386352539062\n",
      "1086/3000 train_loss: 115.45915222167969 test_loss:163.93804931640625\n",
      "1087/3000 train_loss: 124.24947357177734 test_loss:167.48971557617188\n",
      "1088/3000 train_loss: 116.05072021484375 test_loss:169.24188232421875\n",
      "1089/3000 train_loss: 118.46820068359375 test_loss:168.74427795410156\n",
      "1090/3000 train_loss: 119.02544403076172 test_loss:167.1122589111328\n",
      "1091/3000 train_loss: 130.65769958496094 test_loss:166.3080596923828\n",
      "1092/3000 train_loss: 120.42540740966797 test_loss:168.38973999023438\n",
      "1093/3000 train_loss: 133.07908630371094 test_loss:167.6268768310547\n",
      "1094/3000 train_loss: 120.13885498046875 test_loss:165.40330505371094\n",
      "1095/3000 train_loss: 117.71392059326172 test_loss:168.19383239746094\n",
      "1096/3000 train_loss: 123.6217041015625 test_loss:167.6609649658203\n",
      "1097/3000 train_loss: 125.3415298461914 test_loss:166.45578002929688\n",
      "1098/3000 train_loss: 121.03905487060547 test_loss:168.16494750976562\n",
      "1099/3000 train_loss: 107.15226745605469 test_loss:168.91278076171875\n",
      "1100/3000 train_loss: 117.0633544921875 test_loss:168.16033935546875\n",
      "1101/3000 train_loss: 123.26263427734375 test_loss:164.99148559570312\n",
      "1102/3000 train_loss: 119.00846862792969 test_loss:166.615478515625\n",
      "1103/3000 train_loss: 125.17555236816406 test_loss:173.6403350830078\n",
      "1104/3000 train_loss: 120.23634338378906 test_loss:164.05718994140625\n",
      "1105/3000 train_loss: 112.26997375488281 test_loss:164.93350219726562\n",
      "1106/3000 train_loss: 123.84764099121094 test_loss:165.36122131347656\n",
      "1107/3000 train_loss: 124.04207611083984 test_loss:167.08596801757812\n",
      "1108/3000 train_loss: 124.93759155273438 test_loss:170.56167602539062\n",
      "1109/3000 train_loss: 119.28084564208984 test_loss:171.41937255859375\n",
      "1110/3000 train_loss: 135.67979431152344 test_loss:170.5244903564453\n",
      "1111/3000 train_loss: 134.9188690185547 test_loss:172.29611206054688\n",
      "1112/3000 train_loss: 112.53309631347656 test_loss:166.44256591796875\n",
      "1113/3000 train_loss: 112.53947448730469 test_loss:165.27947998046875\n",
      "1114/3000 train_loss: 118.8160171508789 test_loss:166.58859252929688\n",
      "1115/3000 train_loss: 116.87799072265625 test_loss:165.2452850341797\n",
      "1116/3000 train_loss: 122.99418640136719 test_loss:163.32623291015625\n",
      "1117/3000 train_loss: 114.5572738647461 test_loss:162.78390502929688\n",
      "1118/3000 train_loss: 122.6555404663086 test_loss:164.01016235351562\n",
      "1119/3000 train_loss: 133.93675231933594 test_loss:161.90176391601562\n",
      "1120/3000 train_loss: 111.67454528808594 test_loss:164.36184692382812\n",
      "1121/3000 train_loss: 121.37269592285156 test_loss:162.88626098632812\n",
      "1122/3000 train_loss: 122.03428649902344 test_loss:162.5431365966797\n",
      "1123/3000 train_loss: 118.7685546875 test_loss:170.13755798339844\n",
      "1124/3000 train_loss: 116.4616928100586 test_loss:163.48658752441406\n",
      "1125/3000 train_loss: 111.16353607177734 test_loss:166.54733276367188\n",
      "1126/3000 train_loss: 117.31983947753906 test_loss:165.453857421875\n",
      "1127/3000 train_loss: 127.07058715820312 test_loss:164.2442626953125\n",
      "1128/3000 train_loss: 105.41738891601562 test_loss:167.9442901611328\n",
      "1129/3000 train_loss: 135.18023681640625 test_loss:161.91964721679688\n",
      "1130/3000 train_loss: 119.49400329589844 test_loss:161.6717529296875\n",
      "1131/3000 train_loss: 122.28233337402344 test_loss:161.15216064453125\n",
      "1132/3000 train_loss: 130.9744415283203 test_loss:162.4224395751953\n",
      "1133/3000 train_loss: 123.1796646118164 test_loss:167.5436248779297\n",
      "1134/3000 train_loss: 118.92548370361328 test_loss:162.14862060546875\n",
      "1135/3000 train_loss: 110.48681640625 test_loss:161.61570739746094\n",
      "1136/3000 train_loss: 104.5891342163086 test_loss:161.02511596679688\n",
      "1137/3000 train_loss: 111.28933715820312 test_loss:161.6016845703125\n",
      "1138/3000 train_loss: 110.24348449707031 test_loss:161.77670288085938\n",
      "1139/3000 train_loss: 113.18067932128906 test_loss:164.06585693359375\n",
      "1140/3000 train_loss: 120.24024963378906 test_loss:164.69029235839844\n",
      "1141/3000 train_loss: 115.80880737304688 test_loss:163.5919189453125\n",
      "1142/3000 train_loss: 124.82931518554688 test_loss:163.98048400878906\n",
      "1143/3000 train_loss: 111.04993438720703 test_loss:160.6122283935547\n",
      "1144/3000 train_loss: 128.0229034423828 test_loss:162.31539916992188\n",
      "1145/3000 train_loss: 125.787841796875 test_loss:161.6688995361328\n",
      "1146/3000 train_loss: 136.6905975341797 test_loss:170.43508911132812\n",
      "1147/3000 train_loss: 109.82884979248047 test_loss:161.02987670898438\n",
      "1148/3000 train_loss: 125.96917724609375 test_loss:161.81373596191406\n",
      "1149/3000 train_loss: 137.30694580078125 test_loss:162.45404052734375\n",
      "1150/3000 train_loss: 117.32513427734375 test_loss:158.8582305908203\n",
      "1151/3000 train_loss: 136.13214111328125 test_loss:160.4690399169922\n",
      "1152/3000 train_loss: 109.21620178222656 test_loss:161.5762176513672\n",
      "1153/3000 train_loss: 124.69923400878906 test_loss:161.57489013671875\n",
      "1154/3000 train_loss: 119.62802124023438 test_loss:160.12136840820312\n",
      "1155/3000 train_loss: 127.10415649414062 test_loss:166.93450927734375\n",
      "1156/3000 train_loss: 122.7339859008789 test_loss:159.3800811767578\n",
      "1157/3000 train_loss: 131.70213317871094 test_loss:161.954345703125\n",
      "1158/3000 train_loss: 108.4295883178711 test_loss:162.99285888671875\n",
      "1159/3000 train_loss: 126.60188293457031 test_loss:160.09759521484375\n",
      "1160/3000 train_loss: 107.45459747314453 test_loss:158.4432830810547\n",
      "1161/3000 train_loss: 113.47087097167969 test_loss:159.2755889892578\n",
      "1162/3000 train_loss: 106.24295043945312 test_loss:157.8049774169922\n",
      "1163/3000 train_loss: 105.56464385986328 test_loss:158.32806396484375\n",
      "1164/3000 train_loss: 117.61947631835938 test_loss:160.26272583007812\n",
      "1165/3000 train_loss: 116.73712158203125 test_loss:160.09693908691406\n",
      "1166/3000 train_loss: 125.47380065917969 test_loss:157.87109375\n",
      "1167/3000 train_loss: 117.31913757324219 test_loss:165.19178771972656\n",
      "1168/3000 train_loss: 114.49710845947266 test_loss:161.18911743164062\n",
      "1169/3000 train_loss: 107.8608627319336 test_loss:161.8251953125\n",
      "1170/3000 train_loss: 109.34723663330078 test_loss:162.84124755859375\n",
      "1171/3000 train_loss: 112.3955078125 test_loss:159.91494750976562\n",
      "1172/3000 train_loss: 134.61949157714844 test_loss:161.33670043945312\n",
      "1173/3000 train_loss: 110.61798858642578 test_loss:159.97265625\n",
      "1174/3000 train_loss: 119.43575286865234 test_loss:161.18527221679688\n",
      "1175/3000 train_loss: 111.9225845336914 test_loss:161.02877807617188\n",
      "1176/3000 train_loss: 117.57941436767578 test_loss:162.61032104492188\n",
      "1177/3000 train_loss: 107.35905456542969 test_loss:159.92955017089844\n",
      "1178/3000 train_loss: 114.6637191772461 test_loss:161.24514770507812\n",
      "1179/3000 train_loss: 119.03707885742188 test_loss:162.8529815673828\n",
      "1180/3000 train_loss: 109.28715515136719 test_loss:158.7222442626953\n",
      "1181/3000 train_loss: 125.2799072265625 test_loss:159.1282958984375\n",
      "1182/3000 train_loss: 105.86082458496094 test_loss:157.2305908203125\n",
      "1183/3000 train_loss: 127.03173065185547 test_loss:159.2749481201172\n",
      "1184/3000 train_loss: 117.38523864746094 test_loss:156.96559143066406\n",
      "1185/3000 train_loss: 124.21961212158203 test_loss:157.72640991210938\n",
      "1186/3000 train_loss: 131.85789489746094 test_loss:157.7553253173828\n",
      "1187/3000 train_loss: 132.2105712890625 test_loss:163.85655212402344\n",
      "1188/3000 train_loss: 110.73211669921875 test_loss:155.59140014648438\n",
      "1189/3000 train_loss: 111.09147644042969 test_loss:156.90199279785156\n",
      "1190/3000 train_loss: 106.67707824707031 test_loss:158.5276641845703\n",
      "1191/3000 train_loss: 99.22518157958984 test_loss:155.63351440429688\n",
      "1192/3000 train_loss: 113.81373596191406 test_loss:156.3822479248047\n",
      "1193/3000 train_loss: 119.50589752197266 test_loss:161.58840942382812\n",
      "1194/3000 train_loss: 112.79478454589844 test_loss:156.14781188964844\n",
      "1195/3000 train_loss: 120.40673065185547 test_loss:156.1975860595703\n",
      "1196/3000 train_loss: 131.75357055664062 test_loss:155.85572814941406\n",
      "1197/3000 train_loss: 118.19863891601562 test_loss:158.00772094726562\n",
      "1198/3000 train_loss: 119.32772827148438 test_loss:155.3253173828125\n",
      "1199/3000 train_loss: 104.85953521728516 test_loss:156.2621612548828\n",
      "1200/3000 train_loss: 111.78068542480469 test_loss:155.45083618164062\n",
      "1201/3000 train_loss: 109.39510345458984 test_loss:159.113525390625\n",
      "1202/3000 train_loss: 128.91845703125 test_loss:157.6585693359375\n",
      "1203/3000 train_loss: 102.7271728515625 test_loss:156.11154174804688\n",
      "1204/3000 train_loss: 113.92963409423828 test_loss:156.33558654785156\n",
      "1205/3000 train_loss: 103.86112213134766 test_loss:156.52328491210938\n",
      "1206/3000 train_loss: 115.06722259521484 test_loss:154.16180419921875\n",
      "1207/3000 train_loss: 120.43821716308594 test_loss:156.71627807617188\n",
      "1208/3000 train_loss: 119.58586120605469 test_loss:153.8426513671875\n",
      "1209/3000 train_loss: 120.00940704345703 test_loss:157.0920867919922\n",
      "1210/3000 train_loss: 111.04143524169922 test_loss:154.37326049804688\n",
      "1211/3000 train_loss: 114.451416015625 test_loss:157.77740478515625\n",
      "1212/3000 train_loss: 122.00230407714844 test_loss:153.73912048339844\n",
      "1213/3000 train_loss: 112.15559387207031 test_loss:154.07846069335938\n",
      "1214/3000 train_loss: 123.65451049804688 test_loss:160.65542602539062\n",
      "1215/3000 train_loss: 126.30399322509766 test_loss:152.88973999023438\n",
      "1216/3000 train_loss: 139.8463134765625 test_loss:157.05149841308594\n",
      "1217/3000 train_loss: 111.71871185302734 test_loss:159.41773986816406\n",
      "1218/3000 train_loss: 103.30994415283203 test_loss:157.01644897460938\n",
      "1219/3000 train_loss: 134.69273376464844 test_loss:154.79310607910156\n",
      "1220/3000 train_loss: 117.17849731445312 test_loss:156.76121520996094\n",
      "1221/3000 train_loss: 100.13665771484375 test_loss:153.92306518554688\n",
      "1222/3000 train_loss: 113.0289077758789 test_loss:152.4139404296875\n",
      "1223/3000 train_loss: 110.95350646972656 test_loss:155.8217010498047\n",
      "1224/3000 train_loss: 108.9508056640625 test_loss:154.97938537597656\n",
      "1225/3000 train_loss: 121.99565124511719 test_loss:155.4634552001953\n",
      "1226/3000 train_loss: 107.83680725097656 test_loss:157.49966430664062\n",
      "1227/3000 train_loss: 110.83738708496094 test_loss:154.05921936035156\n",
      "1228/3000 train_loss: 109.02749633789062 test_loss:154.72085571289062\n",
      "1229/3000 train_loss: 106.65752410888672 test_loss:152.5069122314453\n",
      "1230/3000 train_loss: 120.01588439941406 test_loss:155.99093627929688\n",
      "1231/3000 train_loss: 101.05229949951172 test_loss:152.10806274414062\n",
      "1232/3000 train_loss: 113.17019653320312 test_loss:153.73757934570312\n",
      "1233/3000 train_loss: 112.72523498535156 test_loss:151.10389709472656\n",
      "1234/3000 train_loss: 100.11093139648438 test_loss:152.9242706298828\n",
      "1235/3000 train_loss: 106.2529525756836 test_loss:151.023193359375\n",
      "1236/3000 train_loss: 103.07080841064453 test_loss:150.0621337890625\n",
      "1237/3000 train_loss: 102.47281646728516 test_loss:152.29885864257812\n",
      "1238/3000 train_loss: 104.93292999267578 test_loss:152.47689819335938\n",
      "1239/3000 train_loss: 99.33533477783203 test_loss:152.65414428710938\n",
      "1240/3000 train_loss: 106.13592529296875 test_loss:151.921142578125\n",
      "1241/3000 train_loss: 112.33961486816406 test_loss:151.64956665039062\n",
      "1242/3000 train_loss: 103.10118103027344 test_loss:152.15940856933594\n",
      "1243/3000 train_loss: 105.91079711914062 test_loss:150.27560424804688\n",
      "1244/3000 train_loss: 116.25989532470703 test_loss:155.5140380859375\n",
      "1245/3000 train_loss: 111.57269287109375 test_loss:157.6697235107422\n",
      "1246/3000 train_loss: 113.0617904663086 test_loss:155.21575927734375\n",
      "1247/3000 train_loss: 109.4683609008789 test_loss:153.77447509765625\n",
      "1248/3000 train_loss: 100.63304138183594 test_loss:152.80198669433594\n",
      "1249/3000 train_loss: 106.15180206298828 test_loss:149.4156951904297\n",
      "1250/3000 train_loss: 105.1856460571289 test_loss:151.11244201660156\n",
      "1251/3000 train_loss: 95.8398208618164 test_loss:157.49688720703125\n",
      "1252/3000 train_loss: 119.33072662353516 test_loss:152.2606658935547\n",
      "1253/3000 train_loss: 109.0764389038086 test_loss:152.13583374023438\n",
      "1254/3000 train_loss: 109.51461029052734 test_loss:152.8489532470703\n",
      "1255/3000 train_loss: 109.17539978027344 test_loss:151.2251434326172\n",
      "1256/3000 train_loss: 104.46379852294922 test_loss:152.79713439941406\n",
      "1257/3000 train_loss: 117.38074493408203 test_loss:160.96583557128906\n",
      "1258/3000 train_loss: 116.56901550292969 test_loss:161.0819091796875\n",
      "1259/3000 train_loss: 114.49882507324219 test_loss:163.1775360107422\n",
      "1260/3000 train_loss: 118.8299789428711 test_loss:162.8663787841797\n",
      "1261/3000 train_loss: 108.84617614746094 test_loss:157.1528778076172\n",
      "1262/3000 train_loss: 110.4610366821289 test_loss:157.26010131835938\n",
      "1263/3000 train_loss: 115.36154174804688 test_loss:155.9209442138672\n",
      "1264/3000 train_loss: 108.6219253540039 test_loss:154.7726593017578\n",
      "1265/3000 train_loss: 109.84336853027344 test_loss:155.9428253173828\n",
      "1266/3000 train_loss: 111.50393676757812 test_loss:154.37152099609375\n",
      "1267/3000 train_loss: 104.5826416015625 test_loss:153.15444946289062\n",
      "1268/3000 train_loss: 95.7429428100586 test_loss:152.9365997314453\n",
      "1269/3000 train_loss: 110.74583435058594 test_loss:154.87428283691406\n",
      "1270/3000 train_loss: 102.51393127441406 test_loss:151.9338836669922\n",
      "1271/3000 train_loss: 111.10086059570312 test_loss:152.12339782714844\n",
      "1272/3000 train_loss: 95.89990997314453 test_loss:155.9293212890625\n",
      "1273/3000 train_loss: 118.73189544677734 test_loss:152.95526123046875\n",
      "1274/3000 train_loss: 105.16078186035156 test_loss:154.02171325683594\n",
      "1275/3000 train_loss: 101.92781066894531 test_loss:154.22421264648438\n",
      "1276/3000 train_loss: 100.53390502929688 test_loss:154.1494140625\n",
      "1277/3000 train_loss: 99.05171966552734 test_loss:155.51490783691406\n",
      "1278/3000 train_loss: 114.41715240478516 test_loss:153.46646118164062\n",
      "1279/3000 train_loss: 107.73845672607422 test_loss:151.90823364257812\n",
      "1280/3000 train_loss: 94.48588562011719 test_loss:152.27638244628906\n",
      "1281/3000 train_loss: 107.5075912475586 test_loss:153.37400817871094\n",
      "1282/3000 train_loss: 118.35518646240234 test_loss:159.26551818847656\n",
      "1283/3000 train_loss: 108.18356323242188 test_loss:162.63803100585938\n",
      "1284/3000 train_loss: 113.7080078125 test_loss:152.96185302734375\n",
      "1285/3000 train_loss: 119.02316284179688 test_loss:152.11390686035156\n",
      "1286/3000 train_loss: 114.12353515625 test_loss:151.05755615234375\n",
      "1287/3000 train_loss: 109.77045440673828 test_loss:148.26565551757812\n",
      "1288/3000 train_loss: 126.25150299072266 test_loss:150.54930114746094\n",
      "1289/3000 train_loss: 112.51081085205078 test_loss:151.09078979492188\n",
      "1290/3000 train_loss: 103.54557037353516 test_loss:154.20912170410156\n",
      "1291/3000 train_loss: 113.71712493896484 test_loss:148.7477264404297\n",
      "1292/3000 train_loss: 105.68717193603516 test_loss:155.52578735351562\n",
      "1293/3000 train_loss: 102.91327667236328 test_loss:147.5330810546875\n",
      "1294/3000 train_loss: 107.50068664550781 test_loss:150.48390197753906\n",
      "1295/3000 train_loss: 95.35620880126953 test_loss:150.54776000976562\n",
      "1296/3000 train_loss: 97.03854370117188 test_loss:148.35861206054688\n",
      "1297/3000 train_loss: 108.03462219238281 test_loss:149.8598175048828\n",
      "1298/3000 train_loss: 102.10735321044922 test_loss:147.60740661621094\n",
      "1299/3000 train_loss: 105.41514587402344 test_loss:147.24887084960938\n",
      "1300/3000 train_loss: 110.88091278076172 test_loss:150.41639709472656\n",
      "1301/3000 train_loss: 99.3166732788086 test_loss:146.31455993652344\n",
      "1302/3000 train_loss: 102.71617126464844 test_loss:145.9674530029297\n",
      "1303/3000 train_loss: 105.61624908447266 test_loss:148.703125\n",
      "1304/3000 train_loss: 97.81256103515625 test_loss:147.52821350097656\n",
      "1305/3000 train_loss: 104.16259765625 test_loss:148.61607360839844\n",
      "1306/3000 train_loss: 103.5379867553711 test_loss:153.36720275878906\n",
      "1307/3000 train_loss: 105.46873474121094 test_loss:147.4234619140625\n",
      "1308/3000 train_loss: 103.88902282714844 test_loss:147.97438049316406\n",
      "1309/3000 train_loss: 99.91842651367188 test_loss:147.92987060546875\n",
      "1310/3000 train_loss: 99.79439544677734 test_loss:147.4114532470703\n",
      "1311/3000 train_loss: 102.87425994873047 test_loss:148.4874725341797\n",
      "1312/3000 train_loss: 117.09672546386719 test_loss:144.94703674316406\n",
      "1313/3000 train_loss: 103.2583999633789 test_loss:145.13284301757812\n",
      "1314/3000 train_loss: 100.53276824951172 test_loss:146.83718872070312\n",
      "1315/3000 train_loss: 98.77540588378906 test_loss:145.9099884033203\n",
      "1316/3000 train_loss: 102.15345001220703 test_loss:148.8883056640625\n",
      "1317/3000 train_loss: 99.24053955078125 test_loss:149.22169494628906\n",
      "1318/3000 train_loss: 113.93649291992188 test_loss:147.9589385986328\n",
      "1319/3000 train_loss: 103.16048431396484 test_loss:148.14117431640625\n",
      "1320/3000 train_loss: 119.83818054199219 test_loss:146.61439514160156\n",
      "1321/3000 train_loss: 110.58876037597656 test_loss:150.61355590820312\n",
      "1322/3000 train_loss: 95.36590576171875 test_loss:148.21047973632812\n",
      "1323/3000 train_loss: 115.78125 test_loss:149.3035430908203\n",
      "1324/3000 train_loss: 106.87691497802734 test_loss:150.32200622558594\n",
      "1325/3000 train_loss: 104.12494659423828 test_loss:145.69100952148438\n",
      "1326/3000 train_loss: 100.19793701171875 test_loss:146.54464721679688\n",
      "1327/3000 train_loss: 92.59434509277344 test_loss:145.48890686035156\n",
      "1328/3000 train_loss: 108.16738891601562 test_loss:150.6285400390625\n",
      "1329/3000 train_loss: 101.68621826171875 test_loss:144.90811157226562\n",
      "1330/3000 train_loss: 119.01258850097656 test_loss:143.75668334960938\n",
      "1331/3000 train_loss: 99.1041488647461 test_loss:144.7080535888672\n",
      "1332/3000 train_loss: 88.79276275634766 test_loss:145.65052795410156\n",
      "1333/3000 train_loss: 105.94867706298828 test_loss:146.43829345703125\n",
      "1334/3000 train_loss: 91.3681411743164 test_loss:145.45123291015625\n",
      "1335/3000 train_loss: 95.50874328613281 test_loss:147.119873046875\n",
      "1336/3000 train_loss: 109.85993957519531 test_loss:144.41949462890625\n",
      "1337/3000 train_loss: 104.13229370117188 test_loss:144.30064392089844\n",
      "1338/3000 train_loss: 95.68902587890625 test_loss:146.1243896484375\n",
      "1339/3000 train_loss: 96.71330261230469 test_loss:144.77001953125\n",
      "1340/3000 train_loss: 105.25614166259766 test_loss:146.65228271484375\n",
      "1341/3000 train_loss: 102.08397674560547 test_loss:145.64779663085938\n",
      "1342/3000 train_loss: 105.26299285888672 test_loss:144.42921447753906\n",
      "1343/3000 train_loss: 100.08668518066406 test_loss:144.27688598632812\n",
      "1344/3000 train_loss: 98.27816772460938 test_loss:142.0889434814453\n",
      "1345/3000 train_loss: 105.1895523071289 test_loss:143.00030517578125\n",
      "1346/3000 train_loss: 101.71825408935547 test_loss:147.6995849609375\n",
      "1347/3000 train_loss: 115.98664093017578 test_loss:147.80862426757812\n",
      "1348/3000 train_loss: 106.59744262695312 test_loss:144.55044555664062\n",
      "1349/3000 train_loss: 95.88034057617188 test_loss:143.52096557617188\n",
      "1350/3000 train_loss: 92.30834197998047 test_loss:144.388671875\n",
      "1351/3000 train_loss: 92.20848846435547 test_loss:146.6214141845703\n",
      "1352/3000 train_loss: 105.09021759033203 test_loss:143.12806701660156\n",
      "1353/3000 train_loss: 84.947265625 test_loss:143.94351196289062\n",
      "1354/3000 train_loss: 92.61419677734375 test_loss:146.3168487548828\n",
      "1355/3000 train_loss: 101.5421142578125 test_loss:144.40980529785156\n",
      "1356/3000 train_loss: 102.73109436035156 test_loss:143.3904266357422\n",
      "1357/3000 train_loss: 96.50566101074219 test_loss:147.34481811523438\n",
      "1358/3000 train_loss: 106.18963623046875 test_loss:144.65679931640625\n",
      "1359/3000 train_loss: 100.20458221435547 test_loss:144.48065185546875\n",
      "1360/3000 train_loss: 100.61940002441406 test_loss:145.34226989746094\n",
      "1361/3000 train_loss: 96.67868041992188 test_loss:145.23223876953125\n",
      "1362/3000 train_loss: 106.52880859375 test_loss:143.79776000976562\n",
      "1363/3000 train_loss: 98.46571350097656 test_loss:144.36492919921875\n",
      "1364/3000 train_loss: 93.8873519897461 test_loss:141.68563842773438\n",
      "1365/3000 train_loss: 114.78923797607422 test_loss:143.4632110595703\n",
      "1366/3000 train_loss: 88.75729370117188 test_loss:145.23794555664062\n",
      "1367/3000 train_loss: 96.72834777832031 test_loss:142.4147186279297\n",
      "1368/3000 train_loss: 97.60601806640625 test_loss:143.81971740722656\n",
      "1369/3000 train_loss: 95.4013900756836 test_loss:144.36758422851562\n",
      "1370/3000 train_loss: 102.91858673095703 test_loss:145.9980926513672\n",
      "1371/3000 train_loss: 88.01402282714844 test_loss:142.87918090820312\n",
      "1372/3000 train_loss: 106.33061981201172 test_loss:142.8604736328125\n",
      "1373/3000 train_loss: 100.3012466430664 test_loss:144.31629943847656\n",
      "1374/3000 train_loss: 101.32860565185547 test_loss:144.28851318359375\n",
      "1375/3000 train_loss: 92.22527313232422 test_loss:147.08163452148438\n",
      "1376/3000 train_loss: 105.92665100097656 test_loss:141.69781494140625\n",
      "1377/3000 train_loss: 94.36068725585938 test_loss:141.8334503173828\n",
      "1378/3000 train_loss: 90.8724136352539 test_loss:141.59397888183594\n",
      "1379/3000 train_loss: 113.082275390625 test_loss:146.0720977783203\n",
      "1380/3000 train_loss: 106.86820220947266 test_loss:143.06752014160156\n",
      "1381/3000 train_loss: 95.41069030761719 test_loss:142.93057250976562\n",
      "1382/3000 train_loss: 94.74299621582031 test_loss:145.4253692626953\n",
      "1383/3000 train_loss: 103.52032470703125 test_loss:143.96920776367188\n",
      "1384/3000 train_loss: 104.05862426757812 test_loss:144.01834106445312\n",
      "1385/3000 train_loss: 106.16187286376953 test_loss:143.2979736328125\n",
      "1386/3000 train_loss: 94.7801742553711 test_loss:143.2927703857422\n",
      "1387/3000 train_loss: 120.5205307006836 test_loss:148.6656951904297\n",
      "1388/3000 train_loss: 105.49645233154297 test_loss:144.37017822265625\n",
      "1389/3000 train_loss: 99.09760284423828 test_loss:150.34661865234375\n",
      "1390/3000 train_loss: 102.9688949584961 test_loss:142.97540283203125\n",
      "1391/3000 train_loss: 98.7553939819336 test_loss:144.9388885498047\n",
      "1392/3000 train_loss: 103.92029571533203 test_loss:142.5648193359375\n",
      "1393/3000 train_loss: 93.61054992675781 test_loss:142.90960693359375\n",
      "1394/3000 train_loss: 110.31810760498047 test_loss:143.2716827392578\n",
      "1395/3000 train_loss: 97.67488098144531 test_loss:142.33395385742188\n",
      "1396/3000 train_loss: 95.92285919189453 test_loss:142.43185424804688\n",
      "1397/3000 train_loss: 86.93840026855469 test_loss:144.0410919189453\n",
      "1398/3000 train_loss: 93.65544891357422 test_loss:143.60328674316406\n",
      "1399/3000 train_loss: 97.2073974609375 test_loss:142.54054260253906\n",
      "1400/3000 train_loss: 96.52192687988281 test_loss:142.54708862304688\n",
      "1401/3000 train_loss: 104.54448699951172 test_loss:143.68763732910156\n",
      "1402/3000 train_loss: 100.50464630126953 test_loss:147.43946838378906\n",
      "1403/3000 train_loss: 95.46461486816406 test_loss:140.57156372070312\n",
      "1404/3000 train_loss: 97.46856689453125 test_loss:140.5556640625\n",
      "1405/3000 train_loss: 89.14353942871094 test_loss:139.91525268554688\n",
      "1406/3000 train_loss: 98.96456146240234 test_loss:143.4302978515625\n",
      "1407/3000 train_loss: 91.84568786621094 test_loss:143.23805236816406\n",
      "1408/3000 train_loss: 89.7699966430664 test_loss:145.1523895263672\n",
      "1409/3000 train_loss: 92.942138671875 test_loss:140.28866577148438\n",
      "1410/3000 train_loss: 93.07889556884766 test_loss:139.83822631835938\n",
      "1411/3000 train_loss: 88.07311248779297 test_loss:138.12490844726562\n",
      "1412/3000 train_loss: 103.86092376708984 test_loss:141.11981201171875\n",
      "1413/3000 train_loss: 88.21717071533203 test_loss:141.26792907714844\n",
      "1414/3000 train_loss: 95.29289245605469 test_loss:141.48193359375\n",
      "1415/3000 train_loss: 93.88800048828125 test_loss:142.45602416992188\n",
      "1416/3000 train_loss: 95.04200744628906 test_loss:141.80235290527344\n",
      "1417/3000 train_loss: 103.46517944335938 test_loss:143.31524658203125\n",
      "1418/3000 train_loss: 97.16891479492188 test_loss:147.87879943847656\n",
      "1419/3000 train_loss: 86.48870849609375 test_loss:144.26377868652344\n",
      "1420/3000 train_loss: 90.60547637939453 test_loss:142.863037109375\n",
      "1421/3000 train_loss: 85.73226165771484 test_loss:140.5000762939453\n",
      "1422/3000 train_loss: 87.59400939941406 test_loss:143.5458221435547\n",
      "1423/3000 train_loss: 99.01432037353516 test_loss:141.7960205078125\n",
      "1424/3000 train_loss: 91.52738189697266 test_loss:146.40260314941406\n",
      "1425/3000 train_loss: 83.86783599853516 test_loss:142.2796173095703\n",
      "1426/3000 train_loss: 112.98918151855469 test_loss:140.4933319091797\n",
      "1427/3000 train_loss: 93.30485534667969 test_loss:139.2205810546875\n",
      "1428/3000 train_loss: 87.28587341308594 test_loss:144.0074920654297\n",
      "1429/3000 train_loss: 98.70463562011719 test_loss:139.26553344726562\n",
      "1430/3000 train_loss: 89.385009765625 test_loss:142.12278747558594\n",
      "1431/3000 train_loss: 97.86833190917969 test_loss:139.1973419189453\n",
      "1432/3000 train_loss: 98.24371337890625 test_loss:139.07774353027344\n",
      "1433/3000 train_loss: 93.77333068847656 test_loss:138.0179443359375\n",
      "1434/3000 train_loss: 85.41082763671875 test_loss:139.47332763671875\n",
      "1435/3000 train_loss: 104.04988098144531 test_loss:137.99349975585938\n",
      "1436/3000 train_loss: 101.17585754394531 test_loss:141.64308166503906\n",
      "1437/3000 train_loss: 110.53101348876953 test_loss:146.3289337158203\n",
      "1438/3000 train_loss: 94.67568969726562 test_loss:139.53494262695312\n",
      "1439/3000 train_loss: 91.80020141601562 test_loss:140.34793090820312\n",
      "1440/3000 train_loss: 85.96607971191406 test_loss:141.6491241455078\n",
      "1441/3000 train_loss: 98.815673828125 test_loss:139.35360717773438\n",
      "1442/3000 train_loss: 94.75182342529297 test_loss:142.13943481445312\n",
      "1443/3000 train_loss: 98.63998413085938 test_loss:138.4354248046875\n",
      "1444/3000 train_loss: 85.03972625732422 test_loss:136.60842895507812\n",
      "1445/3000 train_loss: 100.57206726074219 test_loss:138.4637908935547\n",
      "1446/3000 train_loss: 97.4178695678711 test_loss:136.2977752685547\n",
      "1447/3000 train_loss: 94.69892883300781 test_loss:137.1492462158203\n",
      "1448/3000 train_loss: 97.84868621826172 test_loss:138.90362548828125\n",
      "1449/3000 train_loss: 93.77053833007812 test_loss:137.1120147705078\n",
      "1450/3000 train_loss: 89.853515625 test_loss:140.49234008789062\n",
      "1451/3000 train_loss: 87.64750671386719 test_loss:137.24049377441406\n",
      "1452/3000 train_loss: 101.75410461425781 test_loss:138.00933837890625\n",
      "1453/3000 train_loss: 89.15426635742188 test_loss:138.92538452148438\n",
      "1454/3000 train_loss: 89.45532989501953 test_loss:139.99659729003906\n",
      "1455/3000 train_loss: 85.09580993652344 test_loss:137.10733032226562\n",
      "1456/3000 train_loss: 93.66293334960938 test_loss:137.31858825683594\n",
      "1457/3000 train_loss: 91.76654052734375 test_loss:138.68453979492188\n",
      "1458/3000 train_loss: 87.27852630615234 test_loss:137.75030517578125\n",
      "1459/3000 train_loss: 89.99949645996094 test_loss:143.37606811523438\n",
      "1460/3000 train_loss: 89.30191040039062 test_loss:136.90435791015625\n",
      "1461/3000 train_loss: 83.97787475585938 test_loss:138.45693969726562\n",
      "1462/3000 train_loss: 89.10747528076172 test_loss:136.31459045410156\n",
      "1463/3000 train_loss: 104.9248275756836 test_loss:134.3358154296875\n",
      "1464/3000 train_loss: 92.91191864013672 test_loss:135.49546813964844\n",
      "1465/3000 train_loss: 97.46041107177734 test_loss:135.59129333496094\n",
      "1466/3000 train_loss: 85.55157470703125 test_loss:136.84375\n",
      "1467/3000 train_loss: 92.88105773925781 test_loss:134.97096252441406\n",
      "1468/3000 train_loss: 118.33078002929688 test_loss:136.03457641601562\n",
      "1469/3000 train_loss: 90.9781494140625 test_loss:138.91586303710938\n",
      "1470/3000 train_loss: 84.13398742675781 test_loss:137.5048065185547\n",
      "1471/3000 train_loss: 88.91312408447266 test_loss:138.13507080078125\n",
      "1472/3000 train_loss: 90.66004180908203 test_loss:136.7325439453125\n",
      "1473/3000 train_loss: 101.89215087890625 test_loss:136.79916381835938\n",
      "1474/3000 train_loss: 84.89730834960938 test_loss:135.28411865234375\n",
      "1475/3000 train_loss: 78.4007339477539 test_loss:139.5236053466797\n",
      "1476/3000 train_loss: 83.36481475830078 test_loss:136.11141967773438\n",
      "1477/3000 train_loss: 91.63831329345703 test_loss:140.74232482910156\n",
      "1478/3000 train_loss: 89.94538879394531 test_loss:136.6903076171875\n",
      "1479/3000 train_loss: 87.00969696044922 test_loss:138.40252685546875\n",
      "1480/3000 train_loss: 91.26963806152344 test_loss:134.9750518798828\n",
      "1481/3000 train_loss: 87.60865020751953 test_loss:136.83421325683594\n",
      "1482/3000 train_loss: 87.53975677490234 test_loss:137.6380615234375\n",
      "1483/3000 train_loss: 79.6741943359375 test_loss:135.0955810546875\n",
      "1484/3000 train_loss: 91.42925262451172 test_loss:135.4806671142578\n",
      "1485/3000 train_loss: 81.0333251953125 test_loss:132.75350952148438\n",
      "1486/3000 train_loss: 88.41315460205078 test_loss:134.122314453125\n",
      "1487/3000 train_loss: 87.65543365478516 test_loss:135.49765014648438\n",
      "1488/3000 train_loss: 85.81800842285156 test_loss:136.30279541015625\n",
      "1489/3000 train_loss: 88.15914154052734 test_loss:134.77589416503906\n",
      "1490/3000 train_loss: 85.76963806152344 test_loss:133.90623474121094\n",
      "1491/3000 train_loss: 96.55654907226562 test_loss:135.29464721679688\n",
      "1492/3000 train_loss: 82.84420013427734 test_loss:142.05601501464844\n",
      "1493/3000 train_loss: 91.32201385498047 test_loss:136.52476501464844\n",
      "1494/3000 train_loss: 77.86140441894531 test_loss:138.96841430664062\n",
      "1495/3000 train_loss: 78.19161224365234 test_loss:135.3907928466797\n",
      "1496/3000 train_loss: 82.91313934326172 test_loss:133.25845336914062\n",
      "1497/3000 train_loss: 84.42559051513672 test_loss:136.48277282714844\n",
      "1498/3000 train_loss: 92.14250946044922 test_loss:133.74996948242188\n",
      "1499/3000 train_loss: 85.09178924560547 test_loss:133.12522888183594\n",
      "1500/3000 train_loss: 82.74596405029297 test_loss:132.55747985839844\n",
      "1501/3000 train_loss: 80.96483612060547 test_loss:133.0957794189453\n",
      "1502/3000 train_loss: 86.7466812133789 test_loss:136.24517822265625\n",
      "1503/3000 train_loss: 82.5777359008789 test_loss:133.30052185058594\n",
      "1504/3000 train_loss: 83.24447631835938 test_loss:131.27427673339844\n",
      "1505/3000 train_loss: 91.89823913574219 test_loss:130.8357391357422\n",
      "1506/3000 train_loss: 89.1257553100586 test_loss:132.5443115234375\n",
      "1507/3000 train_loss: 86.20301818847656 test_loss:135.1724853515625\n",
      "1508/3000 train_loss: 84.77766418457031 test_loss:134.91839599609375\n",
      "1509/3000 train_loss: 78.5810317993164 test_loss:131.93161010742188\n",
      "1510/3000 train_loss: 96.25597381591797 test_loss:132.0718536376953\n",
      "1511/3000 train_loss: 82.8973617553711 test_loss:132.87503051757812\n",
      "1512/3000 train_loss: 83.00601196289062 test_loss:132.68453979492188\n",
      "1513/3000 train_loss: 94.94014739990234 test_loss:133.6929168701172\n",
      "1514/3000 train_loss: 89.12069702148438 test_loss:137.49282836914062\n",
      "1515/3000 train_loss: 99.95194244384766 test_loss:137.9196014404297\n",
      "1516/3000 train_loss: 94.17937469482422 test_loss:135.66793823242188\n",
      "1517/3000 train_loss: 80.40753173828125 test_loss:134.41458129882812\n",
      "1518/3000 train_loss: 100.78485107421875 test_loss:134.21063232421875\n",
      "1519/3000 train_loss: 92.05561065673828 test_loss:134.4093475341797\n",
      "1520/3000 train_loss: 82.18248748779297 test_loss:132.73280334472656\n",
      "1521/3000 train_loss: 79.97162628173828 test_loss:133.23086547851562\n",
      "1522/3000 train_loss: 88.32613372802734 test_loss:131.2491912841797\n",
      "1523/3000 train_loss: 85.42630004882812 test_loss:133.57655334472656\n",
      "1524/3000 train_loss: 80.30589294433594 test_loss:133.35427856445312\n",
      "1525/3000 train_loss: 90.81385803222656 test_loss:131.20591735839844\n",
      "1526/3000 train_loss: 93.32093048095703 test_loss:135.41795349121094\n",
      "1527/3000 train_loss: 84.26873779296875 test_loss:129.01040649414062\n",
      "1528/3000 train_loss: 96.71194458007812 test_loss:129.84347534179688\n",
      "1529/3000 train_loss: 92.26361083984375 test_loss:132.2573699951172\n",
      "1530/3000 train_loss: 97.86563873291016 test_loss:129.79783630371094\n",
      "1531/3000 train_loss: 88.70406341552734 test_loss:133.39108276367188\n",
      "1532/3000 train_loss: 85.74000549316406 test_loss:132.98709106445312\n",
      "1533/3000 train_loss: 85.96768188476562 test_loss:130.85931396484375\n",
      "1534/3000 train_loss: 80.0825424194336 test_loss:131.9219207763672\n",
      "1535/3000 train_loss: 82.4721908569336 test_loss:129.76123046875\n",
      "1536/3000 train_loss: 79.9196548461914 test_loss:131.06163024902344\n",
      "1537/3000 train_loss: 83.24031829833984 test_loss:129.85382080078125\n",
      "1538/3000 train_loss: 79.7564468383789 test_loss:132.752685546875\n",
      "1539/3000 train_loss: 91.87520599365234 test_loss:131.5032196044922\n",
      "1540/3000 train_loss: 82.3031234741211 test_loss:129.9145965576172\n",
      "1541/3000 train_loss: 81.6016616821289 test_loss:129.62551879882812\n",
      "1542/3000 train_loss: 87.2358169555664 test_loss:131.2986602783203\n",
      "1543/3000 train_loss: 89.12264251708984 test_loss:129.56187438964844\n",
      "1544/3000 train_loss: 84.26050567626953 test_loss:129.3729248046875\n",
      "1545/3000 train_loss: 84.33195495605469 test_loss:129.9434356689453\n",
      "1546/3000 train_loss: 90.39093780517578 test_loss:129.15965270996094\n",
      "1547/3000 train_loss: 75.47749328613281 test_loss:132.49037170410156\n",
      "1548/3000 train_loss: 84.51746368408203 test_loss:129.73333740234375\n",
      "1549/3000 train_loss: 93.25487518310547 test_loss:133.72842407226562\n",
      "1550/3000 train_loss: 93.50926208496094 test_loss:129.5126953125\n",
      "1551/3000 train_loss: 100.19489288330078 test_loss:130.47573852539062\n",
      "1552/3000 train_loss: 77.80197143554688 test_loss:131.93849182128906\n",
      "1553/3000 train_loss: 91.55298614501953 test_loss:132.3352813720703\n",
      "1554/3000 train_loss: 80.59689331054688 test_loss:132.56710815429688\n",
      "1555/3000 train_loss: 88.1669692993164 test_loss:130.18426513671875\n",
      "1556/3000 train_loss: 89.79102325439453 test_loss:132.4681854248047\n",
      "1557/3000 train_loss: 84.63816833496094 test_loss:128.0943603515625\n",
      "1558/3000 train_loss: 79.40795135498047 test_loss:133.896728515625\n",
      "1559/3000 train_loss: 83.78750610351562 test_loss:128.1068115234375\n",
      "1560/3000 train_loss: 67.48751068115234 test_loss:128.40725708007812\n",
      "1561/3000 train_loss: 80.79405212402344 test_loss:130.19464111328125\n",
      "1562/3000 train_loss: 77.16473388671875 test_loss:129.3061981201172\n",
      "1563/3000 train_loss: 85.69224548339844 test_loss:127.61695098876953\n",
      "1564/3000 train_loss: 86.21746826171875 test_loss:135.15956115722656\n",
      "1565/3000 train_loss: 79.57476043701172 test_loss:128.13568115234375\n",
      "1566/3000 train_loss: 80.242431640625 test_loss:127.10310363769531\n",
      "1567/3000 train_loss: 86.30853271484375 test_loss:126.56373596191406\n",
      "1568/3000 train_loss: 96.12144470214844 test_loss:127.88623046875\n",
      "1569/3000 train_loss: 85.44558715820312 test_loss:129.378173828125\n",
      "1570/3000 train_loss: 81.88434600830078 test_loss:127.61266326904297\n",
      "1571/3000 train_loss: 74.13816833496094 test_loss:126.19749450683594\n",
      "1572/3000 train_loss: 76.71439361572266 test_loss:129.64044189453125\n",
      "1573/3000 train_loss: 74.8475570678711 test_loss:128.85540771484375\n",
      "1574/3000 train_loss: 80.27656555175781 test_loss:125.66943359375\n",
      "1575/3000 train_loss: 81.88775634765625 test_loss:128.41468811035156\n",
      "1576/3000 train_loss: 80.58040618896484 test_loss:127.01766967773438\n",
      "1577/3000 train_loss: 80.21458435058594 test_loss:130.4550323486328\n",
      "1578/3000 train_loss: 80.23401641845703 test_loss:126.644775390625\n",
      "1579/3000 train_loss: 86.5484390258789 test_loss:126.38377380371094\n",
      "1580/3000 train_loss: 72.5738296508789 test_loss:126.84614562988281\n",
      "1581/3000 train_loss: 82.38831329345703 test_loss:126.083740234375\n",
      "1582/3000 train_loss: 84.77351379394531 test_loss:128.41934204101562\n",
      "1583/3000 train_loss: 75.1424789428711 test_loss:127.3952407836914\n",
      "1584/3000 train_loss: 74.37198638916016 test_loss:127.8719482421875\n",
      "1585/3000 train_loss: 84.74615478515625 test_loss:129.4888153076172\n",
      "1586/3000 train_loss: 69.79817199707031 test_loss:130.2762908935547\n",
      "1587/3000 train_loss: 80.64053344726562 test_loss:129.19227600097656\n",
      "1588/3000 train_loss: 72.14917755126953 test_loss:126.90132141113281\n",
      "1589/3000 train_loss: 86.28372955322266 test_loss:126.67274475097656\n",
      "1590/3000 train_loss: 72.46417236328125 test_loss:129.08880615234375\n",
      "1591/3000 train_loss: 73.65323638916016 test_loss:129.41444396972656\n",
      "1592/3000 train_loss: 71.56285095214844 test_loss:132.12249755859375\n",
      "1593/3000 train_loss: 78.36175537109375 test_loss:131.04220581054688\n",
      "1594/3000 train_loss: 77.80333709716797 test_loss:129.3686065673828\n",
      "1595/3000 train_loss: 88.4028549194336 test_loss:137.3897705078125\n",
      "1596/3000 train_loss: 76.71650695800781 test_loss:126.47474670410156\n",
      "1597/3000 train_loss: 81.08747863769531 test_loss:127.36663818359375\n",
      "1598/3000 train_loss: 75.1509017944336 test_loss:132.8223419189453\n",
      "1599/3000 train_loss: 83.03446960449219 test_loss:127.10369110107422\n",
      "1600/3000 train_loss: 75.9507827758789 test_loss:130.58291625976562\n",
      "1601/3000 train_loss: 79.07512664794922 test_loss:126.33914184570312\n",
      "1602/3000 train_loss: 73.77075958251953 test_loss:127.90489196777344\n",
      "1603/3000 train_loss: 95.6002197265625 test_loss:127.41380310058594\n",
      "1604/3000 train_loss: 103.439697265625 test_loss:131.722900390625\n",
      "1605/3000 train_loss: 91.79827880859375 test_loss:136.93194580078125\n",
      "1606/3000 train_loss: 78.06304931640625 test_loss:128.92579650878906\n",
      "1607/3000 train_loss: 77.30683135986328 test_loss:125.50715637207031\n",
      "1608/3000 train_loss: 77.86244201660156 test_loss:126.9756088256836\n",
      "1609/3000 train_loss: 76.33731842041016 test_loss:127.27787780761719\n",
      "1610/3000 train_loss: 68.96673583984375 test_loss:128.59786987304688\n",
      "1611/3000 train_loss: 80.75682067871094 test_loss:134.3430633544922\n",
      "1612/3000 train_loss: 76.18218231201172 test_loss:129.06280517578125\n",
      "1613/3000 train_loss: 88.21532440185547 test_loss:131.11827087402344\n",
      "1614/3000 train_loss: 70.13726043701172 test_loss:128.76901245117188\n",
      "1615/3000 train_loss: 82.4255142211914 test_loss:127.51629638671875\n",
      "1616/3000 train_loss: 86.20320892333984 test_loss:131.19285583496094\n",
      "1617/3000 train_loss: 84.36235809326172 test_loss:127.70236206054688\n",
      "1618/3000 train_loss: 103.67240905761719 test_loss:126.73847961425781\n",
      "1619/3000 train_loss: 76.74185180664062 test_loss:127.05278015136719\n",
      "1620/3000 train_loss: 78.49663543701172 test_loss:125.4417724609375\n",
      "1621/3000 train_loss: 75.0679931640625 test_loss:126.43254089355469\n",
      "1622/3000 train_loss: 71.0714340209961 test_loss:129.30126953125\n",
      "1623/3000 train_loss: 77.87539672851562 test_loss:131.98446655273438\n",
      "1624/3000 train_loss: 83.8453140258789 test_loss:129.87631225585938\n",
      "1625/3000 train_loss: 80.5359878540039 test_loss:130.19110107421875\n",
      "1626/3000 train_loss: 78.557373046875 test_loss:125.02925109863281\n",
      "1627/3000 train_loss: 88.20220184326172 test_loss:125.11796569824219\n",
      "1628/3000 train_loss: 79.69991302490234 test_loss:130.57144165039062\n",
      "1629/3000 train_loss: 78.35372924804688 test_loss:130.4252471923828\n",
      "1630/3000 train_loss: 78.44941711425781 test_loss:127.8682861328125\n",
      "1631/3000 train_loss: 75.83480834960938 test_loss:127.59181213378906\n",
      "1632/3000 train_loss: 79.78960418701172 test_loss:126.09954833984375\n",
      "1633/3000 train_loss: 77.62326049804688 test_loss:125.45797729492188\n",
      "1634/3000 train_loss: 82.4686508178711 test_loss:128.59072875976562\n",
      "1635/3000 train_loss: 82.17935943603516 test_loss:127.89776611328125\n",
      "1636/3000 train_loss: 86.56481170654297 test_loss:126.60884094238281\n",
      "1637/3000 train_loss: 82.58162689208984 test_loss:125.47392272949219\n",
      "1638/3000 train_loss: 94.48231506347656 test_loss:127.39767456054688\n",
      "1639/3000 train_loss: 79.9399642944336 test_loss:137.33595275878906\n",
      "1640/3000 train_loss: 85.13333129882812 test_loss:134.32167053222656\n",
      "1641/3000 train_loss: 82.89888763427734 test_loss:135.06837463378906\n",
      "1642/3000 train_loss: 80.26891326904297 test_loss:141.9935302734375\n",
      "1643/3000 train_loss: 75.65076446533203 test_loss:129.3223419189453\n",
      "1644/3000 train_loss: 84.23524475097656 test_loss:126.8493881225586\n",
      "1645/3000 train_loss: 71.95280456542969 test_loss:130.7947998046875\n",
      "1646/3000 train_loss: 76.35575103759766 test_loss:128.71058654785156\n",
      "1647/3000 train_loss: 84.1548843383789 test_loss:127.5633544921875\n",
      "1648/3000 train_loss: 78.05323028564453 test_loss:126.63912963867188\n",
      "1649/3000 train_loss: 76.81227111816406 test_loss:131.42030334472656\n",
      "1650/3000 train_loss: 97.22447967529297 test_loss:134.19248962402344\n",
      "1651/3000 train_loss: 78.89033508300781 test_loss:132.55906677246094\n",
      "1652/3000 train_loss: 79.55437469482422 test_loss:130.17681884765625\n",
      "1653/3000 train_loss: 73.88842010498047 test_loss:129.5261688232422\n",
      "1654/3000 train_loss: 77.92847442626953 test_loss:126.65144348144531\n",
      "1655/3000 train_loss: 84.15821838378906 test_loss:125.70753479003906\n",
      "1656/3000 train_loss: 87.37976837158203 test_loss:130.71566772460938\n",
      "1657/3000 train_loss: 75.41567993164062 test_loss:123.12749481201172\n",
      "1658/3000 train_loss: 78.83575439453125 test_loss:124.695556640625\n",
      "1659/3000 train_loss: 70.48448181152344 test_loss:122.31144714355469\n",
      "1660/3000 train_loss: 72.85321044921875 test_loss:123.39110565185547\n",
      "1661/3000 train_loss: 79.70567321777344 test_loss:123.90965270996094\n",
      "1662/3000 train_loss: 84.85566711425781 test_loss:124.83029174804688\n",
      "1663/3000 train_loss: 74.193603515625 test_loss:125.9814453125\n",
      "1664/3000 train_loss: 73.59224700927734 test_loss:126.60569763183594\n",
      "1665/3000 train_loss: 80.09687042236328 test_loss:125.76016235351562\n",
      "1666/3000 train_loss: 68.88777923583984 test_loss:123.72404479980469\n",
      "1667/3000 train_loss: 74.81438446044922 test_loss:122.99198913574219\n",
      "1668/3000 train_loss: 84.31768035888672 test_loss:122.75459289550781\n",
      "1669/3000 train_loss: 80.10851287841797 test_loss:122.00173950195312\n",
      "1670/3000 train_loss: 69.82562255859375 test_loss:123.21919250488281\n",
      "1671/3000 train_loss: 66.86939239501953 test_loss:129.2899627685547\n",
      "1672/3000 train_loss: 81.39795684814453 test_loss:127.28952026367188\n",
      "1673/3000 train_loss: 80.05016326904297 test_loss:129.2798309326172\n",
      "1674/3000 train_loss: 64.6203842163086 test_loss:128.28236389160156\n",
      "1675/3000 train_loss: 80.78410339355469 test_loss:130.7732696533203\n",
      "1676/3000 train_loss: 67.3934326171875 test_loss:124.91560363769531\n",
      "1677/3000 train_loss: 88.66765594482422 test_loss:125.61776733398438\n",
      "1678/3000 train_loss: 86.47096252441406 test_loss:129.63418579101562\n",
      "1679/3000 train_loss: 78.26030731201172 test_loss:122.44345092773438\n",
      "1680/3000 train_loss: 74.74607849121094 test_loss:121.90277099609375\n",
      "1681/3000 train_loss: 83.773681640625 test_loss:124.04878234863281\n",
      "1682/3000 train_loss: 78.03034973144531 test_loss:126.79158020019531\n",
      "1683/3000 train_loss: 71.66780090332031 test_loss:121.92535400390625\n",
      "1684/3000 train_loss: 67.57028198242188 test_loss:122.69900512695312\n",
      "1685/3000 train_loss: 73.98858642578125 test_loss:125.15065002441406\n",
      "1686/3000 train_loss: 85.86470794677734 test_loss:125.41612243652344\n",
      "1687/3000 train_loss: 67.71035766601562 test_loss:122.58920288085938\n",
      "1688/3000 train_loss: 74.79107666015625 test_loss:122.37024688720703\n",
      "1689/3000 train_loss: 100.7526626586914 test_loss:121.56986999511719\n",
      "1690/3000 train_loss: 77.97684478759766 test_loss:127.43534851074219\n",
      "1691/3000 train_loss: 75.13969421386719 test_loss:126.08013916015625\n",
      "1692/3000 train_loss: 77.4834976196289 test_loss:122.0726318359375\n",
      "1693/3000 train_loss: 75.7667007446289 test_loss:122.47068786621094\n",
      "1694/3000 train_loss: 79.3232650756836 test_loss:121.29969024658203\n",
      "1695/3000 train_loss: 73.70026397705078 test_loss:127.4036636352539\n",
      "1696/3000 train_loss: 89.25342559814453 test_loss:127.90158081054688\n",
      "1697/3000 train_loss: 71.24449920654297 test_loss:125.60688781738281\n",
      "1698/3000 train_loss: 65.3974380493164 test_loss:123.53076171875\n",
      "1699/3000 train_loss: 75.88114929199219 test_loss:123.23774719238281\n",
      "1700/3000 train_loss: 73.81716918945312 test_loss:124.53578186035156\n",
      "1701/3000 train_loss: 75.52764129638672 test_loss:126.64909362792969\n",
      "1702/3000 train_loss: 66.36293029785156 test_loss:126.32551574707031\n",
      "1703/3000 train_loss: 78.02652740478516 test_loss:125.5028076171875\n",
      "1704/3000 train_loss: 70.1292495727539 test_loss:130.52943420410156\n",
      "1705/3000 train_loss: 72.28094482421875 test_loss:124.14131164550781\n",
      "1706/3000 train_loss: 72.0192642211914 test_loss:123.04559326171875\n",
      "1707/3000 train_loss: 64.86701202392578 test_loss:122.98358154296875\n",
      "1708/3000 train_loss: 75.00397491455078 test_loss:123.51470947265625\n",
      "1709/3000 train_loss: 68.91426086425781 test_loss:124.15505981445312\n",
      "1710/3000 train_loss: 71.82447052001953 test_loss:129.86215209960938\n",
      "1711/3000 train_loss: 68.17738342285156 test_loss:120.56486511230469\n",
      "1712/3000 train_loss: 75.35504913330078 test_loss:122.00747680664062\n",
      "1713/3000 train_loss: 65.11048889160156 test_loss:122.40426635742188\n",
      "1714/3000 train_loss: 72.41001892089844 test_loss:120.74018096923828\n",
      "1715/3000 train_loss: 62.245758056640625 test_loss:125.23948669433594\n",
      "1716/3000 train_loss: 70.90624237060547 test_loss:123.14544677734375\n",
      "1717/3000 train_loss: 83.66129302978516 test_loss:125.01116943359375\n",
      "1718/3000 train_loss: 71.13630676269531 test_loss:121.9718017578125\n",
      "1719/3000 train_loss: 76.88050079345703 test_loss:124.879150390625\n",
      "1720/3000 train_loss: 66.26248168945312 test_loss:123.61320495605469\n",
      "1721/3000 train_loss: 67.07201385498047 test_loss:125.44976806640625\n",
      "1722/3000 train_loss: 79.02859497070312 test_loss:122.63160705566406\n",
      "1723/3000 train_loss: 66.70500183105469 test_loss:122.97807312011719\n",
      "1724/3000 train_loss: 71.72174835205078 test_loss:120.96328735351562\n",
      "1725/3000 train_loss: 72.91707611083984 test_loss:124.95150756835938\n",
      "1726/3000 train_loss: 75.35391998291016 test_loss:120.50558471679688\n",
      "1727/3000 train_loss: 73.44954681396484 test_loss:120.11126708984375\n",
      "1728/3000 train_loss: 74.16670989990234 test_loss:119.91928100585938\n",
      "1729/3000 train_loss: 64.00798034667969 test_loss:122.58985900878906\n",
      "1730/3000 train_loss: 65.07164001464844 test_loss:119.90454864501953\n",
      "1731/3000 train_loss: 63.48706817626953 test_loss:120.34695434570312\n",
      "1732/3000 train_loss: 79.52861022949219 test_loss:119.23298645019531\n",
      "1733/3000 train_loss: 76.75110626220703 test_loss:121.239990234375\n",
      "1734/3000 train_loss: 78.2347412109375 test_loss:122.16029357910156\n",
      "1735/3000 train_loss: 76.03545379638672 test_loss:121.36674499511719\n",
      "1736/3000 train_loss: 79.231689453125 test_loss:124.46916198730469\n",
      "1737/3000 train_loss: 72.69818878173828 test_loss:130.2383270263672\n",
      "1738/3000 train_loss: 78.92639923095703 test_loss:124.10071563720703\n",
      "1739/3000 train_loss: 72.30701446533203 test_loss:129.71746826171875\n",
      "1740/3000 train_loss: 70.1514663696289 test_loss:123.90150451660156\n",
      "1741/3000 train_loss: 69.20450592041016 test_loss:123.14895629882812\n",
      "1742/3000 train_loss: 70.98307800292969 test_loss:123.05418395996094\n",
      "1743/3000 train_loss: 73.00393676757812 test_loss:122.87779235839844\n",
      "1744/3000 train_loss: 70.16677856445312 test_loss:120.49520874023438\n",
      "1745/3000 train_loss: 83.46104431152344 test_loss:121.02267456054688\n",
      "1746/3000 train_loss: 74.06507873535156 test_loss:121.6910400390625\n",
      "1747/3000 train_loss: 75.75390625 test_loss:120.50274658203125\n",
      "1748/3000 train_loss: 67.2233657836914 test_loss:126.03570556640625\n",
      "1749/3000 train_loss: 71.6440658569336 test_loss:121.0709228515625\n",
      "1750/3000 train_loss: 68.44097137451172 test_loss:121.17547607421875\n",
      "1751/3000 train_loss: 70.79310607910156 test_loss:121.13243103027344\n",
      "1752/3000 train_loss: 91.5485610961914 test_loss:121.8988265991211\n",
      "1753/3000 train_loss: 71.76248168945312 test_loss:124.50686645507812\n",
      "1754/3000 train_loss: 71.87963104248047 test_loss:122.32960510253906\n",
      "1755/3000 train_loss: 79.26795959472656 test_loss:127.40252685546875\n",
      "1756/3000 train_loss: 70.37220764160156 test_loss:119.70933532714844\n",
      "1757/3000 train_loss: 61.79776382446289 test_loss:120.13851928710938\n",
      "1758/3000 train_loss: 78.61505889892578 test_loss:121.78482055664062\n",
      "1759/3000 train_loss: 67.34034729003906 test_loss:121.55715942382812\n",
      "1760/3000 train_loss: 82.6128158569336 test_loss:121.47003173828125\n",
      "1761/3000 train_loss: 81.06732177734375 test_loss:119.86036682128906\n",
      "1762/3000 train_loss: 67.38817596435547 test_loss:117.05601501464844\n",
      "1763/3000 train_loss: 64.40618896484375 test_loss:122.68719482421875\n",
      "1764/3000 train_loss: 79.10729217529297 test_loss:122.67198181152344\n",
      "1765/3000 train_loss: 79.1795883178711 test_loss:120.03031921386719\n",
      "1766/3000 train_loss: 61.642303466796875 test_loss:121.0318603515625\n",
      "1767/3000 train_loss: 74.95702362060547 test_loss:122.86466979980469\n",
      "1768/3000 train_loss: 73.1437759399414 test_loss:119.40385437011719\n",
      "1769/3000 train_loss: 80.24717712402344 test_loss:122.75924682617188\n",
      "1770/3000 train_loss: 76.29020690917969 test_loss:120.55503845214844\n",
      "1771/3000 train_loss: 71.64622497558594 test_loss:119.78176879882812\n",
      "1772/3000 train_loss: 78.23481750488281 test_loss:118.86962890625\n",
      "1773/3000 train_loss: 70.1773681640625 test_loss:122.41181945800781\n",
      "1774/3000 train_loss: 67.1015396118164 test_loss:124.12864685058594\n",
      "1775/3000 train_loss: 70.16035461425781 test_loss:123.38314819335938\n",
      "1776/3000 train_loss: 73.09785461425781 test_loss:120.76673889160156\n",
      "1777/3000 train_loss: 66.91615295410156 test_loss:123.1944580078125\n",
      "1778/3000 train_loss: 70.76080322265625 test_loss:119.2322998046875\n",
      "1779/3000 train_loss: 69.32227325439453 test_loss:123.05972290039062\n",
      "1780/3000 train_loss: 84.36542510986328 test_loss:120.02146911621094\n",
      "1781/3000 train_loss: 77.1063232421875 test_loss:121.61158752441406\n",
      "1782/3000 train_loss: 76.30333709716797 test_loss:123.46243286132812\n",
      "1783/3000 train_loss: 73.9838638305664 test_loss:118.16567993164062\n",
      "1784/3000 train_loss: 80.67263793945312 test_loss:116.91233825683594\n",
      "1785/3000 train_loss: 70.0342025756836 test_loss:120.89509582519531\n",
      "1786/3000 train_loss: 66.87005615234375 test_loss:119.16059875488281\n",
      "1787/3000 train_loss: 68.69623565673828 test_loss:118.92630004882812\n",
      "1788/3000 train_loss: 66.17693328857422 test_loss:118.80787658691406\n",
      "1789/3000 train_loss: 64.94290924072266 test_loss:117.2791748046875\n",
      "1790/3000 train_loss: 63.34324645996094 test_loss:118.23399353027344\n",
      "1791/3000 train_loss: 58.217803955078125 test_loss:118.779052734375\n",
      "1792/3000 train_loss: 74.31539154052734 test_loss:117.56396484375\n",
      "1793/3000 train_loss: 66.69597625732422 test_loss:119.42974853515625\n",
      "1794/3000 train_loss: 66.95086669921875 test_loss:119.1751708984375\n",
      "1795/3000 train_loss: 73.68914031982422 test_loss:124.14736938476562\n",
      "1796/3000 train_loss: 85.3961181640625 test_loss:130.30661010742188\n",
      "1797/3000 train_loss: 74.45774841308594 test_loss:123.04231262207031\n",
      "1798/3000 train_loss: 61.83943176269531 test_loss:124.00590515136719\n",
      "1799/3000 train_loss: 83.75422668457031 test_loss:122.82061767578125\n",
      "1800/3000 train_loss: 63.74869918823242 test_loss:124.53772735595703\n",
      "1801/3000 train_loss: 65.29488372802734 test_loss:121.38900756835938\n",
      "1802/3000 train_loss: 85.68882751464844 test_loss:121.99313354492188\n",
      "1803/3000 train_loss: 79.32553100585938 test_loss:124.81210327148438\n",
      "1804/3000 train_loss: 69.19823455810547 test_loss:120.06993103027344\n",
      "1805/3000 train_loss: 79.14793395996094 test_loss:118.89408874511719\n",
      "1806/3000 train_loss: 65.11427307128906 test_loss:123.78628540039062\n",
      "1807/3000 train_loss: 69.88890838623047 test_loss:118.28102111816406\n",
      "1808/3000 train_loss: 78.01791381835938 test_loss:121.23336791992188\n",
      "1809/3000 train_loss: 83.2969970703125 test_loss:118.32461547851562\n",
      "1810/3000 train_loss: 69.43145751953125 test_loss:118.88456726074219\n",
      "1811/3000 train_loss: 65.23986053466797 test_loss:123.53480529785156\n",
      "1812/3000 train_loss: 83.61233520507812 test_loss:124.27338409423828\n",
      "1813/3000 train_loss: 64.1529541015625 test_loss:120.63322448730469\n",
      "1814/3000 train_loss: 67.3277587890625 test_loss:119.14468383789062\n",
      "1815/3000 train_loss: 66.2324447631836 test_loss:116.73370361328125\n",
      "1816/3000 train_loss: 61.418739318847656 test_loss:118.45468139648438\n",
      "1817/3000 train_loss: 66.98957061767578 test_loss:118.39623260498047\n",
      "1818/3000 train_loss: 65.45121765136719 test_loss:117.6512451171875\n",
      "1819/3000 train_loss: 69.59330749511719 test_loss:115.74932098388672\n",
      "1820/3000 train_loss: 72.02195739746094 test_loss:121.749267578125\n",
      "1821/3000 train_loss: 83.65126037597656 test_loss:124.08088684082031\n",
      "1822/3000 train_loss: 80.88494873046875 test_loss:122.24234008789062\n",
      "1823/3000 train_loss: 59.44441223144531 test_loss:123.4771728515625\n",
      "1824/3000 train_loss: 75.34246826171875 test_loss:119.43592834472656\n",
      "1825/3000 train_loss: 67.6290283203125 test_loss:117.6981201171875\n",
      "1826/3000 train_loss: 66.57838439941406 test_loss:115.75506591796875\n",
      "1827/3000 train_loss: 76.91532897949219 test_loss:118.80438232421875\n",
      "1828/3000 train_loss: 64.93458557128906 test_loss:118.61471557617188\n",
      "1829/3000 train_loss: 72.03312683105469 test_loss:119.28713989257812\n",
      "1830/3000 train_loss: 79.97661590576172 test_loss:120.01533508300781\n",
      "1831/3000 train_loss: 68.73747253417969 test_loss:118.65818786621094\n",
      "1832/3000 train_loss: 60.574058532714844 test_loss:116.87968444824219\n",
      "1833/3000 train_loss: 68.5989990234375 test_loss:117.33712768554688\n",
      "1834/3000 train_loss: 71.0887451171875 test_loss:121.27317810058594\n",
      "1835/3000 train_loss: 66.33612060546875 test_loss:122.00589752197266\n",
      "1836/3000 train_loss: 71.07842254638672 test_loss:121.87718200683594\n",
      "1837/3000 train_loss: 68.63285064697266 test_loss:117.60688781738281\n",
      "1838/3000 train_loss: 62.550071716308594 test_loss:119.51362609863281\n",
      "1839/3000 train_loss: 81.27625274658203 test_loss:119.88267517089844\n",
      "1840/3000 train_loss: 75.41468811035156 test_loss:120.23114013671875\n",
      "1841/3000 train_loss: 67.00750732421875 test_loss:119.25083923339844\n",
      "1842/3000 train_loss: 60.718536376953125 test_loss:123.24832153320312\n",
      "1843/3000 train_loss: 76.78981018066406 test_loss:120.19219970703125\n",
      "1844/3000 train_loss: 68.61373138427734 test_loss:117.39488983154297\n",
      "1845/3000 train_loss: 65.9729232788086 test_loss:118.17767333984375\n",
      "1846/3000 train_loss: 75.91940307617188 test_loss:117.999755859375\n",
      "1847/3000 train_loss: 59.04975128173828 test_loss:114.69741821289062\n",
      "1848/3000 train_loss: 63.52729797363281 test_loss:118.42927551269531\n",
      "1849/3000 train_loss: 64.1579360961914 test_loss:119.22785949707031\n",
      "1850/3000 train_loss: 65.50006866455078 test_loss:118.52662658691406\n",
      "1851/3000 train_loss: 71.01246643066406 test_loss:116.76152038574219\n",
      "1852/3000 train_loss: 63.44968795776367 test_loss:114.97299194335938\n",
      "1853/3000 train_loss: 67.65379333496094 test_loss:113.42999267578125\n",
      "1854/3000 train_loss: 76.81332397460938 test_loss:113.94149780273438\n",
      "1855/3000 train_loss: 86.78963470458984 test_loss:118.70954895019531\n",
      "1856/3000 train_loss: 71.44609832763672 test_loss:119.98341369628906\n",
      "1857/3000 train_loss: 65.3221664428711 test_loss:119.58282470703125\n",
      "1858/3000 train_loss: 62.062583923339844 test_loss:120.57392883300781\n",
      "1859/3000 train_loss: 69.07595825195312 test_loss:118.01475524902344\n",
      "1860/3000 train_loss: 63.39303207397461 test_loss:114.80012512207031\n",
      "1861/3000 train_loss: 61.43516159057617 test_loss:116.3624496459961\n",
      "1862/3000 train_loss: 64.1061782836914 test_loss:117.96385192871094\n",
      "1863/3000 train_loss: 73.51826477050781 test_loss:119.26667785644531\n",
      "1864/3000 train_loss: 67.69857025146484 test_loss:120.37248992919922\n",
      "1865/3000 train_loss: 63.894187927246094 test_loss:120.65184020996094\n",
      "1866/3000 train_loss: 75.7904281616211 test_loss:117.09546661376953\n",
      "1867/3000 train_loss: 70.7464370727539 test_loss:113.72769927978516\n",
      "1868/3000 train_loss: 62.10858917236328 test_loss:115.24522399902344\n",
      "1869/3000 train_loss: 58.576045989990234 test_loss:118.70028686523438\n",
      "1870/3000 train_loss: 71.46648406982422 test_loss:118.16372680664062\n",
      "1871/3000 train_loss: 68.8009033203125 test_loss:118.58491516113281\n",
      "1872/3000 train_loss: 68.1817398071289 test_loss:119.44918823242188\n",
      "1873/3000 train_loss: 74.36949157714844 test_loss:114.12628173828125\n",
      "1874/3000 train_loss: 70.73487091064453 test_loss:119.99287414550781\n",
      "1875/3000 train_loss: 71.60484313964844 test_loss:116.76614379882812\n",
      "1876/3000 train_loss: 82.88119506835938 test_loss:119.49652099609375\n",
      "1877/3000 train_loss: 63.8660888671875 test_loss:115.02806091308594\n",
      "1878/3000 train_loss: 60.55226135253906 test_loss:116.07620239257812\n",
      "1879/3000 train_loss: 81.75843811035156 test_loss:122.10205078125\n",
      "1880/3000 train_loss: 77.13031005859375 test_loss:115.62136840820312\n",
      "1881/3000 train_loss: 59.97964859008789 test_loss:114.30625915527344\n",
      "1882/3000 train_loss: 80.68115234375 test_loss:115.30319213867188\n",
      "1883/3000 train_loss: 78.52667236328125 test_loss:116.56486511230469\n",
      "1884/3000 train_loss: 64.92227935791016 test_loss:118.42443084716797\n",
      "1885/3000 train_loss: 71.94031524658203 test_loss:115.1710205078125\n",
      "1886/3000 train_loss: 69.82888793945312 test_loss:116.16423034667969\n",
      "1887/3000 train_loss: 65.17375946044922 test_loss:114.92247009277344\n",
      "1888/3000 train_loss: 70.88331604003906 test_loss:115.83773803710938\n",
      "1889/3000 train_loss: 60.793800354003906 test_loss:118.30815124511719\n",
      "1890/3000 train_loss: 73.60740661621094 test_loss:118.52212524414062\n",
      "1891/3000 train_loss: 56.66086196899414 test_loss:117.91830444335938\n",
      "1892/3000 train_loss: 53.800636291503906 test_loss:112.85528564453125\n",
      "1893/3000 train_loss: 71.4632568359375 test_loss:114.5321044921875\n",
      "1894/3000 train_loss: 68.66106414794922 test_loss:116.37127685546875\n",
      "1895/3000 train_loss: 86.26781463623047 test_loss:111.76351928710938\n",
      "1896/3000 train_loss: 59.786842346191406 test_loss:111.43843078613281\n",
      "1897/3000 train_loss: 68.74290466308594 test_loss:115.27645874023438\n",
      "1898/3000 train_loss: 58.70563888549805 test_loss:115.15083312988281\n",
      "1899/3000 train_loss: 65.37966918945312 test_loss:121.25820922851562\n",
      "1900/3000 train_loss: 71.78125762939453 test_loss:112.42294311523438\n",
      "1901/3000 train_loss: 63.552490234375 test_loss:115.71990966796875\n",
      "1902/3000 train_loss: 63.36825180053711 test_loss:117.63880157470703\n",
      "1903/3000 train_loss: 65.23140716552734 test_loss:112.88301086425781\n",
      "1904/3000 train_loss: 70.258544921875 test_loss:115.22055053710938\n",
      "1905/3000 train_loss: 71.98760223388672 test_loss:113.66183471679688\n",
      "1906/3000 train_loss: 66.82022857666016 test_loss:113.29008483886719\n",
      "1907/3000 train_loss: 73.43677520751953 test_loss:113.40255737304688\n",
      "1908/3000 train_loss: 62.58006286621094 test_loss:118.86006164550781\n",
      "1909/3000 train_loss: 69.86407470703125 test_loss:113.6494140625\n",
      "1910/3000 train_loss: 65.5999755859375 test_loss:115.02632141113281\n",
      "1911/3000 train_loss: 68.34089660644531 test_loss:115.54939270019531\n",
      "1912/3000 train_loss: 65.10195922851562 test_loss:114.67716979980469\n",
      "1913/3000 train_loss: 77.84353637695312 test_loss:114.25334167480469\n",
      "1914/3000 train_loss: 65.18557739257812 test_loss:112.95343017578125\n",
      "1915/3000 train_loss: 70.03128814697266 test_loss:113.87062072753906\n",
      "1916/3000 train_loss: 70.94694519042969 test_loss:115.988525390625\n",
      "1917/3000 train_loss: 58.80385208129883 test_loss:114.65376281738281\n",
      "1918/3000 train_loss: 56.87568664550781 test_loss:114.32693481445312\n",
      "1919/3000 train_loss: 85.72283172607422 test_loss:114.74043273925781\n",
      "1920/3000 train_loss: 62.68966293334961 test_loss:116.76084899902344\n",
      "1921/3000 train_loss: 67.2184829711914 test_loss:114.14984130859375\n",
      "1922/3000 train_loss: 70.55308532714844 test_loss:111.13034057617188\n",
      "1923/3000 train_loss: 72.74687957763672 test_loss:112.12289428710938\n",
      "1924/3000 train_loss: 71.0064468383789 test_loss:112.98452758789062\n",
      "1925/3000 train_loss: 53.15068817138672 test_loss:115.26521301269531\n",
      "1926/3000 train_loss: 75.93574523925781 test_loss:113.96932220458984\n",
      "1927/3000 train_loss: 57.48246765136719 test_loss:113.24559020996094\n",
      "1928/3000 train_loss: 74.71244049072266 test_loss:117.06341552734375\n",
      "1929/3000 train_loss: 59.796043395996094 test_loss:114.56900024414062\n",
      "1930/3000 train_loss: 64.7411117553711 test_loss:115.52383422851562\n",
      "1931/3000 train_loss: 63.1118049621582 test_loss:116.6949462890625\n",
      "1932/3000 train_loss: 76.04449462890625 test_loss:113.98191833496094\n",
      "1933/3000 train_loss: 71.08451843261719 test_loss:112.1822509765625\n",
      "1934/3000 train_loss: 71.42527770996094 test_loss:110.7923583984375\n",
      "1935/3000 train_loss: 56.63193130493164 test_loss:112.5361328125\n",
      "1936/3000 train_loss: 62.323158264160156 test_loss:112.14566040039062\n",
      "1937/3000 train_loss: 64.87151336669922 test_loss:110.84132385253906\n",
      "1938/3000 train_loss: 70.5309066772461 test_loss:113.83055877685547\n",
      "1939/3000 train_loss: 73.06175994873047 test_loss:109.22431945800781\n",
      "1940/3000 train_loss: 66.55094909667969 test_loss:110.71304321289062\n",
      "1941/3000 train_loss: 54.387451171875 test_loss:113.66459655761719\n",
      "1942/3000 train_loss: 61.04148864746094 test_loss:112.87777709960938\n",
      "1943/3000 train_loss: 64.20320892333984 test_loss:114.074462890625\n",
      "1944/3000 train_loss: 62.34518051147461 test_loss:111.96044158935547\n",
      "1945/3000 train_loss: 64.5005111694336 test_loss:112.94985961914062\n",
      "1946/3000 train_loss: 65.2422103881836 test_loss:113.919677734375\n",
      "1947/3000 train_loss: 69.14236450195312 test_loss:119.28778839111328\n",
      "1948/3000 train_loss: 54.772499084472656 test_loss:116.14755249023438\n",
      "1949/3000 train_loss: 69.5506820678711 test_loss:113.09982299804688\n",
      "1950/3000 train_loss: 57.24149703979492 test_loss:111.76873016357422\n",
      "1951/3000 train_loss: 60.336204528808594 test_loss:113.90606689453125\n",
      "1952/3000 train_loss: 57.83012390136719 test_loss:114.8635482788086\n",
      "1953/3000 train_loss: 64.1942138671875 test_loss:115.2978286743164\n",
      "1954/3000 train_loss: 58.887516021728516 test_loss:114.18455505371094\n",
      "1955/3000 train_loss: 58.38412094116211 test_loss:113.32331848144531\n",
      "1956/3000 train_loss: 55.61090087890625 test_loss:111.98040008544922\n",
      "1957/3000 train_loss: 65.56683349609375 test_loss:110.16802215576172\n",
      "1958/3000 train_loss: 65.66215515136719 test_loss:106.68807983398438\n",
      "1959/3000 train_loss: 65.4520492553711 test_loss:110.26469421386719\n",
      "1960/3000 train_loss: 62.60485076904297 test_loss:115.17353820800781\n",
      "1961/3000 train_loss: 68.24600982666016 test_loss:116.47752380371094\n",
      "1962/3000 train_loss: 58.65397644042969 test_loss:113.03744506835938\n",
      "1963/3000 train_loss: 62.17961883544922 test_loss:114.88485717773438\n",
      "1964/3000 train_loss: 60.85504913330078 test_loss:115.56333923339844\n",
      "1965/3000 train_loss: 65.64701843261719 test_loss:117.84696960449219\n",
      "1966/3000 train_loss: 84.41619110107422 test_loss:111.6427001953125\n",
      "1967/3000 train_loss: 66.23430633544922 test_loss:118.51806640625\n",
      "1968/3000 train_loss: 72.83909606933594 test_loss:115.0985107421875\n",
      "1969/3000 train_loss: 65.36433410644531 test_loss:110.66936492919922\n",
      "1970/3000 train_loss: 60.66225814819336 test_loss:111.87680053710938\n",
      "1971/3000 train_loss: 75.14865112304688 test_loss:111.89973449707031\n",
      "1972/3000 train_loss: 63.87564468383789 test_loss:109.55455017089844\n",
      "1973/3000 train_loss: 60.445396423339844 test_loss:111.64836120605469\n",
      "1974/3000 train_loss: 60.20990753173828 test_loss:110.21450805664062\n",
      "1975/3000 train_loss: 69.45157623291016 test_loss:109.06829071044922\n",
      "1976/3000 train_loss: 64.20490264892578 test_loss:109.14739990234375\n",
      "1977/3000 train_loss: 62.90449142456055 test_loss:107.50283813476562\n",
      "1978/3000 train_loss: 67.42118072509766 test_loss:108.43016052246094\n",
      "1979/3000 train_loss: 58.25335693359375 test_loss:106.96745300292969\n",
      "1980/3000 train_loss: 56.36012268066406 test_loss:109.60281372070312\n",
      "1981/3000 train_loss: 58.27619171142578 test_loss:111.80633544921875\n",
      "1982/3000 train_loss: 61.41378402709961 test_loss:108.7158203125\n",
      "1983/3000 train_loss: 60.385711669921875 test_loss:108.88397216796875\n",
      "1984/3000 train_loss: 77.30037689208984 test_loss:109.87345886230469\n",
      "1985/3000 train_loss: 62.82360076904297 test_loss:110.37681579589844\n",
      "1986/3000 train_loss: 64.2711181640625 test_loss:112.75578308105469\n",
      "1987/3000 train_loss: 64.00865173339844 test_loss:110.52603149414062\n",
      "1988/3000 train_loss: 64.22163391113281 test_loss:110.34419250488281\n",
      "1989/3000 train_loss: 60.59303283691406 test_loss:110.32816314697266\n",
      "1990/3000 train_loss: 60.66529083251953 test_loss:110.44233703613281\n",
      "1991/3000 train_loss: 56.75751495361328 test_loss:107.87289428710938\n",
      "1992/3000 train_loss: 61.63621139526367 test_loss:107.74336242675781\n",
      "1993/3000 train_loss: 59.744834899902344 test_loss:108.32638549804688\n",
      "1994/3000 train_loss: 93.35569763183594 test_loss:107.30086517333984\n",
      "1995/3000 train_loss: 60.86163330078125 test_loss:106.84298706054688\n",
      "1996/3000 train_loss: 80.86831665039062 test_loss:109.80729675292969\n",
      "1997/3000 train_loss: 65.87358093261719 test_loss:110.96653747558594\n",
      "1998/3000 train_loss: 64.7796630859375 test_loss:109.47343444824219\n",
      "1999/3000 train_loss: 58.294734954833984 test_loss:108.77397155761719\n",
      "2000/3000 train_loss: 58.28847122192383 test_loss:110.64331817626953\n",
      "2001/3000 train_loss: 59.45537567138672 test_loss:109.75122833251953\n",
      "2002/3000 train_loss: 61.120216369628906 test_loss:111.88468170166016\n",
      "2003/3000 train_loss: 59.44407653808594 test_loss:109.98017883300781\n",
      "2004/3000 train_loss: 66.43269348144531 test_loss:108.98829650878906\n",
      "2005/3000 train_loss: 51.12651443481445 test_loss:109.36631774902344\n",
      "2006/3000 train_loss: 60.41257858276367 test_loss:108.29689025878906\n",
      "2007/3000 train_loss: 68.68777465820312 test_loss:107.98747253417969\n",
      "2008/3000 train_loss: 68.63037109375 test_loss:106.45689392089844\n",
      "2009/3000 train_loss: 61.26536560058594 test_loss:108.17044067382812\n",
      "2010/3000 train_loss: 55.070457458496094 test_loss:107.38522338867188\n",
      "2011/3000 train_loss: 59.029571533203125 test_loss:108.36138916015625\n",
      "2012/3000 train_loss: 63.4791259765625 test_loss:109.37054443359375\n",
      "2013/3000 train_loss: 60.28407669067383 test_loss:109.00624084472656\n",
      "2014/3000 train_loss: 68.15093231201172 test_loss:112.69375610351562\n",
      "2015/3000 train_loss: 56.31336975097656 test_loss:113.88784790039062\n",
      "2016/3000 train_loss: 53.39885330200195 test_loss:110.52562713623047\n",
      "2017/3000 train_loss: 56.08351135253906 test_loss:110.08165740966797\n",
      "2018/3000 train_loss: 64.4889907836914 test_loss:110.93563842773438\n",
      "2019/3000 train_loss: 63.771461486816406 test_loss:115.46315002441406\n",
      "2020/3000 train_loss: 62.35972595214844 test_loss:114.69014739990234\n",
      "2021/3000 train_loss: 59.3809928894043 test_loss:113.13096618652344\n",
      "2022/3000 train_loss: 74.21958923339844 test_loss:111.28865051269531\n",
      "2023/3000 train_loss: 66.33389282226562 test_loss:114.05073547363281\n",
      "2024/3000 train_loss: 71.41990661621094 test_loss:110.45341491699219\n",
      "2025/3000 train_loss: 55.945289611816406 test_loss:112.08416748046875\n",
      "2026/3000 train_loss: 57.12390899658203 test_loss:110.85472106933594\n",
      "2027/3000 train_loss: 60.81013107299805 test_loss:111.18116760253906\n",
      "2028/3000 train_loss: 62.47105407714844 test_loss:109.86085510253906\n",
      "2029/3000 train_loss: 68.45352172851562 test_loss:107.5257568359375\n",
      "2030/3000 train_loss: 60.16529846191406 test_loss:113.04707336425781\n",
      "2031/3000 train_loss: 66.86641693115234 test_loss:109.35291290283203\n",
      "2032/3000 train_loss: 68.60662078857422 test_loss:112.11717224121094\n",
      "2033/3000 train_loss: 60.67669677734375 test_loss:109.03291320800781\n",
      "2034/3000 train_loss: 67.20346069335938 test_loss:110.91891479492188\n",
      "2035/3000 train_loss: 61.96059799194336 test_loss:108.03631591796875\n",
      "2036/3000 train_loss: 64.04458618164062 test_loss:107.49830627441406\n",
      "2037/3000 train_loss: 74.76496887207031 test_loss:112.244384765625\n",
      "2038/3000 train_loss: 67.49857330322266 test_loss:110.01896667480469\n",
      "2039/3000 train_loss: 57.05031204223633 test_loss:111.81538391113281\n",
      "2040/3000 train_loss: 62.88399887084961 test_loss:116.67413330078125\n",
      "2041/3000 train_loss: 49.29248046875 test_loss:114.8770751953125\n",
      "2042/3000 train_loss: 67.13080596923828 test_loss:119.22384643554688\n",
      "2043/3000 train_loss: 61.006980895996094 test_loss:115.85728454589844\n",
      "2044/3000 train_loss: 54.20881652832031 test_loss:115.50839233398438\n",
      "2045/3000 train_loss: 58.91996765136719 test_loss:112.42044830322266\n",
      "2046/3000 train_loss: 62.10300827026367 test_loss:109.71562194824219\n",
      "2047/3000 train_loss: 60.62782287597656 test_loss:108.04315185546875\n",
      "2048/3000 train_loss: 70.968017578125 test_loss:108.42431640625\n",
      "2049/3000 train_loss: 56.79754638671875 test_loss:112.80813598632812\n",
      "2050/3000 train_loss: 59.31233215332031 test_loss:110.80956268310547\n",
      "2051/3000 train_loss: 61.59477233886719 test_loss:107.00554656982422\n",
      "2052/3000 train_loss: 59.71765899658203 test_loss:108.66813659667969\n",
      "2053/3000 train_loss: 70.31246185302734 test_loss:107.20987701416016\n",
      "2054/3000 train_loss: 75.26227569580078 test_loss:111.38185119628906\n",
      "2055/3000 train_loss: 60.83108139038086 test_loss:109.11119842529297\n",
      "2056/3000 train_loss: 60.10385513305664 test_loss:107.97413635253906\n",
      "2057/3000 train_loss: 60.68851852416992 test_loss:108.96949768066406\n",
      "2058/3000 train_loss: 61.65949249267578 test_loss:107.10247802734375\n",
      "2059/3000 train_loss: 62.14067840576172 test_loss:109.44081115722656\n",
      "2060/3000 train_loss: 61.40424728393555 test_loss:108.43373107910156\n",
      "2061/3000 train_loss: 58.685874938964844 test_loss:108.85106658935547\n",
      "2062/3000 train_loss: 58.80482864379883 test_loss:111.55834197998047\n",
      "2063/3000 train_loss: 64.14610290527344 test_loss:110.99957275390625\n",
      "2064/3000 train_loss: 58.12584686279297 test_loss:111.60198974609375\n",
      "2065/3000 train_loss: 58.17680358886719 test_loss:109.06136322021484\n",
      "2066/3000 train_loss: 68.26404571533203 test_loss:110.171630859375\n",
      "2067/3000 train_loss: 67.35673522949219 test_loss:109.41887664794922\n",
      "2068/3000 train_loss: 63.93914031982422 test_loss:107.74827575683594\n",
      "2069/3000 train_loss: 54.2613410949707 test_loss:104.98597717285156\n",
      "2070/3000 train_loss: 62.688140869140625 test_loss:109.01441955566406\n",
      "2071/3000 train_loss: 57.08989334106445 test_loss:108.460205078125\n",
      "2072/3000 train_loss: 63.92477798461914 test_loss:107.98835754394531\n",
      "2073/3000 train_loss: 60.82324981689453 test_loss:112.79708862304688\n",
      "2074/3000 train_loss: 60.806278228759766 test_loss:107.57386779785156\n",
      "2075/3000 train_loss: 74.11009979248047 test_loss:107.37031555175781\n",
      "2076/3000 train_loss: 50.93263244628906 test_loss:108.50395202636719\n",
      "2077/3000 train_loss: 54.16763687133789 test_loss:105.92231750488281\n",
      "2078/3000 train_loss: 57.76625061035156 test_loss:104.79029846191406\n",
      "2079/3000 train_loss: 63.03289794921875 test_loss:108.08587646484375\n",
      "2080/3000 train_loss: 54.97819519042969 test_loss:107.71903991699219\n",
      "2081/3000 train_loss: 58.54798889160156 test_loss:107.95236206054688\n",
      "2082/3000 train_loss: 59.87024688720703 test_loss:110.70042419433594\n",
      "2083/3000 train_loss: 61.16445541381836 test_loss:108.06307983398438\n",
      "2084/3000 train_loss: 51.9381217956543 test_loss:106.52696228027344\n",
      "2085/3000 train_loss: 58.95264434814453 test_loss:105.01080322265625\n",
      "2086/3000 train_loss: 58.82536315917969 test_loss:104.88330841064453\n",
      "2087/3000 train_loss: 72.87764739990234 test_loss:107.34170532226562\n",
      "2088/3000 train_loss: 70.3209228515625 test_loss:111.99630737304688\n",
      "2089/3000 train_loss: 59.5412712097168 test_loss:109.23870849609375\n",
      "2090/3000 train_loss: 65.32945251464844 test_loss:107.04293823242188\n",
      "2091/3000 train_loss: 68.07893371582031 test_loss:103.6800308227539\n",
      "2092/3000 train_loss: 57.4601936340332 test_loss:106.00093078613281\n",
      "2093/3000 train_loss: 65.04789733886719 test_loss:104.64178466796875\n",
      "2094/3000 train_loss: 71.61878204345703 test_loss:109.77261352539062\n",
      "2095/3000 train_loss: 52.44622039794922 test_loss:108.20095825195312\n",
      "2096/3000 train_loss: 54.41773986816406 test_loss:108.56700134277344\n",
      "2097/3000 train_loss: 57.78712463378906 test_loss:110.19552612304688\n",
      "2098/3000 train_loss: 63.62010192871094 test_loss:107.821044921875\n",
      "2099/3000 train_loss: 70.62332153320312 test_loss:109.2744140625\n",
      "2100/3000 train_loss: 63.053375244140625 test_loss:108.21855163574219\n",
      "2101/3000 train_loss: 56.894466400146484 test_loss:106.48789978027344\n",
      "2102/3000 train_loss: 52.168006896972656 test_loss:105.78642272949219\n",
      "2103/3000 train_loss: 54.60899353027344 test_loss:108.68695831298828\n",
      "2104/3000 train_loss: 57.443267822265625 test_loss:107.4557876586914\n",
      "2105/3000 train_loss: 67.11833190917969 test_loss:111.09963989257812\n",
      "2106/3000 train_loss: 55.05766296386719 test_loss:104.53268432617188\n",
      "2107/3000 train_loss: 72.07989501953125 test_loss:102.0593490600586\n",
      "2108/3000 train_loss: 61.266685485839844 test_loss:109.21293640136719\n",
      "2109/3000 train_loss: 59.279258728027344 test_loss:104.80442810058594\n",
      "2110/3000 train_loss: 60.88338851928711 test_loss:105.74049377441406\n",
      "2111/3000 train_loss: 68.35228729248047 test_loss:112.41085815429688\n",
      "2112/3000 train_loss: 53.50642776489258 test_loss:103.01724243164062\n",
      "2113/3000 train_loss: 58.746360778808594 test_loss:107.64620971679688\n",
      "2114/3000 train_loss: 54.672454833984375 test_loss:104.81730651855469\n",
      "2115/3000 train_loss: 67.1000747680664 test_loss:105.97742462158203\n",
      "2116/3000 train_loss: 50.8215446472168 test_loss:105.65501403808594\n",
      "2117/3000 train_loss: 60.05741882324219 test_loss:106.83357238769531\n",
      "2118/3000 train_loss: 55.137840270996094 test_loss:106.97344970703125\n",
      "2119/3000 train_loss: 57.28832244873047 test_loss:109.26667022705078\n",
      "2120/3000 train_loss: 63.46276092529297 test_loss:107.07855224609375\n",
      "2121/3000 train_loss: 62.68631362915039 test_loss:103.35128784179688\n",
      "2122/3000 train_loss: 63.69165802001953 test_loss:108.09797668457031\n",
      "2123/3000 train_loss: 56.071754455566406 test_loss:106.80270385742188\n",
      "2124/3000 train_loss: 53.65620803833008 test_loss:105.46415710449219\n",
      "2125/3000 train_loss: 56.13669967651367 test_loss:107.16683959960938\n",
      "2126/3000 train_loss: 56.406715393066406 test_loss:108.11943054199219\n",
      "2127/3000 train_loss: 62.53371810913086 test_loss:105.71823120117188\n",
      "2128/3000 train_loss: 65.99726104736328 test_loss:105.60641479492188\n",
      "2129/3000 train_loss: 57.35991668701172 test_loss:103.53792572021484\n",
      "2130/3000 train_loss: 62.376190185546875 test_loss:105.20309448242188\n",
      "2131/3000 train_loss: 65.96887969970703 test_loss:105.17008972167969\n",
      "2132/3000 train_loss: 63.738704681396484 test_loss:105.84915161132812\n",
      "2133/3000 train_loss: 58.27758026123047 test_loss:104.37835693359375\n",
      "2134/3000 train_loss: 47.983306884765625 test_loss:102.87196350097656\n",
      "2135/3000 train_loss: 66.14903259277344 test_loss:102.68124389648438\n",
      "2136/3000 train_loss: 61.51436996459961 test_loss:103.69084167480469\n",
      "2137/3000 train_loss: 48.91175079345703 test_loss:103.43125915527344\n",
      "2138/3000 train_loss: 53.39496612548828 test_loss:105.0766830444336\n",
      "2139/3000 train_loss: 57.32722854614258 test_loss:104.54171752929688\n",
      "2140/3000 train_loss: 68.06507873535156 test_loss:105.0849609375\n",
      "2141/3000 train_loss: 60.34823989868164 test_loss:108.3585433959961\n",
      "2142/3000 train_loss: 60.54804992675781 test_loss:107.78045654296875\n",
      "2143/3000 train_loss: 60.08766174316406 test_loss:105.30155944824219\n",
      "2144/3000 train_loss: 57.671443939208984 test_loss:105.20246887207031\n",
      "2145/3000 train_loss: 54.454227447509766 test_loss:106.15560913085938\n",
      "2146/3000 train_loss: 63.67509078979492 test_loss:109.0805435180664\n",
      "2147/3000 train_loss: 65.64682006835938 test_loss:104.87734985351562\n",
      "2148/3000 train_loss: 65.25193786621094 test_loss:105.88020324707031\n",
      "2149/3000 train_loss: 63.99754333496094 test_loss:106.48204040527344\n",
      "2150/3000 train_loss: 50.24472427368164 test_loss:107.73477172851562\n",
      "2151/3000 train_loss: 59.48197937011719 test_loss:105.06822204589844\n",
      "2152/3000 train_loss: 45.524986267089844 test_loss:104.87384033203125\n",
      "2153/3000 train_loss: 64.8850326538086 test_loss:105.07537841796875\n",
      "2154/3000 train_loss: 50.57782745361328 test_loss:102.33212280273438\n",
      "2155/3000 train_loss: 58.45232009887695 test_loss:101.2876968383789\n",
      "2156/3000 train_loss: 61.80463409423828 test_loss:102.92514038085938\n",
      "2157/3000 train_loss: 56.52997589111328 test_loss:104.29914855957031\n",
      "2158/3000 train_loss: 57.47908401489258 test_loss:103.97999572753906\n",
      "2159/3000 train_loss: 54.6597900390625 test_loss:103.27690124511719\n",
      "2160/3000 train_loss: 55.879547119140625 test_loss:103.10700988769531\n",
      "2161/3000 train_loss: 48.29294967651367 test_loss:107.93872833251953\n",
      "2162/3000 train_loss: 48.81079864501953 test_loss:104.9662094116211\n",
      "2163/3000 train_loss: 51.973453521728516 test_loss:105.41947937011719\n",
      "2164/3000 train_loss: 56.076622009277344 test_loss:104.80232238769531\n",
      "2165/3000 train_loss: 62.488250732421875 test_loss:109.8619384765625\n",
      "2166/3000 train_loss: 57.059688568115234 test_loss:105.95541381835938\n",
      "2167/3000 train_loss: 52.98725128173828 test_loss:107.39566040039062\n",
      "2168/3000 train_loss: 48.20399475097656 test_loss:102.75010681152344\n",
      "2169/3000 train_loss: 63.60143280029297 test_loss:104.14263153076172\n",
      "2170/3000 train_loss: 66.16777038574219 test_loss:108.17786407470703\n",
      "2171/3000 train_loss: 53.54182434082031 test_loss:104.73104858398438\n",
      "2172/3000 train_loss: 51.08201217651367 test_loss:101.19181823730469\n",
      "2173/3000 train_loss: 60.33918762207031 test_loss:104.28413391113281\n",
      "2174/3000 train_loss: 54.523048400878906 test_loss:101.216552734375\n",
      "2175/3000 train_loss: 50.5852165222168 test_loss:100.69102478027344\n",
      "2176/3000 train_loss: 46.95011520385742 test_loss:101.71463012695312\n",
      "2177/3000 train_loss: 56.02913284301758 test_loss:103.10195922851562\n",
      "2178/3000 train_loss: 57.45408248901367 test_loss:105.59912109375\n",
      "2179/3000 train_loss: 52.797279357910156 test_loss:103.2389907836914\n",
      "2180/3000 train_loss: 61.50932312011719 test_loss:102.29072570800781\n",
      "2181/3000 train_loss: 55.01724624633789 test_loss:102.40918731689453\n",
      "2182/3000 train_loss: 54.55499267578125 test_loss:101.11398315429688\n",
      "2183/3000 train_loss: 57.56736755371094 test_loss:104.59080505371094\n",
      "2184/3000 train_loss: 58.513816833496094 test_loss:102.91877746582031\n",
      "2185/3000 train_loss: 50.23942184448242 test_loss:103.79244232177734\n",
      "2186/3000 train_loss: 71.01707458496094 test_loss:103.24917602539062\n",
      "2187/3000 train_loss: 52.49162673950195 test_loss:101.16896057128906\n",
      "2188/3000 train_loss: 50.0423698425293 test_loss:102.74897003173828\n",
      "2189/3000 train_loss: 61.3659553527832 test_loss:100.84701538085938\n",
      "2190/3000 train_loss: 50.15489959716797 test_loss:101.5208740234375\n",
      "2191/3000 train_loss: 54.685264587402344 test_loss:101.9893798828125\n",
      "2192/3000 train_loss: 48.174537658691406 test_loss:102.737548828125\n",
      "2193/3000 train_loss: 54.244964599609375 test_loss:103.50953674316406\n",
      "2194/3000 train_loss: 60.48438262939453 test_loss:102.0274658203125\n",
      "2195/3000 train_loss: 58.031700134277344 test_loss:101.06723022460938\n",
      "2196/3000 train_loss: 57.32234191894531 test_loss:102.59281921386719\n",
      "2197/3000 train_loss: 69.11656951904297 test_loss:102.72981262207031\n",
      "2198/3000 train_loss: 53.85893249511719 test_loss:105.52850341796875\n",
      "2199/3000 train_loss: 60.06711196899414 test_loss:103.35400390625\n",
      "2200/3000 train_loss: 55.11131286621094 test_loss:100.2545166015625\n",
      "2201/3000 train_loss: 60.16089630126953 test_loss:99.43693542480469\n",
      "2202/3000 train_loss: 54.73968505859375 test_loss:100.84302520751953\n",
      "2203/3000 train_loss: 46.95305252075195 test_loss:100.64430236816406\n",
      "2204/3000 train_loss: 72.61930847167969 test_loss:102.51551818847656\n",
      "2205/3000 train_loss: 53.117313385009766 test_loss:98.95198059082031\n",
      "2206/3000 train_loss: 58.81806564331055 test_loss:101.00513458251953\n",
      "2207/3000 train_loss: 54.652610778808594 test_loss:99.23731994628906\n",
      "2208/3000 train_loss: 47.222103118896484 test_loss:104.850341796875\n",
      "2209/3000 train_loss: 60.69831466674805 test_loss:103.30319213867188\n",
      "2210/3000 train_loss: 56.618221282958984 test_loss:102.42610931396484\n",
      "2211/3000 train_loss: 52.083412170410156 test_loss:102.54566955566406\n",
      "2212/3000 train_loss: 54.00290298461914 test_loss:98.80540466308594\n",
      "2213/3000 train_loss: 52.72274398803711 test_loss:99.98712158203125\n",
      "2214/3000 train_loss: 54.707881927490234 test_loss:101.38645935058594\n",
      "2215/3000 train_loss: 49.422462463378906 test_loss:101.51358032226562\n",
      "2216/3000 train_loss: 53.17438507080078 test_loss:100.38886260986328\n",
      "2217/3000 train_loss: 53.80227279663086 test_loss:100.330322265625\n",
      "2218/3000 train_loss: 64.34121704101562 test_loss:101.18025207519531\n",
      "2219/3000 train_loss: 58.72341537475586 test_loss:100.77877807617188\n",
      "2220/3000 train_loss: 58.70938491821289 test_loss:99.39051818847656\n",
      "2221/3000 train_loss: 56.011314392089844 test_loss:99.6407470703125\n",
      "2222/3000 train_loss: 58.3791618347168 test_loss:100.57194519042969\n",
      "2223/3000 train_loss: 52.19282531738281 test_loss:100.44306945800781\n",
      "2224/3000 train_loss: 66.24896240234375 test_loss:101.06859588623047\n",
      "2225/3000 train_loss: 69.06720733642578 test_loss:100.84112548828125\n",
      "2226/3000 train_loss: 49.97056198120117 test_loss:100.4874038696289\n",
      "2227/3000 train_loss: 58.716697692871094 test_loss:100.74223327636719\n",
      "2228/3000 train_loss: 54.009071350097656 test_loss:101.02354431152344\n",
      "2229/3000 train_loss: 60.68677520751953 test_loss:98.07034301757812\n",
      "2230/3000 train_loss: 52.9262580871582 test_loss:96.88104248046875\n",
      "2231/3000 train_loss: 54.4674072265625 test_loss:97.47122192382812\n",
      "2232/3000 train_loss: 54.866554260253906 test_loss:98.95393371582031\n",
      "2233/3000 train_loss: 48.014549255371094 test_loss:97.68978881835938\n",
      "2234/3000 train_loss: 49.12565612792969 test_loss:96.82489013671875\n",
      "2235/3000 train_loss: 59.169517517089844 test_loss:98.9622802734375\n",
      "2236/3000 train_loss: 56.46115493774414 test_loss:99.63970184326172\n",
      "2237/3000 train_loss: 68.43370056152344 test_loss:100.76163482666016\n",
      "2238/3000 train_loss: 52.26457214355469 test_loss:97.82311248779297\n",
      "2239/3000 train_loss: 64.99297332763672 test_loss:99.14111328125\n",
      "2240/3000 train_loss: 56.6823844909668 test_loss:98.74009704589844\n",
      "2241/3000 train_loss: 63.55865478515625 test_loss:101.71417999267578\n",
      "2242/3000 train_loss: 63.14948272705078 test_loss:103.64277648925781\n",
      "2243/3000 train_loss: 64.14596557617188 test_loss:99.2777099609375\n",
      "2244/3000 train_loss: 49.67982864379883 test_loss:100.51602172851562\n",
      "2245/3000 train_loss: 56.26620101928711 test_loss:98.88148498535156\n",
      "2246/3000 train_loss: 45.261959075927734 test_loss:99.09721374511719\n",
      "2247/3000 train_loss: 48.50789260864258 test_loss:98.8396224975586\n",
      "2248/3000 train_loss: 51.4559326171875 test_loss:98.7186508178711\n",
      "2249/3000 train_loss: 56.869140625 test_loss:103.9788589477539\n",
      "2250/3000 train_loss: 55.94860076904297 test_loss:99.6026611328125\n",
      "2251/3000 train_loss: 56.11628723144531 test_loss:101.94398498535156\n",
      "2252/3000 train_loss: 60.625099182128906 test_loss:102.77880096435547\n",
      "2253/3000 train_loss: 50.729270935058594 test_loss:103.767578125\n",
      "2254/3000 train_loss: 51.85355758666992 test_loss:101.72625732421875\n",
      "2255/3000 train_loss: 55.38838195800781 test_loss:100.22795104980469\n",
      "2256/3000 train_loss: 51.67810821533203 test_loss:100.42051696777344\n",
      "2257/3000 train_loss: 49.62807083129883 test_loss:100.47720336914062\n",
      "2258/3000 train_loss: 50.15626525878906 test_loss:102.23599243164062\n",
      "2259/3000 train_loss: 47.141319274902344 test_loss:99.95783996582031\n",
      "2260/3000 train_loss: 50.57841873168945 test_loss:100.3145980834961\n",
      "2261/3000 train_loss: 55.16769790649414 test_loss:100.60677337646484\n",
      "2262/3000 train_loss: 55.72789764404297 test_loss:99.10841369628906\n",
      "2263/3000 train_loss: 68.24056243896484 test_loss:100.1578369140625\n",
      "2264/3000 train_loss: 56.15552520751953 test_loss:99.67910766601562\n",
      "2265/3000 train_loss: 55.379703521728516 test_loss:101.19906616210938\n",
      "2266/3000 train_loss: 59.16654968261719 test_loss:100.79632568359375\n",
      "2267/3000 train_loss: 52.37859344482422 test_loss:104.11335754394531\n",
      "2268/3000 train_loss: 53.531089782714844 test_loss:100.84489440917969\n",
      "2269/3000 train_loss: 61.5526123046875 test_loss:98.17037200927734\n",
      "2270/3000 train_loss: 57.192161560058594 test_loss:96.76366424560547\n",
      "2271/3000 train_loss: 55.166542053222656 test_loss:96.19863891601562\n",
      "2272/3000 train_loss: 58.622535705566406 test_loss:96.453369140625\n",
      "2273/3000 train_loss: 52.17264175415039 test_loss:95.97584533691406\n",
      "2274/3000 train_loss: 58.45821762084961 test_loss:98.67123413085938\n",
      "2275/3000 train_loss: 55.98289108276367 test_loss:98.43084716796875\n",
      "2276/3000 train_loss: 58.56383514404297 test_loss:99.0230712890625\n",
      "2277/3000 train_loss: 55.925331115722656 test_loss:96.31019592285156\n",
      "2278/3000 train_loss: 54.90565490722656 test_loss:98.15719604492188\n",
      "2279/3000 train_loss: 62.88132858276367 test_loss:100.20732116699219\n",
      "2280/3000 train_loss: 63.40636444091797 test_loss:99.26945495605469\n",
      "2281/3000 train_loss: 54.21528625488281 test_loss:100.97776794433594\n",
      "2282/3000 train_loss: 51.50562286376953 test_loss:99.05598449707031\n",
      "2283/3000 train_loss: 47.646244049072266 test_loss:97.9884033203125\n",
      "2284/3000 train_loss: 59.2794189453125 test_loss:100.18904113769531\n",
      "2285/3000 train_loss: 51.07754898071289 test_loss:98.60505676269531\n",
      "2286/3000 train_loss: 46.33240509033203 test_loss:98.56312561035156\n",
      "2287/3000 train_loss: 50.40448760986328 test_loss:100.93399047851562\n",
      "2288/3000 train_loss: 49.95547866821289 test_loss:102.56546020507812\n",
      "2289/3000 train_loss: 51.213890075683594 test_loss:100.60533142089844\n",
      "2290/3000 train_loss: 51.885719299316406 test_loss:98.47235870361328\n",
      "2291/3000 train_loss: 53.546024322509766 test_loss:97.39411926269531\n",
      "2292/3000 train_loss: 49.97411346435547 test_loss:96.09439849853516\n",
      "2293/3000 train_loss: 51.03863525390625 test_loss:98.4380874633789\n",
      "2294/3000 train_loss: 47.60145950317383 test_loss:97.43115997314453\n",
      "2295/3000 train_loss: 52.84426498413086 test_loss:98.95022583007812\n",
      "2296/3000 train_loss: 51.82596969604492 test_loss:97.49675750732422\n",
      "2297/3000 train_loss: 55.71062469482422 test_loss:98.30850982666016\n",
      "2298/3000 train_loss: 48.356483459472656 test_loss:97.35362243652344\n",
      "2299/3000 train_loss: 55.903076171875 test_loss:97.03447723388672\n",
      "2300/3000 train_loss: 48.61701965332031 test_loss:100.1720962524414\n",
      "2301/3000 train_loss: 52.078067779541016 test_loss:99.10877227783203\n",
      "2302/3000 train_loss: 59.24176025390625 test_loss:98.71280670166016\n",
      "2303/3000 train_loss: 51.583595275878906 test_loss:98.35401916503906\n",
      "2304/3000 train_loss: 46.273643493652344 test_loss:97.71379089355469\n",
      "2305/3000 train_loss: 52.472225189208984 test_loss:97.73966979980469\n",
      "2306/3000 train_loss: 45.980323791503906 test_loss:96.61028289794922\n",
      "2307/3000 train_loss: 53.78791427612305 test_loss:96.55860900878906\n",
      "2308/3000 train_loss: 49.67811965942383 test_loss:95.44837951660156\n",
      "2309/3000 train_loss: 53.593204498291016 test_loss:97.60952758789062\n",
      "2310/3000 train_loss: 50.260108947753906 test_loss:97.35185241699219\n",
      "2311/3000 train_loss: 54.05915451049805 test_loss:97.81636047363281\n",
      "2312/3000 train_loss: 50.911460876464844 test_loss:96.16423034667969\n",
      "2313/3000 train_loss: 47.414283752441406 test_loss:95.21784973144531\n",
      "2314/3000 train_loss: 50.50337600708008 test_loss:96.2205810546875\n",
      "2315/3000 train_loss: 48.34413146972656 test_loss:95.28839111328125\n",
      "2316/3000 train_loss: 54.89765930175781 test_loss:94.48963928222656\n",
      "2317/3000 train_loss: 50.97782516479492 test_loss:95.99645233154297\n",
      "2318/3000 train_loss: 56.177696228027344 test_loss:98.69631958007812\n",
      "2319/3000 train_loss: 69.1517333984375 test_loss:98.76063537597656\n",
      "2320/3000 train_loss: 53.47533416748047 test_loss:97.06819152832031\n",
      "2321/3000 train_loss: 49.458492279052734 test_loss:96.50213623046875\n",
      "2322/3000 train_loss: 57.75823211669922 test_loss:97.32318115234375\n",
      "2323/3000 train_loss: 50.69867706298828 test_loss:97.88491821289062\n",
      "2324/3000 train_loss: 53.4525260925293 test_loss:99.13722229003906\n",
      "2325/3000 train_loss: 42.52438735961914 test_loss:98.56452941894531\n",
      "2326/3000 train_loss: 49.7523078918457 test_loss:97.16947937011719\n",
      "2327/3000 train_loss: 46.679283142089844 test_loss:96.89788818359375\n",
      "2328/3000 train_loss: 53.46971893310547 test_loss:96.8363037109375\n",
      "2329/3000 train_loss: 49.459495544433594 test_loss:95.39569091796875\n",
      "2330/3000 train_loss: 60.44004821777344 test_loss:96.04733276367188\n",
      "2331/3000 train_loss: 62.486473083496094 test_loss:97.23680877685547\n",
      "2332/3000 train_loss: 57.66023635864258 test_loss:99.67373657226562\n",
      "2333/3000 train_loss: 51.63326644897461 test_loss:95.89665222167969\n",
      "2334/3000 train_loss: 50.85829162597656 test_loss:95.86184692382812\n",
      "2335/3000 train_loss: 56.24526596069336 test_loss:95.97295379638672\n",
      "2336/3000 train_loss: 45.15435791015625 test_loss:95.8477554321289\n",
      "2337/3000 train_loss: 58.93718719482422 test_loss:94.17158508300781\n",
      "2338/3000 train_loss: 48.65909194946289 test_loss:95.02459716796875\n",
      "2339/3000 train_loss: 48.97698211669922 test_loss:95.02938842773438\n",
      "2340/3000 train_loss: 53.547706604003906 test_loss:94.42446899414062\n",
      "2341/3000 train_loss: 54.67366027832031 test_loss:95.17521667480469\n",
      "2342/3000 train_loss: 57.9859619140625 test_loss:94.69798278808594\n",
      "2343/3000 train_loss: 56.0308723449707 test_loss:94.88046264648438\n",
      "2344/3000 train_loss: 48.54167175292969 test_loss:95.93771362304688\n",
      "2345/3000 train_loss: 45.79957580566406 test_loss:97.26050567626953\n",
      "2346/3000 train_loss: 49.57548904418945 test_loss:95.90996551513672\n",
      "2347/3000 train_loss: 52.559349060058594 test_loss:99.09715270996094\n",
      "2348/3000 train_loss: 61.78135299682617 test_loss:95.45513916015625\n",
      "2349/3000 train_loss: 57.92347717285156 test_loss:96.32803344726562\n",
      "2350/3000 train_loss: 49.105133056640625 test_loss:93.28709411621094\n",
      "2351/3000 train_loss: 51.184654235839844 test_loss:92.87269592285156\n",
      "2352/3000 train_loss: 51.85147476196289 test_loss:94.64696502685547\n",
      "2353/3000 train_loss: 63.65299987792969 test_loss:96.43888854980469\n",
      "2354/3000 train_loss: 63.469608306884766 test_loss:97.82132720947266\n",
      "2355/3000 train_loss: 52.47068405151367 test_loss:95.92585754394531\n",
      "2356/3000 train_loss: 50.68115234375 test_loss:96.18173217773438\n",
      "2357/3000 train_loss: 52.98933792114258 test_loss:96.6954345703125\n",
      "2358/3000 train_loss: 50.41988754272461 test_loss:97.20930480957031\n",
      "2359/3000 train_loss: 56.020381927490234 test_loss:97.68460083007812\n",
      "2360/3000 train_loss: 47.77629470825195 test_loss:94.46260070800781\n",
      "2361/3000 train_loss: 52.517860412597656 test_loss:93.89453125\n",
      "2362/3000 train_loss: 59.627357482910156 test_loss:95.69548797607422\n",
      "2363/3000 train_loss: 56.90502166748047 test_loss:95.14849853515625\n",
      "2364/3000 train_loss: 48.20418930053711 test_loss:94.86614990234375\n",
      "2365/3000 train_loss: 48.53929901123047 test_loss:93.65902709960938\n",
      "2366/3000 train_loss: 56.810943603515625 test_loss:95.76225280761719\n",
      "2367/3000 train_loss: 52.40381622314453 test_loss:96.38430786132812\n",
      "2368/3000 train_loss: 59.83671951293945 test_loss:95.4157485961914\n",
      "2369/3000 train_loss: 61.82609176635742 test_loss:97.5654067993164\n",
      "2370/3000 train_loss: 53.18034362792969 test_loss:97.85227966308594\n",
      "2371/3000 train_loss: 62.30731201171875 test_loss:94.11271667480469\n",
      "2372/3000 train_loss: 48.362213134765625 test_loss:93.70014953613281\n",
      "2373/3000 train_loss: 53.36585998535156 test_loss:95.55018615722656\n",
      "2374/3000 train_loss: 51.80449676513672 test_loss:93.55690002441406\n",
      "2375/3000 train_loss: 55.43119812011719 test_loss:95.169189453125\n",
      "2376/3000 train_loss: 54.226436614990234 test_loss:95.36866760253906\n",
      "2377/3000 train_loss: 62.34820556640625 test_loss:98.50770568847656\n",
      "2378/3000 train_loss: 50.190189361572266 test_loss:96.06758117675781\n",
      "2379/3000 train_loss: 48.18824005126953 test_loss:99.9066162109375\n",
      "2380/3000 train_loss: 59.060665130615234 test_loss:96.9061279296875\n",
      "2381/3000 train_loss: 54.362060546875 test_loss:95.10043334960938\n",
      "2382/3000 train_loss: 49.07030487060547 test_loss:93.89643859863281\n",
      "2383/3000 train_loss: 52.861392974853516 test_loss:92.60575866699219\n",
      "2384/3000 train_loss: 49.68730926513672 test_loss:94.19566345214844\n",
      "2385/3000 train_loss: 58.93380355834961 test_loss:97.94889831542969\n",
      "2386/3000 train_loss: 50.45530319213867 test_loss:94.86076354980469\n",
      "2387/3000 train_loss: 48.948631286621094 test_loss:93.75892639160156\n",
      "2388/3000 train_loss: 47.18202590942383 test_loss:93.24214172363281\n",
      "2389/3000 train_loss: 48.66372299194336 test_loss:92.96171569824219\n",
      "2390/3000 train_loss: 54.98678970336914 test_loss:93.63148498535156\n",
      "2391/3000 train_loss: 52.65705490112305 test_loss:95.19050598144531\n",
      "2392/3000 train_loss: 47.77375793457031 test_loss:97.28361511230469\n",
      "2393/3000 train_loss: 56.39809799194336 test_loss:95.68651580810547\n",
      "2394/3000 train_loss: 62.91858673095703 test_loss:96.94148254394531\n",
      "2395/3000 train_loss: 47.11992263793945 test_loss:94.9860610961914\n",
      "2396/3000 train_loss: 61.746646881103516 test_loss:95.68408203125\n",
      "2397/3000 train_loss: 47.3372802734375 test_loss:94.27056884765625\n",
      "2398/3000 train_loss: 52.2157096862793 test_loss:93.68122863769531\n",
      "2399/3000 train_loss: 48.078304290771484 test_loss:93.63655090332031\n",
      "2400/3000 train_loss: 54.852073669433594 test_loss:95.19416809082031\n",
      "2401/3000 train_loss: 46.51987838745117 test_loss:95.46142578125\n",
      "2402/3000 train_loss: 41.092247009277344 test_loss:92.88819885253906\n",
      "2403/3000 train_loss: 54.30992126464844 test_loss:94.03740692138672\n",
      "2404/3000 train_loss: 63.42521667480469 test_loss:94.28392028808594\n",
      "2405/3000 train_loss: 46.576759338378906 test_loss:93.50665283203125\n",
      "2406/3000 train_loss: 51.01996612548828 test_loss:93.25263977050781\n",
      "2407/3000 train_loss: 51.58980178833008 test_loss:94.11192321777344\n",
      "2408/3000 train_loss: 44.1109619140625 test_loss:95.73068237304688\n",
      "2409/3000 train_loss: 46.96652603149414 test_loss:94.81420135498047\n",
      "2410/3000 train_loss: 49.6300048828125 test_loss:92.51054382324219\n",
      "2411/3000 train_loss: 58.73191833496094 test_loss:95.28536224365234\n",
      "2412/3000 train_loss: 51.420997619628906 test_loss:97.90327453613281\n",
      "2413/3000 train_loss: 46.946990966796875 test_loss:97.152587890625\n",
      "2414/3000 train_loss: 47.91633224487305 test_loss:96.74826049804688\n",
      "2415/3000 train_loss: 50.46180725097656 test_loss:97.85169982910156\n",
      "2416/3000 train_loss: 58.40851593017578 test_loss:95.82081604003906\n",
      "2417/3000 train_loss: 52.465545654296875 test_loss:97.16888427734375\n",
      "2418/3000 train_loss: 51.587745666503906 test_loss:96.44688415527344\n",
      "2419/3000 train_loss: 43.262290954589844 test_loss:94.41394805908203\n",
      "2420/3000 train_loss: 47.871795654296875 test_loss:94.37527465820312\n",
      "2421/3000 train_loss: 49.83796310424805 test_loss:93.55598449707031\n",
      "2422/3000 train_loss: 48.393585205078125 test_loss:95.39265441894531\n",
      "2423/3000 train_loss: 48.25819396972656 test_loss:98.72362518310547\n",
      "2424/3000 train_loss: 54.47578430175781 test_loss:94.68745422363281\n",
      "2425/3000 train_loss: 49.33835983276367 test_loss:96.95858001708984\n",
      "2426/3000 train_loss: 49.400962829589844 test_loss:93.19135284423828\n",
      "2427/3000 train_loss: 68.22601318359375 test_loss:93.250244140625\n",
      "2428/3000 train_loss: 42.51628494262695 test_loss:94.30216979980469\n",
      "2429/3000 train_loss: 42.46308135986328 test_loss:94.94596862792969\n",
      "2430/3000 train_loss: 50.29258728027344 test_loss:96.36328125\n",
      "2431/3000 train_loss: 53.33149337768555 test_loss:98.60057067871094\n",
      "2432/3000 train_loss: 58.95170211791992 test_loss:94.78927612304688\n",
      "2433/3000 train_loss: 60.68547821044922 test_loss:97.05015563964844\n",
      "2434/3000 train_loss: 54.66093063354492 test_loss:97.65231323242188\n",
      "2435/3000 train_loss: 53.089622497558594 test_loss:94.00891876220703\n",
      "2436/3000 train_loss: 52.745948791503906 test_loss:97.64056396484375\n",
      "2437/3000 train_loss: 54.80797576904297 test_loss:100.33598327636719\n",
      "2438/3000 train_loss: 50.34233474731445 test_loss:94.87638092041016\n",
      "2439/3000 train_loss: 45.77799606323242 test_loss:93.78788757324219\n",
      "2440/3000 train_loss: 43.456417083740234 test_loss:95.12557983398438\n",
      "2441/3000 train_loss: 49.72999572753906 test_loss:89.55003356933594\n",
      "2442/3000 train_loss: 48.543975830078125 test_loss:90.20704650878906\n",
      "2443/3000 train_loss: 46.70686721801758 test_loss:91.42436218261719\n",
      "2444/3000 train_loss: 53.279197692871094 test_loss:92.1425552368164\n",
      "2445/3000 train_loss: 65.9871826171875 test_loss:90.78633117675781\n",
      "2446/3000 train_loss: 44.96028518676758 test_loss:92.01277160644531\n",
      "2447/3000 train_loss: 49.08051681518555 test_loss:91.63446807861328\n",
      "2448/3000 train_loss: 63.018157958984375 test_loss:92.82672119140625\n",
      "2449/3000 train_loss: 53.860843658447266 test_loss:91.32688903808594\n",
      "2450/3000 train_loss: 54.126182556152344 test_loss:90.7334976196289\n",
      "2451/3000 train_loss: 51.95138168334961 test_loss:92.48892974853516\n",
      "2452/3000 train_loss: 50.998836517333984 test_loss:92.55738067626953\n",
      "2453/3000 train_loss: 53.379188537597656 test_loss:90.94570922851562\n",
      "2454/3000 train_loss: 51.2349853515625 test_loss:94.04927062988281\n",
      "2455/3000 train_loss: 54.36688995361328 test_loss:91.57215881347656\n",
      "2456/3000 train_loss: 45.3558349609375 test_loss:91.45481872558594\n",
      "2457/3000 train_loss: 41.25077819824219 test_loss:90.63690185546875\n",
      "2458/3000 train_loss: 61.12535095214844 test_loss:91.92826843261719\n",
      "2459/3000 train_loss: 49.72496032714844 test_loss:91.37428283691406\n",
      "2460/3000 train_loss: 44.26805877685547 test_loss:92.96505737304688\n",
      "2461/3000 train_loss: 58.12376403808594 test_loss:100.23814392089844\n",
      "2462/3000 train_loss: 60.58231735229492 test_loss:104.72089385986328\n",
      "2463/3000 train_loss: 52.59790802001953 test_loss:97.29818725585938\n",
      "2464/3000 train_loss: 51.50286102294922 test_loss:92.31932067871094\n",
      "2465/3000 train_loss: 45.27524948120117 test_loss:92.79997253417969\n",
      "2466/3000 train_loss: 50.0128059387207 test_loss:92.58309936523438\n",
      "2467/3000 train_loss: 50.099666595458984 test_loss:91.29969787597656\n",
      "2468/3000 train_loss: 51.5760383605957 test_loss:91.13922119140625\n",
      "2469/3000 train_loss: 46.5970458984375 test_loss:92.51850891113281\n",
      "2470/3000 train_loss: 44.42703628540039 test_loss:91.59845733642578\n",
      "2471/3000 train_loss: 45.881675720214844 test_loss:92.0953369140625\n",
      "2472/3000 train_loss: 55.47382736206055 test_loss:92.75271606445312\n",
      "2473/3000 train_loss: 52.354209899902344 test_loss:93.13894653320312\n",
      "2474/3000 train_loss: 49.784080505371094 test_loss:91.67140197753906\n",
      "2475/3000 train_loss: 51.39082336425781 test_loss:90.7800064086914\n",
      "2476/3000 train_loss: 54.36574935913086 test_loss:91.11924743652344\n",
      "2477/3000 train_loss: 69.8141098022461 test_loss:91.95116424560547\n",
      "2478/3000 train_loss: 51.195457458496094 test_loss:91.70571899414062\n",
      "2479/3000 train_loss: 50.20331573486328 test_loss:92.50894165039062\n",
      "2480/3000 train_loss: 50.279869079589844 test_loss:95.57994842529297\n",
      "2481/3000 train_loss: 54.93980026245117 test_loss:96.2220458984375\n",
      "2482/3000 train_loss: 55.43426513671875 test_loss:92.2022705078125\n",
      "2483/3000 train_loss: 51.01286697387695 test_loss:92.15357971191406\n",
      "2484/3000 train_loss: 51.897727966308594 test_loss:92.74618530273438\n",
      "2485/3000 train_loss: 44.94932556152344 test_loss:94.06776428222656\n",
      "2486/3000 train_loss: 59.69725799560547 test_loss:104.44686889648438\n",
      "2487/3000 train_loss: 71.02376556396484 test_loss:126.7808837890625\n",
      "2488/3000 train_loss: 81.00691223144531 test_loss:119.68690490722656\n",
      "2489/3000 train_loss: 69.70706176757812 test_loss:114.71897888183594\n",
      "2490/3000 train_loss: 63.725685119628906 test_loss:107.25051879882812\n",
      "2491/3000 train_loss: 75.05845642089844 test_loss:104.01109313964844\n",
      "2492/3000 train_loss: 58.6527214050293 test_loss:102.68498229980469\n",
      "2493/3000 train_loss: 50.701316833496094 test_loss:99.80008697509766\n",
      "2494/3000 train_loss: 55.68293380737305 test_loss:96.6090087890625\n",
      "2495/3000 train_loss: 68.87782287597656 test_loss:97.66998291015625\n",
      "2496/3000 train_loss: 53.17556381225586 test_loss:97.26927947998047\n",
      "2497/3000 train_loss: 56.76554489135742 test_loss:95.65071105957031\n",
      "2498/3000 train_loss: 46.01422119140625 test_loss:96.19625854492188\n",
      "2499/3000 train_loss: 49.53201675415039 test_loss:94.83968353271484\n",
      "2500/3000 train_loss: 57.505882263183594 test_loss:95.51397705078125\n",
      "2501/3000 train_loss: 51.17719268798828 test_loss:95.33812713623047\n",
      "2502/3000 train_loss: 52.587066650390625 test_loss:95.89120483398438\n",
      "2503/3000 train_loss: 47.98732376098633 test_loss:94.26657104492188\n",
      "2504/3000 train_loss: 53.40688705444336 test_loss:94.78807067871094\n",
      "2505/3000 train_loss: 64.34475708007812 test_loss:94.4018325805664\n",
      "2506/3000 train_loss: 46.259498596191406 test_loss:95.45111083984375\n",
      "2507/3000 train_loss: 48.96303939819336 test_loss:97.1248779296875\n",
      "2508/3000 train_loss: 44.21078109741211 test_loss:94.27801513671875\n",
      "2509/3000 train_loss: 47.167518615722656 test_loss:93.58809661865234\n",
      "2510/3000 train_loss: 60.6982536315918 test_loss:96.37931823730469\n",
      "2511/3000 train_loss: 57.71564483642578 test_loss:98.06742858886719\n",
      "2512/3000 train_loss: 50.311798095703125 test_loss:98.50572204589844\n",
      "2513/3000 train_loss: 49.494781494140625 test_loss:94.15449523925781\n",
      "2514/3000 train_loss: 57.453399658203125 test_loss:93.42739868164062\n",
      "2515/3000 train_loss: 58.032108306884766 test_loss:93.48002624511719\n",
      "2516/3000 train_loss: 74.04346466064453 test_loss:94.93326568603516\n",
      "2517/3000 train_loss: 49.435420989990234 test_loss:97.69645690917969\n",
      "2518/3000 train_loss: 52.025596618652344 test_loss:95.8726806640625\n",
      "2519/3000 train_loss: 49.062103271484375 test_loss:95.36845397949219\n",
      "2520/3000 train_loss: 49.92200469970703 test_loss:93.5576171875\n",
      "2521/3000 train_loss: 58.61579895019531 test_loss:96.48502349853516\n",
      "2522/3000 train_loss: 44.53823471069336 test_loss:93.7208251953125\n",
      "2523/3000 train_loss: 56.67302322387695 test_loss:93.1268310546875\n",
      "2524/3000 train_loss: 63.90862274169922 test_loss:93.04122161865234\n",
      "2525/3000 train_loss: 49.250118255615234 test_loss:91.0211410522461\n",
      "2526/3000 train_loss: 51.93492126464844 test_loss:92.73297119140625\n",
      "2527/3000 train_loss: 56.32250213623047 test_loss:93.63907623291016\n",
      "2528/3000 train_loss: 49.983551025390625 test_loss:96.50209045410156\n",
      "2529/3000 train_loss: 47.350868225097656 test_loss:94.60308837890625\n",
      "2530/3000 train_loss: 41.583221435546875 test_loss:92.65291595458984\n",
      "2531/3000 train_loss: 45.158164978027344 test_loss:91.50808715820312\n",
      "2532/3000 train_loss: 48.55501937866211 test_loss:91.80718231201172\n",
      "2533/3000 train_loss: 60.05335235595703 test_loss:92.30618286132812\n",
      "2534/3000 train_loss: 51.439178466796875 test_loss:93.18769073486328\n",
      "2535/3000 train_loss: 50.106414794921875 test_loss:93.97917175292969\n",
      "2536/3000 train_loss: 52.75226593017578 test_loss:94.77822875976562\n",
      "2537/3000 train_loss: 54.99833679199219 test_loss:92.99781799316406\n",
      "2538/3000 train_loss: 53.97259521484375 test_loss:90.65200805664062\n",
      "2539/3000 train_loss: 47.42472457885742 test_loss:93.68251037597656\n",
      "2540/3000 train_loss: 60.10263442993164 test_loss:93.72187805175781\n",
      "2541/3000 train_loss: 59.17874526977539 test_loss:97.03506469726562\n",
      "2542/3000 train_loss: 54.318443298339844 test_loss:93.29234313964844\n",
      "2543/3000 train_loss: 58.42086410522461 test_loss:93.99105834960938\n",
      "2544/3000 train_loss: 52.243568420410156 test_loss:94.96443176269531\n",
      "2545/3000 train_loss: 49.39651107788086 test_loss:93.17964172363281\n",
      "2546/3000 train_loss: 46.907142639160156 test_loss:94.93684387207031\n",
      "2547/3000 train_loss: 43.76705551147461 test_loss:92.39230346679688\n",
      "2548/3000 train_loss: 42.68400573730469 test_loss:93.92301940917969\n",
      "2549/3000 train_loss: 45.58036422729492 test_loss:93.89260864257812\n",
      "2550/3000 train_loss: 46.64830017089844 test_loss:94.67987060546875\n",
      "2551/3000 train_loss: 46.592960357666016 test_loss:92.66755676269531\n",
      "2552/3000 train_loss: 55.281288146972656 test_loss:91.18760681152344\n",
      "2553/3000 train_loss: 47.98619079589844 test_loss:91.64733123779297\n",
      "2554/3000 train_loss: 56.82584762573242 test_loss:93.03359985351562\n",
      "2555/3000 train_loss: 55.17686080932617 test_loss:94.05642700195312\n",
      "2556/3000 train_loss: 41.9860954284668 test_loss:93.00154113769531\n",
      "2557/3000 train_loss: 44.39828872680664 test_loss:90.04853057861328\n",
      "2558/3000 train_loss: 70.67296600341797 test_loss:90.06524658203125\n",
      "2559/3000 train_loss: 48.70621109008789 test_loss:93.60609436035156\n",
      "2560/3000 train_loss: 49.195640563964844 test_loss:92.54658508300781\n",
      "2561/3000 train_loss: 48.22513961791992 test_loss:90.92716979980469\n",
      "2562/3000 train_loss: 50.448158264160156 test_loss:90.14279174804688\n",
      "2563/3000 train_loss: 53.72608184814453 test_loss:89.87030029296875\n",
      "2564/3000 train_loss: 44.345314025878906 test_loss:90.01309204101562\n",
      "2565/3000 train_loss: 41.79774856567383 test_loss:90.13447570800781\n",
      "2566/3000 train_loss: 45.92547607421875 test_loss:91.78382873535156\n",
      "2567/3000 train_loss: 52.594703674316406 test_loss:91.84947967529297\n",
      "2568/3000 train_loss: 54.535057067871094 test_loss:91.1619873046875\n",
      "2569/3000 train_loss: 62.58991622924805 test_loss:91.20761108398438\n",
      "2570/3000 train_loss: 46.732643127441406 test_loss:93.15322875976562\n",
      "2571/3000 train_loss: 41.81707000732422 test_loss:93.41593933105469\n",
      "2572/3000 train_loss: 67.4583511352539 test_loss:92.185791015625\n",
      "2573/3000 train_loss: 45.59866714477539 test_loss:91.14535522460938\n",
      "2574/3000 train_loss: 47.163841247558594 test_loss:89.90135192871094\n",
      "2575/3000 train_loss: 42.069976806640625 test_loss:90.24705505371094\n",
      "2576/3000 train_loss: 50.0888786315918 test_loss:92.61515808105469\n",
      "2577/3000 train_loss: 48.76514434814453 test_loss:93.29634094238281\n",
      "2578/3000 train_loss: 60.58308410644531 test_loss:92.65974426269531\n",
      "2579/3000 train_loss: 49.71510314941406 test_loss:91.6114273071289\n",
      "2580/3000 train_loss: 48.76814270019531 test_loss:92.06248474121094\n",
      "2581/3000 train_loss: 44.98065948486328 test_loss:91.30433654785156\n",
      "2582/3000 train_loss: 42.308101654052734 test_loss:92.76451873779297\n",
      "2583/3000 train_loss: 53.78655242919922 test_loss:93.58695983886719\n",
      "2584/3000 train_loss: 43.56045913696289 test_loss:93.21330261230469\n",
      "2585/3000 train_loss: 40.97882080078125 test_loss:91.67088317871094\n",
      "2586/3000 train_loss: 56.62104034423828 test_loss:91.72303771972656\n",
      "2587/3000 train_loss: 49.42263412475586 test_loss:92.07549285888672\n",
      "2588/3000 train_loss: 55.87251281738281 test_loss:92.821044921875\n",
      "2589/3000 train_loss: 50.62506103515625 test_loss:92.05882263183594\n",
      "2590/3000 train_loss: 50.01560592651367 test_loss:91.54696655273438\n",
      "2591/3000 train_loss: 49.198238372802734 test_loss:92.30364990234375\n",
      "2592/3000 train_loss: 46.759910583496094 test_loss:89.52462768554688\n",
      "2593/3000 train_loss: 43.31401062011719 test_loss:88.77293395996094\n",
      "2594/3000 train_loss: 45.79758834838867 test_loss:88.46707153320312\n",
      "2595/3000 train_loss: 55.62875747680664 test_loss:88.98106384277344\n",
      "2596/3000 train_loss: 44.12456512451172 test_loss:87.88349914550781\n",
      "2597/3000 train_loss: 44.13529968261719 test_loss:88.79753112792969\n",
      "2598/3000 train_loss: 48.022117614746094 test_loss:88.42001342773438\n",
      "2599/3000 train_loss: 51.40364456176758 test_loss:91.70845794677734\n",
      "2600/3000 train_loss: 42.5179443359375 test_loss:90.15261840820312\n",
      "2601/3000 train_loss: 52.563297271728516 test_loss:90.55271911621094\n",
      "2602/3000 train_loss: 48.04381561279297 test_loss:89.65289306640625\n",
      "2603/3000 train_loss: 57.377655029296875 test_loss:90.36711883544922\n",
      "2604/3000 train_loss: 49.472835540771484 test_loss:93.98918151855469\n",
      "2605/3000 train_loss: 48.973777770996094 test_loss:93.36170959472656\n",
      "2606/3000 train_loss: 55.103126525878906 test_loss:90.88478088378906\n",
      "2607/3000 train_loss: 47.496337890625 test_loss:91.4735107421875\n",
      "2608/3000 train_loss: 50.03887939453125 test_loss:92.2486572265625\n",
      "2609/3000 train_loss: 52.71601486206055 test_loss:92.91006469726562\n",
      "2610/3000 train_loss: 43.46635437011719 test_loss:90.10242462158203\n",
      "2611/3000 train_loss: 53.34748840332031 test_loss:90.22394561767578\n",
      "2612/3000 train_loss: 52.10494613647461 test_loss:90.73870849609375\n",
      "2613/3000 train_loss: 40.80216598510742 test_loss:89.48575592041016\n",
      "2614/3000 train_loss: 43.61362075805664 test_loss:89.86087036132812\n",
      "2615/3000 train_loss: 63.5829963684082 test_loss:90.07579040527344\n",
      "2616/3000 train_loss: 51.12957763671875 test_loss:88.54585266113281\n",
      "2617/3000 train_loss: 41.949493408203125 test_loss:87.7972640991211\n",
      "2618/3000 train_loss: 45.97939682006836 test_loss:88.1759262084961\n",
      "2619/3000 train_loss: 45.83567810058594 test_loss:90.94549560546875\n",
      "2620/3000 train_loss: 52.68975067138672 test_loss:89.91783142089844\n",
      "2621/3000 train_loss: 48.93351745605469 test_loss:90.69618225097656\n",
      "2622/3000 train_loss: 51.35121154785156 test_loss:87.9066162109375\n",
      "2623/3000 train_loss: 47.05136489868164 test_loss:91.10137939453125\n",
      "2624/3000 train_loss: 49.24540710449219 test_loss:91.11006164550781\n",
      "2625/3000 train_loss: 41.11398696899414 test_loss:90.6024398803711\n",
      "2626/3000 train_loss: 63.00102996826172 test_loss:89.02605438232422\n",
      "2627/3000 train_loss: 47.818607330322266 test_loss:89.6705322265625\n",
      "2628/3000 train_loss: 51.18924331665039 test_loss:91.88407135009766\n",
      "2629/3000 train_loss: 49.087249755859375 test_loss:90.24697875976562\n",
      "2630/3000 train_loss: 45.98896026611328 test_loss:93.46583557128906\n",
      "2631/3000 train_loss: 50.65830993652344 test_loss:87.92074584960938\n",
      "2632/3000 train_loss: 49.002685546875 test_loss:90.48373413085938\n",
      "2633/3000 train_loss: 49.84859848022461 test_loss:90.57734680175781\n",
      "2634/3000 train_loss: 54.448707580566406 test_loss:91.15934753417969\n",
      "2635/3000 train_loss: 42.0970458984375 test_loss:90.7161865234375\n",
      "2636/3000 train_loss: 56.36342239379883 test_loss:89.84380340576172\n",
      "2637/3000 train_loss: 41.52580642700195 test_loss:88.66968536376953\n",
      "2638/3000 train_loss: 50.825035095214844 test_loss:89.49302673339844\n",
      "2639/3000 train_loss: 41.71339416503906 test_loss:89.74484252929688\n",
      "2640/3000 train_loss: 45.556365966796875 test_loss:91.42182922363281\n",
      "2641/3000 train_loss: 51.81798553466797 test_loss:91.435791015625\n",
      "2642/3000 train_loss: 53.955108642578125 test_loss:91.86183166503906\n",
      "2643/3000 train_loss: 61.8126335144043 test_loss:91.22705841064453\n",
      "2644/3000 train_loss: 49.13328552246094 test_loss:88.41049194335938\n",
      "2645/3000 train_loss: 50.838687896728516 test_loss:88.24635314941406\n",
      "2646/3000 train_loss: 50.07117462158203 test_loss:90.01606750488281\n",
      "2647/3000 train_loss: 48.93357849121094 test_loss:88.07203674316406\n",
      "2648/3000 train_loss: 53.242191314697266 test_loss:87.3391342163086\n",
      "2649/3000 train_loss: 43.614078521728516 test_loss:89.76876831054688\n",
      "2650/3000 train_loss: 44.72807312011719 test_loss:89.81280517578125\n",
      "2651/3000 train_loss: 46.9313850402832 test_loss:91.28677368164062\n",
      "2652/3000 train_loss: 49.24512481689453 test_loss:88.41896057128906\n",
      "2653/3000 train_loss: 41.807674407958984 test_loss:89.97602081298828\n",
      "2654/3000 train_loss: 53.12053298950195 test_loss:88.10032653808594\n",
      "2655/3000 train_loss: 40.798580169677734 test_loss:88.48212432861328\n",
      "2656/3000 train_loss: 46.14381408691406 test_loss:88.0899658203125\n",
      "2657/3000 train_loss: 46.64623260498047 test_loss:87.77175903320312\n",
      "2658/3000 train_loss: 42.10614776611328 test_loss:90.61332702636719\n",
      "2659/3000 train_loss: 50.703433990478516 test_loss:89.7886734008789\n",
      "2660/3000 train_loss: 71.7359848022461 test_loss:93.5476303100586\n",
      "2661/3000 train_loss: 50.416656494140625 test_loss:89.37425231933594\n",
      "2662/3000 train_loss: 41.56189727783203 test_loss:92.51211547851562\n",
      "2663/3000 train_loss: 54.98272705078125 test_loss:89.33050537109375\n",
      "2664/3000 train_loss: 49.793785095214844 test_loss:89.50439453125\n",
      "2665/3000 train_loss: 45.62376403808594 test_loss:88.08219909667969\n",
      "2666/3000 train_loss: 45.969600677490234 test_loss:88.89054870605469\n",
      "2667/3000 train_loss: 52.134490966796875 test_loss:86.25906372070312\n",
      "2668/3000 train_loss: 46.526344299316406 test_loss:89.049560546875\n",
      "2669/3000 train_loss: 49.44188690185547 test_loss:89.99939727783203\n",
      "2670/3000 train_loss: 44.980316162109375 test_loss:87.86971282958984\n",
      "2671/3000 train_loss: 50.38902282714844 test_loss:86.88848114013672\n",
      "2672/3000 train_loss: 50.00490951538086 test_loss:87.91609954833984\n",
      "2673/3000 train_loss: 49.19480514526367 test_loss:87.21821594238281\n",
      "2674/3000 train_loss: 45.84592056274414 test_loss:88.26704406738281\n",
      "2675/3000 train_loss: 44.8455810546875 test_loss:87.07212829589844\n",
      "2676/3000 train_loss: 40.749271392822266 test_loss:87.85943603515625\n",
      "2677/3000 train_loss: 45.91372299194336 test_loss:87.31353759765625\n",
      "2678/3000 train_loss: 42.480804443359375 test_loss:87.65827941894531\n",
      "2679/3000 train_loss: 49.417030334472656 test_loss:87.3880615234375\n",
      "2680/3000 train_loss: 46.869293212890625 test_loss:89.65425109863281\n",
      "2681/3000 train_loss: 40.50940704345703 test_loss:88.21833801269531\n",
      "2682/3000 train_loss: 36.72513961791992 test_loss:90.07025146484375\n",
      "2683/3000 train_loss: 39.92387008666992 test_loss:87.48780822753906\n",
      "2684/3000 train_loss: 52.96910858154297 test_loss:88.36369323730469\n",
      "2685/3000 train_loss: 43.782684326171875 test_loss:87.84527587890625\n",
      "2686/3000 train_loss: 45.02790069580078 test_loss:90.27896118164062\n",
      "2687/3000 train_loss: 58.99903869628906 test_loss:90.363037109375\n",
      "2688/3000 train_loss: 37.65308380126953 test_loss:89.76254272460938\n",
      "2689/3000 train_loss: 43.10853576660156 test_loss:89.53471374511719\n",
      "2690/3000 train_loss: 42.78854751586914 test_loss:87.9272689819336\n",
      "2691/3000 train_loss: 48.06256103515625 test_loss:86.42694091796875\n",
      "2692/3000 train_loss: 49.18428039550781 test_loss:88.83373260498047\n",
      "2693/3000 train_loss: 39.32247543334961 test_loss:90.07679748535156\n",
      "2694/3000 train_loss: 42.664878845214844 test_loss:89.66810607910156\n",
      "2695/3000 train_loss: 61.92036819458008 test_loss:88.20943450927734\n",
      "2696/3000 train_loss: 43.50565719604492 test_loss:88.9134292602539\n",
      "2697/3000 train_loss: 52.15909194946289 test_loss:88.54137420654297\n",
      "2698/3000 train_loss: 41.218692779541016 test_loss:90.93186950683594\n",
      "2699/3000 train_loss: 56.509098052978516 test_loss:91.25985717773438\n",
      "2700/3000 train_loss: 54.426368713378906 test_loss:90.42758178710938\n",
      "2701/3000 train_loss: 44.01490783691406 test_loss:90.6752700805664\n",
      "2702/3000 train_loss: 43.056053161621094 test_loss:90.72792053222656\n",
      "2703/3000 train_loss: 58.57899475097656 test_loss:88.880859375\n",
      "2704/3000 train_loss: 42.195369720458984 test_loss:92.09785461425781\n",
      "2705/3000 train_loss: 51.43950653076172 test_loss:90.85903930664062\n",
      "2706/3000 train_loss: 48.38855743408203 test_loss:95.59242248535156\n",
      "2707/3000 train_loss: 62.3495979309082 test_loss:91.76563262939453\n",
      "2708/3000 train_loss: 49.370872497558594 test_loss:91.05703735351562\n",
      "2709/3000 train_loss: 56.01427459716797 test_loss:92.83403015136719\n",
      "2710/3000 train_loss: 44.32666015625 test_loss:90.47015380859375\n",
      "2711/3000 train_loss: 57.51797866821289 test_loss:90.28269958496094\n",
      "2712/3000 train_loss: 48.850894927978516 test_loss:89.37684631347656\n",
      "2713/3000 train_loss: 47.7822151184082 test_loss:89.309326171875\n",
      "2714/3000 train_loss: 38.58026123046875 test_loss:88.47001647949219\n",
      "2715/3000 train_loss: 43.796939849853516 test_loss:89.86685180664062\n",
      "2716/3000 train_loss: 53.36960983276367 test_loss:90.66996002197266\n",
      "2717/3000 train_loss: 45.70950698852539 test_loss:93.18971252441406\n",
      "2718/3000 train_loss: 51.509918212890625 test_loss:89.30610656738281\n",
      "2719/3000 train_loss: 55.433990478515625 test_loss:90.93853759765625\n",
      "2720/3000 train_loss: 43.9282112121582 test_loss:90.71669006347656\n",
      "2721/3000 train_loss: 47.370460510253906 test_loss:90.48359680175781\n",
      "2722/3000 train_loss: 43.60184097290039 test_loss:91.93427276611328\n",
      "2723/3000 train_loss: 37.50259780883789 test_loss:90.940673828125\n",
      "2724/3000 train_loss: 41.29175567626953 test_loss:92.57742309570312\n",
      "2725/3000 train_loss: 42.881980895996094 test_loss:88.37649536132812\n",
      "2726/3000 train_loss: 51.57219314575195 test_loss:90.78752136230469\n",
      "2727/3000 train_loss: 47.820770263671875 test_loss:91.19709777832031\n",
      "2728/3000 train_loss: 53.451316833496094 test_loss:92.19328308105469\n",
      "2729/3000 train_loss: 36.52105712890625 test_loss:89.70297241210938\n",
      "2730/3000 train_loss: 46.648651123046875 test_loss:88.89116668701172\n",
      "2731/3000 train_loss: 37.28761672973633 test_loss:90.27369689941406\n",
      "2732/3000 train_loss: 51.76765441894531 test_loss:89.00831604003906\n",
      "2733/3000 train_loss: 48.41294860839844 test_loss:92.07746887207031\n",
      "2734/3000 train_loss: 58.51759719848633 test_loss:90.52250671386719\n",
      "2735/3000 train_loss: 47.27143096923828 test_loss:89.94198608398438\n",
      "2736/3000 train_loss: 42.034053802490234 test_loss:89.88261413574219\n",
      "2737/3000 train_loss: 42.41401672363281 test_loss:90.36492919921875\n",
      "2738/3000 train_loss: 46.57384490966797 test_loss:89.75865173339844\n",
      "2739/3000 train_loss: 42.63182067871094 test_loss:89.95343017578125\n",
      "2740/3000 train_loss: 38.75190353393555 test_loss:91.87671661376953\n",
      "2741/3000 train_loss: 51.34721374511719 test_loss:91.00012969970703\n",
      "2742/3000 train_loss: 49.838626861572266 test_loss:93.29393005371094\n",
      "2743/3000 train_loss: 40.922508239746094 test_loss:90.99143981933594\n",
      "2744/3000 train_loss: 37.070030212402344 test_loss:91.71946716308594\n",
      "2745/3000 train_loss: 53.72333526611328 test_loss:90.59368896484375\n",
      "2746/3000 train_loss: 41.094703674316406 test_loss:91.24493408203125\n",
      "2747/3000 train_loss: 40.55573654174805 test_loss:88.87908172607422\n",
      "2748/3000 train_loss: 50.94112777709961 test_loss:87.68563842773438\n",
      "2749/3000 train_loss: 49.25679016113281 test_loss:86.67155456542969\n",
      "2750/3000 train_loss: 40.18995666503906 test_loss:90.35791015625\n",
      "2751/3000 train_loss: 45.54747009277344 test_loss:90.26420593261719\n",
      "2752/3000 train_loss: 44.90220642089844 test_loss:90.67953491210938\n",
      "2753/3000 train_loss: 44.542171478271484 test_loss:87.75149536132812\n",
      "2754/3000 train_loss: 40.470943450927734 test_loss:88.49546813964844\n",
      "2755/3000 train_loss: 42.08773422241211 test_loss:88.01441955566406\n",
      "2756/3000 train_loss: 51.60643768310547 test_loss:89.15845489501953\n",
      "2757/3000 train_loss: 53.553977966308594 test_loss:88.06890869140625\n",
      "2758/3000 train_loss: 46.192081451416016 test_loss:88.66850280761719\n",
      "2759/3000 train_loss: 47.605377197265625 test_loss:90.84310913085938\n",
      "2760/3000 train_loss: 45.28916931152344 test_loss:90.41270446777344\n",
      "2761/3000 train_loss: 44.19810104370117 test_loss:86.3323974609375\n",
      "2762/3000 train_loss: 36.299842834472656 test_loss:88.0307388305664\n",
      "2763/3000 train_loss: 40.994773864746094 test_loss:87.14228820800781\n",
      "2764/3000 train_loss: 47.53116989135742 test_loss:88.17390441894531\n",
      "2765/3000 train_loss: 48.00053405761719 test_loss:88.15621948242188\n",
      "2766/3000 train_loss: 44.39640426635742 test_loss:87.15377807617188\n",
      "2767/3000 train_loss: 37.243858337402344 test_loss:85.6683349609375\n",
      "2768/3000 train_loss: 50.62543487548828 test_loss:85.85844421386719\n",
      "2769/3000 train_loss: 39.06149673461914 test_loss:84.83718872070312\n",
      "2770/3000 train_loss: 60.25273513793945 test_loss:87.17109680175781\n",
      "2771/3000 train_loss: 44.411773681640625 test_loss:88.8992919921875\n",
      "2772/3000 train_loss: 45.94684982299805 test_loss:88.88554382324219\n",
      "2773/3000 train_loss: 40.54145431518555 test_loss:87.39202880859375\n",
      "2774/3000 train_loss: 54.25038146972656 test_loss:88.82221984863281\n",
      "2775/3000 train_loss: 50.826622009277344 test_loss:90.53770446777344\n",
      "2776/3000 train_loss: 56.923675537109375 test_loss:87.55741882324219\n",
      "2777/3000 train_loss: 41.21222686767578 test_loss:88.58670043945312\n",
      "2778/3000 train_loss: 48.1119384765625 test_loss:86.73365020751953\n",
      "2779/3000 train_loss: 35.836280822753906 test_loss:87.9298095703125\n",
      "2780/3000 train_loss: 47.064476013183594 test_loss:88.45928955078125\n",
      "2781/3000 train_loss: 47.96427917480469 test_loss:87.93646240234375\n",
      "2782/3000 train_loss: 40.24701690673828 test_loss:87.13121032714844\n",
      "2783/3000 train_loss: 42.180355072021484 test_loss:84.71630859375\n",
      "2784/3000 train_loss: 41.59021759033203 test_loss:85.33407592773438\n",
      "2785/3000 train_loss: 38.93748092651367 test_loss:85.18997192382812\n",
      "2786/3000 train_loss: 38.96722412109375 test_loss:85.04015350341797\n",
      "2787/3000 train_loss: 36.30831527709961 test_loss:85.20426940917969\n",
      "2788/3000 train_loss: 44.77288055419922 test_loss:84.92567443847656\n",
      "2789/3000 train_loss: 42.513038635253906 test_loss:87.13334655761719\n",
      "2790/3000 train_loss: 50.67548370361328 test_loss:89.02481079101562\n",
      "2791/3000 train_loss: 40.519866943359375 test_loss:88.54789733886719\n",
      "2792/3000 train_loss: 40.86370086669922 test_loss:87.21885681152344\n",
      "2793/3000 train_loss: 51.51322555541992 test_loss:88.16548156738281\n",
      "2794/3000 train_loss: 38.424156188964844 test_loss:87.86540222167969\n",
      "2795/3000 train_loss: 40.66590118408203 test_loss:87.00505065917969\n",
      "2796/3000 train_loss: 45.922943115234375 test_loss:86.89797973632812\n",
      "2797/3000 train_loss: 41.39729690551758 test_loss:86.68255615234375\n",
      "2798/3000 train_loss: 39.924095153808594 test_loss:85.6622085571289\n",
      "2799/3000 train_loss: 53.48500061035156 test_loss:90.67394256591797\n",
      "2800/3000 train_loss: 41.65108871459961 test_loss:92.02597045898438\n",
      "2801/3000 train_loss: 43.00700759887695 test_loss:90.0822982788086\n",
      "2802/3000 train_loss: 57.485931396484375 test_loss:88.70606994628906\n",
      "2803/3000 train_loss: 49.0889892578125 test_loss:85.09828186035156\n",
      "2804/3000 train_loss: 45.29470443725586 test_loss:85.42289733886719\n",
      "2805/3000 train_loss: 43.20759963989258 test_loss:86.6986083984375\n",
      "2806/3000 train_loss: 40.26533508300781 test_loss:88.41419219970703\n",
      "2807/3000 train_loss: 43.02890396118164 test_loss:86.05288696289062\n",
      "2808/3000 train_loss: 43.183380126953125 test_loss:85.95515441894531\n",
      "2809/3000 train_loss: 41.88162612915039 test_loss:90.44113159179688\n",
      "2810/3000 train_loss: 47.27167892456055 test_loss:90.12797546386719\n",
      "2811/3000 train_loss: 52.946502685546875 test_loss:89.39337158203125\n",
      "2812/3000 train_loss: 43.15523147583008 test_loss:91.19796752929688\n",
      "2813/3000 train_loss: 41.99275588989258 test_loss:88.51930236816406\n",
      "2814/3000 train_loss: 47.3323860168457 test_loss:87.17404174804688\n",
      "2815/3000 train_loss: 35.08502197265625 test_loss:86.43356323242188\n",
      "2816/3000 train_loss: 44.557376861572266 test_loss:84.3210678100586\n",
      "2817/3000 train_loss: 52.90038299560547 test_loss:85.81130981445312\n",
      "2818/3000 train_loss: 47.644012451171875 test_loss:88.22994995117188\n",
      "2819/3000 train_loss: 41.6838493347168 test_loss:89.00502014160156\n",
      "2820/3000 train_loss: 37.17626190185547 test_loss:87.08173370361328\n",
      "2821/3000 train_loss: 41.74921798706055 test_loss:87.08451080322266\n",
      "2822/3000 train_loss: 43.29991912841797 test_loss:88.35804748535156\n",
      "2823/3000 train_loss: 45.339447021484375 test_loss:86.53968811035156\n",
      "2824/3000 train_loss: 36.96660232543945 test_loss:84.84793853759766\n",
      "2825/3000 train_loss: 51.72611999511719 test_loss:90.53382873535156\n",
      "2826/3000 train_loss: 47.69338607788086 test_loss:89.78999328613281\n",
      "2827/3000 train_loss: 45.82170867919922 test_loss:90.58528137207031\n",
      "2828/3000 train_loss: 48.755802154541016 test_loss:91.44636535644531\n",
      "2829/3000 train_loss: 49.69493103027344 test_loss:85.97251892089844\n",
      "2830/3000 train_loss: 40.736366271972656 test_loss:84.39248657226562\n",
      "2831/3000 train_loss: 37.32809829711914 test_loss:84.25652313232422\n",
      "2832/3000 train_loss: 40.678619384765625 test_loss:84.56753540039062\n",
      "2833/3000 train_loss: 38.47832107543945 test_loss:84.71635437011719\n",
      "2834/3000 train_loss: 41.08674240112305 test_loss:84.79939270019531\n",
      "2835/3000 train_loss: 42.320369720458984 test_loss:84.18817901611328\n",
      "2836/3000 train_loss: 37.335205078125 test_loss:84.13690185546875\n",
      "2837/3000 train_loss: 38.806663513183594 test_loss:85.11817932128906\n",
      "2838/3000 train_loss: 45.77427673339844 test_loss:86.17991638183594\n",
      "2839/3000 train_loss: 44.88664627075195 test_loss:83.43826293945312\n",
      "2840/3000 train_loss: 41.93286895751953 test_loss:86.37974548339844\n",
      "2841/3000 train_loss: 51.184165954589844 test_loss:86.51473999023438\n",
      "2842/3000 train_loss: 45.24979782104492 test_loss:84.93443298339844\n",
      "2843/3000 train_loss: 38.88473892211914 test_loss:83.42357635498047\n",
      "2844/3000 train_loss: 42.474185943603516 test_loss:83.64546966552734\n",
      "2845/3000 train_loss: 53.04362487792969 test_loss:85.43644714355469\n",
      "2846/3000 train_loss: 45.24271011352539 test_loss:87.4359130859375\n",
      "2847/3000 train_loss: 47.224430084228516 test_loss:89.36349487304688\n",
      "2848/3000 train_loss: 42.41366958618164 test_loss:85.093017578125\n",
      "2849/3000 train_loss: 48.94731521606445 test_loss:85.78892517089844\n",
      "2850/3000 train_loss: 41.01715850830078 test_loss:87.65234375\n",
      "2851/3000 train_loss: 45.98548126220703 test_loss:87.01609802246094\n",
      "2852/3000 train_loss: 42.291343688964844 test_loss:90.46821594238281\n",
      "2853/3000 train_loss: 42.84956741333008 test_loss:86.43293762207031\n",
      "2854/3000 train_loss: 41.059486389160156 test_loss:85.40461730957031\n",
      "2855/3000 train_loss: 48.34501647949219 test_loss:82.2513427734375\n",
      "2856/3000 train_loss: 42.18456268310547 test_loss:83.71505737304688\n",
      "2857/3000 train_loss: 35.7722282409668 test_loss:85.23880004882812\n",
      "2858/3000 train_loss: 40.867897033691406 test_loss:84.40780639648438\n",
      "2859/3000 train_loss: 36.72348403930664 test_loss:86.31916809082031\n",
      "2860/3000 train_loss: 37.462440490722656 test_loss:83.97514343261719\n",
      "2861/3000 train_loss: 41.16231155395508 test_loss:84.74465942382812\n",
      "2862/3000 train_loss: 36.45138931274414 test_loss:84.6763916015625\n",
      "2863/3000 train_loss: 41.495147705078125 test_loss:84.1745834350586\n",
      "2864/3000 train_loss: 43.41648483276367 test_loss:83.51637268066406\n",
      "2865/3000 train_loss: 46.79047775268555 test_loss:83.4949951171875\n",
      "2866/3000 train_loss: 42.59449768066406 test_loss:84.97843933105469\n",
      "2867/3000 train_loss: 51.091678619384766 test_loss:83.44490051269531\n",
      "2868/3000 train_loss: 43.40543746948242 test_loss:85.12781524658203\n",
      "2869/3000 train_loss: 41.28697967529297 test_loss:85.8450927734375\n",
      "2870/3000 train_loss: 37.98097229003906 test_loss:83.72659301757812\n",
      "2871/3000 train_loss: 42.08871841430664 test_loss:82.76642608642578\n",
      "2872/3000 train_loss: 44.60028839111328 test_loss:83.87704467773438\n",
      "2873/3000 train_loss: 41.29691696166992 test_loss:83.52863311767578\n",
      "2874/3000 train_loss: 40.72932434082031 test_loss:82.92726135253906\n",
      "2875/3000 train_loss: 40.02935028076172 test_loss:82.09541320800781\n",
      "2876/3000 train_loss: 43.66289138793945 test_loss:83.00778198242188\n",
      "2877/3000 train_loss: 42.296791076660156 test_loss:83.40862274169922\n",
      "2878/3000 train_loss: 44.351585388183594 test_loss:84.47352600097656\n",
      "2879/3000 train_loss: 44.284297943115234 test_loss:86.24552154541016\n",
      "2880/3000 train_loss: 41.5712890625 test_loss:86.32420349121094\n",
      "2881/3000 train_loss: 49.63341522216797 test_loss:88.96672821044922\n",
      "2882/3000 train_loss: 42.19964599609375 test_loss:86.27660369873047\n",
      "2883/3000 train_loss: 57.617496490478516 test_loss:86.08653259277344\n",
      "2884/3000 train_loss: 43.99504852294922 test_loss:87.88105773925781\n",
      "2885/3000 train_loss: 41.29515075683594 test_loss:87.34307861328125\n",
      "2886/3000 train_loss: 47.46941375732422 test_loss:90.79559326171875\n",
      "2887/3000 train_loss: 39.38559341430664 test_loss:88.43875885009766\n",
      "2888/3000 train_loss: 43.306915283203125 test_loss:84.45639038085938\n",
      "2889/3000 train_loss: 37.71903991699219 test_loss:86.07073974609375\n",
      "2890/3000 train_loss: 43.11125946044922 test_loss:85.47309875488281\n",
      "2891/3000 train_loss: 45.90894317626953 test_loss:84.35334777832031\n",
      "2892/3000 train_loss: 47.27956771850586 test_loss:85.6756591796875\n",
      "2893/3000 train_loss: 42.608821868896484 test_loss:85.54061889648438\n",
      "2894/3000 train_loss: 40.6779899597168 test_loss:83.48114776611328\n",
      "2895/3000 train_loss: 45.70362091064453 test_loss:83.65819549560547\n",
      "2896/3000 train_loss: 42.27226257324219 test_loss:87.8935775756836\n",
      "2897/3000 train_loss: 41.99800491333008 test_loss:87.33963012695312\n",
      "2898/3000 train_loss: 39.78520584106445 test_loss:84.70657348632812\n",
      "2899/3000 train_loss: 46.161861419677734 test_loss:85.34700012207031\n",
      "2900/3000 train_loss: 44.703670501708984 test_loss:83.86476135253906\n",
      "2901/3000 train_loss: 43.644935607910156 test_loss:82.44052124023438\n",
      "2902/3000 train_loss: 40.38314437866211 test_loss:81.17044067382812\n",
      "2903/3000 train_loss: 42.57573699951172 test_loss:80.14520263671875\n",
      "2904/3000 train_loss: 39.37984085083008 test_loss:80.67744445800781\n",
      "2905/3000 train_loss: 39.92935562133789 test_loss:81.92866516113281\n",
      "2906/3000 train_loss: 40.17171096801758 test_loss:81.80479431152344\n",
      "2907/3000 train_loss: 47.747459411621094 test_loss:82.43470764160156\n",
      "2908/3000 train_loss: 46.918521881103516 test_loss:82.60621643066406\n",
      "2909/3000 train_loss: 47.28849792480469 test_loss:85.31021118164062\n",
      "2910/3000 train_loss: 34.05662536621094 test_loss:82.90562438964844\n",
      "2911/3000 train_loss: 37.60407257080078 test_loss:82.84255981445312\n",
      "2912/3000 train_loss: 44.101463317871094 test_loss:82.93771362304688\n",
      "2913/3000 train_loss: 44.96617889404297 test_loss:83.17604064941406\n",
      "2914/3000 train_loss: 47.206241607666016 test_loss:82.29252624511719\n",
      "2915/3000 train_loss: 50.091575622558594 test_loss:82.38760375976562\n",
      "2916/3000 train_loss: 45.23394012451172 test_loss:83.00588989257812\n",
      "2917/3000 train_loss: 50.649879455566406 test_loss:83.62236022949219\n",
      "2918/3000 train_loss: 43.65278625488281 test_loss:85.77692413330078\n",
      "2919/3000 train_loss: 38.1451416015625 test_loss:84.54435729980469\n",
      "2920/3000 train_loss: 43.04755401611328 test_loss:80.20170593261719\n",
      "2921/3000 train_loss: 44.01868438720703 test_loss:82.42129516601562\n",
      "2922/3000 train_loss: 45.151222229003906 test_loss:82.6978530883789\n",
      "2923/3000 train_loss: 42.64070129394531 test_loss:82.42564392089844\n",
      "2924/3000 train_loss: 37.812931060791016 test_loss:81.58036804199219\n",
      "2925/3000 train_loss: 38.44591522216797 test_loss:80.76322937011719\n",
      "2926/3000 train_loss: 32.94925308227539 test_loss:81.25362396240234\n",
      "2927/3000 train_loss: 41.219688415527344 test_loss:80.56497192382812\n",
      "2928/3000 train_loss: 41.7451171875 test_loss:81.568359375\n",
      "2929/3000 train_loss: 42.66936111450195 test_loss:80.76913452148438\n",
      "2930/3000 train_loss: 37.825199127197266 test_loss:79.4190673828125\n",
      "2931/3000 train_loss: 40.19519805908203 test_loss:80.57772827148438\n",
      "2932/3000 train_loss: 49.5578727722168 test_loss:80.04136657714844\n",
      "2933/3000 train_loss: 49.37775802612305 test_loss:82.41291809082031\n",
      "2934/3000 train_loss: 47.20503616333008 test_loss:84.13185119628906\n",
      "2935/3000 train_loss: 44.819271087646484 test_loss:84.37901306152344\n",
      "2936/3000 train_loss: 45.06882095336914 test_loss:84.43678283691406\n",
      "2937/3000 train_loss: 48.860389709472656 test_loss:82.18331909179688\n",
      "2938/3000 train_loss: 38.089012145996094 test_loss:81.20816040039062\n",
      "2939/3000 train_loss: 42.64544677734375 test_loss:83.22700500488281\n",
      "2940/3000 train_loss: 59.06065368652344 test_loss:82.30499267578125\n",
      "2941/3000 train_loss: 46.19096374511719 test_loss:82.50477600097656\n",
      "2942/3000 train_loss: 51.60805130004883 test_loss:81.91168212890625\n",
      "2943/3000 train_loss: 37.14317321777344 test_loss:82.38585662841797\n",
      "2944/3000 train_loss: 44.73763656616211 test_loss:79.40641784667969\n",
      "2945/3000 train_loss: 38.477230072021484 test_loss:80.59573364257812\n",
      "2946/3000 train_loss: 40.32826232910156 test_loss:79.11593627929688\n",
      "2947/3000 train_loss: 41.13261413574219 test_loss:82.646484375\n",
      "2948/3000 train_loss: 45.678985595703125 test_loss:82.26519775390625\n",
      "2949/3000 train_loss: 47.530357360839844 test_loss:83.59517669677734\n",
      "2950/3000 train_loss: 38.69263458251953 test_loss:84.24834442138672\n",
      "2951/3000 train_loss: 42.52117156982422 test_loss:81.62753295898438\n",
      "2952/3000 train_loss: 48.900123596191406 test_loss:81.5710220336914\n",
      "2953/3000 train_loss: 45.250152587890625 test_loss:84.65654754638672\n",
      "2954/3000 train_loss: 43.19263458251953 test_loss:83.35201263427734\n",
      "2955/3000 train_loss: 56.204437255859375 test_loss:82.55366516113281\n",
      "2956/3000 train_loss: 39.069244384765625 test_loss:83.12654113769531\n",
      "2957/3000 train_loss: 42.304134368896484 test_loss:82.77435302734375\n",
      "2958/3000 train_loss: 47.39142990112305 test_loss:83.37711334228516\n",
      "2959/3000 train_loss: 44.85354232788086 test_loss:81.75282287597656\n",
      "2960/3000 train_loss: 43.24851608276367 test_loss:83.56560516357422\n",
      "2961/3000 train_loss: 39.83425521850586 test_loss:83.00143432617188\n",
      "2962/3000 train_loss: 38.29530715942383 test_loss:81.11761474609375\n",
      "2963/3000 train_loss: 44.18280792236328 test_loss:82.0551528930664\n",
      "2964/3000 train_loss: 38.890323638916016 test_loss:80.94911193847656\n",
      "2965/3000 train_loss: 41.3529167175293 test_loss:82.7685546875\n",
      "2966/3000 train_loss: 41.961280822753906 test_loss:83.56167602539062\n",
      "2967/3000 train_loss: 45.648014068603516 test_loss:83.95285034179688\n",
      "2968/3000 train_loss: 43.67416000366211 test_loss:87.34504699707031\n",
      "2969/3000 train_loss: 38.1429328918457 test_loss:85.13497161865234\n",
      "2970/3000 train_loss: 41.753597259521484 test_loss:85.03567504882812\n",
      "2971/3000 train_loss: 39.5888786315918 test_loss:85.19583892822266\n",
      "2972/3000 train_loss: 33.29386901855469 test_loss:86.09461975097656\n",
      "2973/3000 train_loss: 36.26381301879883 test_loss:83.91635131835938\n",
      "2974/3000 train_loss: 43.86784744262695 test_loss:81.97891235351562\n",
      "2975/3000 train_loss: 52.314170837402344 test_loss:83.37971496582031\n",
      "2976/3000 train_loss: 39.7841911315918 test_loss:85.44474792480469\n",
      "2977/3000 train_loss: 39.40810012817383 test_loss:83.78522491455078\n",
      "2978/3000 train_loss: 51.56550216674805 test_loss:83.50585174560547\n",
      "2979/3000 train_loss: 40.93629837036133 test_loss:83.98016357421875\n",
      "2980/3000 train_loss: 51.21625900268555 test_loss:83.20476531982422\n",
      "2981/3000 train_loss: 39.2587776184082 test_loss:83.75990295410156\n",
      "2982/3000 train_loss: 42.19062042236328 test_loss:84.78878784179688\n",
      "2983/3000 train_loss: 48.35795211791992 test_loss:85.41678619384766\n",
      "2984/3000 train_loss: 60.1300048828125 test_loss:85.30904388427734\n",
      "2985/3000 train_loss: 35.975013732910156 test_loss:84.2136459350586\n",
      "2986/3000 train_loss: 53.2144775390625 test_loss:83.16748046875\n",
      "2987/3000 train_loss: 39.46000289916992 test_loss:84.04415893554688\n",
      "2988/3000 train_loss: 41.601318359375 test_loss:82.56071472167969\n",
      "2989/3000 train_loss: 50.69137191772461 test_loss:82.61003112792969\n",
      "2990/3000 train_loss: 44.63762664794922 test_loss:82.39569091796875\n",
      "2991/3000 train_loss: 47.459632873535156 test_loss:83.81906127929688\n",
      "2992/3000 train_loss: 38.61653137207031 test_loss:82.97245788574219\n",
      "2993/3000 train_loss: 49.1308708190918 test_loss:82.14058685302734\n",
      "2994/3000 train_loss: 47.7396240234375 test_loss:86.74723815917969\n",
      "2995/3000 train_loss: 42.148773193359375 test_loss:85.23283386230469\n",
      "2996/3000 train_loss: 47.49579620361328 test_loss:85.576416015625\n",
      "2997/3000 train_loss: 40.329734802246094 test_loss:83.74722290039062\n",
      "2998/3000 train_loss: 37.92995834350586 test_loss:82.59701538085938\n",
      "2999/3000 train_loss: 44.32875061035156 test_loss:83.18852233886719\n",
      "3000/3000 train_loss: 44.78605651855469 test_loss:83.63282775878906\n"
     ]
    }
   ],
   "source": [
    "losses = train(model = unet, optimizer = optimizer, criterion=criterion, data_tr=train_data,\n",
    "               data_val = test_data, scheduler = scheduler,device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "6Ew7_F0-q7aL",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "41780229-11e0-403b-8b96-a510522b8817"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(83.6328)"
      ]
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "source": [
    "avg_loss = 0\n",
    "# unet.eval()\n",
    "# unet.train()\n",
    "preds = []\n",
    "i = 0\n",
    "test_anomaly_losses = []\n",
    "test_normal_losses = []\n",
    "test_losses = []\n",
    "test_real = y_test.tolist()\n",
    "for batch in test_data:\n",
    "  with torch.no_grad():\n",
    "    # unet.train()\n",
    "    predictions = unet(batch.to(device)).cpu()\n",
    "    preds.append(predictions)\n",
    "  loss = criterion(predictions, batch.cpu())\n",
    "  for j in range(len(predictions)):\n",
    "    if int(y_test[i]) == 1:\n",
    "      test_anomaly_losses.append(float(criterion(predictions[j], batch[j])))\n",
    "    else:\n",
    "      test_normal_losses.append(float(criterion(predictions[j], batch[j])))\n",
    "    i += 1\n",
    "    test_losses.append(criterion(predictions[j], batch[j]))\n",
    "  # print(loss)\n",
    "  # print(loss)\n",
    "  avg_loss += loss / len(test_data)\n",
    "# avg_loss\n",
    "\n",
    "avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "VpDKorrRso9o",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "00b43ac6-7b88-4ae3-f225-e24eef83f044"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(97.86798472886674, 70.30054347038269)"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "sum(test_anomaly_losses)/len(test_anomaly_losses) , sum(test_normal_losses)/len(test_normal_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5LEvbZKYuh7J",
    "outputId": "100c7fcb-4580-4789-e59f-a7682ac6aa44"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.7869101123595506\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "vals = np.arange(40, 900, 0.5).tolist()\n",
    "for threshold in vals:\n",
    "  preds = []\n",
    "  for j in range(len(test_losses)):\n",
    "    if test_losses[j] > threshold:\n",
    "      preds.append(1)\n",
    "    else:\n",
    "      preds.append(0)\n",
    "  \n",
    "  results.append(roc_auc_score(test_real,preds))\n",
    "\n",
    "print(max(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QaSSqG8SbAw2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
