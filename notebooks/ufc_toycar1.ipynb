{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9SStKf4G0V5H"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim import Adam\n",
        "import torchaudio\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from skimage.util import img_as_ubyte\n",
        "import pandas as pd\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import librosa\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import io\n",
        "from sklearn.metrics import roc_auc_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XtxbKLZq5KX",
        "outputId": "0c6de58c-f736-440b-85ce-a6ad2ae77c29"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYxHegIM0Z4i",
        "outputId": "8b35e1ba-3d36-4aca-c569-e94fbad47a14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "h9DATQwS0ivD"
      },
      "outputs": [],
      "source": [
        "class MimiiDataset(Dataset):\n",
        "    def __init__(self,audio_dir, n_fft = 1024, win_length = 1024,\n",
        "                 hop_length = 512,power = 2,n_mels = 128,pad_mode = 'reflect',\n",
        "                 sr = 16000,center = True,norm = None):\n",
        "      \n",
        "        super(MimiiDataset, self).__init__()\n",
        "        self.audio_dir = audio_dir\n",
        "        self.n_mels = n_mels\n",
        "        self.n_fft = n_fft\n",
        "        self.win_length = win_length\n",
        "        self.hop_length = hop_length\n",
        "        self.power = power\n",
        "        self.pad_mode = pad_mode\n",
        "        self.sr = sr\n",
        "        self.center = center\n",
        "        self.norm = norm\n",
        "\n",
        "    def get_files(self):\n",
        "       return self.train_files, self.test_files\n",
        "    \n",
        "    def get_data(self,device, id):\n",
        "        \n",
        "        self.train_files, self.train_labels = self._train_file_list(device, id)\n",
        "        self.test_files, self.test_labels = self._test_file_list(device, id)\n",
        "        \n",
        "        self.train_data = self.get_audios(self.train_files)\n",
        "        self.test_data = self.get_audios(self.test_files)\n",
        "        \n",
        "        return self.train_data, self.test_data, self.train_labels, self.test_labels\n",
        "    \n",
        "    def _train_file_list(self, device, id):\n",
        "        query = os.path.abspath(\n",
        "            f\"{self.audio_dir}/{device}/train/normal_id_0{id}*.wav\"\n",
        "        )\n",
        "        train_normal_files = sorted(glob.glob(query))\n",
        "        train_normal_labels = np.zeros(len(train_normal_files))\n",
        "        \n",
        "        query = os.path.abspath(\n",
        "                f\"{self.audio_dir}/{device}/train/anomaly_id_0{id}*.wav\"\n",
        "            )\n",
        "        train_anomaly_files = sorted(glob.glob(query))\n",
        "        train_anomaly_labels = np.ones(len(train_anomaly_files))\n",
        "        \n",
        "        train_file_list = np.concatenate((train_normal_files, train_anomaly_files), axis=0)\n",
        "        train_labels = np.concatenate((train_normal_labels, train_anomaly_labels), axis=0)\n",
        "        \n",
        "        return train_file_list, train_labels\n",
        "    \n",
        "    def _test_file_list(self, device, id):     \n",
        "        query = os.path.abspath(\n",
        "            f\"{self.audio_dir}/{device}/test/normal_id_0{id}*.wav\"\n",
        "            )\n",
        "        test_normal_files = sorted(glob.glob(query))\n",
        "        test_normal_labels = np.zeros(len(test_normal_files))\n",
        "        \n",
        "        query = os.path.abspath(\n",
        "            f\"{self.audio_dir}/{device}/test/anomaly_id_0{id}*.wav\"\n",
        "            )\n",
        "        test_anomaly_files = sorted(glob.glob(query))\n",
        "        test_anomaly_labels = np.ones(len(test_anomaly_files))\n",
        "        \n",
        "        test_file_list = np.concatenate((test_normal_files, \n",
        "                                          test_anomaly_files), axis=0)\n",
        "        test_labels = np.concatenate((test_normal_labels,\n",
        "                                      test_anomaly_labels), axis=0)\n",
        "          \n",
        "        return test_file_list, test_labels\n",
        "\n",
        "    def normalize(self,tensor):\n",
        "        tensor_minusmean = tensor - tensor.mean()\n",
        "        return tensor_minusmean/np.absolute(tensor_minusmean).max()\n",
        "\n",
        "    def make0min(self,tensornd):\n",
        "        tensor = tensornd.numpy()\n",
        "        res = np.where(tensor == 0, 1E-19 , tensor)\n",
        "        return torch.from_numpy(res)\n",
        "\n",
        "    def spectrogrameToImage(self,specgram):\n",
        "        # specgram = torchaudio.transforms.MelSpectrogram(n_fft=1024, win_length=1024, \n",
        "        #                                                 hop_length=512, power=2, \n",
        "        #                                                 normalized=True, n_mels=128)(waveform )\n",
        "        specgram= self.make0min(specgram)\n",
        "        specgram = specgram.log2()[0,:,:].numpy()\n",
        "        \n",
        "        tr2image = transforms.Compose([transforms.ToPILImage()])\n",
        "\n",
        "        specgram= self.normalize(specgram)\n",
        "        # specgram = img_as_ubyte(specgram)\n",
        "        specgramImage = tr2image(specgram)\n",
        "        return specgramImage\n",
        "\n",
        "    def get_logmelspectrogram(self, waveform):\n",
        "        melspec = librosa.feature.melspectrogram(\n",
        "          n_fft=self.n_fft, win_length=self.win_length, hop_length=self.hop_length,\n",
        "          power=self.power,n_mels=self.n_mels,pad_mode=self.pad_mode,sr=self.sr,\n",
        "          center=self.center,norm=self.norm,htk=True,\n",
        "          y = waveform.numpy()\n",
        "        )\n",
        "\n",
        "        logmelspec = librosa.power_to_db(melspec)\n",
        "\n",
        "        return logmelspec\n",
        "\n",
        "    def get_melspectrogram(self,waveform):\n",
        "        melspec = librosa.feature.melspectrogram(\n",
        "            n_fft=self.n_fft, win_length=self.win_length, hop_length=self.hop_length,\n",
        "            power=self.power,n_mels=self.n_mels,pad_mode=self.pad_mode,sr=self.sr,\n",
        "            center=self.center,norm=self.norm,htk=True,\n",
        "            y = waveform.numpy()\n",
        "        )\n",
        "\n",
        "        return melspec\n",
        "    \n",
        "    def get_mfcc(self,waveform):\n",
        "        mfcc = librosa.feature.mfcc(    \n",
        "            n_fft=self.n_fft, win_length=self.win_length, \n",
        "            hop_length=self.hop_length,pad_mode=self.pad_mode,sr=self.sr,\n",
        "            center=self.center,norm=self.norm,n_mfcc=40,\n",
        "            y = waveform.numpy()\n",
        "        )\n",
        "\n",
        "        return mfcc\n",
        "\n",
        "    def get_chroma_stft(self,waveform):\n",
        "        stft = librosa.feature.chroma_stft(\n",
        "            n_fft=self.n_fft, win_length=self.win_length, \n",
        "            hop_length=self.hop_length,pad_mode=self.pad_mode,sr=self.sr,\n",
        "            center=self.center,norm=self.norm,n_chroma=12,\n",
        "            y=waveform.numpy()\n",
        "        )\n",
        "\n",
        "        return stft\n",
        "\n",
        "    def get_spectral_contrast(self,waveform):\n",
        "        spec_contrast = librosa.feature.spectral_contrast(    \n",
        "            n_fft=self.n_fft, win_length=self.win_length,center=self.center,\n",
        "            hop_length=self.hop_length,pad_mode=self.pad_mode,sr=self.sr,\n",
        "            y = waveform.numpy()\n",
        "        )\n",
        "\n",
        "        return spec_contrast\n",
        "    \n",
        "    def get_tonnetz(self,waveform):\n",
        "        harmonic = librosa.effects.harmonic(waveform.numpy())\n",
        "        tonnetz = librosa.feature.tonnetz(y=harmonic,sr=self.sr)\n",
        "\n",
        "        return tonnetz\n",
        "\n",
        "    def get_audios(self, file_list):\n",
        "        data = []\n",
        "        for i in range(len(file_list)):\n",
        "          y, sr = torchaudio.load(file_list[i])  \n",
        "          data.append(y)\n",
        "\n",
        "        return data\n",
        "    def _derive_data(self, file_list):\n",
        "        train_data = []\n",
        "        test_data = []\n",
        "        train_mode = True\n",
        "        for file_list in [self.train_files, self.test_files]:\n",
        "          tr2tensor = transforms.Compose([transforms.PILToTensor()])\n",
        "          data = []\n",
        "          for j in range(len(file_list)):\n",
        "            y, sr = torchaudio.load(file_list[j])  \n",
        "            spec = self.get_melspectrogram(y)\n",
        "            spec = self.spectrogrameToImage(spec)\n",
        "            spec = spec.convert('RGB')\n",
        "            vectors = tr2tensor(spec)\n",
        "            if train_mode:     \n",
        "              train_data.append(vectors)\n",
        "            else:\n",
        "              test_data.append(vectors)\n",
        "            \n",
        "          train_mode = False\n",
        "                \n",
        "        return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "S96soeIc0o13"
      },
      "outputs": [],
      "source": [
        "dataset = MimiiDataset('/content/drive/MyDrive/mimii')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "Gn2zdn92doi1"
      },
      "outputs": [],
      "source": [
        "_, _, y_train, y_test = dataset.get_data('ToyCar', 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "SgjpeWy_RV1C"
      },
      "outputs": [],
      "source": [
        "train_mixed_f = torch.load('/content/drive/MyDrive/mixed_features/train_mf_toycar1.pt')\n",
        "test_mixed_f = torch.load('/content/drive/MyDrive/mixed_features/test_mf_toycar1.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEl3qOh-mZVK",
        "outputId": "184c9398-375e-4ec7-c7d4-a87e474e6641"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([434, 193])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "train_mixed_f.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "jWMPVGu1qiEq"
      },
      "outputs": [],
      "source": [
        "train_data = DataLoader(train_mixed_f, batch_size=64, shuffle = True)\n",
        "test_data = DataLoader(test_mixed_f, batch_size = 64, shuffle= False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "vNTBTRe6qnBq"
      },
      "outputs": [],
      "source": [
        "class UNet_FC(nn.Module):\n",
        "\n",
        "  def __init__(self, in_features):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.bn = nn.BatchNorm1d(128)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "    self.fc0 = nn.Linear(in_features=in_features,out_features=in_features)\n",
        "\n",
        "    # encoder\n",
        "    self.fc1 = nn.Linear(in_features=in_features, out_features=128)\n",
        "    self.fc2 = nn.Linear(in_features=128,out_features=128)\n",
        "    self.fc3 = nn.Linear(in_features=128, out_features=128)\n",
        "    self.fc4 = nn.Linear(in_features=128, out_features=128)\n",
        "    self.fc5 = nn.Linear(in_features=128, out_features=8)\n",
        "\n",
        "    # decoder\n",
        "    self.fc6 = nn.Linear(in_features=8, out_features=128)\n",
        "    self.fc7 = nn.Linear(in_features=128*2, out_features=128)\n",
        "    self.fc8 = nn.Linear(in_features=128*2, out_features=128)\n",
        "    self.fc9 = nn.Linear(in_features=128*2, out_features=128)\n",
        "\n",
        "    self.out = nn.Linear(in_features=128*2, out_features=in_features)\n",
        "\n",
        "  def encoder(self, x):\n",
        "    input = self.fc0(x)\n",
        "\n",
        "    x1 = self.relu(self.bn(self.fc1(input)))\n",
        "    x2 = self.relu(self.bn(self.fc2(x1)))\n",
        "    x3 = self.relu(self.bn(self.fc3(x2)))\n",
        "    x4 = self.relu(self.bn(self.fc4(x3)))\n",
        "    x5 = self.relu(self.fc5(x4))\n",
        "\n",
        "    return [x5, x4, x3, x2, x1]\n",
        "\n",
        "  def decoder(self, x):\n",
        "    x6 = self.relu(self.fc6(x[0]))\n",
        "    con1 = torch.cat((x6,x[1]), 1) \n",
        "    x7 = self.relu(self.bn(self.fc7(con1)))\n",
        "    con2 = torch.cat((x7,x[2]), 1)\n",
        "    x8 = self.relu(self.bn(self.fc8(con2)))\n",
        "    con3 = torch.cat((x8,x[3]), 1)\n",
        "    x9 = self.relu(self.bn(self.fc9(con3)))\n",
        "    con4 = torch.cat((x9,x[4]), 1)\n",
        "\n",
        "    x10 = self.out(con4)\n",
        "\n",
        "    return x10\n",
        "  \n",
        "  def forward(self, x):\n",
        "    # encoded = self.encoder(x)\n",
        "\n",
        "    # decoded = self.decoder(encoded)\n",
        "    input = self.fc0(x)\n",
        "\n",
        "    x1 = self.relu(self.bn(self.fc1(input)))\n",
        "    x2 = self.relu(self.bn(self.fc2(x1)))\n",
        "    x3 = self.relu(self.bn(self.fc3(x2)))\n",
        "    x4 = self.relu(self.bn(self.fc4(x3)))\n",
        "    x5 = self.relu(self.fc5(x4))\n",
        "\n",
        "    xy = [x5, x4, x3, x2, x1]\n",
        "\n",
        "    x6 = self.relu(self.fc6(xy[0]))\n",
        "    con1 = torch.cat((x6,xy[1]), 1) \n",
        "    x7 = self.relu(self.bn(self.fc7(con1)))\n",
        "    con2 = torch.cat((x7,xy[2]), 1)\n",
        "    x8 = self.relu(self.bn(self.fc8(con2)))\n",
        "    con3 = torch.cat((x8,xy[3]), 1)\n",
        "    x9 = self.relu(self.bn(self.fc9(con3)))\n",
        "    con4 = torch.cat((x9,xy[4]), 1)\n",
        "\n",
        "    x10 = self.out(con4)\n",
        "\n",
        "    # return decoded\n",
        "    return x10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "ZfgcBtQ3qn5l"
      },
      "outputs": [],
      "source": [
        "def train(model, optimizer, criterion, data_tr, data_val, scheduler = None,\n",
        "          epochs = 3000, device = 'cpu'):\n",
        "    # X_val, Y_val = next(iter(data_val))\n",
        "    losses = []\n",
        "    prev_avg_loss = 100000\n",
        "    for epoch in range(epochs):\n",
        "        train_avg_loss = 0\n",
        "        test_avg_loss = 0\n",
        "        # model.train()  # train mode\n",
        "        for batch in data_tr:\n",
        "          # data to device\n",
        "          batch = batch.to(device)\n",
        "          # set parameter gradients to zero\n",
        "          optimizer.zero_grad()\n",
        "          # forward\n",
        "          # print(Y_batch.shape)\n",
        "          predictions = model(batch)\n",
        "          loss = criterion(predictions, batch)\n",
        "          loss.backward() # backward-pass\n",
        "          optimizer.step()  # update weights\n",
        "          # calculate loss to show the user\n",
        "          if scheduler:\n",
        "            scheduler.step(loss)\n",
        "          train_avg_loss += loss / len(data_tr)\n",
        "\n",
        "        # model.eval()\n",
        "        for batch in data_val:\n",
        "          with torch.no_grad():\n",
        "            preds = model(batch.to(device)).cpu()\n",
        "            loss = criterion(preds,batch)\n",
        "            test_avg_loss += loss / len(data_val)\n",
        "                    \n",
        "        losses.append(train_avg_loss.item())\n",
        "        # if (epoch+1)%50 == 0:\n",
        "        print(\"{}/{} train_loss: {} test_loss:{}\".format(epoch+1, epochs, train_avg_loss, test_avg_loss))\n",
        "        # if test_avg_loss < 70:\n",
        "        #   break\n",
        "    return losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "ptkVTF55quOL"
      },
      "outputs": [],
      "source": [
        "unet = UNet_FC(in_features=193).to(device)\n",
        "optimizer = Adam(params = unet.parameters(), lr = 10e-3)\n",
        "# optimizer = Adam(params = unet.parameters())\n",
        "criterion = nn.MSELoss()\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, \n",
        "                                                       min_lr=10e-4, mode = 'min',\n",
        "                                                       patience = 30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkfmYl9oXhcB",
        "outputId": "af4e040d-52fd-4f5e-fea4-3ab1a9b8ff90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/3000 train_loss: 298451.3125 test_loss:289280.3125\n",
            "2/3000 train_loss: 294553.75 test_loss:284259.9375\n",
            "3/3000 train_loss: 288626.34375 test_loss:277262.0625\n",
            "4/3000 train_loss: 280487.65625 test_loss:268218.25\n",
            "5/3000 train_loss: 270148.75 test_loss:256776.96875\n",
            "6/3000 train_loss: 257176.4375 test_loss:242369.40625\n",
            "7/3000 train_loss: 241078.5625 test_loss:225109.546875\n",
            "8/3000 train_loss: 222939.5 test_loss:206655.046875\n",
            "9/3000 train_loss: 203700.0 test_loss:187373.625\n",
            "10/3000 train_loss: 184375.1875 test_loss:168023.9375\n",
            "11/3000 train_loss: 164088.15625 test_loss:147639.4375\n",
            "12/3000 train_loss: 143430.5625 test_loss:127956.8125\n",
            "13/3000 train_loss: 123798.625 test_loss:109532.390625\n",
            "14/3000 train_loss: 105276.359375 test_loss:91996.2578125\n",
            "15/3000 train_loss: 88037.1484375 test_loss:75902.8359375\n",
            "16/3000 train_loss: 71924.1015625 test_loss:61399.93359375\n",
            "17/3000 train_loss: 57955.6328125 test_loss:48688.25\n",
            "18/3000 train_loss: 45662.51953125 test_loss:37557.671875\n",
            "19/3000 train_loss: 35169.87109375 test_loss:28494.27734375\n",
            "20/3000 train_loss: 26235.04296875 test_loss:21090.705078125\n",
            "21/3000 train_loss: 20032.671875 test_loss:15385.1328125\n",
            "22/3000 train_loss: 13922.5166015625 test_loss:10893.7265625\n",
            "23/3000 train_loss: 10157.7392578125 test_loss:7743.5263671875\n",
            "24/3000 train_loss: 7099.70849609375 test_loss:5409.93896484375\n",
            "25/3000 train_loss: 4939.154296875 test_loss:3785.40625\n",
            "26/3000 train_loss: 3295.8544921875 test_loss:2524.86865234375\n",
            "27/3000 train_loss: 2407.14111328125 test_loss:2032.841796875\n",
            "28/3000 train_loss: 1551.1351318359375 test_loss:1385.42529296875\n",
            "29/3000 train_loss: 1057.8316650390625 test_loss:1045.8287353515625\n",
            "30/3000 train_loss: 745.0911865234375 test_loss:844.1824340820312\n",
            "31/3000 train_loss: 892.5693969726562 test_loss:730.6337890625\n",
            "32/3000 train_loss: 665.5206298828125 test_loss:679.8641357421875\n",
            "33/3000 train_loss: 434.81512451171875 test_loss:599.7416381835938\n",
            "34/3000 train_loss: 506.161376953125 test_loss:595.8740234375\n",
            "35/3000 train_loss: 545.9973754882812 test_loss:526.3627319335938\n",
            "36/3000 train_loss: 305.2384033203125 test_loss:627.9382934570312\n",
            "37/3000 train_loss: 393.65679931640625 test_loss:537.72607421875\n",
            "38/3000 train_loss: 364.302978515625 test_loss:562.4133911132812\n",
            "39/3000 train_loss: 340.3550109863281 test_loss:458.45709228515625\n",
            "40/3000 train_loss: 303.0768127441406 test_loss:450.13018798828125\n",
            "41/3000 train_loss: 361.50762939453125 test_loss:451.22607421875\n",
            "42/3000 train_loss: 300.7984619140625 test_loss:432.7116394042969\n",
            "43/3000 train_loss: 444.5766296386719 test_loss:424.55902099609375\n",
            "44/3000 train_loss: 299.77325439453125 test_loss:420.7179260253906\n",
            "45/3000 train_loss: 347.8599548339844 test_loss:434.9576416015625\n",
            "46/3000 train_loss: 384.9996337890625 test_loss:420.15924072265625\n",
            "47/3000 train_loss: 255.75035095214844 test_loss:435.14306640625\n",
            "48/3000 train_loss: 239.96734619140625 test_loss:414.9322509765625\n",
            "49/3000 train_loss: 243.2940216064453 test_loss:437.9162292480469\n",
            "50/3000 train_loss: 250.3468780517578 test_loss:419.885986328125\n",
            "51/3000 train_loss: 351.9391174316406 test_loss:418.6103210449219\n",
            "52/3000 train_loss: 223.13926696777344 test_loss:431.4427490234375\n",
            "53/3000 train_loss: 331.2354736328125 test_loss:412.6076965332031\n",
            "54/3000 train_loss: 247.86659240722656 test_loss:423.7213134765625\n",
            "55/3000 train_loss: 242.1055145263672 test_loss:431.53778076171875\n",
            "56/3000 train_loss: 273.8520812988281 test_loss:398.77001953125\n",
            "57/3000 train_loss: 234.87213134765625 test_loss:420.877197265625\n",
            "58/3000 train_loss: 256.58837890625 test_loss:400.32421875\n",
            "59/3000 train_loss: 219.3450927734375 test_loss:415.94659423828125\n",
            "60/3000 train_loss: 301.0340881347656 test_loss:406.3907470703125\n",
            "61/3000 train_loss: 279.5328369140625 test_loss:411.971435546875\n",
            "62/3000 train_loss: 306.1500549316406 test_loss:407.3868713378906\n",
            "63/3000 train_loss: 245.86485290527344 test_loss:403.59759521484375\n",
            "64/3000 train_loss: 231.2451934814453 test_loss:412.9888000488281\n",
            "65/3000 train_loss: 246.68450927734375 test_loss:406.25384521484375\n",
            "66/3000 train_loss: 332.6955871582031 test_loss:421.3656005859375\n",
            "67/3000 train_loss: 369.2256774902344 test_loss:391.091064453125\n",
            "68/3000 train_loss: 322.5548095703125 test_loss:401.4225769042969\n",
            "69/3000 train_loss: 228.80450439453125 test_loss:407.60479736328125\n",
            "70/3000 train_loss: 260.11260986328125 test_loss:401.4375305175781\n",
            "71/3000 train_loss: 336.213134765625 test_loss:377.763671875\n",
            "72/3000 train_loss: 334.19256591796875 test_loss:408.38470458984375\n",
            "73/3000 train_loss: 232.46168518066406 test_loss:374.8480529785156\n",
            "74/3000 train_loss: 303.48052978515625 test_loss:392.4471435546875\n",
            "75/3000 train_loss: 274.52252197265625 test_loss:399.11907958984375\n",
            "76/3000 train_loss: 268.0223388671875 test_loss:381.21844482421875\n",
            "77/3000 train_loss: 208.44595336914062 test_loss:380.503173828125\n",
            "78/3000 train_loss: 224.8555450439453 test_loss:372.157470703125\n",
            "79/3000 train_loss: 246.6216583251953 test_loss:383.83062744140625\n",
            "80/3000 train_loss: 233.76670837402344 test_loss:367.30584716796875\n",
            "81/3000 train_loss: 228.99232482910156 test_loss:401.5850524902344\n",
            "82/3000 train_loss: 273.8684997558594 test_loss:361.8775634765625\n",
            "83/3000 train_loss: 284.23114013671875 test_loss:380.9339904785156\n",
            "84/3000 train_loss: 238.84671020507812 test_loss:372.8182373046875\n",
            "85/3000 train_loss: 216.67213439941406 test_loss:375.9847717285156\n",
            "86/3000 train_loss: 189.99887084960938 test_loss:392.5598449707031\n",
            "87/3000 train_loss: 195.0670166015625 test_loss:363.2185974121094\n",
            "88/3000 train_loss: 260.79058837890625 test_loss:362.8780212402344\n",
            "89/3000 train_loss: 263.9146423339844 test_loss:410.14556884765625\n",
            "90/3000 train_loss: 280.503662109375 test_loss:362.0729675292969\n",
            "91/3000 train_loss: 226.14739990234375 test_loss:379.4919128417969\n",
            "92/3000 train_loss: 239.08860778808594 test_loss:356.6467590332031\n",
            "93/3000 train_loss: 226.34768676757812 test_loss:360.12591552734375\n",
            "94/3000 train_loss: 207.08615112304688 test_loss:368.7247314453125\n",
            "95/3000 train_loss: 224.4601287841797 test_loss:358.8202209472656\n",
            "96/3000 train_loss: 206.7710418701172 test_loss:355.74688720703125\n",
            "97/3000 train_loss: 210.7918701171875 test_loss:384.2246398925781\n",
            "98/3000 train_loss: 293.6604919433594 test_loss:352.11578369140625\n",
            "99/3000 train_loss: 222.54440307617188 test_loss:361.5438537597656\n",
            "100/3000 train_loss: 363.509033203125 test_loss:350.06890869140625\n",
            "101/3000 train_loss: 401.56011962890625 test_loss:384.2106628417969\n",
            "102/3000 train_loss: 226.8633270263672 test_loss:335.085693359375\n",
            "103/3000 train_loss: 208.4022979736328 test_loss:373.1603698730469\n",
            "104/3000 train_loss: 258.94171142578125 test_loss:336.18609619140625\n",
            "105/3000 train_loss: 221.30958557128906 test_loss:340.3606262207031\n",
            "106/3000 train_loss: 195.4143829345703 test_loss:354.7713928222656\n",
            "107/3000 train_loss: 231.1392364501953 test_loss:354.29095458984375\n",
            "108/3000 train_loss: 290.1636657714844 test_loss:347.7115783691406\n",
            "109/3000 train_loss: 234.78335571289062 test_loss:328.6642150878906\n",
            "110/3000 train_loss: 187.5597686767578 test_loss:335.2949523925781\n",
            "111/3000 train_loss: 246.2744903564453 test_loss:336.8876647949219\n",
            "112/3000 train_loss: 214.29925537109375 test_loss:326.5899963378906\n",
            "113/3000 train_loss: 182.54241943359375 test_loss:366.9774475097656\n",
            "114/3000 train_loss: 191.80084228515625 test_loss:339.15606689453125\n",
            "115/3000 train_loss: 177.77777099609375 test_loss:355.7933349609375\n",
            "116/3000 train_loss: 179.44647216796875 test_loss:340.55572509765625\n",
            "117/3000 train_loss: 199.63967895507812 test_loss:342.3642578125\n",
            "118/3000 train_loss: 193.21929931640625 test_loss:339.94573974609375\n",
            "119/3000 train_loss: 322.63201904296875 test_loss:369.65118408203125\n",
            "120/3000 train_loss: 188.40017700195312 test_loss:348.5738220214844\n",
            "121/3000 train_loss: 203.00157165527344 test_loss:327.4062805175781\n",
            "122/3000 train_loss: 219.0736083984375 test_loss:338.7147216796875\n",
            "123/3000 train_loss: 327.31201171875 test_loss:341.90386962890625\n",
            "124/3000 train_loss: 237.84176635742188 test_loss:345.6470031738281\n",
            "125/3000 train_loss: 295.26751708984375 test_loss:330.51043701171875\n",
            "126/3000 train_loss: 279.5668029785156 test_loss:338.2254943847656\n",
            "127/3000 train_loss: 246.35243225097656 test_loss:328.8941650390625\n",
            "128/3000 train_loss: 216.76402282714844 test_loss:326.80633544921875\n",
            "129/3000 train_loss: 227.5889434814453 test_loss:335.4281005859375\n",
            "130/3000 train_loss: 223.59262084960938 test_loss:331.1250915527344\n",
            "131/3000 train_loss: 176.03802490234375 test_loss:340.1678466796875\n",
            "132/3000 train_loss: 227.158447265625 test_loss:338.6378173828125\n",
            "133/3000 train_loss: 224.91258239746094 test_loss:322.3584289550781\n",
            "134/3000 train_loss: 196.43704223632812 test_loss:337.58441162109375\n",
            "135/3000 train_loss: 197.761962890625 test_loss:324.2381896972656\n",
            "136/3000 train_loss: 180.73593139648438 test_loss:323.022216796875\n",
            "137/3000 train_loss: 164.04685974121094 test_loss:344.7997741699219\n",
            "138/3000 train_loss: 191.5477752685547 test_loss:332.8122253417969\n",
            "139/3000 train_loss: 159.80291748046875 test_loss:336.2052001953125\n",
            "140/3000 train_loss: 249.0166015625 test_loss:321.61444091796875\n",
            "141/3000 train_loss: 175.16073608398438 test_loss:324.68035888671875\n",
            "142/3000 train_loss: 184.61929321289062 test_loss:374.11224365234375\n",
            "143/3000 train_loss: 184.0767059326172 test_loss:328.5580749511719\n",
            "144/3000 train_loss: 260.4621276855469 test_loss:332.0817565917969\n",
            "145/3000 train_loss: 174.50930786132812 test_loss:316.3446044921875\n",
            "146/3000 train_loss: 203.65553283691406 test_loss:322.6737060546875\n",
            "147/3000 train_loss: 185.20846557617188 test_loss:311.6173400878906\n",
            "148/3000 train_loss: 176.55215454101562 test_loss:338.229248046875\n",
            "149/3000 train_loss: 189.14837646484375 test_loss:325.609130859375\n",
            "150/3000 train_loss: 182.4658660888672 test_loss:312.3911437988281\n",
            "151/3000 train_loss: 179.3074951171875 test_loss:314.6121826171875\n",
            "152/3000 train_loss: 195.19541931152344 test_loss:322.9298095703125\n",
            "153/3000 train_loss: 183.36553955078125 test_loss:335.40380859375\n",
            "154/3000 train_loss: 204.24461364746094 test_loss:331.0597839355469\n",
            "155/3000 train_loss: 175.7391815185547 test_loss:313.2150573730469\n",
            "156/3000 train_loss: 189.87750244140625 test_loss:319.46826171875\n",
            "157/3000 train_loss: 232.61785888671875 test_loss:321.57244873046875\n",
            "158/3000 train_loss: 216.37594604492188 test_loss:313.1791687011719\n",
            "159/3000 train_loss: 171.7360382080078 test_loss:333.9158630371094\n",
            "160/3000 train_loss: 185.48129272460938 test_loss:303.3466491699219\n",
            "161/3000 train_loss: 186.11314392089844 test_loss:306.1558837890625\n",
            "162/3000 train_loss: 203.3447265625 test_loss:320.3973388671875\n",
            "163/3000 train_loss: 162.95565795898438 test_loss:298.5081481933594\n",
            "164/3000 train_loss: 173.85586547851562 test_loss:317.4578857421875\n",
            "165/3000 train_loss: 156.86097717285156 test_loss:299.4752502441406\n",
            "166/3000 train_loss: 151.9459991455078 test_loss:328.3731384277344\n",
            "167/3000 train_loss: 143.7509307861328 test_loss:328.9566650390625\n",
            "168/3000 train_loss: 187.51300048828125 test_loss:312.17816162109375\n",
            "169/3000 train_loss: 163.75930786132812 test_loss:295.83013916015625\n",
            "170/3000 train_loss: 178.31443786621094 test_loss:321.1181640625\n",
            "171/3000 train_loss: 254.3846435546875 test_loss:329.3192443847656\n",
            "172/3000 train_loss: 182.03384399414062 test_loss:315.4527587890625\n",
            "173/3000 train_loss: 199.56219482421875 test_loss:295.8430480957031\n",
            "174/3000 train_loss: 257.6942138671875 test_loss:330.3704833984375\n",
            "175/3000 train_loss: 364.2916564941406 test_loss:366.5071716308594\n",
            "176/3000 train_loss: 199.00148010253906 test_loss:322.0058898925781\n",
            "177/3000 train_loss: 190.07484436035156 test_loss:329.2847900390625\n",
            "178/3000 train_loss: 164.37750244140625 test_loss:309.2786865234375\n",
            "179/3000 train_loss: 162.2469024658203 test_loss:312.9619140625\n",
            "180/3000 train_loss: 170.81700134277344 test_loss:326.209228515625\n",
            "181/3000 train_loss: 182.7943878173828 test_loss:308.6884765625\n",
            "182/3000 train_loss: 205.03648376464844 test_loss:304.38604736328125\n",
            "183/3000 train_loss: 196.02191162109375 test_loss:317.8496398925781\n",
            "184/3000 train_loss: 160.6171417236328 test_loss:346.2242431640625\n",
            "185/3000 train_loss: 199.57064819335938 test_loss:300.2425842285156\n",
            "186/3000 train_loss: 160.60848999023438 test_loss:298.1719055175781\n",
            "187/3000 train_loss: 199.5735626220703 test_loss:299.88946533203125\n",
            "188/3000 train_loss: 175.4940948486328 test_loss:303.5487060546875\n",
            "189/3000 train_loss: 172.14950561523438 test_loss:319.5240173339844\n",
            "190/3000 train_loss: 170.2786102294922 test_loss:301.0618591308594\n",
            "191/3000 train_loss: 184.82762145996094 test_loss:294.83563232421875\n",
            "192/3000 train_loss: 237.28878784179688 test_loss:300.1903076171875\n",
            "193/3000 train_loss: 187.80165100097656 test_loss:312.1950988769531\n",
            "194/3000 train_loss: 185.59335327148438 test_loss:326.0876159667969\n",
            "195/3000 train_loss: 150.89698791503906 test_loss:292.515869140625\n",
            "196/3000 train_loss: 138.09292602539062 test_loss:309.14447021484375\n",
            "197/3000 train_loss: 192.6392364501953 test_loss:306.2447204589844\n",
            "198/3000 train_loss: 151.33651733398438 test_loss:309.7473449707031\n",
            "199/3000 train_loss: 147.3009490966797 test_loss:304.8234558105469\n",
            "200/3000 train_loss: 145.83328247070312 test_loss:287.2762451171875\n",
            "201/3000 train_loss: 150.03736877441406 test_loss:294.6425476074219\n",
            "202/3000 train_loss: 245.27105712890625 test_loss:289.8232727050781\n",
            "203/3000 train_loss: 140.7631378173828 test_loss:304.322021484375\n",
            "204/3000 train_loss: 162.81234741210938 test_loss:318.9915771484375\n",
            "205/3000 train_loss: 236.20118713378906 test_loss:281.6929626464844\n",
            "206/3000 train_loss: 181.2042694091797 test_loss:291.57867431640625\n",
            "207/3000 train_loss: 204.025390625 test_loss:300.7062683105469\n",
            "208/3000 train_loss: 213.07489013671875 test_loss:287.9709167480469\n",
            "209/3000 train_loss: 166.8020477294922 test_loss:276.3889465332031\n",
            "210/3000 train_loss: 149.14329528808594 test_loss:283.3692626953125\n",
            "211/3000 train_loss: 167.02545166015625 test_loss:268.75982666015625\n",
            "212/3000 train_loss: 158.57064819335938 test_loss:300.3948974609375\n",
            "213/3000 train_loss: 245.64871215820312 test_loss:271.1279296875\n",
            "214/3000 train_loss: 176.28277587890625 test_loss:280.3465270996094\n",
            "215/3000 train_loss: 138.8601531982422 test_loss:276.15887451171875\n",
            "216/3000 train_loss: 152.5919647216797 test_loss:304.14739990234375\n",
            "217/3000 train_loss: 140.45213317871094 test_loss:292.3569030761719\n",
            "218/3000 train_loss: 147.53773498535156 test_loss:268.2740478515625\n",
            "219/3000 train_loss: 203.4519500732422 test_loss:279.1075439453125\n",
            "220/3000 train_loss: 156.82809448242188 test_loss:288.8822326660156\n",
            "221/3000 train_loss: 171.35302734375 test_loss:266.36688232421875\n",
            "222/3000 train_loss: 175.12545776367188 test_loss:287.6809387207031\n",
            "223/3000 train_loss: 256.47576904296875 test_loss:279.3021545410156\n",
            "224/3000 train_loss: 143.32452392578125 test_loss:304.78485107421875\n",
            "225/3000 train_loss: 225.24081420898438 test_loss:282.9954528808594\n",
            "226/3000 train_loss: 163.1913299560547 test_loss:266.96258544921875\n",
            "227/3000 train_loss: 191.48019409179688 test_loss:285.727294921875\n",
            "228/3000 train_loss: 168.6338348388672 test_loss:274.61663818359375\n",
            "229/3000 train_loss: 139.9656524658203 test_loss:291.07501220703125\n",
            "230/3000 train_loss: 163.3046112060547 test_loss:269.9405517578125\n",
            "231/3000 train_loss: 159.0528564453125 test_loss:276.2534484863281\n",
            "232/3000 train_loss: 151.2088165283203 test_loss:272.3844299316406\n",
            "233/3000 train_loss: 152.09375 test_loss:263.7052307128906\n",
            "234/3000 train_loss: 148.92127990722656 test_loss:288.8926696777344\n",
            "235/3000 train_loss: 133.51609802246094 test_loss:275.7008056640625\n",
            "236/3000 train_loss: 165.50054931640625 test_loss:269.55780029296875\n",
            "237/3000 train_loss: 260.0941467285156 test_loss:265.7445068359375\n",
            "238/3000 train_loss: 153.84950256347656 test_loss:269.01556396484375\n",
            "239/3000 train_loss: 160.12037658691406 test_loss:317.5210266113281\n",
            "240/3000 train_loss: 230.8072509765625 test_loss:279.12725830078125\n",
            "241/3000 train_loss: 179.89312744140625 test_loss:261.4529724121094\n",
            "242/3000 train_loss: 137.32464599609375 test_loss:283.47869873046875\n",
            "243/3000 train_loss: 130.8838653564453 test_loss:273.8999328613281\n",
            "244/3000 train_loss: 183.53773498535156 test_loss:266.1451110839844\n",
            "245/3000 train_loss: 134.71102905273438 test_loss:284.137451171875\n",
            "246/3000 train_loss: 144.6240234375 test_loss:257.733154296875\n",
            "247/3000 train_loss: 181.12831115722656 test_loss:274.1742858886719\n",
            "248/3000 train_loss: 136.50283813476562 test_loss:293.9152526855469\n",
            "249/3000 train_loss: 136.62258911132812 test_loss:288.08831787109375\n",
            "250/3000 train_loss: 204.38601684570312 test_loss:263.298095703125\n",
            "251/3000 train_loss: 200.95315551757812 test_loss:254.44345092773438\n",
            "252/3000 train_loss: 153.13241577148438 test_loss:250.22381591796875\n",
            "253/3000 train_loss: 157.3120880126953 test_loss:259.8733825683594\n",
            "254/3000 train_loss: 137.14161682128906 test_loss:293.1860656738281\n",
            "255/3000 train_loss: 184.09678649902344 test_loss:265.7928466796875\n",
            "256/3000 train_loss: 144.31263732910156 test_loss:259.58074951171875\n",
            "257/3000 train_loss: 145.0270538330078 test_loss:258.9311828613281\n",
            "258/3000 train_loss: 154.8951416015625 test_loss:259.1344909667969\n",
            "259/3000 train_loss: 181.18052673339844 test_loss:248.11404418945312\n",
            "260/3000 train_loss: 156.56683349609375 test_loss:247.9656219482422\n",
            "261/3000 train_loss: 196.84730529785156 test_loss:256.87774658203125\n",
            "262/3000 train_loss: 148.0162353515625 test_loss:252.31398010253906\n",
            "263/3000 train_loss: 132.22792053222656 test_loss:282.0868835449219\n",
            "264/3000 train_loss: 267.3464050292969 test_loss:244.93226623535156\n",
            "265/3000 train_loss: 151.90528869628906 test_loss:256.90777587890625\n",
            "266/3000 train_loss: 132.8344268798828 test_loss:259.6043395996094\n",
            "267/3000 train_loss: 168.20445251464844 test_loss:271.24755859375\n",
            "268/3000 train_loss: 291.4637451171875 test_loss:298.679931640625\n",
            "269/3000 train_loss: 143.36549377441406 test_loss:267.85284423828125\n",
            "270/3000 train_loss: 141.41693115234375 test_loss:262.92681884765625\n",
            "271/3000 train_loss: 153.82484436035156 test_loss:298.0129699707031\n",
            "272/3000 train_loss: 157.27542114257812 test_loss:241.8253936767578\n",
            "273/3000 train_loss: 126.26808166503906 test_loss:245.13882446289062\n",
            "274/3000 train_loss: 136.64300537109375 test_loss:259.82647705078125\n",
            "275/3000 train_loss: 136.3580322265625 test_loss:258.8016357421875\n",
            "276/3000 train_loss: 118.7611312866211 test_loss:253.33981323242188\n",
            "277/3000 train_loss: 177.2539520263672 test_loss:263.0328674316406\n",
            "278/3000 train_loss: 134.17820739746094 test_loss:246.15594482421875\n",
            "279/3000 train_loss: 129.35830688476562 test_loss:258.75341796875\n",
            "280/3000 train_loss: 159.25527954101562 test_loss:258.1250305175781\n",
            "281/3000 train_loss: 127.10260009765625 test_loss:244.0821075439453\n",
            "282/3000 train_loss: 152.4392547607422 test_loss:240.29652404785156\n",
            "283/3000 train_loss: 127.55168151855469 test_loss:261.45648193359375\n",
            "284/3000 train_loss: 121.145751953125 test_loss:241.70660400390625\n",
            "285/3000 train_loss: 176.99574279785156 test_loss:248.74185180664062\n",
            "286/3000 train_loss: 134.83839416503906 test_loss:250.69003295898438\n",
            "287/3000 train_loss: 137.39892578125 test_loss:239.3500518798828\n",
            "288/3000 train_loss: 120.84195709228516 test_loss:229.7068328857422\n",
            "289/3000 train_loss: 161.64291381835938 test_loss:266.2531433105469\n",
            "290/3000 train_loss: 152.1016387939453 test_loss:241.88211059570312\n",
            "291/3000 train_loss: 117.10967254638672 test_loss:251.16134643554688\n",
            "292/3000 train_loss: 122.20398712158203 test_loss:237.24710083007812\n",
            "293/3000 train_loss: 125.28749084472656 test_loss:236.35903930664062\n",
            "294/3000 train_loss: 132.36888122558594 test_loss:257.8472595214844\n",
            "295/3000 train_loss: 128.57151794433594 test_loss:237.63824462890625\n",
            "296/3000 train_loss: 132.2252655029297 test_loss:233.6149444580078\n",
            "297/3000 train_loss: 129.34869384765625 test_loss:237.8260955810547\n",
            "298/3000 train_loss: 120.57974243164062 test_loss:236.8245849609375\n",
            "299/3000 train_loss: 123.66551208496094 test_loss:256.7705078125\n",
            "300/3000 train_loss: 130.33749389648438 test_loss:247.0352325439453\n",
            "301/3000 train_loss: 133.9658203125 test_loss:235.2889404296875\n",
            "302/3000 train_loss: 124.25831604003906 test_loss:232.979248046875\n",
            "303/3000 train_loss: 124.29610443115234 test_loss:260.014892578125\n",
            "304/3000 train_loss: 116.37145233154297 test_loss:253.4710693359375\n",
            "305/3000 train_loss: 114.78564453125 test_loss:243.8401336669922\n",
            "306/3000 train_loss: 111.5926742553711 test_loss:222.13394165039062\n",
            "307/3000 train_loss: 119.10836791992188 test_loss:243.61680603027344\n",
            "308/3000 train_loss: 209.13323974609375 test_loss:233.16848754882812\n",
            "309/3000 train_loss: 133.80661010742188 test_loss:227.3719482421875\n",
            "310/3000 train_loss: 144.08067321777344 test_loss:272.7245178222656\n",
            "311/3000 train_loss: 130.5966339111328 test_loss:229.38400268554688\n",
            "312/3000 train_loss: 127.74867248535156 test_loss:235.556884765625\n",
            "313/3000 train_loss: 120.77224731445312 test_loss:244.701416015625\n",
            "314/3000 train_loss: 131.84197998046875 test_loss:233.54763793945312\n",
            "315/3000 train_loss: 123.40505981445312 test_loss:234.9134521484375\n",
            "316/3000 train_loss: 153.24993896484375 test_loss:223.46319580078125\n",
            "317/3000 train_loss: 146.21994018554688 test_loss:232.18687438964844\n",
            "318/3000 train_loss: 123.97064208984375 test_loss:230.62149047851562\n",
            "319/3000 train_loss: 144.19125366210938 test_loss:226.17776489257812\n",
            "320/3000 train_loss: 116.73117065429688 test_loss:247.14126586914062\n",
            "321/3000 train_loss: 131.20986938476562 test_loss:248.5532684326172\n",
            "322/3000 train_loss: 130.52378845214844 test_loss:224.9049835205078\n",
            "323/3000 train_loss: 124.62377166748047 test_loss:233.6387939453125\n",
            "324/3000 train_loss: 125.20187377929688 test_loss:250.2933349609375\n",
            "325/3000 train_loss: 105.02348327636719 test_loss:223.95217895507812\n",
            "326/3000 train_loss: 164.83543395996094 test_loss:234.58053588867188\n",
            "327/3000 train_loss: 131.4626007080078 test_loss:256.6219482421875\n",
            "328/3000 train_loss: 124.54685974121094 test_loss:223.53555297851562\n",
            "329/3000 train_loss: 131.8821563720703 test_loss:218.32862854003906\n",
            "330/3000 train_loss: 105.99097442626953 test_loss:227.99783325195312\n",
            "331/3000 train_loss: 121.96989440917969 test_loss:232.388671875\n",
            "332/3000 train_loss: 109.26042938232422 test_loss:227.6254119873047\n",
            "333/3000 train_loss: 175.26806640625 test_loss:224.7026824951172\n",
            "334/3000 train_loss: 115.32361602783203 test_loss:226.55755615234375\n",
            "335/3000 train_loss: 103.56790924072266 test_loss:229.71839904785156\n",
            "336/3000 train_loss: 114.31230163574219 test_loss:215.49508666992188\n",
            "337/3000 train_loss: 113.14433288574219 test_loss:232.90538024902344\n",
            "338/3000 train_loss: 141.70596313476562 test_loss:219.22215270996094\n",
            "339/3000 train_loss: 101.42168426513672 test_loss:223.47512817382812\n",
            "340/3000 train_loss: 90.35595703125 test_loss:223.06015014648438\n",
            "341/3000 train_loss: 113.1676254272461 test_loss:214.13369750976562\n",
            "342/3000 train_loss: 110.0755615234375 test_loss:219.42755126953125\n",
            "343/3000 train_loss: 116.53352355957031 test_loss:213.49920654296875\n",
            "344/3000 train_loss: 161.4618377685547 test_loss:230.39321899414062\n",
            "345/3000 train_loss: 98.36430358886719 test_loss:218.3952178955078\n",
            "346/3000 train_loss: 99.26972198486328 test_loss:210.6517333984375\n",
            "347/3000 train_loss: 114.3725357055664 test_loss:210.92715454101562\n",
            "348/3000 train_loss: 113.55982208251953 test_loss:227.56512451171875\n",
            "349/3000 train_loss: 109.9989242553711 test_loss:214.07920837402344\n",
            "350/3000 train_loss: 117.1265869140625 test_loss:208.3258514404297\n",
            "351/3000 train_loss: 114.56083679199219 test_loss:239.52459716796875\n",
            "352/3000 train_loss: 107.33283996582031 test_loss:221.3427276611328\n",
            "353/3000 train_loss: 116.26649475097656 test_loss:207.55889892578125\n",
            "354/3000 train_loss: 108.14624786376953 test_loss:212.35047912597656\n",
            "355/3000 train_loss: 108.76387023925781 test_loss:215.6005096435547\n",
            "356/3000 train_loss: 119.02472686767578 test_loss:211.28379821777344\n",
            "357/3000 train_loss: 103.56661987304688 test_loss:205.41119384765625\n",
            "358/3000 train_loss: 103.7034912109375 test_loss:204.787841796875\n",
            "359/3000 train_loss: 108.28378295898438 test_loss:207.58822631835938\n",
            "360/3000 train_loss: 127.8388442993164 test_loss:203.2977294921875\n",
            "361/3000 train_loss: 143.2105712890625 test_loss:244.35400390625\n",
            "362/3000 train_loss: 132.8883056640625 test_loss:210.76425170898438\n",
            "363/3000 train_loss: 109.7592544555664 test_loss:200.83395385742188\n",
            "364/3000 train_loss: 125.85488891601562 test_loss:206.20449829101562\n",
            "365/3000 train_loss: 93.44256591796875 test_loss:214.09649658203125\n",
            "366/3000 train_loss: 109.94625091552734 test_loss:207.07737731933594\n",
            "367/3000 train_loss: 100.20858001708984 test_loss:205.42495727539062\n",
            "368/3000 train_loss: 112.562255859375 test_loss:221.60972595214844\n",
            "369/3000 train_loss: 99.86306762695312 test_loss:201.42160034179688\n",
            "370/3000 train_loss: 99.06319427490234 test_loss:199.4605712890625\n",
            "371/3000 train_loss: 106.87672424316406 test_loss:221.8888702392578\n",
            "372/3000 train_loss: 95.5036849975586 test_loss:196.58282470703125\n",
            "373/3000 train_loss: 96.13614654541016 test_loss:191.97769165039062\n",
            "374/3000 train_loss: 132.87628173828125 test_loss:200.33355712890625\n",
            "375/3000 train_loss: 104.6876220703125 test_loss:208.8909454345703\n",
            "376/3000 train_loss: 96.45221710205078 test_loss:201.86752319335938\n",
            "377/3000 train_loss: 87.03860473632812 test_loss:205.09518432617188\n",
            "378/3000 train_loss: 118.14501190185547 test_loss:209.01303100585938\n",
            "379/3000 train_loss: 93.1255874633789 test_loss:221.62403869628906\n",
            "380/3000 train_loss: 106.5414810180664 test_loss:197.97109985351562\n",
            "381/3000 train_loss: 132.33714294433594 test_loss:197.3748016357422\n",
            "382/3000 train_loss: 111.31688690185547 test_loss:212.0372772216797\n",
            "383/3000 train_loss: 104.87142181396484 test_loss:200.79824829101562\n",
            "384/3000 train_loss: 92.76799011230469 test_loss:223.71542358398438\n",
            "385/3000 train_loss: 100.47174835205078 test_loss:220.56427001953125\n",
            "386/3000 train_loss: 90.06952667236328 test_loss:196.17672729492188\n",
            "387/3000 train_loss: 133.27871704101562 test_loss:204.74270629882812\n",
            "388/3000 train_loss: 99.6099624633789 test_loss:224.08450317382812\n",
            "389/3000 train_loss: 118.77386474609375 test_loss:206.66650390625\n",
            "390/3000 train_loss: 84.84848022460938 test_loss:208.18685913085938\n",
            "391/3000 train_loss: 94.06807708740234 test_loss:189.2738037109375\n",
            "392/3000 train_loss: 113.89151000976562 test_loss:201.86590576171875\n",
            "393/3000 train_loss: 96.44146728515625 test_loss:197.974609375\n",
            "394/3000 train_loss: 118.93065643310547 test_loss:221.41294860839844\n",
            "395/3000 train_loss: 117.45194244384766 test_loss:198.90126037597656\n",
            "396/3000 train_loss: 103.38748931884766 test_loss:203.62242126464844\n",
            "397/3000 train_loss: 168.7631378173828 test_loss:204.99899291992188\n",
            "398/3000 train_loss: 120.6478500366211 test_loss:214.64602661132812\n",
            "399/3000 train_loss: 93.58924102783203 test_loss:200.40853881835938\n",
            "400/3000 train_loss: 94.61029052734375 test_loss:193.5247802734375\n",
            "401/3000 train_loss: 102.84220123291016 test_loss:198.26531982421875\n",
            "402/3000 train_loss: 114.40180969238281 test_loss:194.22421264648438\n",
            "403/3000 train_loss: 107.3971176147461 test_loss:190.09225463867188\n",
            "404/3000 train_loss: 108.29412841796875 test_loss:192.52630615234375\n",
            "405/3000 train_loss: 98.35810852050781 test_loss:182.84339904785156\n",
            "406/3000 train_loss: 128.5400390625 test_loss:198.02593994140625\n",
            "407/3000 train_loss: 105.70850372314453 test_loss:190.080810546875\n",
            "408/3000 train_loss: 91.80826568603516 test_loss:211.31158447265625\n",
            "409/3000 train_loss: 95.55078125 test_loss:190.38333129882812\n",
            "410/3000 train_loss: 88.9874267578125 test_loss:184.42811584472656\n",
            "411/3000 train_loss: 117.7341537475586 test_loss:184.92724609375\n",
            "412/3000 train_loss: 96.2389907836914 test_loss:196.24380493164062\n",
            "413/3000 train_loss: 89.92361450195312 test_loss:197.79995727539062\n",
            "414/3000 train_loss: 110.67045593261719 test_loss:220.3603973388672\n",
            "415/3000 train_loss: 116.22260284423828 test_loss:205.21017456054688\n",
            "416/3000 train_loss: 96.94660949707031 test_loss:211.67820739746094\n",
            "417/3000 train_loss: 96.0015869140625 test_loss:191.71527099609375\n",
            "418/3000 train_loss: 101.00286865234375 test_loss:189.30020141601562\n",
            "419/3000 train_loss: 105.60480499267578 test_loss:192.63565063476562\n",
            "420/3000 train_loss: 97.66825866699219 test_loss:201.77276611328125\n",
            "421/3000 train_loss: 89.28451538085938 test_loss:209.48260498046875\n",
            "422/3000 train_loss: 93.89471435546875 test_loss:198.1696319580078\n",
            "423/3000 train_loss: 94.1520004272461 test_loss:200.9178466796875\n",
            "424/3000 train_loss: 87.38341522216797 test_loss:204.25926208496094\n",
            "425/3000 train_loss: 129.3722686767578 test_loss:195.344970703125\n",
            "426/3000 train_loss: 94.29075622558594 test_loss:191.24362182617188\n",
            "427/3000 train_loss: 92.99656677246094 test_loss:188.3367919921875\n",
            "428/3000 train_loss: 96.38504028320312 test_loss:185.0670166015625\n",
            "429/3000 train_loss: 83.75272369384766 test_loss:204.93502807617188\n",
            "430/3000 train_loss: 94.75277709960938 test_loss:190.24000549316406\n",
            "431/3000 train_loss: 90.81437683105469 test_loss:174.0229034423828\n",
            "432/3000 train_loss: 92.57589721679688 test_loss:184.9436798095703\n",
            "433/3000 train_loss: 85.05730438232422 test_loss:177.5242156982422\n",
            "434/3000 train_loss: 106.29888153076172 test_loss:183.43923950195312\n",
            "435/3000 train_loss: 155.99691772460938 test_loss:248.9756317138672\n",
            "436/3000 train_loss: 115.54618835449219 test_loss:201.85800170898438\n",
            "437/3000 train_loss: 89.84866333007812 test_loss:172.36697387695312\n",
            "438/3000 train_loss: 102.56090545654297 test_loss:196.5458221435547\n",
            "439/3000 train_loss: 94.20907592773438 test_loss:198.37356567382812\n",
            "440/3000 train_loss: 92.89970397949219 test_loss:185.96240234375\n",
            "441/3000 train_loss: 88.72185516357422 test_loss:191.15272521972656\n",
            "442/3000 train_loss: 109.46125030517578 test_loss:192.68283081054688\n",
            "443/3000 train_loss: 83.8851089477539 test_loss:187.37765502929688\n",
            "444/3000 train_loss: 87.36714172363281 test_loss:180.60707092285156\n",
            "445/3000 train_loss: 90.78136444091797 test_loss:188.18283081054688\n",
            "446/3000 train_loss: 118.80289459228516 test_loss:188.10086059570312\n",
            "447/3000 train_loss: 88.73770904541016 test_loss:182.31631469726562\n",
            "448/3000 train_loss: 93.9517593383789 test_loss:176.26361083984375\n",
            "449/3000 train_loss: 91.01261901855469 test_loss:175.037109375\n",
            "450/3000 train_loss: 90.50802612304688 test_loss:176.69638061523438\n",
            "451/3000 train_loss: 103.36544036865234 test_loss:183.85244750976562\n",
            "452/3000 train_loss: 89.0020751953125 test_loss:180.38348388671875\n",
            "453/3000 train_loss: 88.00261688232422 test_loss:179.21212768554688\n",
            "454/3000 train_loss: 102.4913330078125 test_loss:170.90573120117188\n",
            "455/3000 train_loss: 89.05433654785156 test_loss:186.13731384277344\n",
            "456/3000 train_loss: 79.9193344116211 test_loss:183.76101684570312\n",
            "457/3000 train_loss: 87.97527313232422 test_loss:174.08047485351562\n",
            "458/3000 train_loss: 77.03936767578125 test_loss:175.27716064453125\n",
            "459/3000 train_loss: 78.31217956542969 test_loss:181.4551239013672\n",
            "460/3000 train_loss: 80.64933013916016 test_loss:173.5130615234375\n",
            "461/3000 train_loss: 91.41954040527344 test_loss:175.0185546875\n",
            "462/3000 train_loss: 98.88357543945312 test_loss:188.63705444335938\n",
            "463/3000 train_loss: 87.5455322265625 test_loss:187.43777465820312\n",
            "464/3000 train_loss: 79.73322296142578 test_loss:178.7431640625\n",
            "465/3000 train_loss: 83.53876495361328 test_loss:181.7091827392578\n",
            "466/3000 train_loss: 87.0571517944336 test_loss:190.0218505859375\n",
            "467/3000 train_loss: 77.85682678222656 test_loss:171.44976806640625\n",
            "468/3000 train_loss: 93.65425872802734 test_loss:174.58509826660156\n",
            "469/3000 train_loss: 98.92744445800781 test_loss:179.6822052001953\n",
            "470/3000 train_loss: 95.25357055664062 test_loss:184.87091064453125\n",
            "471/3000 train_loss: 104.8987808227539 test_loss:185.5554962158203\n",
            "472/3000 train_loss: 90.4697265625 test_loss:176.46295166015625\n",
            "473/3000 train_loss: 109.11048889160156 test_loss:189.70562744140625\n",
            "474/3000 train_loss: 80.92106628417969 test_loss:175.72991943359375\n",
            "475/3000 train_loss: 93.22464752197266 test_loss:182.49835205078125\n",
            "476/3000 train_loss: 95.33478546142578 test_loss:178.54254150390625\n",
            "477/3000 train_loss: 86.78131103515625 test_loss:171.40037536621094\n",
            "478/3000 train_loss: 108.07634735107422 test_loss:176.90472412109375\n",
            "479/3000 train_loss: 85.57018280029297 test_loss:190.03785705566406\n",
            "480/3000 train_loss: 85.62169647216797 test_loss:183.8966827392578\n",
            "481/3000 train_loss: 135.20469665527344 test_loss:197.91343688964844\n",
            "482/3000 train_loss: 101.21001434326172 test_loss:196.92181396484375\n",
            "483/3000 train_loss: 83.07209014892578 test_loss:185.30801391601562\n",
            "484/3000 train_loss: 96.2606430053711 test_loss:172.12451171875\n",
            "485/3000 train_loss: 94.89220428466797 test_loss:175.41424560546875\n",
            "486/3000 train_loss: 78.91026306152344 test_loss:172.77024841308594\n",
            "487/3000 train_loss: 86.3512191772461 test_loss:169.38287353515625\n",
            "488/3000 train_loss: 81.82547760009766 test_loss:169.81729125976562\n",
            "489/3000 train_loss: 87.02853393554688 test_loss:168.57440185546875\n",
            "490/3000 train_loss: 82.97103881835938 test_loss:178.2443389892578\n",
            "491/3000 train_loss: 89.19527435302734 test_loss:177.79351806640625\n",
            "492/3000 train_loss: 90.21172332763672 test_loss:170.45626831054688\n",
            "493/3000 train_loss: 85.65727233886719 test_loss:172.16275024414062\n",
            "494/3000 train_loss: 75.95857238769531 test_loss:182.2893524169922\n",
            "495/3000 train_loss: 83.96942901611328 test_loss:182.77293395996094\n",
            "496/3000 train_loss: 108.15574645996094 test_loss:172.65447998046875\n",
            "497/3000 train_loss: 80.77684783935547 test_loss:180.590576171875\n",
            "498/3000 train_loss: 80.12109375 test_loss:177.4658203125\n",
            "499/3000 train_loss: 75.4904556274414 test_loss:164.9608917236328\n",
            "500/3000 train_loss: 82.85836029052734 test_loss:168.27813720703125\n",
            "501/3000 train_loss: 89.9871597290039 test_loss:177.99325561523438\n",
            "502/3000 train_loss: 104.92906188964844 test_loss:181.04025268554688\n",
            "503/3000 train_loss: 101.28417205810547 test_loss:175.98330688476562\n",
            "504/3000 train_loss: 80.18333435058594 test_loss:163.24496459960938\n",
            "505/3000 train_loss: 93.7396469116211 test_loss:182.78628540039062\n",
            "506/3000 train_loss: 87.43296813964844 test_loss:190.05828857421875\n",
            "507/3000 train_loss: 75.45561981201172 test_loss:169.4753875732422\n",
            "508/3000 train_loss: 77.7825698852539 test_loss:170.80377197265625\n",
            "509/3000 train_loss: 74.56689453125 test_loss:173.3380126953125\n",
            "510/3000 train_loss: 78.98040771484375 test_loss:175.55474853515625\n",
            "511/3000 train_loss: 73.43579864501953 test_loss:169.0272216796875\n",
            "512/3000 train_loss: 88.0189208984375 test_loss:178.2187042236328\n",
            "513/3000 train_loss: 89.9344711303711 test_loss:180.7391357421875\n",
            "514/3000 train_loss: 74.84529113769531 test_loss:165.33523559570312\n",
            "515/3000 train_loss: 82.8525161743164 test_loss:163.07232666015625\n",
            "516/3000 train_loss: 83.84956359863281 test_loss:165.25442504882812\n",
            "517/3000 train_loss: 73.12733459472656 test_loss:162.47850036621094\n",
            "518/3000 train_loss: 72.94166564941406 test_loss:163.78448486328125\n",
            "519/3000 train_loss: 81.76991271972656 test_loss:179.30577087402344\n",
            "520/3000 train_loss: 86.40991973876953 test_loss:167.88772583007812\n",
            "521/3000 train_loss: 76.62822723388672 test_loss:159.24871826171875\n",
            "522/3000 train_loss: 70.68582153320312 test_loss:158.17977905273438\n",
            "523/3000 train_loss: 74.28953552246094 test_loss:163.02212524414062\n",
            "524/3000 train_loss: 80.74517822265625 test_loss:167.1164093017578\n",
            "525/3000 train_loss: 70.5414047241211 test_loss:158.37583923339844\n",
            "526/3000 train_loss: 99.31592559814453 test_loss:159.1343231201172\n",
            "527/3000 train_loss: 92.24443817138672 test_loss:176.94955444335938\n",
            "528/3000 train_loss: 80.5965347290039 test_loss:188.88900756835938\n",
            "529/3000 train_loss: 84.69945526123047 test_loss:178.80136108398438\n",
            "530/3000 train_loss: 73.5696029663086 test_loss:165.29556274414062\n",
            "531/3000 train_loss: 86.33980560302734 test_loss:167.7423095703125\n",
            "532/3000 train_loss: 109.0748291015625 test_loss:187.18099975585938\n",
            "533/3000 train_loss: 78.54251861572266 test_loss:176.11270141601562\n",
            "534/3000 train_loss: 81.86251831054688 test_loss:162.34024047851562\n",
            "535/3000 train_loss: 74.84465789794922 test_loss:170.1234130859375\n",
            "536/3000 train_loss: 71.37873840332031 test_loss:162.45401000976562\n",
            "537/3000 train_loss: 72.46505737304688 test_loss:163.02352905273438\n",
            "538/3000 train_loss: 69.5555648803711 test_loss:158.17735290527344\n",
            "539/3000 train_loss: 72.40153503417969 test_loss:161.0675048828125\n",
            "540/3000 train_loss: 75.4455337524414 test_loss:163.70726013183594\n",
            "541/3000 train_loss: 75.8054428100586 test_loss:164.85687255859375\n",
            "542/3000 train_loss: 76.70477294921875 test_loss:160.73077392578125\n",
            "543/3000 train_loss: 77.11190032958984 test_loss:164.02859497070312\n",
            "544/3000 train_loss: 78.70207214355469 test_loss:166.94577026367188\n",
            "545/3000 train_loss: 76.21541595458984 test_loss:164.34974670410156\n",
            "546/3000 train_loss: 84.6760025024414 test_loss:168.9153594970703\n",
            "547/3000 train_loss: 68.35232543945312 test_loss:161.25048828125\n",
            "548/3000 train_loss: 79.3480224609375 test_loss:157.03662109375\n",
            "549/3000 train_loss: 108.29737854003906 test_loss:156.6614990234375\n",
            "550/3000 train_loss: 68.39517974853516 test_loss:169.15737915039062\n",
            "551/3000 train_loss: 75.59040069580078 test_loss:159.16270446777344\n",
            "552/3000 train_loss: 85.25782012939453 test_loss:160.85308837890625\n",
            "553/3000 train_loss: 74.85647583007812 test_loss:160.60391235351562\n",
            "554/3000 train_loss: 72.28327178955078 test_loss:152.32772827148438\n",
            "555/3000 train_loss: 77.79273986816406 test_loss:172.65884399414062\n",
            "556/3000 train_loss: 88.86579132080078 test_loss:168.11715698242188\n",
            "557/3000 train_loss: 83.29264831542969 test_loss:166.76425170898438\n",
            "558/3000 train_loss: 67.56782531738281 test_loss:159.11724853515625\n",
            "559/3000 train_loss: 75.72858428955078 test_loss:157.70697021484375\n",
            "560/3000 train_loss: 79.19031524658203 test_loss:159.65769958496094\n",
            "561/3000 train_loss: 72.24382019042969 test_loss:152.35833740234375\n",
            "562/3000 train_loss: 78.7057113647461 test_loss:161.77597045898438\n",
            "563/3000 train_loss: 78.18426513671875 test_loss:161.0321502685547\n",
            "564/3000 train_loss: 74.90845489501953 test_loss:163.63943481445312\n",
            "565/3000 train_loss: 67.57989501953125 test_loss:159.59408569335938\n",
            "566/3000 train_loss: 74.23457336425781 test_loss:151.98956298828125\n",
            "567/3000 train_loss: 77.66788482666016 test_loss:164.6278076171875\n",
            "568/3000 train_loss: 66.97984313964844 test_loss:156.13661193847656\n",
            "569/3000 train_loss: 88.609130859375 test_loss:164.20364379882812\n",
            "570/3000 train_loss: 85.17674255371094 test_loss:180.84326171875\n",
            "571/3000 train_loss: 70.93569946289062 test_loss:162.70059204101562\n",
            "572/3000 train_loss: 75.2767562866211 test_loss:181.07223510742188\n",
            "573/3000 train_loss: 75.01068115234375 test_loss:165.0730743408203\n",
            "574/3000 train_loss: 58.9059944152832 test_loss:150.17483520507812\n",
            "575/3000 train_loss: 87.56330871582031 test_loss:158.9864959716797\n",
            "576/3000 train_loss: 71.85311126708984 test_loss:159.9679718017578\n",
            "577/3000 train_loss: 70.4929428100586 test_loss:155.83566284179688\n",
            "578/3000 train_loss: 59.485198974609375 test_loss:154.83428955078125\n",
            "579/3000 train_loss: 71.01464080810547 test_loss:161.94500732421875\n",
            "580/3000 train_loss: 71.22592163085938 test_loss:170.1387939453125\n",
            "581/3000 train_loss: 74.88001251220703 test_loss:154.03782653808594\n",
            "582/3000 train_loss: 65.80628204345703 test_loss:148.93003845214844\n",
            "583/3000 train_loss: 78.46855163574219 test_loss:154.6046142578125\n",
            "584/3000 train_loss: 67.32609558105469 test_loss:159.14186096191406\n",
            "585/3000 train_loss: 62.24391174316406 test_loss:163.18467712402344\n",
            "586/3000 train_loss: 123.23636627197266 test_loss:165.83090209960938\n",
            "587/3000 train_loss: 77.52749633789062 test_loss:170.3507843017578\n",
            "588/3000 train_loss: 82.51253509521484 test_loss:172.8283233642578\n",
            "589/3000 train_loss: 83.74201202392578 test_loss:171.51498413085938\n",
            "590/3000 train_loss: 71.1864013671875 test_loss:164.72509765625\n",
            "591/3000 train_loss: 68.57917022705078 test_loss:162.7560272216797\n",
            "592/3000 train_loss: 105.21562194824219 test_loss:176.303955078125\n",
            "593/3000 train_loss: 102.25711059570312 test_loss:189.32589721679688\n",
            "594/3000 train_loss: 73.8310546875 test_loss:158.5532989501953\n",
            "595/3000 train_loss: 76.21320343017578 test_loss:156.37588500976562\n",
            "596/3000 train_loss: 69.27849578857422 test_loss:154.6915740966797\n",
            "597/3000 train_loss: 74.59478759765625 test_loss:153.53330993652344\n",
            "598/3000 train_loss: 69.50894927978516 test_loss:149.53515625\n",
            "599/3000 train_loss: 72.43299102783203 test_loss:153.7431182861328\n",
            "600/3000 train_loss: 84.83069610595703 test_loss:155.5437469482422\n",
            "601/3000 train_loss: 88.41654968261719 test_loss:162.7472686767578\n",
            "602/3000 train_loss: 78.86040496826172 test_loss:163.9649658203125\n",
            "603/3000 train_loss: 68.69564056396484 test_loss:158.07676696777344\n",
            "604/3000 train_loss: 76.763916015625 test_loss:154.62191772460938\n",
            "605/3000 train_loss: 77.06782531738281 test_loss:162.71182250976562\n",
            "606/3000 train_loss: 89.46781921386719 test_loss:162.99095153808594\n",
            "607/3000 train_loss: 81.62178802490234 test_loss:154.91712951660156\n",
            "608/3000 train_loss: 70.76095581054688 test_loss:152.4881591796875\n",
            "609/3000 train_loss: 74.23350524902344 test_loss:150.9163818359375\n",
            "610/3000 train_loss: 74.37557983398438 test_loss:156.77174377441406\n",
            "611/3000 train_loss: 71.1700439453125 test_loss:154.31768798828125\n",
            "612/3000 train_loss: 63.37211608886719 test_loss:155.0160369873047\n",
            "613/3000 train_loss: 61.396087646484375 test_loss:160.8831787109375\n",
            "614/3000 train_loss: 73.03295135498047 test_loss:173.8336944580078\n",
            "615/3000 train_loss: 77.73773956298828 test_loss:157.3321533203125\n",
            "616/3000 train_loss: 76.77259826660156 test_loss:152.52159118652344\n",
            "617/3000 train_loss: 64.92730712890625 test_loss:155.2922821044922\n",
            "618/3000 train_loss: 75.72614288330078 test_loss:157.23898315429688\n",
            "619/3000 train_loss: 72.69731903076172 test_loss:146.3428497314453\n",
            "620/3000 train_loss: 70.67694091796875 test_loss:153.90756225585938\n",
            "621/3000 train_loss: 68.49208068847656 test_loss:151.66485595703125\n",
            "622/3000 train_loss: 80.91139221191406 test_loss:151.63880920410156\n",
            "623/3000 train_loss: 68.05634307861328 test_loss:157.76437377929688\n",
            "624/3000 train_loss: 79.4273681640625 test_loss:159.10113525390625\n",
            "625/3000 train_loss: 79.99346160888672 test_loss:158.96145629882812\n",
            "626/3000 train_loss: 63.96258544921875 test_loss:155.8918914794922\n",
            "627/3000 train_loss: 66.87773895263672 test_loss:151.6291046142578\n",
            "628/3000 train_loss: 68.35449981689453 test_loss:155.48960876464844\n",
            "629/3000 train_loss: 71.81515502929688 test_loss:157.1403045654297\n",
            "630/3000 train_loss: 65.36768341064453 test_loss:146.38270568847656\n",
            "631/3000 train_loss: 70.8354263305664 test_loss:153.7003631591797\n",
            "632/3000 train_loss: 64.089111328125 test_loss:157.6663818359375\n",
            "633/3000 train_loss: 66.20249938964844 test_loss:162.07464599609375\n",
            "634/3000 train_loss: 63.56663513183594 test_loss:150.76983642578125\n",
            "635/3000 train_loss: 70.85240173339844 test_loss:143.65394592285156\n",
            "636/3000 train_loss: 68.99336242675781 test_loss:155.17103576660156\n",
            "637/3000 train_loss: 69.11821746826172 test_loss:154.23756408691406\n",
            "638/3000 train_loss: 68.96602630615234 test_loss:146.4514617919922\n",
            "639/3000 train_loss: 60.05575942993164 test_loss:150.45481872558594\n",
            "640/3000 train_loss: 74.3404769897461 test_loss:148.14500427246094\n",
            "641/3000 train_loss: 69.53019714355469 test_loss:146.42599487304688\n",
            "642/3000 train_loss: 65.63130187988281 test_loss:151.81515502929688\n",
            "643/3000 train_loss: 65.36792755126953 test_loss:152.99008178710938\n",
            "644/3000 train_loss: 74.58445739746094 test_loss:152.12925720214844\n",
            "645/3000 train_loss: 76.48124694824219 test_loss:151.08277893066406\n",
            "646/3000 train_loss: 66.90231323242188 test_loss:150.4898223876953\n",
            "647/3000 train_loss: 67.98753356933594 test_loss:144.47164916992188\n",
            "648/3000 train_loss: 72.54537200927734 test_loss:145.42745971679688\n",
            "649/3000 train_loss: 64.7597427368164 test_loss:144.6051483154297\n",
            "650/3000 train_loss: 90.1817398071289 test_loss:155.90414428710938\n",
            "651/3000 train_loss: 61.99597930908203 test_loss:143.3802490234375\n",
            "652/3000 train_loss: 70.63584899902344 test_loss:139.18504333496094\n",
            "653/3000 train_loss: 89.7155990600586 test_loss:153.02410888671875\n",
            "654/3000 train_loss: 73.92803192138672 test_loss:152.1261444091797\n",
            "655/3000 train_loss: 60.50473403930664 test_loss:143.1729278564453\n",
            "656/3000 train_loss: 71.4863052368164 test_loss:140.66790771484375\n",
            "657/3000 train_loss: 64.78590393066406 test_loss:141.1892547607422\n",
            "658/3000 train_loss: 65.70674896240234 test_loss:147.99899291992188\n",
            "659/3000 train_loss: 73.27774047851562 test_loss:137.63235473632812\n",
            "660/3000 train_loss: 57.00646209716797 test_loss:139.18603515625\n",
            "661/3000 train_loss: 71.28042602539062 test_loss:150.22596740722656\n",
            "662/3000 train_loss: 68.1993179321289 test_loss:151.32423400878906\n",
            "663/3000 train_loss: 60.39613342285156 test_loss:143.3807830810547\n",
            "664/3000 train_loss: 93.10604858398438 test_loss:143.0718536376953\n",
            "665/3000 train_loss: 71.68453979492188 test_loss:151.16151428222656\n",
            "666/3000 train_loss: 81.94530487060547 test_loss:148.07437133789062\n",
            "667/3000 train_loss: 68.8360824584961 test_loss:149.0057373046875\n",
            "668/3000 train_loss: 68.83052825927734 test_loss:140.1942901611328\n",
            "669/3000 train_loss: 61.17224884033203 test_loss:149.7783660888672\n",
            "670/3000 train_loss: 67.27764129638672 test_loss:153.48989868164062\n",
            "671/3000 train_loss: 62.604576110839844 test_loss:137.62454223632812\n",
            "672/3000 train_loss: 54.32538986206055 test_loss:141.06246948242188\n",
            "673/3000 train_loss: 66.41748809814453 test_loss:143.36441040039062\n",
            "674/3000 train_loss: 65.05023956298828 test_loss:148.77647399902344\n",
            "675/3000 train_loss: 58.78561019897461 test_loss:138.61376953125\n",
            "676/3000 train_loss: 59.342376708984375 test_loss:138.39312744140625\n",
            "677/3000 train_loss: 57.331565856933594 test_loss:136.50180053710938\n",
            "678/3000 train_loss: 56.039424896240234 test_loss:143.2594757080078\n",
            "679/3000 train_loss: 64.05967712402344 test_loss:139.7200469970703\n",
            "680/3000 train_loss: 69.07785034179688 test_loss:158.59457397460938\n",
            "681/3000 train_loss: 62.01023864746094 test_loss:140.0834197998047\n",
            "682/3000 train_loss: 56.380855560302734 test_loss:135.17091369628906\n",
            "683/3000 train_loss: 60.365333557128906 test_loss:139.9019775390625\n",
            "684/3000 train_loss: 57.587432861328125 test_loss:139.2814178466797\n",
            "685/3000 train_loss: 66.87564849853516 test_loss:144.39901733398438\n",
            "686/3000 train_loss: 62.585838317871094 test_loss:136.2101593017578\n",
            "687/3000 train_loss: 58.84540939331055 test_loss:138.591796875\n",
            "688/3000 train_loss: 58.464630126953125 test_loss:137.30157470703125\n",
            "689/3000 train_loss: 67.93306732177734 test_loss:137.7734832763672\n",
            "690/3000 train_loss: 72.86734008789062 test_loss:137.70199584960938\n",
            "691/3000 train_loss: 64.1373519897461 test_loss:142.61537170410156\n",
            "692/3000 train_loss: 80.526123046875 test_loss:136.32037353515625\n",
            "693/3000 train_loss: 72.31224060058594 test_loss:141.28482055664062\n",
            "694/3000 train_loss: 66.04011535644531 test_loss:135.00265502929688\n",
            "695/3000 train_loss: 79.11366271972656 test_loss:147.4334716796875\n",
            "696/3000 train_loss: 72.06895446777344 test_loss:141.0318603515625\n",
            "697/3000 train_loss: 62.353302001953125 test_loss:140.74423217773438\n",
            "698/3000 train_loss: 64.94938659667969 test_loss:142.02841186523438\n",
            "699/3000 train_loss: 59.838314056396484 test_loss:135.3806610107422\n",
            "700/3000 train_loss: 68.09003448486328 test_loss:144.1587677001953\n",
            "701/3000 train_loss: 56.906070709228516 test_loss:148.51611328125\n",
            "702/3000 train_loss: 59.26250076293945 test_loss:143.95599365234375\n",
            "703/3000 train_loss: 61.65517807006836 test_loss:139.54074096679688\n",
            "704/3000 train_loss: 68.88645935058594 test_loss:141.2224884033203\n",
            "705/3000 train_loss: 61.324668884277344 test_loss:142.99874877929688\n",
            "706/3000 train_loss: 73.95081329345703 test_loss:140.02850341796875\n",
            "707/3000 train_loss: 63.78334045410156 test_loss:140.43157958984375\n",
            "708/3000 train_loss: 79.90653991699219 test_loss:149.259521484375\n",
            "709/3000 train_loss: 60.8780517578125 test_loss:139.07843017578125\n",
            "710/3000 train_loss: 61.562644958496094 test_loss:133.3988037109375\n",
            "711/3000 train_loss: 65.81333923339844 test_loss:140.98707580566406\n",
            "712/3000 train_loss: 56.555233001708984 test_loss:137.20962524414062\n",
            "713/3000 train_loss: 62.63411331176758 test_loss:143.36190795898438\n",
            "714/3000 train_loss: 61.42618942260742 test_loss:141.8927001953125\n",
            "715/3000 train_loss: 60.533424377441406 test_loss:138.62905883789062\n",
            "716/3000 train_loss: 57.83396530151367 test_loss:130.30091857910156\n",
            "717/3000 train_loss: 54.93646240234375 test_loss:130.39373779296875\n",
            "718/3000 train_loss: 71.72823333740234 test_loss:134.06539916992188\n",
            "719/3000 train_loss: 56.76056671142578 test_loss:132.44268798828125\n",
            "720/3000 train_loss: 57.904964447021484 test_loss:137.92495727539062\n",
            "721/3000 train_loss: 59.03348922729492 test_loss:135.44810485839844\n",
            "722/3000 train_loss: 79.17095947265625 test_loss:137.23617553710938\n",
            "723/3000 train_loss: 57.94566345214844 test_loss:130.50753784179688\n",
            "724/3000 train_loss: 60.31016540527344 test_loss:132.78529357910156\n",
            "725/3000 train_loss: 62.29821014404297 test_loss:133.48361206054688\n",
            "726/3000 train_loss: 74.88666534423828 test_loss:131.8487091064453\n",
            "727/3000 train_loss: 63.906494140625 test_loss:131.65789794921875\n",
            "728/3000 train_loss: 93.15322875976562 test_loss:146.79689025878906\n",
            "729/3000 train_loss: 57.607017517089844 test_loss:135.19129943847656\n",
            "730/3000 train_loss: 71.72015380859375 test_loss:141.09054565429688\n",
            "731/3000 train_loss: 52.78041458129883 test_loss:135.4153594970703\n",
            "732/3000 train_loss: 64.42120361328125 test_loss:132.0906219482422\n",
            "733/3000 train_loss: 68.8546371459961 test_loss:149.30702209472656\n",
            "734/3000 train_loss: 55.55314636230469 test_loss:128.15066528320312\n",
            "735/3000 train_loss: 61.005977630615234 test_loss:132.67605590820312\n",
            "736/3000 train_loss: 66.006591796875 test_loss:132.63063049316406\n",
            "737/3000 train_loss: 65.97254180908203 test_loss:130.43710327148438\n",
            "738/3000 train_loss: 61.36867904663086 test_loss:138.03858947753906\n",
            "739/3000 train_loss: 68.37370300292969 test_loss:128.50233459472656\n",
            "740/3000 train_loss: 61.5260009765625 test_loss:132.69891357421875\n",
            "741/3000 train_loss: 55.13412094116211 test_loss:129.62942504882812\n",
            "742/3000 train_loss: 64.71089935302734 test_loss:134.9393768310547\n",
            "743/3000 train_loss: 66.78038024902344 test_loss:136.69766235351562\n",
            "744/3000 train_loss: 61.19550704956055 test_loss:131.29518127441406\n",
            "745/3000 train_loss: 63.923919677734375 test_loss:135.9460906982422\n",
            "746/3000 train_loss: 75.71212005615234 test_loss:135.10606384277344\n",
            "747/3000 train_loss: 59.32011795043945 test_loss:136.6220703125\n",
            "748/3000 train_loss: 56.41804122924805 test_loss:128.79965209960938\n",
            "749/3000 train_loss: 59.280662536621094 test_loss:132.27207946777344\n",
            "750/3000 train_loss: 60.11539840698242 test_loss:128.9115447998047\n",
            "751/3000 train_loss: 59.789031982421875 test_loss:130.2061004638672\n",
            "752/3000 train_loss: 53.238258361816406 test_loss:125.710693359375\n",
            "753/3000 train_loss: 58.93364715576172 test_loss:128.46788024902344\n",
            "754/3000 train_loss: 55.88105392456055 test_loss:125.91675567626953\n",
            "755/3000 train_loss: 53.93605041503906 test_loss:133.34324645996094\n",
            "756/3000 train_loss: 65.2638931274414 test_loss:142.95516967773438\n",
            "757/3000 train_loss: 57.908016204833984 test_loss:130.17726135253906\n",
            "758/3000 train_loss: 67.84507751464844 test_loss:134.9083251953125\n",
            "759/3000 train_loss: 63.36961364746094 test_loss:130.30850219726562\n",
            "760/3000 train_loss: 60.0711669921875 test_loss:137.87403869628906\n",
            "761/3000 train_loss: 92.7125244140625 test_loss:144.8802947998047\n",
            "762/3000 train_loss: 56.44884490966797 test_loss:148.66246032714844\n",
            "763/3000 train_loss: 65.8853530883789 test_loss:144.2641143798828\n",
            "764/3000 train_loss: 51.41778564453125 test_loss:129.23965454101562\n",
            "765/3000 train_loss: 62.28840637207031 test_loss:130.83741760253906\n",
            "766/3000 train_loss: 55.368011474609375 test_loss:128.22219848632812\n",
            "767/3000 train_loss: 52.593109130859375 test_loss:123.5654296875\n",
            "768/3000 train_loss: 59.0417366027832 test_loss:129.90846252441406\n",
            "769/3000 train_loss: 58.32705307006836 test_loss:128.4868621826172\n",
            "770/3000 train_loss: 66.29887390136719 test_loss:139.7344970703125\n",
            "771/3000 train_loss: 61.06050109863281 test_loss:122.57012176513672\n",
            "772/3000 train_loss: 55.683135986328125 test_loss:128.64413452148438\n",
            "773/3000 train_loss: 54.59935760498047 test_loss:132.17027282714844\n",
            "774/3000 train_loss: 57.08910369873047 test_loss:144.47793579101562\n",
            "775/3000 train_loss: 57.15540313720703 test_loss:141.6735076904297\n",
            "776/3000 train_loss: 78.00736999511719 test_loss:128.55029296875\n",
            "777/3000 train_loss: 50.739768981933594 test_loss:131.5986328125\n",
            "778/3000 train_loss: 45.7341194152832 test_loss:122.84223937988281\n",
            "779/3000 train_loss: 54.13557434082031 test_loss:127.32133483886719\n",
            "780/3000 train_loss: 52.215301513671875 test_loss:131.3079376220703\n",
            "781/3000 train_loss: 59.32435989379883 test_loss:127.41949462890625\n",
            "782/3000 train_loss: 53.83458709716797 test_loss:122.9669189453125\n",
            "783/3000 train_loss: 63.24672317504883 test_loss:129.48219299316406\n",
            "784/3000 train_loss: 66.91944885253906 test_loss:128.1685333251953\n",
            "785/3000 train_loss: 52.19438934326172 test_loss:130.5192413330078\n",
            "786/3000 train_loss: 65.56890869140625 test_loss:130.33462524414062\n",
            "787/3000 train_loss: 62.75259780883789 test_loss:130.3772735595703\n",
            "788/3000 train_loss: 58.654335021972656 test_loss:129.89573669433594\n",
            "789/3000 train_loss: 48.102352142333984 test_loss:126.43614196777344\n",
            "790/3000 train_loss: 78.83792877197266 test_loss:138.85931396484375\n",
            "791/3000 train_loss: 66.83342742919922 test_loss:129.1846923828125\n",
            "792/3000 train_loss: 65.27313232421875 test_loss:125.32656860351562\n",
            "793/3000 train_loss: 55.262725830078125 test_loss:127.39846801757812\n",
            "794/3000 train_loss: 52.292396545410156 test_loss:121.87875366210938\n",
            "795/3000 train_loss: 50.597782135009766 test_loss:130.0062255859375\n",
            "796/3000 train_loss: 69.16354370117188 test_loss:127.12971496582031\n",
            "797/3000 train_loss: 59.3912467956543 test_loss:125.27713775634766\n",
            "798/3000 train_loss: 59.021575927734375 test_loss:129.39651489257812\n",
            "799/3000 train_loss: 49.734596252441406 test_loss:128.02084350585938\n",
            "800/3000 train_loss: 56.08366012573242 test_loss:126.33763122558594\n",
            "801/3000 train_loss: 57.44446563720703 test_loss:128.6363983154297\n",
            "802/3000 train_loss: 62.43960189819336 test_loss:136.847900390625\n",
            "803/3000 train_loss: 50.94658279418945 test_loss:127.21452331542969\n",
            "804/3000 train_loss: 47.318233489990234 test_loss:130.01345825195312\n",
            "805/3000 train_loss: 64.52229309082031 test_loss:127.4797592163086\n",
            "806/3000 train_loss: 67.03886413574219 test_loss:127.5009994506836\n",
            "807/3000 train_loss: 53.34275436401367 test_loss:127.50536346435547\n",
            "808/3000 train_loss: 48.67652130126953 test_loss:124.3505859375\n",
            "809/3000 train_loss: 49.2327766418457 test_loss:128.88296508789062\n",
            "810/3000 train_loss: 53.69485092163086 test_loss:123.27398681640625\n",
            "811/3000 train_loss: 48.3187255859375 test_loss:126.47003936767578\n",
            "812/3000 train_loss: 52.47767639160156 test_loss:124.37940216064453\n",
            "813/3000 train_loss: 60.27820587158203 test_loss:127.8476333618164\n",
            "814/3000 train_loss: 59.785667419433594 test_loss:117.72942352294922\n",
            "815/3000 train_loss: 61.98640060424805 test_loss:119.91360473632812\n",
            "816/3000 train_loss: 49.09023666381836 test_loss:120.35008239746094\n",
            "817/3000 train_loss: 57.276187896728516 test_loss:119.17417907714844\n",
            "818/3000 train_loss: 58.78599548339844 test_loss:132.17616271972656\n",
            "819/3000 train_loss: 55.253204345703125 test_loss:133.38121032714844\n",
            "820/3000 train_loss: 64.3318862915039 test_loss:130.53961181640625\n",
            "821/3000 train_loss: 56.34580612182617 test_loss:121.02206420898438\n",
            "822/3000 train_loss: 162.71400451660156 test_loss:125.74079132080078\n",
            "823/3000 train_loss: 110.74454498291016 test_loss:139.17506408691406\n",
            "824/3000 train_loss: 84.7373046875 test_loss:132.89385986328125\n",
            "825/3000 train_loss: 72.02709197998047 test_loss:130.1632080078125\n",
            "826/3000 train_loss: 56.49868392944336 test_loss:124.32176971435547\n",
            "827/3000 train_loss: 58.213130950927734 test_loss:129.6908416748047\n",
            "828/3000 train_loss: 60.23506164550781 test_loss:130.2389678955078\n",
            "829/3000 train_loss: 53.003849029541016 test_loss:116.89402770996094\n",
            "830/3000 train_loss: 57.57637405395508 test_loss:117.95933532714844\n",
            "831/3000 train_loss: 48.510433197021484 test_loss:124.58831787109375\n",
            "832/3000 train_loss: 50.62079620361328 test_loss:125.752197265625\n",
            "833/3000 train_loss: 51.49778366088867 test_loss:120.36009979248047\n",
            "834/3000 train_loss: 49.347930908203125 test_loss:120.94090270996094\n",
            "835/3000 train_loss: 66.18216705322266 test_loss:130.9450225830078\n",
            "836/3000 train_loss: 46.46660614013672 test_loss:118.60061645507812\n",
            "837/3000 train_loss: 54.77411651611328 test_loss:118.5357666015625\n",
            "838/3000 train_loss: 45.357460021972656 test_loss:118.85929107666016\n",
            "839/3000 train_loss: 59.331451416015625 test_loss:115.38960266113281\n",
            "840/3000 train_loss: 48.47321319580078 test_loss:118.91742706298828\n",
            "841/3000 train_loss: 50.36516571044922 test_loss:121.15158081054688\n",
            "842/3000 train_loss: 51.52625274658203 test_loss:120.33766174316406\n",
            "843/3000 train_loss: 50.325523376464844 test_loss:116.37821960449219\n",
            "844/3000 train_loss: 48.967063903808594 test_loss:114.47671508789062\n",
            "845/3000 train_loss: 44.84636688232422 test_loss:119.18954467773438\n",
            "846/3000 train_loss: 49.88877487182617 test_loss:122.05706787109375\n",
            "847/3000 train_loss: 53.80815887451172 test_loss:119.25874328613281\n",
            "848/3000 train_loss: 58.5002326965332 test_loss:125.99325561523438\n",
            "849/3000 train_loss: 53.7393913269043 test_loss:121.44441223144531\n",
            "850/3000 train_loss: 49.921417236328125 test_loss:128.50372314453125\n",
            "851/3000 train_loss: 60.281959533691406 test_loss:122.84463500976562\n",
            "852/3000 train_loss: 67.75354766845703 test_loss:131.31317138671875\n",
            "853/3000 train_loss: 53.315887451171875 test_loss:120.13720703125\n",
            "854/3000 train_loss: 52.20907211303711 test_loss:118.2662582397461\n",
            "855/3000 train_loss: 51.16313552856445 test_loss:119.19482421875\n",
            "856/3000 train_loss: 50.912071228027344 test_loss:119.38143157958984\n",
            "857/3000 train_loss: 54.50651931762695 test_loss:112.67092895507812\n",
            "858/3000 train_loss: 53.66382598876953 test_loss:120.73527526855469\n",
            "859/3000 train_loss: 45.815677642822266 test_loss:118.14830017089844\n",
            "860/3000 train_loss: 45.09139633178711 test_loss:113.70034790039062\n",
            "861/3000 train_loss: 46.707237243652344 test_loss:120.2945785522461\n",
            "862/3000 train_loss: 48.63661193847656 test_loss:117.99859619140625\n",
            "863/3000 train_loss: 52.12236022949219 test_loss:120.37911987304688\n",
            "864/3000 train_loss: 53.55843734741211 test_loss:117.60139465332031\n",
            "865/3000 train_loss: 51.13789749145508 test_loss:117.1341552734375\n",
            "866/3000 train_loss: 45.95249557495117 test_loss:111.49284362792969\n",
            "867/3000 train_loss: 44.85666275024414 test_loss:114.46269226074219\n",
            "868/3000 train_loss: 49.99959945678711 test_loss:119.02691650390625\n",
            "869/3000 train_loss: 60.154869079589844 test_loss:122.48429870605469\n",
            "870/3000 train_loss: 55.36728286743164 test_loss:121.48670196533203\n",
            "871/3000 train_loss: 61.7591438293457 test_loss:132.37698364257812\n",
            "872/3000 train_loss: 47.07721710205078 test_loss:138.3791961669922\n",
            "873/3000 train_loss: 63.912147521972656 test_loss:118.09774780273438\n",
            "874/3000 train_loss: 49.48594665527344 test_loss:123.113037109375\n",
            "875/3000 train_loss: 49.886871337890625 test_loss:120.6707763671875\n",
            "876/3000 train_loss: 57.89790344238281 test_loss:120.32098388671875\n",
            "877/3000 train_loss: 48.759769439697266 test_loss:122.3658447265625\n",
            "878/3000 train_loss: 50.5286865234375 test_loss:115.50839233398438\n",
            "879/3000 train_loss: 53.51011276245117 test_loss:123.48896789550781\n",
            "880/3000 train_loss: 52.82179260253906 test_loss:119.927490234375\n",
            "881/3000 train_loss: 55.89854431152344 test_loss:114.31224060058594\n",
            "882/3000 train_loss: 47.14435577392578 test_loss:115.21218872070312\n",
            "883/3000 train_loss: 48.591426849365234 test_loss:114.79589080810547\n",
            "884/3000 train_loss: 49.84513473510742 test_loss:116.48008728027344\n",
            "885/3000 train_loss: 42.19270706176758 test_loss:121.64955139160156\n",
            "886/3000 train_loss: 46.92155075073242 test_loss:121.86871337890625\n",
            "887/3000 train_loss: 49.12086868286133 test_loss:126.57772064208984\n",
            "888/3000 train_loss: 53.97220230102539 test_loss:124.82682800292969\n",
            "889/3000 train_loss: 47.531471252441406 test_loss:117.52969360351562\n",
            "890/3000 train_loss: 47.1944465637207 test_loss:122.05229949951172\n",
            "891/3000 train_loss: 56.46539306640625 test_loss:118.19286346435547\n",
            "892/3000 train_loss: 50.466758728027344 test_loss:115.88690948486328\n",
            "893/3000 train_loss: 48.937400817871094 test_loss:118.059326171875\n",
            "894/3000 train_loss: 54.816776275634766 test_loss:116.08596801757812\n",
            "895/3000 train_loss: 45.519901275634766 test_loss:115.19096374511719\n",
            "896/3000 train_loss: 47.80591583251953 test_loss:112.98876190185547\n",
            "897/3000 train_loss: 50.25772476196289 test_loss:111.55702209472656\n",
            "898/3000 train_loss: 48.74819564819336 test_loss:112.92865753173828\n",
            "899/3000 train_loss: 47.471168518066406 test_loss:111.53657531738281\n",
            "900/3000 train_loss: 49.722801208496094 test_loss:119.84668731689453\n",
            "901/3000 train_loss: 51.86688232421875 test_loss:116.78549194335938\n",
            "902/3000 train_loss: 50.071044921875 test_loss:122.38253784179688\n",
            "903/3000 train_loss: 50.270233154296875 test_loss:115.49244689941406\n",
            "904/3000 train_loss: 59.83544921875 test_loss:112.80431365966797\n",
            "905/3000 train_loss: 55.692325592041016 test_loss:119.9503402709961\n",
            "906/3000 train_loss: 50.923912048339844 test_loss:128.38113403320312\n",
            "907/3000 train_loss: 56.79640197753906 test_loss:116.13615417480469\n",
            "908/3000 train_loss: 59.21167755126953 test_loss:116.50762176513672\n",
            "909/3000 train_loss: 46.82722854614258 test_loss:108.80252075195312\n",
            "910/3000 train_loss: 46.11048126220703 test_loss:121.6145248413086\n",
            "911/3000 train_loss: 54.3018684387207 test_loss:119.8332290649414\n",
            "912/3000 train_loss: 58.03864288330078 test_loss:120.43208312988281\n",
            "913/3000 train_loss: 39.61195373535156 test_loss:113.20860290527344\n",
            "914/3000 train_loss: 47.45493698120117 test_loss:114.7790756225586\n",
            "915/3000 train_loss: 41.278350830078125 test_loss:113.33351135253906\n",
            "916/3000 train_loss: 54.26679229736328 test_loss:124.79862213134766\n",
            "917/3000 train_loss: 47.68644332885742 test_loss:114.01094055175781\n",
            "918/3000 train_loss: 50.656700134277344 test_loss:117.50651550292969\n",
            "919/3000 train_loss: 50.218868255615234 test_loss:117.47709655761719\n",
            "920/3000 train_loss: 41.70235824584961 test_loss:115.135986328125\n",
            "921/3000 train_loss: 52.71708679199219 test_loss:119.2245101928711\n",
            "922/3000 train_loss: 45.85498809814453 test_loss:110.28583526611328\n",
            "923/3000 train_loss: 40.927528381347656 test_loss:118.20498657226562\n",
            "924/3000 train_loss: 49.222373962402344 test_loss:117.35786437988281\n",
            "925/3000 train_loss: 46.45595169067383 test_loss:111.81739807128906\n",
            "926/3000 train_loss: 51.48991775512695 test_loss:118.19377899169922\n",
            "927/3000 train_loss: 46.41339111328125 test_loss:111.93753051757812\n",
            "928/3000 train_loss: 52.633846282958984 test_loss:110.60589599609375\n",
            "929/3000 train_loss: 61.41489028930664 test_loss:120.37228393554688\n",
            "930/3000 train_loss: 46.66836929321289 test_loss:109.39871215820312\n",
            "931/3000 train_loss: 49.40265655517578 test_loss:113.24491882324219\n",
            "932/3000 train_loss: 49.24402618408203 test_loss:112.06922149658203\n",
            "933/3000 train_loss: 49.966026306152344 test_loss:111.07054138183594\n",
            "934/3000 train_loss: 42.45746994018555 test_loss:117.88191223144531\n",
            "935/3000 train_loss: 48.98341751098633 test_loss:110.40367126464844\n",
            "936/3000 train_loss: 51.91108703613281 test_loss:111.30236053466797\n",
            "937/3000 train_loss: 46.312255859375 test_loss:107.50534057617188\n",
            "938/3000 train_loss: 50.21015548706055 test_loss:112.4759521484375\n",
            "939/3000 train_loss: 43.996681213378906 test_loss:108.01019287109375\n",
            "940/3000 train_loss: 50.18994903564453 test_loss:113.6068115234375\n",
            "941/3000 train_loss: 48.37751770019531 test_loss:113.85298919677734\n",
            "942/3000 train_loss: 48.914085388183594 test_loss:106.38961791992188\n",
            "943/3000 train_loss: 44.71656036376953 test_loss:113.496337890625\n",
            "944/3000 train_loss: 44.677913665771484 test_loss:107.63245391845703\n",
            "945/3000 train_loss: 48.1353645324707 test_loss:109.49146270751953\n",
            "946/3000 train_loss: 48.345115661621094 test_loss:109.34918975830078\n",
            "947/3000 train_loss: 49.21043014526367 test_loss:122.9687728881836\n",
            "948/3000 train_loss: 75.43231201171875 test_loss:120.2964096069336\n",
            "949/3000 train_loss: 48.864112854003906 test_loss:114.12403869628906\n",
            "950/3000 train_loss: 47.951995849609375 test_loss:113.76388549804688\n",
            "951/3000 train_loss: 52.422672271728516 test_loss:117.6849365234375\n",
            "952/3000 train_loss: 49.703460693359375 test_loss:110.68646240234375\n",
            "953/3000 train_loss: 47.935577392578125 test_loss:110.49504089355469\n",
            "954/3000 train_loss: 45.50340270996094 test_loss:115.82970428466797\n",
            "955/3000 train_loss: 48.435462951660156 test_loss:108.28584289550781\n",
            "956/3000 train_loss: 41.405357360839844 test_loss:119.5901870727539\n",
            "957/3000 train_loss: 41.931453704833984 test_loss:109.82614135742188\n",
            "958/3000 train_loss: 48.041385650634766 test_loss:116.61024475097656\n",
            "959/3000 train_loss: 48.76762390136719 test_loss:109.20028686523438\n",
            "960/3000 train_loss: 37.97123718261719 test_loss:115.39299011230469\n",
            "961/3000 train_loss: 63.191707611083984 test_loss:113.6280517578125\n",
            "962/3000 train_loss: 47.75907516479492 test_loss:108.05890655517578\n",
            "963/3000 train_loss: 40.6656494140625 test_loss:109.0518569946289\n",
            "964/3000 train_loss: 45.583534240722656 test_loss:110.35176849365234\n",
            "965/3000 train_loss: 72.40705108642578 test_loss:117.8923110961914\n",
            "966/3000 train_loss: 48.165977478027344 test_loss:118.3473892211914\n",
            "967/3000 train_loss: 49.697383880615234 test_loss:129.46168518066406\n",
            "968/3000 train_loss: 53.281368255615234 test_loss:115.47106170654297\n",
            "969/3000 train_loss: 53.1643180847168 test_loss:114.09933471679688\n",
            "970/3000 train_loss: 46.63109588623047 test_loss:116.14234924316406\n",
            "971/3000 train_loss: 41.11956787109375 test_loss:108.56871032714844\n",
            "972/3000 train_loss: 48.51578903198242 test_loss:110.59618377685547\n",
            "973/3000 train_loss: 52.284767150878906 test_loss:110.21234130859375\n",
            "974/3000 train_loss: 44.15889358520508 test_loss:111.07505798339844\n",
            "975/3000 train_loss: 40.76271438598633 test_loss:114.65383911132812\n",
            "976/3000 train_loss: 42.00986099243164 test_loss:115.22024536132812\n",
            "977/3000 train_loss: 53.17794418334961 test_loss:116.212890625\n",
            "978/3000 train_loss: 48.54429244995117 test_loss:109.02600860595703\n",
            "979/3000 train_loss: 49.390350341796875 test_loss:118.33602905273438\n",
            "980/3000 train_loss: 45.565486907958984 test_loss:111.63507080078125\n",
            "981/3000 train_loss: 46.50703048706055 test_loss:106.27976989746094\n",
            "982/3000 train_loss: 51.23614501953125 test_loss:114.65852355957031\n",
            "983/3000 train_loss: 46.97167205810547 test_loss:108.06028747558594\n",
            "984/3000 train_loss: 46.74074172973633 test_loss:120.63859558105469\n",
            "985/3000 train_loss: 44.33309555053711 test_loss:119.80168151855469\n",
            "986/3000 train_loss: 47.79548263549805 test_loss:110.28680419921875\n",
            "987/3000 train_loss: 47.68330001831055 test_loss:110.50780487060547\n",
            "988/3000 train_loss: 42.71889114379883 test_loss:114.77885437011719\n",
            "989/3000 train_loss: 41.770057678222656 test_loss:112.61790466308594\n",
            "990/3000 train_loss: 41.61394500732422 test_loss:109.93408203125\n",
            "991/3000 train_loss: 46.125885009765625 test_loss:109.83427429199219\n",
            "992/3000 train_loss: 59.88874053955078 test_loss:110.99983215332031\n",
            "993/3000 train_loss: 55.10517120361328 test_loss:111.2471923828125\n",
            "994/3000 train_loss: 48.44819641113281 test_loss:110.55842590332031\n",
            "995/3000 train_loss: 44.15419006347656 test_loss:115.01722717285156\n",
            "996/3000 train_loss: 47.52336120605469 test_loss:114.65069580078125\n",
            "997/3000 train_loss: 44.60684585571289 test_loss:108.26460266113281\n",
            "998/3000 train_loss: 44.825347900390625 test_loss:114.89295196533203\n",
            "999/3000 train_loss: 42.2230339050293 test_loss:109.08636474609375\n",
            "1000/3000 train_loss: 43.33818435668945 test_loss:112.77920532226562\n",
            "1001/3000 train_loss: 41.86052322387695 test_loss:111.3331298828125\n",
            "1002/3000 train_loss: 44.143211364746094 test_loss:111.61772155761719\n",
            "1003/3000 train_loss: 50.13248825073242 test_loss:112.59756469726562\n",
            "1004/3000 train_loss: 54.36943817138672 test_loss:107.12972259521484\n",
            "1005/3000 train_loss: 51.82454299926758 test_loss:108.71907043457031\n",
            "1006/3000 train_loss: 47.34828567504883 test_loss:113.93582153320312\n",
            "1007/3000 train_loss: 58.15863800048828 test_loss:116.32362365722656\n",
            "1008/3000 train_loss: 45.63466262817383 test_loss:113.48991394042969\n",
            "1009/3000 train_loss: 37.98845672607422 test_loss:117.64892578125\n",
            "1010/3000 train_loss: 46.01142883300781 test_loss:107.93527221679688\n",
            "1011/3000 train_loss: 48.263938903808594 test_loss:114.21190643310547\n",
            "1012/3000 train_loss: 46.31291198730469 test_loss:109.98239135742188\n",
            "1013/3000 train_loss: 42.694435119628906 test_loss:110.73116302490234\n",
            "1014/3000 train_loss: 45.662803649902344 test_loss:117.48863220214844\n",
            "1015/3000 train_loss: 46.94938659667969 test_loss:113.09808349609375\n",
            "1016/3000 train_loss: 47.630592346191406 test_loss:119.22640991210938\n",
            "1017/3000 train_loss: 51.81425857543945 test_loss:112.59868621826172\n",
            "1018/3000 train_loss: 46.220420837402344 test_loss:124.97157287597656\n",
            "1019/3000 train_loss: 53.27684783935547 test_loss:108.83917236328125\n",
            "1020/3000 train_loss: 42.14628219604492 test_loss:110.20561218261719\n",
            "1021/3000 train_loss: 45.217403411865234 test_loss:103.46955871582031\n",
            "1022/3000 train_loss: 37.84580993652344 test_loss:108.09757995605469\n",
            "1023/3000 train_loss: 47.1740837097168 test_loss:106.3393325805664\n",
            "1024/3000 train_loss: 53.705257415771484 test_loss:116.87995147705078\n",
            "1025/3000 train_loss: 39.839237213134766 test_loss:116.90103149414062\n",
            "1026/3000 train_loss: 65.82563018798828 test_loss:121.56919860839844\n",
            "1027/3000 train_loss: 57.00558853149414 test_loss:136.17041015625\n",
            "1028/3000 train_loss: 43.9498405456543 test_loss:129.20018005371094\n",
            "1029/3000 train_loss: 47.67024230957031 test_loss:121.74136352539062\n",
            "1030/3000 train_loss: 39.100494384765625 test_loss:123.88627624511719\n",
            "1031/3000 train_loss: 41.88825988769531 test_loss:119.72711181640625\n",
            "1032/3000 train_loss: 40.8108024597168 test_loss:118.33075714111328\n",
            "1033/3000 train_loss: 49.07711410522461 test_loss:114.75634002685547\n",
            "1034/3000 train_loss: 45.54886245727539 test_loss:117.78221130371094\n",
            "1035/3000 train_loss: 42.9040641784668 test_loss:111.26642608642578\n",
            "1036/3000 train_loss: 46.45903015136719 test_loss:114.15043640136719\n",
            "1037/3000 train_loss: 46.06727600097656 test_loss:112.10555267333984\n",
            "1038/3000 train_loss: 45.850589752197266 test_loss:118.25399780273438\n",
            "1039/3000 train_loss: 44.950626373291016 test_loss:115.68887329101562\n",
            "1040/3000 train_loss: 40.04072952270508 test_loss:113.02167510986328\n",
            "1041/3000 train_loss: 42.13821792602539 test_loss:116.342529296875\n",
            "1042/3000 train_loss: 48.54856872558594 test_loss:113.06033325195312\n",
            "1043/3000 train_loss: 46.5516471862793 test_loss:116.17066955566406\n",
            "1044/3000 train_loss: 47.100242614746094 test_loss:120.2304458618164\n",
            "1045/3000 train_loss: 38.024749755859375 test_loss:115.16718292236328\n",
            "1046/3000 train_loss: 50.32965087890625 test_loss:113.1328125\n",
            "1047/3000 train_loss: 43.98734664916992 test_loss:108.3081283569336\n",
            "1048/3000 train_loss: 41.72993469238281 test_loss:114.68624114990234\n",
            "1049/3000 train_loss: 41.000953674316406 test_loss:106.13533782958984\n",
            "1050/3000 train_loss: 46.08422088623047 test_loss:115.1397705078125\n",
            "1051/3000 train_loss: 43.57795715332031 test_loss:112.38155364990234\n",
            "1052/3000 train_loss: 41.167938232421875 test_loss:110.32435607910156\n",
            "1053/3000 train_loss: 49.321712493896484 test_loss:112.27361297607422\n",
            "1054/3000 train_loss: 38.01180648803711 test_loss:106.63461303710938\n",
            "1055/3000 train_loss: 40.70539093017578 test_loss:103.6920394897461\n",
            "1056/3000 train_loss: 43.57640838623047 test_loss:109.4173583984375\n",
            "1057/3000 train_loss: 47.80986022949219 test_loss:118.95072937011719\n",
            "1058/3000 train_loss: 38.30198669433594 test_loss:119.22525787353516\n",
            "1059/3000 train_loss: 39.965938568115234 test_loss:121.75637817382812\n",
            "1060/3000 train_loss: 46.33102035522461 test_loss:117.21553039550781\n",
            "1061/3000 train_loss: 39.89745330810547 test_loss:110.42953491210938\n",
            "1062/3000 train_loss: 39.7346305847168 test_loss:110.40056610107422\n",
            "1063/3000 train_loss: 40.84091567993164 test_loss:108.62089538574219\n",
            "1064/3000 train_loss: 42.11312484741211 test_loss:112.55619812011719\n",
            "1065/3000 train_loss: 40.36565399169922 test_loss:105.85621643066406\n",
            "1066/3000 train_loss: 44.102535247802734 test_loss:111.19593811035156\n",
            "1067/3000 train_loss: 40.29977798461914 test_loss:107.27261352539062\n",
            "1068/3000 train_loss: 41.34648132324219 test_loss:104.72860717773438\n",
            "1069/3000 train_loss: 58.91754150390625 test_loss:108.41624450683594\n",
            "1070/3000 train_loss: 43.01713943481445 test_loss:110.14247131347656\n",
            "1071/3000 train_loss: 41.81845474243164 test_loss:112.78543090820312\n",
            "1072/3000 train_loss: 37.40715789794922 test_loss:112.03775787353516\n",
            "1073/3000 train_loss: 39.115238189697266 test_loss:112.35687255859375\n",
            "1074/3000 train_loss: 57.24298858642578 test_loss:110.71456146240234\n",
            "1075/3000 train_loss: 42.622230529785156 test_loss:116.4620132446289\n",
            "1076/3000 train_loss: 48.548072814941406 test_loss:131.530029296875\n",
            "1077/3000 train_loss: 48.977542877197266 test_loss:113.20626068115234\n",
            "1078/3000 train_loss: 49.162601470947266 test_loss:109.36419677734375\n",
            "1079/3000 train_loss: 41.488094329833984 test_loss:112.347412109375\n",
            "1080/3000 train_loss: 39.038673400878906 test_loss:106.0938720703125\n",
            "1081/3000 train_loss: 43.09392547607422 test_loss:105.69718933105469\n",
            "1082/3000 train_loss: 46.89336395263672 test_loss:107.69723510742188\n",
            "1083/3000 train_loss: 43.0884895324707 test_loss:110.39517974853516\n",
            "1084/3000 train_loss: 41.55748748779297 test_loss:116.56060791015625\n",
            "1085/3000 train_loss: 42.78322982788086 test_loss:115.4598159790039\n",
            "1086/3000 train_loss: 42.80119323730469 test_loss:109.38365173339844\n",
            "1087/3000 train_loss: 42.77336883544922 test_loss:104.7196044921875\n",
            "1088/3000 train_loss: 38.945430755615234 test_loss:105.10452270507812\n",
            "1089/3000 train_loss: 39.426719665527344 test_loss:111.59528350830078\n",
            "1090/3000 train_loss: 35.14137268066406 test_loss:103.28204345703125\n",
            "1091/3000 train_loss: 41.96179962158203 test_loss:111.39930725097656\n",
            "1092/3000 train_loss: 36.75918960571289 test_loss:106.40482330322266\n",
            "1093/3000 train_loss: 44.203651428222656 test_loss:106.37525177001953\n",
            "1094/3000 train_loss: 43.15850067138672 test_loss:116.28587341308594\n",
            "1095/3000 train_loss: 45.66242980957031 test_loss:103.5950698852539\n",
            "1096/3000 train_loss: 43.63285827636719 test_loss:112.69689178466797\n",
            "1097/3000 train_loss: 42.73713302612305 test_loss:105.70108032226562\n",
            "1098/3000 train_loss: 39.36593246459961 test_loss:118.31425476074219\n",
            "1099/3000 train_loss: 42.60572052001953 test_loss:118.7713623046875\n",
            "1100/3000 train_loss: 39.645267486572266 test_loss:117.69001007080078\n",
            "1101/3000 train_loss: 38.63710021972656 test_loss:105.47908782958984\n",
            "1102/3000 train_loss: 37.48999786376953 test_loss:107.52973175048828\n",
            "1103/3000 train_loss: 40.18266296386719 test_loss:107.73358917236328\n",
            "1104/3000 train_loss: 41.42399597167969 test_loss:108.9078369140625\n",
            "1105/3000 train_loss: 40.79273986816406 test_loss:106.72947692871094\n",
            "1106/3000 train_loss: 36.24667739868164 test_loss:111.63915252685547\n",
            "1107/3000 train_loss: 38.506500244140625 test_loss:112.550048828125\n",
            "1108/3000 train_loss: 37.390602111816406 test_loss:107.35833740234375\n",
            "1109/3000 train_loss: 34.2582893371582 test_loss:106.75205993652344\n",
            "1110/3000 train_loss: 39.96673583984375 test_loss:107.86337280273438\n",
            "1111/3000 train_loss: 36.04111862182617 test_loss:104.6657485961914\n",
            "1112/3000 train_loss: 43.46935272216797 test_loss:101.96591186523438\n",
            "1113/3000 train_loss: 39.3347282409668 test_loss:106.75164794921875\n",
            "1114/3000 train_loss: 38.43014907836914 test_loss:100.31216430664062\n",
            "1115/3000 train_loss: 41.81501007080078 test_loss:110.11022186279297\n",
            "1116/3000 train_loss: 34.92985534667969 test_loss:107.07424926757812\n",
            "1117/3000 train_loss: 38.97903060913086 test_loss:103.71724700927734\n",
            "1118/3000 train_loss: 48.966983795166016 test_loss:104.28775024414062\n",
            "1119/3000 train_loss: 40.77425765991211 test_loss:117.98948669433594\n",
            "1120/3000 train_loss: 40.781917572021484 test_loss:104.014892578125\n",
            "1121/3000 train_loss: 40.61837387084961 test_loss:99.88481140136719\n",
            "1122/3000 train_loss: 36.341896057128906 test_loss:105.12090301513672\n",
            "1123/3000 train_loss: 36.37965393066406 test_loss:105.05479431152344\n",
            "1124/3000 train_loss: 41.693302154541016 test_loss:104.60528564453125\n",
            "1125/3000 train_loss: 38.66191482543945 test_loss:103.95723724365234\n",
            "1126/3000 train_loss: 40.36233901977539 test_loss:101.46641540527344\n",
            "1127/3000 train_loss: 37.056819915771484 test_loss:102.55646514892578\n",
            "1128/3000 train_loss: 46.934898376464844 test_loss:105.03400421142578\n",
            "1129/3000 train_loss: 51.044498443603516 test_loss:107.67342376708984\n",
            "1130/3000 train_loss: 43.156524658203125 test_loss:106.90386962890625\n",
            "1131/3000 train_loss: 37.599266052246094 test_loss:103.76174926757812\n",
            "1132/3000 train_loss: 43.39264678955078 test_loss:103.51129150390625\n",
            "1133/3000 train_loss: 48.60186767578125 test_loss:102.5858383178711\n",
            "1134/3000 train_loss: 41.37649154663086 test_loss:107.2518539428711\n",
            "1135/3000 train_loss: 43.81258773803711 test_loss:108.48811340332031\n",
            "1136/3000 train_loss: 44.87458419799805 test_loss:109.69760131835938\n",
            "1137/3000 train_loss: 39.316871643066406 test_loss:105.42298889160156\n",
            "1138/3000 train_loss: 34.72872543334961 test_loss:106.53103637695312\n",
            "1139/3000 train_loss: 42.684417724609375 test_loss:100.06289672851562\n",
            "1140/3000 train_loss: 44.572227478027344 test_loss:103.09040832519531\n",
            "1141/3000 train_loss: 43.98639678955078 test_loss:107.47016906738281\n",
            "1142/3000 train_loss: 46.852264404296875 test_loss:110.3702392578125\n",
            "1143/3000 train_loss: 50.30146789550781 test_loss:111.81039428710938\n",
            "1144/3000 train_loss: 43.000152587890625 test_loss:107.82220458984375\n",
            "1145/3000 train_loss: 45.417110443115234 test_loss:110.08787536621094\n",
            "1146/3000 train_loss: 36.88290023803711 test_loss:105.8531723022461\n",
            "1147/3000 train_loss: 42.305599212646484 test_loss:110.93553161621094\n",
            "1148/3000 train_loss: 45.8272705078125 test_loss:105.11166381835938\n",
            "1149/3000 train_loss: 36.21636199951172 test_loss:103.10026550292969\n",
            "1150/3000 train_loss: 39.944828033447266 test_loss:107.83204650878906\n",
            "1151/3000 train_loss: 40.03303146362305 test_loss:99.15532684326172\n",
            "1152/3000 train_loss: 35.67514419555664 test_loss:102.52604675292969\n",
            "1153/3000 train_loss: 35.67325210571289 test_loss:99.36663818359375\n",
            "1154/3000 train_loss: 36.19747543334961 test_loss:102.62020874023438\n",
            "1155/3000 train_loss: 37.071861267089844 test_loss:100.01893615722656\n",
            "1156/3000 train_loss: 41.33210372924805 test_loss:102.13710021972656\n",
            "1157/3000 train_loss: 39.17079162597656 test_loss:106.67170715332031\n",
            "1158/3000 train_loss: 35.42542266845703 test_loss:99.93814086914062\n",
            "1159/3000 train_loss: 36.1319465637207 test_loss:102.10409545898438\n",
            "1160/3000 train_loss: 33.99443054199219 test_loss:100.22213745117188\n",
            "1161/3000 train_loss: 33.94786834716797 test_loss:104.84044647216797\n",
            "1162/3000 train_loss: 53.584049224853516 test_loss:112.71863555908203\n",
            "1163/3000 train_loss: 40.621131896972656 test_loss:118.11126708984375\n",
            "1164/3000 train_loss: 44.99314498901367 test_loss:117.63813018798828\n",
            "1165/3000 train_loss: 35.08985137939453 test_loss:114.65369415283203\n",
            "1166/3000 train_loss: 37.804771423339844 test_loss:115.79130554199219\n",
            "1167/3000 train_loss: 42.762447357177734 test_loss:112.1718521118164\n",
            "1168/3000 train_loss: 36.49728012084961 test_loss:122.52471923828125\n",
            "1169/3000 train_loss: 41.93650817871094 test_loss:115.45988464355469\n",
            "1170/3000 train_loss: 35.32279968261719 test_loss:113.75567626953125\n",
            "1171/3000 train_loss: 41.22937774658203 test_loss:111.19259643554688\n",
            "1172/3000 train_loss: 37.13737869262695 test_loss:118.10678100585938\n",
            "1173/3000 train_loss: 43.81220626831055 test_loss:111.36705017089844\n",
            "1174/3000 train_loss: 45.86493682861328 test_loss:113.4427490234375\n",
            "1175/3000 train_loss: 37.93290328979492 test_loss:110.90711975097656\n",
            "1176/3000 train_loss: 42.106964111328125 test_loss:108.16825866699219\n",
            "1177/3000 train_loss: 41.36443328857422 test_loss:111.37132263183594\n",
            "1178/3000 train_loss: 32.82134246826172 test_loss:108.10812377929688\n",
            "1179/3000 train_loss: 43.74735641479492 test_loss:106.72370910644531\n",
            "1180/3000 train_loss: 39.109317779541016 test_loss:102.30325317382812\n",
            "1181/3000 train_loss: 37.604034423828125 test_loss:114.87934875488281\n",
            "1182/3000 train_loss: 36.129241943359375 test_loss:105.63945007324219\n",
            "1183/3000 train_loss: 39.79158401489258 test_loss:109.26705169677734\n",
            "1184/3000 train_loss: 40.279296875 test_loss:104.53439331054688\n",
            "1185/3000 train_loss: 37.52777099609375 test_loss:116.245361328125\n",
            "1186/3000 train_loss: 36.015079498291016 test_loss:106.30792236328125\n",
            "1187/3000 train_loss: 39.001930236816406 test_loss:110.74207305908203\n",
            "1188/3000 train_loss: 37.29749298095703 test_loss:111.33403015136719\n",
            "1189/3000 train_loss: 43.55612564086914 test_loss:113.23301696777344\n",
            "1190/3000 train_loss: 40.02603530883789 test_loss:99.93519592285156\n",
            "1191/3000 train_loss: 37.92009735107422 test_loss:110.44010925292969\n",
            "1192/3000 train_loss: 37.387168884277344 test_loss:101.37183380126953\n",
            "1193/3000 train_loss: 39.68202209472656 test_loss:111.83521270751953\n",
            "1194/3000 train_loss: 41.7476921081543 test_loss:105.38441467285156\n",
            "1195/3000 train_loss: 42.09614562988281 test_loss:100.87030792236328\n",
            "1196/3000 train_loss: 33.97648620605469 test_loss:106.11290740966797\n",
            "1197/3000 train_loss: 32.726646423339844 test_loss:99.70812225341797\n",
            "1198/3000 train_loss: 34.172996520996094 test_loss:102.99456787109375\n",
            "1199/3000 train_loss: 36.59934997558594 test_loss:101.03336334228516\n",
            "1200/3000 train_loss: 34.255775451660156 test_loss:98.26188659667969\n",
            "1201/3000 train_loss: 35.303524017333984 test_loss:101.1119384765625\n",
            "1202/3000 train_loss: 42.058780670166016 test_loss:109.04511260986328\n",
            "1203/3000 train_loss: 39.73227310180664 test_loss:101.53128051757812\n",
            "1204/3000 train_loss: 39.25003433227539 test_loss:108.90126037597656\n",
            "1205/3000 train_loss: 38.623619079589844 test_loss:98.01363372802734\n",
            "1206/3000 train_loss: 36.858978271484375 test_loss:105.92481994628906\n",
            "1207/3000 train_loss: 40.7558708190918 test_loss:118.06298828125\n",
            "1208/3000 train_loss: 40.99249267578125 test_loss:103.9267578125\n",
            "1209/3000 train_loss: 43.29411315917969 test_loss:108.69869232177734\n",
            "1210/3000 train_loss: 36.30532455444336 test_loss:98.46053314208984\n",
            "1211/3000 train_loss: 42.55875778198242 test_loss:109.05435180664062\n",
            "1212/3000 train_loss: 39.16145706176758 test_loss:105.92329406738281\n",
            "1213/3000 train_loss: 32.74941635131836 test_loss:107.00128173828125\n",
            "1214/3000 train_loss: 34.016971588134766 test_loss:109.49626159667969\n",
            "1215/3000 train_loss: 34.05448913574219 test_loss:100.80165100097656\n",
            "1216/3000 train_loss: 43.054229736328125 test_loss:103.17076110839844\n",
            "1217/3000 train_loss: 37.6501350402832 test_loss:102.417724609375\n",
            "1218/3000 train_loss: 32.236175537109375 test_loss:103.0609359741211\n",
            "1219/3000 train_loss: 45.87893295288086 test_loss:103.4223403930664\n",
            "1220/3000 train_loss: 36.2876091003418 test_loss:103.98712158203125\n",
            "1221/3000 train_loss: 32.95405960083008 test_loss:99.31672668457031\n",
            "1222/3000 train_loss: 32.46942138671875 test_loss:102.28776550292969\n",
            "1223/3000 train_loss: 41.02663040161133 test_loss:108.66647338867188\n",
            "1224/3000 train_loss: 35.49445343017578 test_loss:99.71707153320312\n",
            "1225/3000 train_loss: 40.321434020996094 test_loss:100.40113830566406\n",
            "1226/3000 train_loss: 37.236053466796875 test_loss:102.16111755371094\n",
            "1227/3000 train_loss: 46.21026611328125 test_loss:100.04739379882812\n",
            "1228/3000 train_loss: 42.662696838378906 test_loss:104.22073364257812\n",
            "1229/3000 train_loss: 42.145206451416016 test_loss:110.36752319335938\n",
            "1230/3000 train_loss: 40.02339553833008 test_loss:108.05728149414062\n",
            "1231/3000 train_loss: 37.1385383605957 test_loss:111.01325988769531\n",
            "1232/3000 train_loss: 40.723941802978516 test_loss:100.367431640625\n",
            "1233/3000 train_loss: 36.912559509277344 test_loss:113.71549224853516\n",
            "1234/3000 train_loss: 35.03329849243164 test_loss:109.41140747070312\n",
            "1235/3000 train_loss: 32.45295715332031 test_loss:104.4345703125\n",
            "1236/3000 train_loss: 40.003360748291016 test_loss:97.512939453125\n",
            "1237/3000 train_loss: 38.105865478515625 test_loss:109.9100341796875\n",
            "1238/3000 train_loss: 36.07062530517578 test_loss:108.98753356933594\n",
            "1239/3000 train_loss: 37.9191780090332 test_loss:109.23271179199219\n",
            "1240/3000 train_loss: 32.344791412353516 test_loss:110.92282104492188\n",
            "1241/3000 train_loss: 35.93693923950195 test_loss:103.6965560913086\n",
            "1242/3000 train_loss: 40.212764739990234 test_loss:104.80939483642578\n",
            "1243/3000 train_loss: 35.67515182495117 test_loss:104.32530212402344\n",
            "1244/3000 train_loss: 33.73534393310547 test_loss:107.80365753173828\n",
            "1245/3000 train_loss: 39.57978820800781 test_loss:117.01457214355469\n",
            "1246/3000 train_loss: 46.358734130859375 test_loss:107.39085388183594\n",
            "1247/3000 train_loss: 43.21269989013672 test_loss:108.79823303222656\n",
            "1248/3000 train_loss: 34.57038116455078 test_loss:103.03467559814453\n",
            "1249/3000 train_loss: 34.802642822265625 test_loss:116.78564453125\n",
            "1250/3000 train_loss: 36.87445831298828 test_loss:100.42337036132812\n",
            "1251/3000 train_loss: 40.965049743652344 test_loss:115.37098693847656\n",
            "1252/3000 train_loss: 35.2918701171875 test_loss:111.83332824707031\n",
            "1253/3000 train_loss: 40.04825973510742 test_loss:111.404296875\n",
            "1254/3000 train_loss: 31.795669555664062 test_loss:105.9013671875\n",
            "1255/3000 train_loss: 32.72019958496094 test_loss:102.3802261352539\n",
            "1256/3000 train_loss: 38.60765075683594 test_loss:111.17080688476562\n",
            "1257/3000 train_loss: 36.04840087890625 test_loss:103.21412658691406\n",
            "1258/3000 train_loss: 35.556888580322266 test_loss:112.4707260131836\n",
            "1259/3000 train_loss: 36.89059829711914 test_loss:107.93571472167969\n",
            "1260/3000 train_loss: 36.71986389160156 test_loss:113.31830596923828\n",
            "1261/3000 train_loss: 42.17980194091797 test_loss:101.24406433105469\n",
            "1262/3000 train_loss: 34.493141174316406 test_loss:108.89691162109375\n",
            "1263/3000 train_loss: 33.77256774902344 test_loss:109.98002624511719\n",
            "1264/3000 train_loss: 34.2095947265625 test_loss:101.44393920898438\n",
            "1265/3000 train_loss: 37.45199966430664 test_loss:105.58686065673828\n",
            "1266/3000 train_loss: 39.979957580566406 test_loss:104.09391784667969\n",
            "1267/3000 train_loss: 39.29729080200195 test_loss:105.86212921142578\n",
            "1268/3000 train_loss: 36.05409240722656 test_loss:104.03520202636719\n",
            "1269/3000 train_loss: 33.903358459472656 test_loss:103.36551666259766\n",
            "1270/3000 train_loss: 39.84205627441406 test_loss:110.18297576904297\n",
            "1271/3000 train_loss: 32.151485443115234 test_loss:95.84429931640625\n",
            "1272/3000 train_loss: 34.87092971801758 test_loss:111.50141906738281\n",
            "1273/3000 train_loss: 36.04899215698242 test_loss:99.1118392944336\n",
            "1274/3000 train_loss: 35.0189323425293 test_loss:110.24182891845703\n",
            "1275/3000 train_loss: 31.74713706970215 test_loss:98.25160217285156\n",
            "1276/3000 train_loss: 33.71755599975586 test_loss:102.25994873046875\n",
            "1277/3000 train_loss: 32.2694091796875 test_loss:112.15521240234375\n",
            "1278/3000 train_loss: 42.49095916748047 test_loss:97.87153625488281\n",
            "1279/3000 train_loss: 35.26449966430664 test_loss:105.14671325683594\n",
            "1280/3000 train_loss: 34.20281219482422 test_loss:99.1364974975586\n",
            "1281/3000 train_loss: 35.62895584106445 test_loss:110.2352066040039\n",
            "1282/3000 train_loss: 33.13908386230469 test_loss:107.13224029541016\n",
            "1283/3000 train_loss: 35.41707229614258 test_loss:97.16778564453125\n",
            "1284/3000 train_loss: 36.45201873779297 test_loss:111.75660705566406\n",
            "1285/3000 train_loss: 39.77238082885742 test_loss:94.76753234863281\n",
            "1286/3000 train_loss: 33.2198371887207 test_loss:102.54962921142578\n",
            "1287/3000 train_loss: 32.15689468383789 test_loss:95.26415252685547\n",
            "1288/3000 train_loss: 29.19217872619629 test_loss:104.617431640625\n",
            "1289/3000 train_loss: 33.6240348815918 test_loss:99.66246032714844\n",
            "1290/3000 train_loss: 33.47994613647461 test_loss:104.96603393554688\n",
            "1291/3000 train_loss: 36.59122085571289 test_loss:107.84971618652344\n",
            "1292/3000 train_loss: 31.678516387939453 test_loss:106.38607025146484\n",
            "1293/3000 train_loss: 33.337982177734375 test_loss:102.80718231201172\n",
            "1294/3000 train_loss: 33.382938385009766 test_loss:102.02183532714844\n",
            "1295/3000 train_loss: 35.23935317993164 test_loss:107.27100372314453\n",
            "1296/3000 train_loss: 36.15024948120117 test_loss:108.22189331054688\n",
            "1297/3000 train_loss: 34.341529846191406 test_loss:98.79948425292969\n",
            "1298/3000 train_loss: 31.245962142944336 test_loss:107.02288055419922\n",
            "1299/3000 train_loss: 36.131465911865234 test_loss:101.43716430664062\n",
            "1300/3000 train_loss: 35.3957633972168 test_loss:108.78584289550781\n",
            "1301/3000 train_loss: 36.12345886230469 test_loss:99.17859649658203\n",
            "1302/3000 train_loss: 33.90105438232422 test_loss:105.305908203125\n",
            "1303/3000 train_loss: 38.66889953613281 test_loss:103.07545471191406\n",
            "1304/3000 train_loss: 33.12729263305664 test_loss:103.21488952636719\n",
            "1305/3000 train_loss: 32.1489143371582 test_loss:103.08438110351562\n",
            "1306/3000 train_loss: 32.08041000366211 test_loss:99.00434875488281\n",
            "1307/3000 train_loss: 39.29265594482422 test_loss:112.84429931640625\n",
            "1308/3000 train_loss: 30.539630889892578 test_loss:96.60354614257812\n",
            "1309/3000 train_loss: 29.559358596801758 test_loss:111.3682861328125\n",
            "1310/3000 train_loss: 34.23832321166992 test_loss:99.76351928710938\n",
            "1311/3000 train_loss: 34.45451736450195 test_loss:104.78828430175781\n",
            "1312/3000 train_loss: 32.000274658203125 test_loss:104.59072875976562\n",
            "1313/3000 train_loss: 32.99982452392578 test_loss:108.6795654296875\n",
            "1314/3000 train_loss: 36.405052185058594 test_loss:97.10798645019531\n",
            "1315/3000 train_loss: 36.95594024658203 test_loss:98.10549926757812\n",
            "1316/3000 train_loss: 37.58460998535156 test_loss:112.61676025390625\n",
            "1317/3000 train_loss: 36.660072326660156 test_loss:98.60478210449219\n",
            "1318/3000 train_loss: 37.350528717041016 test_loss:101.14218139648438\n",
            "1319/3000 train_loss: 34.678627014160156 test_loss:106.11075592041016\n",
            "1320/3000 train_loss: 39.246185302734375 test_loss:103.19140625\n",
            "1321/3000 train_loss: 43.47364044189453 test_loss:109.68984985351562\n",
            "1322/3000 train_loss: 35.131103515625 test_loss:108.54092407226562\n",
            "1323/3000 train_loss: 49.91827392578125 test_loss:114.89734649658203\n",
            "1324/3000 train_loss: 37.17622375488281 test_loss:104.27635192871094\n",
            "1325/3000 train_loss: 33.238304138183594 test_loss:104.64813232421875\n",
            "1326/3000 train_loss: 40.355384826660156 test_loss:98.8980941772461\n",
            "1327/3000 train_loss: 39.74203109741211 test_loss:102.9899673461914\n",
            "1328/3000 train_loss: 32.864009857177734 test_loss:95.90959167480469\n",
            "1329/3000 train_loss: 32.26871871948242 test_loss:101.18415069580078\n",
            "1330/3000 train_loss: 32.81724548339844 test_loss:93.33158111572266\n",
            "1331/3000 train_loss: 32.941650390625 test_loss:104.08584594726562\n",
            "1332/3000 train_loss: 31.623676300048828 test_loss:97.7110366821289\n",
            "1333/3000 train_loss: 35.87244415283203 test_loss:90.62406921386719\n",
            "1334/3000 train_loss: 35.16908645629883 test_loss:101.52084350585938\n",
            "1335/3000 train_loss: 31.930343627929688 test_loss:98.06611633300781\n",
            "1336/3000 train_loss: 51.594566345214844 test_loss:104.65878295898438\n",
            "1337/3000 train_loss: 41.8440055847168 test_loss:105.48969268798828\n",
            "1338/3000 train_loss: 40.8132209777832 test_loss:108.88594818115234\n",
            "1339/3000 train_loss: 33.60702896118164 test_loss:98.37586212158203\n",
            "1340/3000 train_loss: 35.807579040527344 test_loss:96.0772933959961\n",
            "1341/3000 train_loss: 49.25996398925781 test_loss:115.75875854492188\n",
            "1342/3000 train_loss: 41.74279022216797 test_loss:102.57707977294922\n",
            "1343/3000 train_loss: 32.36458969116211 test_loss:104.48014831542969\n",
            "1344/3000 train_loss: 33.00385284423828 test_loss:94.84068298339844\n",
            "1345/3000 train_loss: 39.21812438964844 test_loss:119.80915832519531\n",
            "1346/3000 train_loss: 30.655466079711914 test_loss:98.19287109375\n",
            "1347/3000 train_loss: 36.890262603759766 test_loss:105.42872619628906\n",
            "1348/3000 train_loss: 41.54753875732422 test_loss:99.65629577636719\n",
            "1349/3000 train_loss: 35.64849090576172 test_loss:104.3141098022461\n",
            "1350/3000 train_loss: 33.94446563720703 test_loss:101.44638061523438\n",
            "1351/3000 train_loss: 33.75249481201172 test_loss:94.39282989501953\n",
            "1352/3000 train_loss: 36.926307678222656 test_loss:105.89124298095703\n",
            "1353/3000 train_loss: 32.77307891845703 test_loss:113.80500793457031\n",
            "1354/3000 train_loss: 31.649578094482422 test_loss:102.4468002319336\n",
            "1355/3000 train_loss: 30.515684127807617 test_loss:106.92312622070312\n",
            "1356/3000 train_loss: 32.77653884887695 test_loss:102.26146697998047\n",
            "1357/3000 train_loss: 37.29150390625 test_loss:101.84185791015625\n",
            "1358/3000 train_loss: 35.92241287231445 test_loss:107.96492004394531\n",
            "1359/3000 train_loss: 33.91352844238281 test_loss:97.7685546875\n",
            "1360/3000 train_loss: 34.71225357055664 test_loss:106.83502197265625\n",
            "1361/3000 train_loss: 43.46689224243164 test_loss:105.10404968261719\n",
            "1362/3000 train_loss: 55.20591735839844 test_loss:106.64471435546875\n",
            "1363/3000 train_loss: 32.259429931640625 test_loss:108.29910278320312\n",
            "1364/3000 train_loss: 38.06575393676758 test_loss:99.83963012695312\n",
            "1365/3000 train_loss: 32.54364013671875 test_loss:103.83226013183594\n",
            "1366/3000 train_loss: 34.547306060791016 test_loss:96.87824249267578\n",
            "1367/3000 train_loss: 32.64778137207031 test_loss:99.81118774414062\n",
            "1368/3000 train_loss: 32.86606979370117 test_loss:100.83135223388672\n",
            "1369/3000 train_loss: 32.52107620239258 test_loss:101.77735900878906\n",
            "1370/3000 train_loss: 31.300376892089844 test_loss:97.26805114746094\n",
            "1371/3000 train_loss: 36.00837707519531 test_loss:94.0674819946289\n",
            "1372/3000 train_loss: 37.95082092285156 test_loss:94.35704040527344\n",
            "1373/3000 train_loss: 33.51340103149414 test_loss:101.93501281738281\n",
            "1374/3000 train_loss: 34.172786712646484 test_loss:102.78738403320312\n",
            "1375/3000 train_loss: 29.74411392211914 test_loss:105.14612579345703\n",
            "1376/3000 train_loss: 34.6833381652832 test_loss:97.8777847290039\n",
            "1377/3000 train_loss: 32.957820892333984 test_loss:103.25888061523438\n",
            "1378/3000 train_loss: 32.342071533203125 test_loss:100.04808807373047\n",
            "1379/3000 train_loss: 34.20446014404297 test_loss:99.72442626953125\n",
            "1380/3000 train_loss: 32.51784133911133 test_loss:102.74905395507812\n",
            "1381/3000 train_loss: 28.48288917541504 test_loss:95.13912963867188\n",
            "1382/3000 train_loss: 33.363304138183594 test_loss:99.0467529296875\n",
            "1383/3000 train_loss: 31.26062774658203 test_loss:92.98320007324219\n",
            "1384/3000 train_loss: 33.010093688964844 test_loss:100.4744873046875\n",
            "1385/3000 train_loss: 30.631610870361328 test_loss:119.18172454833984\n",
            "1386/3000 train_loss: 31.531129837036133 test_loss:96.0113525390625\n",
            "1387/3000 train_loss: 31.23688316345215 test_loss:104.09196472167969\n",
            "1388/3000 train_loss: 40.818965911865234 test_loss:101.57154083251953\n",
            "1389/3000 train_loss: 36.82878494262695 test_loss:98.8182601928711\n",
            "1390/3000 train_loss: 28.422210693359375 test_loss:99.99142456054688\n",
            "1391/3000 train_loss: 34.65379333496094 test_loss:99.43158721923828\n",
            "1392/3000 train_loss: 37.853355407714844 test_loss:98.45904541015625\n",
            "1393/3000 train_loss: 37.42395782470703 test_loss:98.05868530273438\n",
            "1394/3000 train_loss: 31.749553680419922 test_loss:94.5716552734375\n",
            "1395/3000 train_loss: 34.32209777832031 test_loss:99.89030456542969\n",
            "1396/3000 train_loss: 29.146774291992188 test_loss:94.42173767089844\n",
            "1397/3000 train_loss: 32.130592346191406 test_loss:99.13801574707031\n",
            "1398/3000 train_loss: 30.638439178466797 test_loss:98.36163330078125\n",
            "1399/3000 train_loss: 30.994972229003906 test_loss:96.1610107421875\n",
            "1400/3000 train_loss: 33.41794967651367 test_loss:95.13481140136719\n",
            "1401/3000 train_loss: 33.52942657470703 test_loss:100.88554382324219\n",
            "1402/3000 train_loss: 30.751028060913086 test_loss:99.49674987792969\n",
            "1403/3000 train_loss: 32.204010009765625 test_loss:100.47118377685547\n",
            "1404/3000 train_loss: 33.55704879760742 test_loss:92.72820281982422\n",
            "1405/3000 train_loss: 33.025718688964844 test_loss:96.74146270751953\n",
            "1406/3000 train_loss: 34.551231384277344 test_loss:101.56222534179688\n",
            "1407/3000 train_loss: 33.59968948364258 test_loss:98.99395751953125\n",
            "1408/3000 train_loss: 32.3775749206543 test_loss:97.55463409423828\n",
            "1409/3000 train_loss: 28.516260147094727 test_loss:102.90679931640625\n",
            "1410/3000 train_loss: 34.88146209716797 test_loss:102.52426147460938\n",
            "1411/3000 train_loss: 31.782649993896484 test_loss:94.90423583984375\n",
            "1412/3000 train_loss: 32.24131774902344 test_loss:98.48997497558594\n",
            "1413/3000 train_loss: 32.684688568115234 test_loss:94.23941802978516\n",
            "1414/3000 train_loss: 29.909412384033203 test_loss:97.7793197631836\n",
            "1415/3000 train_loss: 38.79142761230469 test_loss:94.75824737548828\n",
            "1416/3000 train_loss: 29.46044158935547 test_loss:100.43379211425781\n",
            "1417/3000 train_loss: 36.71434783935547 test_loss:89.11344909667969\n",
            "1418/3000 train_loss: 33.41246795654297 test_loss:89.51786804199219\n",
            "1419/3000 train_loss: 30.000286102294922 test_loss:94.38986206054688\n",
            "1420/3000 train_loss: 34.479515075683594 test_loss:96.71097564697266\n",
            "1421/3000 train_loss: 37.07395553588867 test_loss:95.34964752197266\n",
            "1422/3000 train_loss: 30.423809051513672 test_loss:98.17716979980469\n",
            "1423/3000 train_loss: 28.143518447875977 test_loss:94.15966033935547\n",
            "1424/3000 train_loss: 31.347307205200195 test_loss:102.8135986328125\n",
            "1425/3000 train_loss: 38.78131103515625 test_loss:98.31050872802734\n",
            "1426/3000 train_loss: 33.891048431396484 test_loss:96.56578826904297\n",
            "1427/3000 train_loss: 32.40193557739258 test_loss:107.48979187011719\n",
            "1428/3000 train_loss: 29.498456954956055 test_loss:108.29220581054688\n",
            "1429/3000 train_loss: 35.956199645996094 test_loss:94.37779998779297\n",
            "1430/3000 train_loss: 44.28382110595703 test_loss:98.01019287109375\n",
            "1431/3000 train_loss: 33.81166458129883 test_loss:99.317138671875\n",
            "1432/3000 train_loss: 37.73827362060547 test_loss:95.22974395751953\n",
            "1433/3000 train_loss: 31.69443130493164 test_loss:95.53256225585938\n",
            "1434/3000 train_loss: 31.5916748046875 test_loss:96.22444152832031\n",
            "1435/3000 train_loss: 29.370805740356445 test_loss:90.77264404296875\n",
            "1436/3000 train_loss: 35.17366409301758 test_loss:102.77906799316406\n",
            "1437/3000 train_loss: 34.12668228149414 test_loss:93.56198120117188\n",
            "1438/3000 train_loss: 31.88290786743164 test_loss:100.15019226074219\n",
            "1439/3000 train_loss: 30.766948699951172 test_loss:90.55220031738281\n",
            "1440/3000 train_loss: 26.19382095336914 test_loss:94.10609436035156\n",
            "1441/3000 train_loss: 33.86026382446289 test_loss:100.20893859863281\n",
            "1442/3000 train_loss: 35.69857406616211 test_loss:94.29193115234375\n",
            "1443/3000 train_loss: 39.54312515258789 test_loss:90.74525451660156\n",
            "1444/3000 train_loss: 32.290565490722656 test_loss:95.11853790283203\n",
            "1445/3000 train_loss: 28.46076202392578 test_loss:90.62378692626953\n",
            "1446/3000 train_loss: 35.01560974121094 test_loss:88.31685638427734\n",
            "1447/3000 train_loss: 35.386844635009766 test_loss:110.68193054199219\n",
            "1448/3000 train_loss: 31.33355140686035 test_loss:98.77601623535156\n",
            "1449/3000 train_loss: 31.87486457824707 test_loss:100.58477783203125\n",
            "1450/3000 train_loss: 31.952747344970703 test_loss:92.47267150878906\n",
            "1451/3000 train_loss: 33.4129638671875 test_loss:92.94570922851562\n",
            "1452/3000 train_loss: 28.200414657592773 test_loss:91.88191223144531\n",
            "1453/3000 train_loss: 33.167327880859375 test_loss:102.87386322021484\n",
            "1454/3000 train_loss: 35.61009979248047 test_loss:90.2198257446289\n",
            "1455/3000 train_loss: 31.764860153198242 test_loss:91.15574645996094\n",
            "1456/3000 train_loss: 31.568132400512695 test_loss:86.06153106689453\n",
            "1457/3000 train_loss: 76.18710327148438 test_loss:91.45054626464844\n",
            "1458/3000 train_loss: 49.33085632324219 test_loss:104.78548431396484\n",
            "1459/3000 train_loss: 36.56984329223633 test_loss:96.64976501464844\n",
            "1460/3000 train_loss: 34.429927825927734 test_loss:103.78903198242188\n",
            "1461/3000 train_loss: 30.75192642211914 test_loss:97.2572250366211\n",
            "1462/3000 train_loss: 26.991085052490234 test_loss:86.20008087158203\n",
            "1463/3000 train_loss: 26.8233642578125 test_loss:91.51823425292969\n",
            "1464/3000 train_loss: 28.207202911376953 test_loss:84.14329528808594\n",
            "1465/3000 train_loss: 29.53190803527832 test_loss:101.15924835205078\n",
            "1466/3000 train_loss: 29.189910888671875 test_loss:91.94107818603516\n",
            "1467/3000 train_loss: 25.675004959106445 test_loss:95.69673919677734\n",
            "1468/3000 train_loss: 28.515689849853516 test_loss:89.30964660644531\n",
            "1469/3000 train_loss: 31.0920467376709 test_loss:104.35220336914062\n",
            "1470/3000 train_loss: 31.574506759643555 test_loss:89.80593872070312\n",
            "1471/3000 train_loss: 30.174480438232422 test_loss:97.41288757324219\n",
            "1472/3000 train_loss: 31.49663734436035 test_loss:93.27290344238281\n",
            "1473/3000 train_loss: 31.61311149597168 test_loss:98.73408508300781\n",
            "1474/3000 train_loss: 32.0418815612793 test_loss:91.57801818847656\n",
            "1475/3000 train_loss: 31.50290298461914 test_loss:100.37171173095703\n",
            "1476/3000 train_loss: 32.47663497924805 test_loss:95.28052520751953\n",
            "1477/3000 train_loss: 29.799728393554688 test_loss:97.18270874023438\n",
            "1478/3000 train_loss: 32.40510177612305 test_loss:93.95027923583984\n",
            "1479/3000 train_loss: 32.0079231262207 test_loss:99.63842010498047\n",
            "1480/3000 train_loss: 30.17514419555664 test_loss:96.91659545898438\n",
            "1481/3000 train_loss: 34.45634460449219 test_loss:101.40306091308594\n",
            "1482/3000 train_loss: 30.516746520996094 test_loss:95.63125610351562\n",
            "1483/3000 train_loss: 30.973648071289062 test_loss:97.54661560058594\n",
            "1484/3000 train_loss: 33.247962951660156 test_loss:97.35931396484375\n",
            "1485/3000 train_loss: 32.59348678588867 test_loss:99.52900695800781\n",
            "1486/3000 train_loss: 33.98523712158203 test_loss:89.144775390625\n",
            "1487/3000 train_loss: 26.438161849975586 test_loss:99.37004852294922\n",
            "1488/3000 train_loss: 27.314796447753906 test_loss:96.69966888427734\n",
            "1489/3000 train_loss: 30.83071517944336 test_loss:97.08837890625\n",
            "1490/3000 train_loss: 38.99800109863281 test_loss:103.80315399169922\n",
            "1491/3000 train_loss: 30.282482147216797 test_loss:100.94352722167969\n",
            "1492/3000 train_loss: 28.017778396606445 test_loss:94.62382507324219\n",
            "1493/3000 train_loss: 30.85055160522461 test_loss:105.2848892211914\n",
            "1494/3000 train_loss: 74.07417297363281 test_loss:103.38392639160156\n",
            "1495/3000 train_loss: 38.526588439941406 test_loss:108.13999938964844\n",
            "1496/3000 train_loss: 33.69776153564453 test_loss:100.04244995117188\n",
            "1497/3000 train_loss: 30.421798706054688 test_loss:106.43312072753906\n",
            "1498/3000 train_loss: 29.01930046081543 test_loss:93.46577453613281\n",
            "1499/3000 train_loss: 28.9971923828125 test_loss:99.714599609375\n",
            "1500/3000 train_loss: 30.60615348815918 test_loss:90.36767578125\n",
            "1501/3000 train_loss: 32.284427642822266 test_loss:97.61263275146484\n",
            "1502/3000 train_loss: 26.695484161376953 test_loss:89.09172821044922\n",
            "1503/3000 train_loss: 26.963848114013672 test_loss:89.63980102539062\n",
            "1504/3000 train_loss: 37.045230865478516 test_loss:98.5482177734375\n",
            "1505/3000 train_loss: 28.90467071533203 test_loss:93.41014099121094\n",
            "1506/3000 train_loss: 31.340879440307617 test_loss:101.53927612304688\n",
            "1507/3000 train_loss: 34.60139465332031 test_loss:95.08966064453125\n",
            "1508/3000 train_loss: 32.977691650390625 test_loss:94.01712036132812\n",
            "1509/3000 train_loss: 28.031463623046875 test_loss:103.09226989746094\n",
            "1510/3000 train_loss: 33.55874252319336 test_loss:96.28317260742188\n",
            "1511/3000 train_loss: 30.95616340637207 test_loss:93.46746826171875\n",
            "1512/3000 train_loss: 38.65388870239258 test_loss:91.22955322265625\n",
            "1513/3000 train_loss: 29.997718811035156 test_loss:105.61753845214844\n",
            "1514/3000 train_loss: 30.029722213745117 test_loss:91.87922668457031\n",
            "1515/3000 train_loss: 27.238130569458008 test_loss:94.36210632324219\n",
            "1516/3000 train_loss: 26.299808502197266 test_loss:96.02609252929688\n",
            "1517/3000 train_loss: 26.2852840423584 test_loss:86.03251647949219\n",
            "1518/3000 train_loss: 29.364152908325195 test_loss:92.31129455566406\n",
            "1519/3000 train_loss: 28.101716995239258 test_loss:88.16889953613281\n",
            "1520/3000 train_loss: 25.6294002532959 test_loss:90.63631439208984\n",
            "1521/3000 train_loss: 32.25754928588867 test_loss:105.56459045410156\n",
            "1522/3000 train_loss: 30.865434646606445 test_loss:93.30949401855469\n",
            "1523/3000 train_loss: 27.148845672607422 test_loss:95.94377136230469\n",
            "1524/3000 train_loss: 53.04714584350586 test_loss:101.3222885131836\n",
            "1525/3000 train_loss: 38.01442337036133 test_loss:100.42102813720703\n",
            "1526/3000 train_loss: 30.977375030517578 test_loss:95.99310302734375\n",
            "1527/3000 train_loss: 32.77871322631836 test_loss:105.52532958984375\n",
            "1528/3000 train_loss: 30.44471549987793 test_loss:93.49595642089844\n",
            "1529/3000 train_loss: 25.80560302734375 test_loss:95.85835266113281\n",
            "1530/3000 train_loss: 26.064950942993164 test_loss:93.39100646972656\n",
            "1531/3000 train_loss: 31.089611053466797 test_loss:90.78875732421875\n",
            "1532/3000 train_loss: 26.547679901123047 test_loss:88.61550903320312\n",
            "1533/3000 train_loss: 30.175207138061523 test_loss:96.15386962890625\n",
            "1534/3000 train_loss: 26.69908905029297 test_loss:85.94194030761719\n",
            "1535/3000 train_loss: 27.518104553222656 test_loss:94.20211791992188\n",
            "1536/3000 train_loss: 31.282630920410156 test_loss:88.17265319824219\n",
            "1537/3000 train_loss: 26.708370208740234 test_loss:95.49895477294922\n",
            "1538/3000 train_loss: 25.114959716796875 test_loss:89.47758483886719\n",
            "1539/3000 train_loss: 24.495718002319336 test_loss:92.66551208496094\n",
            "1540/3000 train_loss: 30.266565322875977 test_loss:91.88279724121094\n",
            "1541/3000 train_loss: 28.95330238342285 test_loss:87.62158203125\n",
            "1542/3000 train_loss: 32.187400817871094 test_loss:90.33256530761719\n",
            "1543/3000 train_loss: 30.018728256225586 test_loss:97.93163299560547\n",
            "1544/3000 train_loss: 28.597015380859375 test_loss:97.27876281738281\n",
            "1545/3000 train_loss: 30.193958282470703 test_loss:89.6854248046875\n",
            "1546/3000 train_loss: 30.142730712890625 test_loss:94.7967529296875\n",
            "1547/3000 train_loss: 28.931514739990234 test_loss:92.22162628173828\n",
            "1548/3000 train_loss: 26.032859802246094 test_loss:91.95059204101562\n",
            "1549/3000 train_loss: 29.82724952697754 test_loss:88.08035278320312\n",
            "1550/3000 train_loss: 29.339988708496094 test_loss:98.3266372680664\n",
            "1551/3000 train_loss: 32.819026947021484 test_loss:84.68834686279297\n",
            "1552/3000 train_loss: 40.935150146484375 test_loss:99.72367858886719\n",
            "1553/3000 train_loss: 29.976848602294922 test_loss:92.10757446289062\n",
            "1554/3000 train_loss: 30.175580978393555 test_loss:99.071044921875\n",
            "1555/3000 train_loss: 30.47985076904297 test_loss:100.71723937988281\n",
            "1556/3000 train_loss: 27.75341796875 test_loss:96.76251220703125\n",
            "1557/3000 train_loss: 25.835603713989258 test_loss:89.8323974609375\n",
            "1558/3000 train_loss: 31.652034759521484 test_loss:104.20944213867188\n",
            "1559/3000 train_loss: 29.113428115844727 test_loss:95.94427490234375\n",
            "1560/3000 train_loss: 40.899898529052734 test_loss:94.00657653808594\n",
            "1561/3000 train_loss: 34.09906768798828 test_loss:94.66220092773438\n",
            "1562/3000 train_loss: 33.47882080078125 test_loss:93.35105895996094\n",
            "1563/3000 train_loss: 28.300323486328125 test_loss:94.11781311035156\n",
            "1564/3000 train_loss: 35.95758819580078 test_loss:92.39813995361328\n",
            "1565/3000 train_loss: 29.910573959350586 test_loss:87.77120208740234\n",
            "1566/3000 train_loss: 29.462284088134766 test_loss:90.10604095458984\n",
            "1567/3000 train_loss: 28.148792266845703 test_loss:90.77556610107422\n",
            "1568/3000 train_loss: 26.765865325927734 test_loss:87.2923812866211\n",
            "1569/3000 train_loss: 33.59194564819336 test_loss:90.82655334472656\n",
            "1570/3000 train_loss: 29.44774055480957 test_loss:104.64039611816406\n",
            "1571/3000 train_loss: 28.2697811126709 test_loss:93.08261108398438\n",
            "1572/3000 train_loss: 30.429004669189453 test_loss:87.01026916503906\n",
            "1573/3000 train_loss: 26.072193145751953 test_loss:91.76458740234375\n",
            "1574/3000 train_loss: 30.37324333190918 test_loss:91.34854888916016\n",
            "1575/3000 train_loss: 28.22281265258789 test_loss:90.76104736328125\n",
            "1576/3000 train_loss: 34.095462799072266 test_loss:93.185546875\n",
            "1577/3000 train_loss: 30.409547805786133 test_loss:95.6040267944336\n",
            "1578/3000 train_loss: 34.416072845458984 test_loss:96.44342041015625\n",
            "1579/3000 train_loss: 26.563098907470703 test_loss:95.12289428710938\n",
            "1580/3000 train_loss: 33.159996032714844 test_loss:89.36782836914062\n",
            "1581/3000 train_loss: 29.182865142822266 test_loss:86.29135131835938\n",
            "1582/3000 train_loss: 32.24077606201172 test_loss:92.81231689453125\n",
            "1583/3000 train_loss: 29.618074417114258 test_loss:103.51443481445312\n",
            "1584/3000 train_loss: 32.7912712097168 test_loss:89.87171936035156\n",
            "1585/3000 train_loss: 30.738203048706055 test_loss:89.3370590209961\n",
            "1586/3000 train_loss: 27.07611846923828 test_loss:89.06584167480469\n",
            "1587/3000 train_loss: 29.40603256225586 test_loss:91.05030822753906\n",
            "1588/3000 train_loss: 27.183738708496094 test_loss:95.77072143554688\n",
            "1589/3000 train_loss: 28.55365753173828 test_loss:87.83309936523438\n",
            "1590/3000 train_loss: 29.236581802368164 test_loss:94.00953674316406\n",
            "1591/3000 train_loss: 27.68341064453125 test_loss:93.52498626708984\n",
            "1592/3000 train_loss: 27.56047821044922 test_loss:99.5135498046875\n",
            "1593/3000 train_loss: 31.598363876342773 test_loss:97.63725280761719\n",
            "1594/3000 train_loss: 32.11918640136719 test_loss:89.31024169921875\n",
            "1595/3000 train_loss: 28.728246688842773 test_loss:93.03144836425781\n",
            "1596/3000 train_loss: 24.790443420410156 test_loss:89.1422119140625\n",
            "1597/3000 train_loss: 39.0579948425293 test_loss:85.69589233398438\n",
            "1598/3000 train_loss: 34.84547805786133 test_loss:100.12110900878906\n",
            "1599/3000 train_loss: 39.147483825683594 test_loss:96.41165924072266\n",
            "1600/3000 train_loss: 30.280630111694336 test_loss:94.42802429199219\n",
            "1601/3000 train_loss: 42.19924545288086 test_loss:96.12498474121094\n",
            "1602/3000 train_loss: 28.637130737304688 test_loss:96.91017150878906\n",
            "1603/3000 train_loss: 30.607707977294922 test_loss:96.2270278930664\n",
            "1604/3000 train_loss: 28.45183563232422 test_loss:94.771728515625\n",
            "1605/3000 train_loss: 26.0267391204834 test_loss:98.421142578125\n",
            "1606/3000 train_loss: 34.28693771362305 test_loss:97.29537963867188\n",
            "1607/3000 train_loss: 30.995256423950195 test_loss:97.54638671875\n",
            "1608/3000 train_loss: 28.367156982421875 test_loss:96.38241577148438\n",
            "1609/3000 train_loss: 26.47932243347168 test_loss:87.16899871826172\n",
            "1610/3000 train_loss: 32.618751525878906 test_loss:104.345703125\n",
            "1611/3000 train_loss: 31.82086753845215 test_loss:90.69819641113281\n",
            "1612/3000 train_loss: 30.517425537109375 test_loss:95.95339965820312\n",
            "1613/3000 train_loss: 36.57691192626953 test_loss:102.18571472167969\n",
            "1614/3000 train_loss: 29.835674285888672 test_loss:91.42228698730469\n",
            "1615/3000 train_loss: 29.21520233154297 test_loss:93.36309814453125\n",
            "1616/3000 train_loss: 32.00972366333008 test_loss:91.06950378417969\n",
            "1617/3000 train_loss: 28.56945037841797 test_loss:94.43582916259766\n",
            "1618/3000 train_loss: 25.19886016845703 test_loss:91.04364013671875\n",
            "1619/3000 train_loss: 28.340316772460938 test_loss:90.90412902832031\n",
            "1620/3000 train_loss: 33.06288146972656 test_loss:97.61406707763672\n",
            "1621/3000 train_loss: 32.864524841308594 test_loss:94.04727935791016\n",
            "1622/3000 train_loss: 27.591506958007812 test_loss:94.22643280029297\n",
            "1623/3000 train_loss: 30.030399322509766 test_loss:93.5045166015625\n",
            "1624/3000 train_loss: 37.25018310546875 test_loss:93.95571899414062\n",
            "1625/3000 train_loss: 27.587705612182617 test_loss:98.23860168457031\n",
            "1626/3000 train_loss: 26.712127685546875 test_loss:97.03228759765625\n",
            "1627/3000 train_loss: 31.859600067138672 test_loss:95.58694458007812\n",
            "1628/3000 train_loss: 33.372013092041016 test_loss:118.21702575683594\n",
            "1629/3000 train_loss: 34.19658660888672 test_loss:93.77940368652344\n",
            "1630/3000 train_loss: 31.735454559326172 test_loss:95.82514953613281\n",
            "1631/3000 train_loss: 30.428930282592773 test_loss:95.98572540283203\n",
            "1632/3000 train_loss: 26.178556442260742 test_loss:92.80320739746094\n",
            "1633/3000 train_loss: 25.46194076538086 test_loss:88.52333068847656\n",
            "1634/3000 train_loss: 30.276348114013672 test_loss:97.29791259765625\n",
            "1635/3000 train_loss: 27.254732131958008 test_loss:98.53718566894531\n",
            "1636/3000 train_loss: 27.160682678222656 test_loss:92.82460021972656\n",
            "1637/3000 train_loss: 25.533672332763672 test_loss:98.38880920410156\n",
            "1638/3000 train_loss: 29.126914978027344 test_loss:94.97181701660156\n",
            "1639/3000 train_loss: 28.28449249267578 test_loss:94.39766693115234\n",
            "1640/3000 train_loss: 29.03302001953125 test_loss:88.44438934326172\n",
            "1641/3000 train_loss: 32.72801971435547 test_loss:95.07432556152344\n",
            "1642/3000 train_loss: 31.4769344329834 test_loss:95.28781127929688\n",
            "1643/3000 train_loss: 30.29298973083496 test_loss:90.05374908447266\n",
            "1644/3000 train_loss: 29.56832504272461 test_loss:89.90608978271484\n",
            "1645/3000 train_loss: 29.647075653076172 test_loss:92.61018371582031\n",
            "1646/3000 train_loss: 28.821598052978516 test_loss:90.249755859375\n",
            "1647/3000 train_loss: 27.01617431640625 test_loss:92.78162384033203\n",
            "1648/3000 train_loss: 26.624338150024414 test_loss:97.12591552734375\n",
            "1649/3000 train_loss: 29.0654296875 test_loss:91.88834381103516\n",
            "1650/3000 train_loss: 25.43919563293457 test_loss:91.8797378540039\n",
            "1651/3000 train_loss: 27.267593383789062 test_loss:93.71275329589844\n",
            "1652/3000 train_loss: 26.578319549560547 test_loss:99.10649108886719\n",
            "1653/3000 train_loss: 25.24675941467285 test_loss:96.97148132324219\n",
            "1654/3000 train_loss: 30.99915313720703 test_loss:104.20207214355469\n",
            "1655/3000 train_loss: 26.480693817138672 test_loss:88.25457763671875\n",
            "1656/3000 train_loss: 31.691465377807617 test_loss:99.98222351074219\n",
            "1657/3000 train_loss: 26.38033676147461 test_loss:96.85173034667969\n",
            "1658/3000 train_loss: 27.55258560180664 test_loss:90.64772033691406\n",
            "1659/3000 train_loss: 27.184438705444336 test_loss:97.88836669921875\n",
            "1660/3000 train_loss: 24.4778995513916 test_loss:93.27992248535156\n",
            "1661/3000 train_loss: 26.429410934448242 test_loss:92.09835815429688\n",
            "1662/3000 train_loss: 26.96133804321289 test_loss:92.81427764892578\n",
            "1663/3000 train_loss: 30.898704528808594 test_loss:104.93141174316406\n",
            "1664/3000 train_loss: 27.491601943969727 test_loss:92.67315673828125\n",
            "1665/3000 train_loss: 24.80318832397461 test_loss:101.9498291015625\n",
            "1666/3000 train_loss: 28.808687210083008 test_loss:90.68638610839844\n",
            "1667/3000 train_loss: 25.638967514038086 test_loss:96.50222778320312\n",
            "1668/3000 train_loss: 24.925933837890625 test_loss:91.57366943359375\n",
            "1669/3000 train_loss: 26.265586853027344 test_loss:93.75113677978516\n",
            "1670/3000 train_loss: 29.909923553466797 test_loss:101.12147521972656\n",
            "1671/3000 train_loss: 31.192304611206055 test_loss:95.18824768066406\n",
            "1672/3000 train_loss: 29.748027801513672 test_loss:94.77803039550781\n",
            "1673/3000 train_loss: 27.512161254882812 test_loss:97.54426574707031\n",
            "1674/3000 train_loss: 25.93800926208496 test_loss:91.09396362304688\n",
            "1675/3000 train_loss: 31.008453369140625 test_loss:106.85993194580078\n",
            "1676/3000 train_loss: 29.15172004699707 test_loss:88.31855773925781\n",
            "1677/3000 train_loss: 34.307674407958984 test_loss:88.87875366210938\n",
            "1678/3000 train_loss: 29.414194107055664 test_loss:91.0849609375\n",
            "1679/3000 train_loss: 33.776729583740234 test_loss:97.39546203613281\n",
            "1680/3000 train_loss: 30.571903228759766 test_loss:90.54620361328125\n",
            "1681/3000 train_loss: 29.752180099487305 test_loss:90.73674011230469\n",
            "1682/3000 train_loss: 22.927995681762695 test_loss:88.20773315429688\n",
            "1683/3000 train_loss: 28.241878509521484 test_loss:88.83817291259766\n",
            "1684/3000 train_loss: 27.760568618774414 test_loss:95.95808410644531\n",
            "1685/3000 train_loss: 32.76937484741211 test_loss:94.73717498779297\n",
            "1686/3000 train_loss: 32.105018615722656 test_loss:99.7403793334961\n",
            "1687/3000 train_loss: 29.18805694580078 test_loss:90.60197448730469\n",
            "1688/3000 train_loss: 26.23802375793457 test_loss:85.37378692626953\n",
            "1689/3000 train_loss: 29.829004287719727 test_loss:102.35090637207031\n",
            "1690/3000 train_loss: 26.07627296447754 test_loss:92.88249206542969\n",
            "1691/3000 train_loss: 30.147464752197266 test_loss:93.24490356445312\n",
            "1692/3000 train_loss: 25.924531936645508 test_loss:92.15878295898438\n",
            "1693/3000 train_loss: 30.28672981262207 test_loss:92.45793151855469\n",
            "1694/3000 train_loss: 28.1372013092041 test_loss:89.59022521972656\n",
            "1695/3000 train_loss: 30.526884078979492 test_loss:85.44248962402344\n",
            "1696/3000 train_loss: 33.882110595703125 test_loss:100.23051452636719\n",
            "1697/3000 train_loss: 27.271894454956055 test_loss:94.54785919189453\n",
            "1698/3000 train_loss: 23.958898544311523 test_loss:97.63827514648438\n",
            "1699/3000 train_loss: 23.832744598388672 test_loss:87.09844207763672\n",
            "1700/3000 train_loss: 31.704151153564453 test_loss:90.52397155761719\n",
            "1701/3000 train_loss: 29.060558319091797 test_loss:96.87892150878906\n",
            "1702/3000 train_loss: 26.793292999267578 test_loss:85.40495300292969\n",
            "1703/3000 train_loss: 24.561199188232422 test_loss:87.64356994628906\n",
            "1704/3000 train_loss: 28.09077262878418 test_loss:83.5008316040039\n",
            "1705/3000 train_loss: 23.660463333129883 test_loss:87.23128509521484\n",
            "1706/3000 train_loss: 25.439393997192383 test_loss:84.90306854248047\n",
            "1707/3000 train_loss: 23.98912811279297 test_loss:86.11859130859375\n",
            "1708/3000 train_loss: 23.945783615112305 test_loss:90.26033020019531\n",
            "1709/3000 train_loss: 24.103431701660156 test_loss:89.352294921875\n",
            "1710/3000 train_loss: 23.785552978515625 test_loss:84.75651550292969\n",
            "1711/3000 train_loss: 29.79398536682129 test_loss:87.13878631591797\n",
            "1712/3000 train_loss: 27.037212371826172 test_loss:87.39794921875\n",
            "1713/3000 train_loss: 24.36197853088379 test_loss:87.6845474243164\n",
            "1714/3000 train_loss: 29.719009399414062 test_loss:95.73307800292969\n",
            "1715/3000 train_loss: 26.149856567382812 test_loss:87.11344909667969\n",
            "1716/3000 train_loss: 24.414974212646484 test_loss:98.07646179199219\n",
            "1717/3000 train_loss: 23.29566192626953 test_loss:88.50157165527344\n",
            "1718/3000 train_loss: 26.201555252075195 test_loss:85.58242797851562\n",
            "1719/3000 train_loss: 32.34596252441406 test_loss:89.1417465209961\n",
            "1720/3000 train_loss: 25.092798233032227 test_loss:92.33030700683594\n",
            "1721/3000 train_loss: 25.16779899597168 test_loss:88.6016845703125\n",
            "1722/3000 train_loss: 40.801761627197266 test_loss:110.59552001953125\n",
            "1723/3000 train_loss: 28.12204360961914 test_loss:94.52737426757812\n",
            "1724/3000 train_loss: 24.948890686035156 test_loss:86.49109649658203\n",
            "1725/3000 train_loss: 26.359821319580078 test_loss:92.27808380126953\n",
            "1726/3000 train_loss: 21.725067138671875 test_loss:94.19564819335938\n",
            "1727/3000 train_loss: 29.440584182739258 test_loss:100.87217712402344\n",
            "1728/3000 train_loss: 24.69094467163086 test_loss:93.14154815673828\n",
            "1729/3000 train_loss: 25.676109313964844 test_loss:98.35639190673828\n",
            "1730/3000 train_loss: 31.158754348754883 test_loss:91.7459716796875\n",
            "1731/3000 train_loss: 27.644289016723633 test_loss:96.10697937011719\n",
            "1732/3000 train_loss: 28.98516845703125 test_loss:105.83821105957031\n",
            "1733/3000 train_loss: 26.424415588378906 test_loss:99.62347412109375\n",
            "1734/3000 train_loss: 25.518573760986328 test_loss:92.91513061523438\n",
            "1735/3000 train_loss: 28.033449172973633 test_loss:105.13619995117188\n",
            "1736/3000 train_loss: 26.972888946533203 test_loss:95.91728210449219\n",
            "1737/3000 train_loss: 30.71205711364746 test_loss:90.76948547363281\n",
            "1738/3000 train_loss: 26.032676696777344 test_loss:90.46154022216797\n",
            "1739/3000 train_loss: 24.974456787109375 test_loss:102.28919982910156\n",
            "1740/3000 train_loss: 28.77635383605957 test_loss:93.2493896484375\n",
            "1741/3000 train_loss: 29.05496597290039 test_loss:91.40279388427734\n",
            "1742/3000 train_loss: 29.851186752319336 test_loss:92.27108764648438\n",
            "1743/3000 train_loss: 31.428651809692383 test_loss:97.94029998779297\n",
            "1744/3000 train_loss: 28.164697647094727 test_loss:97.52943420410156\n",
            "1745/3000 train_loss: 26.586362838745117 test_loss:93.14944458007812\n",
            "1746/3000 train_loss: 33.1519889831543 test_loss:99.25819396972656\n",
            "1747/3000 train_loss: 28.656787872314453 test_loss:98.39845275878906\n",
            "1748/3000 train_loss: 24.635223388671875 test_loss:96.57518005371094\n",
            "1749/3000 train_loss: 25.29998016357422 test_loss:95.574951171875\n",
            "1750/3000 train_loss: 26.083803176879883 test_loss:88.1539077758789\n",
            "1751/3000 train_loss: 24.770090103149414 test_loss:95.24818420410156\n",
            "1752/3000 train_loss: 25.58812713623047 test_loss:91.03872680664062\n",
            "1753/3000 train_loss: 29.611400604248047 test_loss:101.74494934082031\n",
            "1754/3000 train_loss: 26.429914474487305 test_loss:92.58647155761719\n",
            "1755/3000 train_loss: 25.39558982849121 test_loss:92.8714599609375\n",
            "1756/3000 train_loss: 27.81858253479004 test_loss:99.80844116210938\n",
            "1757/3000 train_loss: 24.592315673828125 test_loss:91.71846008300781\n",
            "1758/3000 train_loss: 35.44197463989258 test_loss:93.0182113647461\n",
            "1759/3000 train_loss: 30.064891815185547 test_loss:99.65631866455078\n",
            "1760/3000 train_loss: 25.506526947021484 test_loss:95.40941619873047\n",
            "1761/3000 train_loss: 27.855663299560547 test_loss:92.60721588134766\n",
            "1762/3000 train_loss: 26.35399627685547 test_loss:97.46263122558594\n",
            "1763/3000 train_loss: 39.54824447631836 test_loss:99.07991027832031\n",
            "1764/3000 train_loss: 32.32114791870117 test_loss:100.92115020751953\n",
            "1765/3000 train_loss: 30.959169387817383 test_loss:95.35520935058594\n",
            "1766/3000 train_loss: 31.0506591796875 test_loss:107.86387634277344\n",
            "1767/3000 train_loss: 27.83144760131836 test_loss:95.72908020019531\n",
            "1768/3000 train_loss: 28.152257919311523 test_loss:105.87149047851562\n",
            "1769/3000 train_loss: 24.564966201782227 test_loss:105.2297592163086\n",
            "1770/3000 train_loss: 21.514516830444336 test_loss:100.36505126953125\n",
            "1771/3000 train_loss: 28.034883499145508 test_loss:103.52660369873047\n",
            "1772/3000 train_loss: 26.462804794311523 test_loss:94.70075225830078\n",
            "1773/3000 train_loss: 24.70996856689453 test_loss:95.10530090332031\n",
            "1774/3000 train_loss: 21.673934936523438 test_loss:92.668701171875\n",
            "1775/3000 train_loss: 25.227033615112305 test_loss:99.33842468261719\n",
            "1776/3000 train_loss: 24.882122039794922 test_loss:97.33506774902344\n",
            "1777/3000 train_loss: 24.656326293945312 test_loss:95.84431457519531\n",
            "1778/3000 train_loss: 26.141321182250977 test_loss:93.18970489501953\n",
            "1779/3000 train_loss: 20.653316497802734 test_loss:91.82350158691406\n",
            "1780/3000 train_loss: 29.97808074951172 test_loss:100.77001190185547\n",
            "1781/3000 train_loss: 26.92584800720215 test_loss:97.68938446044922\n",
            "1782/3000 train_loss: 29.264841079711914 test_loss:91.50379180908203\n",
            "1783/3000 train_loss: 23.529603958129883 test_loss:89.98561096191406\n",
            "1784/3000 train_loss: 25.176345825195312 test_loss:97.23701477050781\n",
            "1785/3000 train_loss: 24.941892623901367 test_loss:94.14452362060547\n",
            "1786/3000 train_loss: 23.973562240600586 test_loss:93.42987060546875\n",
            "1787/3000 train_loss: 24.68357276916504 test_loss:100.22100830078125\n",
            "1788/3000 train_loss: 26.047426223754883 test_loss:98.1951675415039\n",
            "1789/3000 train_loss: 26.715166091918945 test_loss:90.97142791748047\n",
            "1790/3000 train_loss: 24.897613525390625 test_loss:104.81253051757812\n",
            "1791/3000 train_loss: 29.8555850982666 test_loss:90.13018798828125\n",
            "1792/3000 train_loss: 25.66763687133789 test_loss:98.69420623779297\n",
            "1793/3000 train_loss: 30.082904815673828 test_loss:88.07974243164062\n",
            "1794/3000 train_loss: 23.550167083740234 test_loss:98.07994079589844\n",
            "1795/3000 train_loss: 30.999454498291016 test_loss:95.93281555175781\n",
            "1796/3000 train_loss: 30.54033088684082 test_loss:86.69586181640625\n",
            "1797/3000 train_loss: 22.35698890686035 test_loss:90.91785430908203\n",
            "1798/3000 train_loss: 25.017024993896484 test_loss:88.8419189453125\n",
            "1799/3000 train_loss: 24.130189895629883 test_loss:97.01604461669922\n",
            "1800/3000 train_loss: 24.550273895263672 test_loss:85.32498168945312\n",
            "1801/3000 train_loss: 23.110023498535156 test_loss:93.60234069824219\n",
            "1802/3000 train_loss: 27.60472297668457 test_loss:93.34724426269531\n",
            "1803/3000 train_loss: 24.294218063354492 test_loss:89.89418029785156\n",
            "1804/3000 train_loss: 32.654136657714844 test_loss:88.1260986328125\n",
            "1805/3000 train_loss: 23.35846710205078 test_loss:97.98390197753906\n",
            "1806/3000 train_loss: 22.330053329467773 test_loss:90.74064636230469\n",
            "1807/3000 train_loss: 23.399778366088867 test_loss:93.13892364501953\n",
            "1808/3000 train_loss: 30.14729118347168 test_loss:97.0436782836914\n",
            "1809/3000 train_loss: 23.93242645263672 test_loss:89.69381713867188\n",
            "1810/3000 train_loss: 21.57573890686035 test_loss:88.46893310546875\n",
            "1811/3000 train_loss: 24.00864028930664 test_loss:92.16287231445312\n",
            "1812/3000 train_loss: 23.85991668701172 test_loss:89.5942611694336\n",
            "1813/3000 train_loss: 20.842727661132812 test_loss:92.06092834472656\n",
            "1814/3000 train_loss: 27.728023529052734 test_loss:92.33140563964844\n",
            "1815/3000 train_loss: 26.266082763671875 test_loss:94.1295394897461\n",
            "1816/3000 train_loss: 23.67226791381836 test_loss:93.17523193359375\n",
            "1817/3000 train_loss: 24.908206939697266 test_loss:88.3197250366211\n",
            "1818/3000 train_loss: 23.632728576660156 test_loss:87.03750610351562\n",
            "1819/3000 train_loss: 24.270265579223633 test_loss:81.3996353149414\n",
            "1820/3000 train_loss: 26.70647430419922 test_loss:88.41665649414062\n",
            "1821/3000 train_loss: 26.01095962524414 test_loss:93.47586059570312\n",
            "1822/3000 train_loss: 26.589075088500977 test_loss:94.41580200195312\n",
            "1823/3000 train_loss: 26.660171508789062 test_loss:87.43891143798828\n",
            "1824/3000 train_loss: 30.00079345703125 test_loss:94.46934509277344\n",
            "1825/3000 train_loss: 24.365446090698242 test_loss:95.01799011230469\n",
            "1826/3000 train_loss: 23.18010711669922 test_loss:101.14555358886719\n",
            "1827/3000 train_loss: 24.34413719177246 test_loss:89.30558776855469\n",
            "1828/3000 train_loss: 26.096900939941406 test_loss:89.89866638183594\n",
            "1829/3000 train_loss: 23.250024795532227 test_loss:94.00608825683594\n",
            "1830/3000 train_loss: 30.26289939880371 test_loss:94.16525268554688\n",
            "1831/3000 train_loss: 28.329547882080078 test_loss:91.72177124023438\n",
            "1832/3000 train_loss: 24.752199172973633 test_loss:93.20157623291016\n",
            "1833/3000 train_loss: 26.529930114746094 test_loss:88.62702941894531\n",
            "1834/3000 train_loss: 22.88356590270996 test_loss:97.99757385253906\n",
            "1835/3000 train_loss: 24.647994995117188 test_loss:96.6133804321289\n",
            "1836/3000 train_loss: 23.06795310974121 test_loss:92.94761657714844\n",
            "1837/3000 train_loss: 25.625593185424805 test_loss:94.5509033203125\n",
            "1838/3000 train_loss: 26.595169067382812 test_loss:96.1268310546875\n",
            "1839/3000 train_loss: 20.666305541992188 test_loss:95.86530303955078\n",
            "1840/3000 train_loss: 21.690736770629883 test_loss:85.63382720947266\n",
            "1841/3000 train_loss: 21.557004928588867 test_loss:91.27140808105469\n",
            "1842/3000 train_loss: 26.71208953857422 test_loss:85.85848999023438\n",
            "1843/3000 train_loss: 21.163251876831055 test_loss:102.722900390625\n",
            "1844/3000 train_loss: 20.52561378479004 test_loss:89.01354217529297\n",
            "1845/3000 train_loss: 23.749237060546875 test_loss:93.4059829711914\n",
            "1846/3000 train_loss: 22.750459671020508 test_loss:87.59809875488281\n",
            "1847/3000 train_loss: 22.56113052368164 test_loss:100.12090301513672\n",
            "1848/3000 train_loss: 27.65376091003418 test_loss:99.99720001220703\n",
            "1849/3000 train_loss: 23.373327255249023 test_loss:87.50172424316406\n",
            "1850/3000 train_loss: 25.912744522094727 test_loss:92.32229614257812\n",
            "1851/3000 train_loss: 23.451820373535156 test_loss:90.51910400390625\n",
            "1852/3000 train_loss: 23.600635528564453 test_loss:88.32293701171875\n",
            "1853/3000 train_loss: 30.913537979125977 test_loss:92.4954605102539\n",
            "1854/3000 train_loss: 24.98638916015625 test_loss:90.27030944824219\n",
            "1855/3000 train_loss: 30.05196189880371 test_loss:106.72978973388672\n",
            "1856/3000 train_loss: 30.771678924560547 test_loss:93.71765899658203\n",
            "1857/3000 train_loss: 31.97006607055664 test_loss:96.339599609375\n",
            "1858/3000 train_loss: 26.147445678710938 test_loss:89.23270416259766\n",
            "1859/3000 train_loss: 28.747365951538086 test_loss:85.26219177246094\n",
            "1860/3000 train_loss: 22.467967987060547 test_loss:87.81840515136719\n",
            "1861/3000 train_loss: 24.969926834106445 test_loss:91.58538818359375\n",
            "1862/3000 train_loss: 21.766021728515625 test_loss:97.31098937988281\n",
            "1863/3000 train_loss: 24.77075958251953 test_loss:90.05159759521484\n",
            "1864/3000 train_loss: 25.674142837524414 test_loss:93.28389739990234\n",
            "1865/3000 train_loss: 27.338857650756836 test_loss:92.1649169921875\n",
            "1866/3000 train_loss: 23.035457611083984 test_loss:92.12454223632812\n",
            "1867/3000 train_loss: 28.15839385986328 test_loss:94.60282897949219\n",
            "1868/3000 train_loss: 26.045215606689453 test_loss:104.27462768554688\n",
            "1869/3000 train_loss: 24.224979400634766 test_loss:92.89749145507812\n",
            "1870/3000 train_loss: 21.867618560791016 test_loss:91.7598876953125\n",
            "1871/3000 train_loss: 22.61046600341797 test_loss:93.73959350585938\n",
            "1872/3000 train_loss: 23.285512924194336 test_loss:94.30895233154297\n",
            "1873/3000 train_loss: 33.10122299194336 test_loss:99.68311309814453\n",
            "1874/3000 train_loss: 26.059585571289062 test_loss:95.45751953125\n",
            "1875/3000 train_loss: 24.426891326904297 test_loss:93.91731262207031\n",
            "1876/3000 train_loss: 21.246986389160156 test_loss:108.20304870605469\n",
            "1877/3000 train_loss: 23.779296875 test_loss:98.8353500366211\n",
            "1878/3000 train_loss: 24.351802825927734 test_loss:96.58846282958984\n",
            "1879/3000 train_loss: 21.157684326171875 test_loss:101.8100814819336\n",
            "1880/3000 train_loss: 21.693885803222656 test_loss:101.45831298828125\n",
            "1881/3000 train_loss: 21.713123321533203 test_loss:86.42315673828125\n",
            "1882/3000 train_loss: 27.370309829711914 test_loss:95.22975158691406\n",
            "1883/3000 train_loss: 20.513656616210938 test_loss:92.97846984863281\n",
            "1884/3000 train_loss: 23.08145523071289 test_loss:91.44337463378906\n",
            "1885/3000 train_loss: 25.517318725585938 test_loss:93.59912872314453\n",
            "1886/3000 train_loss: 23.584819793701172 test_loss:82.98458099365234\n",
            "1887/3000 train_loss: 21.75402069091797 test_loss:92.80382537841797\n",
            "1888/3000 train_loss: 23.14259147644043 test_loss:83.86376190185547\n",
            "1889/3000 train_loss: 23.1533203125 test_loss:86.50175476074219\n",
            "1890/3000 train_loss: 24.46826171875 test_loss:92.58016967773438\n",
            "1891/3000 train_loss: 21.680908203125 test_loss:94.46409606933594\n",
            "1892/3000 train_loss: 39.4893798828125 test_loss:94.6138916015625\n",
            "1893/3000 train_loss: 25.670860290527344 test_loss:88.71533203125\n",
            "1894/3000 train_loss: 31.114152908325195 test_loss:83.5324478149414\n",
            "1895/3000 train_loss: 27.2789363861084 test_loss:101.20135498046875\n",
            "1896/3000 train_loss: 24.661157608032227 test_loss:88.73186492919922\n",
            "1897/3000 train_loss: 22.82673454284668 test_loss:87.58154296875\n",
            "1898/3000 train_loss: 24.407398223876953 test_loss:97.65380096435547\n",
            "1899/3000 train_loss: 23.37624740600586 test_loss:85.07765197753906\n",
            "1900/3000 train_loss: 23.208358764648438 test_loss:105.80134582519531\n",
            "1901/3000 train_loss: 24.85239601135254 test_loss:85.14910888671875\n",
            "1902/3000 train_loss: 23.6092472076416 test_loss:88.92889404296875\n",
            "1903/3000 train_loss: 21.243896484375 test_loss:100.90946960449219\n",
            "1904/3000 train_loss: 29.515108108520508 test_loss:92.67608642578125\n",
            "1905/3000 train_loss: 24.844341278076172 test_loss:92.65216827392578\n",
            "1906/3000 train_loss: 21.760129928588867 test_loss:85.12166595458984\n",
            "1907/3000 train_loss: 24.495450973510742 test_loss:102.97819519042969\n",
            "1908/3000 train_loss: 27.52858543395996 test_loss:88.0701904296875\n",
            "1909/3000 train_loss: 26.391603469848633 test_loss:85.0643310546875\n",
            "1910/3000 train_loss: 22.34678840637207 test_loss:102.26284790039062\n",
            "1911/3000 train_loss: 23.989938735961914 test_loss:86.36734771728516\n",
            "1912/3000 train_loss: 22.522003173828125 test_loss:95.53036499023438\n",
            "1913/3000 train_loss: 23.016082763671875 test_loss:83.51112365722656\n",
            "1914/3000 train_loss: 24.04741096496582 test_loss:93.09132385253906\n",
            "1915/3000 train_loss: 23.6613712310791 test_loss:99.32294464111328\n",
            "1916/3000 train_loss: 25.523759841918945 test_loss:99.20477294921875\n",
            "1917/3000 train_loss: 29.666187286376953 test_loss:106.78397369384766\n",
            "1918/3000 train_loss: 30.084217071533203 test_loss:94.8070068359375\n",
            "1919/3000 train_loss: 26.315719604492188 test_loss:109.25595092773438\n",
            "1920/3000 train_loss: 25.841312408447266 test_loss:87.03337097167969\n",
            "1921/3000 train_loss: 26.44231414794922 test_loss:85.61834716796875\n",
            "1922/3000 train_loss: 22.04169273376465 test_loss:95.99803161621094\n",
            "1923/3000 train_loss: 24.932085037231445 test_loss:94.47553253173828\n",
            "1924/3000 train_loss: 25.20696449279785 test_loss:92.15000915527344\n",
            "1925/3000 train_loss: 24.570091247558594 test_loss:88.93415069580078\n",
            "1926/3000 train_loss: 21.030187606811523 test_loss:95.90966033935547\n",
            "1927/3000 train_loss: 21.11522674560547 test_loss:89.41182708740234\n",
            "1928/3000 train_loss: 23.47410011291504 test_loss:99.7159423828125\n",
            "1929/3000 train_loss: 22.99245834350586 test_loss:92.93850708007812\n",
            "1930/3000 train_loss: 24.442251205444336 test_loss:94.66899108886719\n",
            "1931/3000 train_loss: 24.846858978271484 test_loss:95.30181121826172\n",
            "1932/3000 train_loss: 29.15682601928711 test_loss:91.30249786376953\n",
            "1933/3000 train_loss: 25.477802276611328 test_loss:85.91477966308594\n",
            "1934/3000 train_loss: 27.53040885925293 test_loss:113.04410552978516\n",
            "1935/3000 train_loss: 29.796926498413086 test_loss:83.99113464355469\n",
            "1936/3000 train_loss: 26.73890495300293 test_loss:94.06597137451172\n",
            "1937/3000 train_loss: 22.28608512878418 test_loss:90.17750549316406\n",
            "1938/3000 train_loss: 23.779569625854492 test_loss:96.36915588378906\n",
            "1939/3000 train_loss: 21.79817008972168 test_loss:84.10647583007812\n",
            "1940/3000 train_loss: 21.84130859375 test_loss:92.41973876953125\n",
            "1941/3000 train_loss: 23.576749801635742 test_loss:95.3740234375\n",
            "1942/3000 train_loss: 20.995641708374023 test_loss:84.39883422851562\n",
            "1943/3000 train_loss: 19.660341262817383 test_loss:85.90972900390625\n",
            "1944/3000 train_loss: 20.030349731445312 test_loss:89.5857925415039\n",
            "1945/3000 train_loss: 23.07230567932129 test_loss:83.15145111083984\n",
            "1946/3000 train_loss: 21.997867584228516 test_loss:93.20957946777344\n",
            "1947/3000 train_loss: 26.39655113220215 test_loss:90.89204406738281\n",
            "1948/3000 train_loss: 21.974533081054688 test_loss:101.9038314819336\n",
            "1949/3000 train_loss: 24.573347091674805 test_loss:88.26057434082031\n",
            "1950/3000 train_loss: 26.41949462890625 test_loss:101.90489196777344\n",
            "1951/3000 train_loss: 24.754812240600586 test_loss:86.0653076171875\n",
            "1952/3000 train_loss: 26.735002517700195 test_loss:85.42029571533203\n",
            "1953/3000 train_loss: 32.24974822998047 test_loss:115.73844909667969\n",
            "1954/3000 train_loss: 25.8060302734375 test_loss:94.10888671875\n",
            "1955/3000 train_loss: 19.69873809814453 test_loss:90.6077651977539\n",
            "1956/3000 train_loss: 26.221826553344727 test_loss:88.96501159667969\n",
            "1957/3000 train_loss: 24.90966033935547 test_loss:89.42601776123047\n",
            "1958/3000 train_loss: 23.24567985534668 test_loss:88.398681640625\n",
            "1959/3000 train_loss: 20.44590950012207 test_loss:93.5884017944336\n",
            "1960/3000 train_loss: 25.9755916595459 test_loss:95.37313842773438\n",
            "1961/3000 train_loss: 22.028053283691406 test_loss:90.67866516113281\n",
            "1962/3000 train_loss: 24.158178329467773 test_loss:105.66511535644531\n",
            "1963/3000 train_loss: 25.45553970336914 test_loss:89.47357177734375\n",
            "1964/3000 train_loss: 20.863046646118164 test_loss:87.03130340576172\n",
            "1965/3000 train_loss: 21.656553268432617 test_loss:94.897216796875\n",
            "1966/3000 train_loss: 23.832931518554688 test_loss:91.96380615234375\n",
            "1967/3000 train_loss: 22.274093627929688 test_loss:100.8365707397461\n",
            "1968/3000 train_loss: 28.601240158081055 test_loss:92.61997985839844\n",
            "1969/3000 train_loss: 24.03171157836914 test_loss:85.34291076660156\n",
            "1970/3000 train_loss: 22.559789657592773 test_loss:90.10482788085938\n",
            "1971/3000 train_loss: 22.183420181274414 test_loss:100.1314468383789\n",
            "1972/3000 train_loss: 20.74801254272461 test_loss:94.1046142578125\n",
            "1973/3000 train_loss: 19.257909774780273 test_loss:90.53144073486328\n",
            "1974/3000 train_loss: 21.075496673583984 test_loss:85.4224853515625\n",
            "1975/3000 train_loss: 23.005908966064453 test_loss:90.21472930908203\n",
            "1976/3000 train_loss: 21.35931968688965 test_loss:89.94784545898438\n",
            "1977/3000 train_loss: 25.1928768157959 test_loss:84.08061218261719\n",
            "1978/3000 train_loss: 20.499732971191406 test_loss:86.27534484863281\n",
            "1979/3000 train_loss: 22.116914749145508 test_loss:79.97526550292969\n",
            "1980/3000 train_loss: 23.426025390625 test_loss:82.77871704101562\n",
            "1981/3000 train_loss: 22.845746994018555 test_loss:92.80265808105469\n",
            "1982/3000 train_loss: 23.741247177124023 test_loss:90.51844024658203\n",
            "1983/3000 train_loss: 22.901588439941406 test_loss:88.03113555908203\n",
            "1984/3000 train_loss: 22.54452896118164 test_loss:88.14823913574219\n",
            "1985/3000 train_loss: 19.739439010620117 test_loss:83.60110473632812\n",
            "1986/3000 train_loss: 21.283411026000977 test_loss:98.85135650634766\n",
            "1987/3000 train_loss: 28.086833953857422 test_loss:88.00928497314453\n",
            "1988/3000 train_loss: 33.45040512084961 test_loss:90.39643859863281\n",
            "1989/3000 train_loss: 25.780664443969727 test_loss:89.34534454345703\n",
            "1990/3000 train_loss: 23.64680290222168 test_loss:87.26834106445312\n",
            "1991/3000 train_loss: 23.632774353027344 test_loss:89.8343505859375\n",
            "1992/3000 train_loss: 21.51157569885254 test_loss:87.50980377197266\n",
            "1993/3000 train_loss: 20.634803771972656 test_loss:86.48506164550781\n",
            "1994/3000 train_loss: 22.63981819152832 test_loss:86.66291809082031\n",
            "1995/3000 train_loss: 21.779150009155273 test_loss:92.93965148925781\n",
            "1996/3000 train_loss: 20.156997680664062 test_loss:87.12901306152344\n",
            "1997/3000 train_loss: 18.647624969482422 test_loss:88.62080383300781\n",
            "1998/3000 train_loss: 23.10021209716797 test_loss:88.18038940429688\n",
            "1999/3000 train_loss: 19.58672523498535 test_loss:84.53496551513672\n",
            "2000/3000 train_loss: 19.064903259277344 test_loss:90.16648864746094\n",
            "2001/3000 train_loss: 19.668973922729492 test_loss:85.67633056640625\n",
            "2002/3000 train_loss: 25.52683448791504 test_loss:85.89373016357422\n",
            "2003/3000 train_loss: 20.052278518676758 test_loss:94.08206176757812\n",
            "2004/3000 train_loss: 19.757497787475586 test_loss:88.58784484863281\n",
            "2005/3000 train_loss: 20.395240783691406 test_loss:85.92079162597656\n",
            "2006/3000 train_loss: 19.139324188232422 test_loss:86.18658447265625\n",
            "2007/3000 train_loss: 20.590978622436523 test_loss:86.98814392089844\n",
            "2008/3000 train_loss: 25.174997329711914 test_loss:88.10771179199219\n",
            "2009/3000 train_loss: 20.61050796508789 test_loss:90.91000366210938\n",
            "2010/3000 train_loss: 27.506383895874023 test_loss:88.67091369628906\n",
            "2011/3000 train_loss: 21.595006942749023 test_loss:91.70637512207031\n",
            "2012/3000 train_loss: 21.816747665405273 test_loss:85.71122741699219\n",
            "2013/3000 train_loss: 21.469547271728516 test_loss:89.29647064208984\n",
            "2014/3000 train_loss: 19.644201278686523 test_loss:91.54226684570312\n",
            "2015/3000 train_loss: 23.596200942993164 test_loss:98.31692504882812\n",
            "2016/3000 train_loss: 23.9777774810791 test_loss:94.1394271850586\n",
            "2017/3000 train_loss: 21.83780860900879 test_loss:82.33111572265625\n",
            "2018/3000 train_loss: 22.628450393676758 test_loss:86.73686218261719\n",
            "2019/3000 train_loss: 27.604740142822266 test_loss:87.99356079101562\n",
            "2020/3000 train_loss: 20.04593276977539 test_loss:89.81983184814453\n",
            "2021/3000 train_loss: 22.951465606689453 test_loss:85.09092712402344\n",
            "2022/3000 train_loss: 23.802717208862305 test_loss:86.99177551269531\n",
            "2023/3000 train_loss: 22.23358726501465 test_loss:88.82405853271484\n",
            "2024/3000 train_loss: 26.75307273864746 test_loss:84.84626007080078\n",
            "2025/3000 train_loss: 24.008451461791992 test_loss:96.09603881835938\n",
            "2026/3000 train_loss: 23.56854248046875 test_loss:90.16714477539062\n",
            "2027/3000 train_loss: 23.26367950439453 test_loss:85.71357727050781\n",
            "2028/3000 train_loss: 20.637405395507812 test_loss:89.9864273071289\n",
            "2029/3000 train_loss: 19.016998291015625 test_loss:86.4171142578125\n",
            "2030/3000 train_loss: 17.929550170898438 test_loss:89.59917449951172\n",
            "2031/3000 train_loss: 20.31566047668457 test_loss:86.86493682861328\n",
            "2032/3000 train_loss: 22.10853385925293 test_loss:84.25946044921875\n",
            "2033/3000 train_loss: 20.52528953552246 test_loss:85.5124282836914\n",
            "2034/3000 train_loss: 20.837461471557617 test_loss:91.562255859375\n",
            "2035/3000 train_loss: 21.221960067749023 test_loss:84.13131713867188\n",
            "2036/3000 train_loss: 21.543407440185547 test_loss:87.22825622558594\n",
            "2037/3000 train_loss: 23.293121337890625 test_loss:86.37212371826172\n",
            "2038/3000 train_loss: 19.458507537841797 test_loss:96.53599548339844\n",
            "2039/3000 train_loss: 20.791881561279297 test_loss:80.68494415283203\n",
            "2040/3000 train_loss: 22.548662185668945 test_loss:88.7483139038086\n",
            "2041/3000 train_loss: 21.832468032836914 test_loss:92.58938598632812\n",
            "2042/3000 train_loss: 25.95917320251465 test_loss:93.2066421508789\n",
            "2043/3000 train_loss: 20.072711944580078 test_loss:87.51868438720703\n",
            "2044/3000 train_loss: 17.670440673828125 test_loss:87.00375366210938\n",
            "2045/3000 train_loss: 20.098730087280273 test_loss:89.95912170410156\n",
            "2046/3000 train_loss: 20.254573822021484 test_loss:86.02886962890625\n",
            "2047/3000 train_loss: 17.709041595458984 test_loss:85.61671447753906\n",
            "2048/3000 train_loss: 21.440776824951172 test_loss:87.58008575439453\n",
            "2049/3000 train_loss: 18.18907928466797 test_loss:90.30562591552734\n",
            "2050/3000 train_loss: 19.130918502807617 test_loss:85.98385620117188\n",
            "2051/3000 train_loss: 21.168806076049805 test_loss:83.24415588378906\n",
            "2052/3000 train_loss: 21.050621032714844 test_loss:88.44998931884766\n",
            "2053/3000 train_loss: 22.9328670501709 test_loss:76.79998779296875\n",
            "2054/3000 train_loss: 24.63282585144043 test_loss:80.17774963378906\n",
            "2055/3000 train_loss: 24.007122039794922 test_loss:95.90704345703125\n",
            "2056/3000 train_loss: 19.760251998901367 test_loss:89.00967407226562\n",
            "2057/3000 train_loss: 17.517852783203125 test_loss:87.44617462158203\n",
            "2058/3000 train_loss: 25.320858001708984 test_loss:87.23750305175781\n",
            "2059/3000 train_loss: 24.50186538696289 test_loss:85.36080932617188\n",
            "2060/3000 train_loss: 23.177658081054688 test_loss:90.99296569824219\n",
            "2061/3000 train_loss: 24.8896484375 test_loss:84.60045623779297\n",
            "2062/3000 train_loss: 22.28801727294922 test_loss:96.69987487792969\n",
            "2063/3000 train_loss: 21.075746536254883 test_loss:92.065673828125\n",
            "2064/3000 train_loss: 21.888011932373047 test_loss:86.79722595214844\n",
            "2065/3000 train_loss: 26.319442749023438 test_loss:88.52969360351562\n",
            "2066/3000 train_loss: 21.329408645629883 test_loss:91.21375274658203\n",
            "2067/3000 train_loss: 24.070175170898438 test_loss:92.71625518798828\n",
            "2068/3000 train_loss: 28.611019134521484 test_loss:82.12420654296875\n",
            "2069/3000 train_loss: 19.660118103027344 test_loss:93.44097900390625\n",
            "2070/3000 train_loss: 24.548294067382812 test_loss:95.59781646728516\n",
            "2071/3000 train_loss: 23.955434799194336 test_loss:93.66453552246094\n",
            "2072/3000 train_loss: 20.242712020874023 test_loss:87.23074340820312\n",
            "2073/3000 train_loss: 21.23821258544922 test_loss:104.28675842285156\n",
            "2074/3000 train_loss: 20.768953323364258 test_loss:99.19084930419922\n",
            "2075/3000 train_loss: 20.802051544189453 test_loss:85.13037109375\n",
            "2076/3000 train_loss: 23.694400787353516 test_loss:87.61515045166016\n",
            "2077/3000 train_loss: 17.74424171447754 test_loss:97.58059692382812\n",
            "2078/3000 train_loss: 21.92236328125 test_loss:92.66160583496094\n",
            "2079/3000 train_loss: 21.75115203857422 test_loss:92.21739196777344\n",
            "2080/3000 train_loss: 18.541717529296875 test_loss:86.1937026977539\n",
            "2081/3000 train_loss: 22.370737075805664 test_loss:91.40412902832031\n",
            "2082/3000 train_loss: 16.293392181396484 test_loss:95.45030212402344\n",
            "2083/3000 train_loss: 20.645334243774414 test_loss:87.6288070678711\n",
            "2084/3000 train_loss: 24.058544158935547 test_loss:91.2730941772461\n",
            "2085/3000 train_loss: 19.70260238647461 test_loss:89.2740478515625\n",
            "2086/3000 train_loss: 22.899682998657227 test_loss:95.6578140258789\n",
            "2087/3000 train_loss: 20.598491668701172 test_loss:103.91314697265625\n",
            "2088/3000 train_loss: 24.73952865600586 test_loss:94.00927734375\n",
            "2089/3000 train_loss: 20.39154815673828 test_loss:98.00529479980469\n",
            "2090/3000 train_loss: 23.135351181030273 test_loss:91.0862045288086\n",
            "2091/3000 train_loss: 20.43840217590332 test_loss:84.65312194824219\n",
            "2092/3000 train_loss: 22.960275650024414 test_loss:95.3294677734375\n",
            "2093/3000 train_loss: 19.185768127441406 test_loss:90.74127960205078\n",
            "2094/3000 train_loss: 18.8420467376709 test_loss:93.39945983886719\n",
            "2095/3000 train_loss: 20.065828323364258 test_loss:88.2860107421875\n",
            "2096/3000 train_loss: 22.75577735900879 test_loss:93.35678100585938\n",
            "2097/3000 train_loss: 18.11488914489746 test_loss:85.79373168945312\n",
            "2098/3000 train_loss: 20.49764633178711 test_loss:88.85678100585938\n",
            "2099/3000 train_loss: 21.411746978759766 test_loss:86.02885437011719\n",
            "2100/3000 train_loss: 24.399045944213867 test_loss:102.96796417236328\n",
            "2101/3000 train_loss: 23.931135177612305 test_loss:88.56178283691406\n",
            "2102/3000 train_loss: 19.36273193359375 test_loss:93.04864501953125\n",
            "2103/3000 train_loss: 17.255977630615234 test_loss:86.35392761230469\n",
            "2104/3000 train_loss: 27.47621726989746 test_loss:92.00485229492188\n",
            "2105/3000 train_loss: 25.391464233398438 test_loss:100.96831512451172\n",
            "2106/3000 train_loss: 20.254823684692383 test_loss:94.7183837890625\n",
            "2107/3000 train_loss: 21.293546676635742 test_loss:96.5168228149414\n",
            "2108/3000 train_loss: 20.699874877929688 test_loss:95.36042022705078\n",
            "2109/3000 train_loss: 21.760114669799805 test_loss:108.11332702636719\n",
            "2110/3000 train_loss: 21.212711334228516 test_loss:90.21929931640625\n",
            "2111/3000 train_loss: 19.7078914642334 test_loss:89.36387634277344\n",
            "2112/3000 train_loss: 20.151884078979492 test_loss:99.59300231933594\n",
            "2113/3000 train_loss: 21.695722579956055 test_loss:86.88511657714844\n",
            "2114/3000 train_loss: 19.92142677307129 test_loss:93.7601089477539\n",
            "2115/3000 train_loss: 19.06611442565918 test_loss:87.66232299804688\n",
            "2116/3000 train_loss: 18.69209861755371 test_loss:90.63395690917969\n",
            "2117/3000 train_loss: 20.929372787475586 test_loss:89.37944793701172\n",
            "2118/3000 train_loss: 36.01997375488281 test_loss:87.98017120361328\n",
            "2119/3000 train_loss: 28.190906524658203 test_loss:98.62660217285156\n",
            "2120/3000 train_loss: 30.147001266479492 test_loss:93.36810302734375\n",
            "2121/3000 train_loss: 21.7775821685791 test_loss:85.1761703491211\n",
            "2122/3000 train_loss: 20.865406036376953 test_loss:90.79944610595703\n",
            "2123/3000 train_loss: 19.508644104003906 test_loss:94.15560150146484\n",
            "2124/3000 train_loss: 20.888208389282227 test_loss:86.29153442382812\n",
            "2125/3000 train_loss: 17.093299865722656 test_loss:92.77127838134766\n",
            "2126/3000 train_loss: 19.165922164916992 test_loss:88.66181945800781\n",
            "2127/3000 train_loss: 21.709674835205078 test_loss:86.11833190917969\n",
            "2128/3000 train_loss: 19.327251434326172 test_loss:91.32652282714844\n",
            "2129/3000 train_loss: 18.367521286010742 test_loss:86.8135757446289\n",
            "2130/3000 train_loss: 20.457170486450195 test_loss:92.83628845214844\n",
            "2131/3000 train_loss: 24.593490600585938 test_loss:92.47900390625\n",
            "2132/3000 train_loss: 17.31060028076172 test_loss:84.04244232177734\n",
            "2133/3000 train_loss: 19.990215301513672 test_loss:91.1331787109375\n",
            "2134/3000 train_loss: 19.353919982910156 test_loss:88.37776947021484\n",
            "2135/3000 train_loss: 17.72027587890625 test_loss:99.29303741455078\n",
            "2136/3000 train_loss: 17.40677261352539 test_loss:89.42207336425781\n",
            "2137/3000 train_loss: 19.298471450805664 test_loss:88.2523193359375\n",
            "2138/3000 train_loss: 18.648448944091797 test_loss:91.60940551757812\n",
            "2139/3000 train_loss: 19.70856285095215 test_loss:92.06188201904297\n",
            "2140/3000 train_loss: 23.608537673950195 test_loss:85.79081726074219\n",
            "2141/3000 train_loss: 19.819721221923828 test_loss:90.94890594482422\n",
            "2142/3000 train_loss: 21.499034881591797 test_loss:83.99564361572266\n",
            "2143/3000 train_loss: 19.700693130493164 test_loss:94.70103454589844\n",
            "2144/3000 train_loss: 22.77652931213379 test_loss:93.55714416503906\n",
            "2145/3000 train_loss: 22.485280990600586 test_loss:89.04278564453125\n",
            "2146/3000 train_loss: 19.541051864624023 test_loss:84.34164428710938\n",
            "2147/3000 train_loss: 18.523666381835938 test_loss:95.45059204101562\n",
            "2148/3000 train_loss: 21.863239288330078 test_loss:86.6670150756836\n",
            "2149/3000 train_loss: 21.957809448242188 test_loss:82.23268127441406\n",
            "2150/3000 train_loss: 20.124454498291016 test_loss:95.1539306640625\n",
            "2151/3000 train_loss: 17.970266342163086 test_loss:85.91961669921875\n",
            "2152/3000 train_loss: 19.864225387573242 test_loss:85.73701477050781\n",
            "2153/3000 train_loss: 19.99196434020996 test_loss:85.6110610961914\n",
            "2154/3000 train_loss: 18.535921096801758 test_loss:83.01190185546875\n",
            "2155/3000 train_loss: 20.141828536987305 test_loss:82.63204956054688\n",
            "2156/3000 train_loss: 17.6068115234375 test_loss:86.10101318359375\n",
            "2157/3000 train_loss: 26.743221282958984 test_loss:93.33148956298828\n",
            "2158/3000 train_loss: 22.370325088500977 test_loss:84.98594665527344\n",
            "2159/3000 train_loss: 19.534488677978516 test_loss:88.11019897460938\n",
            "2160/3000 train_loss: 17.627920150756836 test_loss:89.11680603027344\n",
            "2161/3000 train_loss: 19.351776123046875 test_loss:88.19420623779297\n",
            "2162/3000 train_loss: 18.125625610351562 test_loss:90.55448913574219\n",
            "2163/3000 train_loss: 20.58397102355957 test_loss:88.91691589355469\n",
            "2164/3000 train_loss: 21.787792205810547 test_loss:80.1808853149414\n",
            "2165/3000 train_loss: 20.382556915283203 test_loss:85.10584259033203\n",
            "2166/3000 train_loss: 18.540348052978516 test_loss:89.42623901367188\n",
            "2167/3000 train_loss: 17.719865798950195 test_loss:89.48269653320312\n",
            "2168/3000 train_loss: 18.114416122436523 test_loss:81.480712890625\n",
            "2169/3000 train_loss: 23.006919860839844 test_loss:87.01602935791016\n",
            "2170/3000 train_loss: 20.408756256103516 test_loss:91.45579528808594\n",
            "2171/3000 train_loss: 17.564409255981445 test_loss:84.60057067871094\n",
            "2172/3000 train_loss: 20.500322341918945 test_loss:87.01416015625\n",
            "2173/3000 train_loss: 21.05384063720703 test_loss:95.91574096679688\n",
            "2174/3000 train_loss: 26.735868453979492 test_loss:84.03510284423828\n",
            "2175/3000 train_loss: 21.52926254272461 test_loss:95.42961120605469\n",
            "2176/3000 train_loss: 19.39764404296875 test_loss:89.70025634765625\n",
            "2177/3000 train_loss: 19.367061614990234 test_loss:94.91976928710938\n",
            "2178/3000 train_loss: 17.795318603515625 test_loss:101.46759033203125\n",
            "2179/3000 train_loss: 17.514820098876953 test_loss:102.36518096923828\n",
            "2180/3000 train_loss: 18.925037384033203 test_loss:86.2311019897461\n",
            "2181/3000 train_loss: 20.738574981689453 test_loss:93.56175231933594\n",
            "2182/3000 train_loss: 17.577533721923828 test_loss:93.87667846679688\n",
            "2183/3000 train_loss: 15.73324966430664 test_loss:99.01387023925781\n",
            "2184/3000 train_loss: 20.536422729492188 test_loss:92.29443359375\n",
            "2185/3000 train_loss: 22.37827491760254 test_loss:83.41548919677734\n",
            "2186/3000 train_loss: 21.81142807006836 test_loss:104.07974243164062\n",
            "2187/3000 train_loss: 20.57575225830078 test_loss:95.18270874023438\n",
            "2188/3000 train_loss: 19.48631477355957 test_loss:85.45264434814453\n",
            "2189/3000 train_loss: 18.96346092224121 test_loss:99.07012939453125\n",
            "2190/3000 train_loss: 18.462350845336914 test_loss:83.46604919433594\n",
            "2191/3000 train_loss: 17.516008377075195 test_loss:83.37178039550781\n",
            "2192/3000 train_loss: 19.313217163085938 test_loss:89.90696716308594\n",
            "2193/3000 train_loss: 20.625473022460938 test_loss:92.34053039550781\n",
            "2194/3000 train_loss: 22.875675201416016 test_loss:88.07093048095703\n",
            "2195/3000 train_loss: 23.934085845947266 test_loss:96.36724853515625\n",
            "2196/3000 train_loss: 18.432430267333984 test_loss:85.04743957519531\n",
            "2197/3000 train_loss: 14.799083709716797 test_loss:89.2713623046875\n",
            "2198/3000 train_loss: 25.219697952270508 test_loss:95.99427795410156\n",
            "2199/3000 train_loss: 20.05644416809082 test_loss:83.37229919433594\n",
            "2200/3000 train_loss: 23.9471435546875 test_loss:99.01380920410156\n",
            "2201/3000 train_loss: 21.237884521484375 test_loss:86.61234283447266\n",
            "2202/3000 train_loss: 19.899112701416016 test_loss:93.14097595214844\n",
            "2203/3000 train_loss: 17.037334442138672 test_loss:93.59181213378906\n",
            "2204/3000 train_loss: 23.041166305541992 test_loss:94.6673583984375\n",
            "2205/3000 train_loss: 18.374561309814453 test_loss:85.244873046875\n",
            "2206/3000 train_loss: 17.465688705444336 test_loss:92.90177917480469\n",
            "2207/3000 train_loss: 23.506847381591797 test_loss:84.47462463378906\n",
            "2208/3000 train_loss: 20.008052825927734 test_loss:91.57492065429688\n",
            "2209/3000 train_loss: 23.433149337768555 test_loss:86.0765380859375\n",
            "2210/3000 train_loss: 20.458364486694336 test_loss:85.04492950439453\n",
            "2211/3000 train_loss: 21.62651252746582 test_loss:80.4988021850586\n",
            "2212/3000 train_loss: 25.267532348632812 test_loss:87.50914001464844\n",
            "2213/3000 train_loss: 20.961278915405273 test_loss:95.26885986328125\n",
            "2214/3000 train_loss: 16.595373153686523 test_loss:85.25616455078125\n",
            "2215/3000 train_loss: 18.027219772338867 test_loss:86.718505859375\n",
            "2216/3000 train_loss: 15.826898574829102 test_loss:87.6546630859375\n",
            "2217/3000 train_loss: 16.99662208557129 test_loss:77.84122467041016\n",
            "2218/3000 train_loss: 21.562255859375 test_loss:87.08094787597656\n",
            "2219/3000 train_loss: 20.1796875 test_loss:98.56902313232422\n",
            "2220/3000 train_loss: 18.648061752319336 test_loss:94.16565704345703\n",
            "2221/3000 train_loss: 18.007225036621094 test_loss:84.78194427490234\n",
            "2222/3000 train_loss: 17.577926635742188 test_loss:83.40794372558594\n",
            "2223/3000 train_loss: 19.576202392578125 test_loss:86.03364562988281\n",
            "2224/3000 train_loss: 17.535404205322266 test_loss:83.39701843261719\n",
            "2225/3000 train_loss: 20.228368759155273 test_loss:84.86028289794922\n",
            "2226/3000 train_loss: 21.865015029907227 test_loss:96.51863098144531\n",
            "2227/3000 train_loss: 23.28196907043457 test_loss:97.11642456054688\n",
            "2228/3000 train_loss: 19.00029182434082 test_loss:91.8643798828125\n",
            "2229/3000 train_loss: 18.213781356811523 test_loss:82.56277465820312\n",
            "2230/3000 train_loss: 16.545421600341797 test_loss:85.66819763183594\n",
            "2231/3000 train_loss: 19.40777587890625 test_loss:87.70252990722656\n",
            "2232/3000 train_loss: 18.544824600219727 test_loss:84.47425079345703\n",
            "2233/3000 train_loss: 17.769611358642578 test_loss:88.90946960449219\n",
            "2234/3000 train_loss: 18.491559982299805 test_loss:97.1259994506836\n",
            "2235/3000 train_loss: 16.93556022644043 test_loss:89.91175079345703\n",
            "2236/3000 train_loss: 21.158832550048828 test_loss:85.60234069824219\n",
            "2237/3000 train_loss: 20.043590545654297 test_loss:91.78488159179688\n",
            "2238/3000 train_loss: 20.48230743408203 test_loss:83.94207763671875\n",
            "2239/3000 train_loss: 23.435710906982422 test_loss:89.887451171875\n",
            "2240/3000 train_loss: 22.313884735107422 test_loss:83.85246276855469\n",
            "2241/3000 train_loss: 21.012271881103516 test_loss:84.97771453857422\n",
            "2242/3000 train_loss: 20.86766242980957 test_loss:89.35531616210938\n",
            "2243/3000 train_loss: 19.662357330322266 test_loss:86.22367858886719\n",
            "2244/3000 train_loss: 20.256303787231445 test_loss:84.82150268554688\n",
            "2245/3000 train_loss: 18.628433227539062 test_loss:92.45648193359375\n",
            "2246/3000 train_loss: 17.086240768432617 test_loss:82.89839172363281\n",
            "2247/3000 train_loss: 23.587047576904297 test_loss:88.0050048828125\n",
            "2248/3000 train_loss: 18.053009033203125 test_loss:84.82298278808594\n",
            "2249/3000 train_loss: 19.88241958618164 test_loss:92.0155029296875\n",
            "2250/3000 train_loss: 19.87806510925293 test_loss:89.87705993652344\n",
            "2251/3000 train_loss: 21.335641860961914 test_loss:88.24784851074219\n",
            "2252/3000 train_loss: 16.206539154052734 test_loss:84.47311401367188\n",
            "2253/3000 train_loss: 20.28319549560547 test_loss:87.47586059570312\n",
            "2254/3000 train_loss: 17.887035369873047 test_loss:87.78158569335938\n",
            "2255/3000 train_loss: 16.32373809814453 test_loss:85.4004898071289\n",
            "2256/3000 train_loss: 21.22464370727539 test_loss:93.72549438476562\n",
            "2257/3000 train_loss: 19.424970626831055 test_loss:85.29600524902344\n",
            "2258/3000 train_loss: 18.19325828552246 test_loss:80.08454895019531\n",
            "2259/3000 train_loss: 17.252098083496094 test_loss:89.29367065429688\n",
            "2260/3000 train_loss: 17.24056625366211 test_loss:83.20436096191406\n",
            "2261/3000 train_loss: 21.137826919555664 test_loss:79.82364654541016\n",
            "2262/3000 train_loss: 19.778240203857422 test_loss:82.13816833496094\n",
            "2263/3000 train_loss: 18.200761795043945 test_loss:77.0008773803711\n",
            "2264/3000 train_loss: 16.505395889282227 test_loss:86.3638916015625\n",
            "2265/3000 train_loss: 19.278667449951172 test_loss:77.52947998046875\n",
            "2266/3000 train_loss: 21.031312942504883 test_loss:84.33169555664062\n",
            "2267/3000 train_loss: 18.139646530151367 test_loss:84.1659927368164\n",
            "2268/3000 train_loss: 21.175016403198242 test_loss:83.74136352539062\n",
            "2269/3000 train_loss: 19.383073806762695 test_loss:77.47977447509766\n",
            "2270/3000 train_loss: 22.40690040588379 test_loss:84.18817901611328\n",
            "2271/3000 train_loss: 16.984554290771484 test_loss:81.65938568115234\n",
            "2272/3000 train_loss: 16.417158126831055 test_loss:84.24252319335938\n",
            "2273/3000 train_loss: 17.480064392089844 test_loss:87.09708404541016\n",
            "2274/3000 train_loss: 15.19322681427002 test_loss:83.59944152832031\n",
            "2275/3000 train_loss: 18.578767776489258 test_loss:85.29759216308594\n",
            "2276/3000 train_loss: 22.859514236450195 test_loss:84.48426818847656\n",
            "2277/3000 train_loss: 19.928546905517578 test_loss:86.00056457519531\n",
            "2278/3000 train_loss: 17.977983474731445 test_loss:88.95530700683594\n",
            "2279/3000 train_loss: 20.652421951293945 test_loss:87.78330993652344\n",
            "2280/3000 train_loss: 17.552465438842773 test_loss:88.60580444335938\n",
            "2281/3000 train_loss: 19.651714324951172 test_loss:83.60557556152344\n",
            "2282/3000 train_loss: 19.351144790649414 test_loss:87.56875610351562\n",
            "2283/3000 train_loss: 69.09907531738281 test_loss:91.08599090576172\n",
            "2284/3000 train_loss: 47.31819534301758 test_loss:130.0429229736328\n",
            "2285/3000 train_loss: 34.007320404052734 test_loss:109.04673767089844\n",
            "2286/3000 train_loss: 26.89788246154785 test_loss:102.58068084716797\n",
            "2287/3000 train_loss: 28.569255828857422 test_loss:97.16454315185547\n",
            "2288/3000 train_loss: 25.565166473388672 test_loss:93.55038452148438\n",
            "2289/3000 train_loss: 21.161453247070312 test_loss:89.3155746459961\n",
            "2290/3000 train_loss: 18.443660736083984 test_loss:86.32118225097656\n",
            "2291/3000 train_loss: 24.60663604736328 test_loss:82.54783630371094\n",
            "2292/3000 train_loss: 23.52685546875 test_loss:84.00114440917969\n",
            "2293/3000 train_loss: 22.757583618164062 test_loss:81.2734146118164\n",
            "2294/3000 train_loss: 20.973379135131836 test_loss:80.2703857421875\n",
            "2295/3000 train_loss: 20.232810974121094 test_loss:87.94743347167969\n",
            "2296/3000 train_loss: 16.884796142578125 test_loss:85.82513427734375\n",
            "2297/3000 train_loss: 19.513185501098633 test_loss:85.78547668457031\n",
            "2298/3000 train_loss: 22.456438064575195 test_loss:95.83447265625\n",
            "2299/3000 train_loss: 22.061288833618164 test_loss:83.97128295898438\n",
            "2300/3000 train_loss: 19.371601104736328 test_loss:92.28150939941406\n",
            "2301/3000 train_loss: 16.961383819580078 test_loss:84.04297637939453\n",
            "2302/3000 train_loss: 16.577980041503906 test_loss:77.96764373779297\n",
            "2303/3000 train_loss: 17.91768455505371 test_loss:89.25396728515625\n",
            "2304/3000 train_loss: 17.83866310119629 test_loss:78.85086059570312\n",
            "2305/3000 train_loss: 19.592803955078125 test_loss:81.4721908569336\n",
            "2306/3000 train_loss: 20.212331771850586 test_loss:84.09732818603516\n",
            "2307/3000 train_loss: 18.77452850341797 test_loss:83.56292724609375\n",
            "2308/3000 train_loss: 21.61136245727539 test_loss:81.4234390258789\n",
            "2309/3000 train_loss: 20.01308250427246 test_loss:91.13128662109375\n",
            "2310/3000 train_loss: 19.224855422973633 test_loss:86.03018951416016\n",
            "2311/3000 train_loss: 19.474973678588867 test_loss:86.66030883789062\n",
            "2312/3000 train_loss: 16.6772518157959 test_loss:84.4268798828125\n",
            "2313/3000 train_loss: 16.287948608398438 test_loss:87.76803588867188\n",
            "2314/3000 train_loss: 16.279008865356445 test_loss:83.1550064086914\n",
            "2315/3000 train_loss: 21.606006622314453 test_loss:83.30272674560547\n",
            "2316/3000 train_loss: 17.856340408325195 test_loss:90.10491180419922\n",
            "2317/3000 train_loss: 19.657228469848633 test_loss:81.9365234375\n",
            "2318/3000 train_loss: 20.662567138671875 test_loss:94.04120635986328\n",
            "2319/3000 train_loss: 15.224891662597656 test_loss:84.13108825683594\n",
            "2320/3000 train_loss: 15.512496948242188 test_loss:79.90827941894531\n",
            "2321/3000 train_loss: 17.61524772644043 test_loss:80.6292953491211\n",
            "2322/3000 train_loss: 20.548267364501953 test_loss:86.53787231445312\n",
            "2323/3000 train_loss: 23.818815231323242 test_loss:84.24754333496094\n",
            "2324/3000 train_loss: 16.91866683959961 test_loss:87.10177612304688\n",
            "2325/3000 train_loss: 18.147523880004883 test_loss:89.62054443359375\n",
            "2326/3000 train_loss: 17.709285736083984 test_loss:104.83478546142578\n",
            "2327/3000 train_loss: 24.30280303955078 test_loss:87.29985046386719\n",
            "2328/3000 train_loss: 24.215150833129883 test_loss:88.26046752929688\n",
            "2329/3000 train_loss: 20.1566104888916 test_loss:88.43049621582031\n",
            "2330/3000 train_loss: 17.289257049560547 test_loss:88.09147644042969\n",
            "2331/3000 train_loss: 21.025545120239258 test_loss:84.74295806884766\n",
            "2332/3000 train_loss: 17.332096099853516 test_loss:94.14024353027344\n",
            "2333/3000 train_loss: 20.965072631835938 test_loss:82.26537322998047\n",
            "2334/3000 train_loss: 16.11297035217285 test_loss:84.6667251586914\n",
            "2335/3000 train_loss: 17.0914306640625 test_loss:93.93208312988281\n",
            "2336/3000 train_loss: 18.79861831665039 test_loss:86.9135971069336\n",
            "2337/3000 train_loss: 17.619718551635742 test_loss:93.41917419433594\n",
            "2338/3000 train_loss: 17.42276954650879 test_loss:82.9073486328125\n",
            "2339/3000 train_loss: 18.285911560058594 test_loss:86.86607360839844\n",
            "2340/3000 train_loss: 15.823244094848633 test_loss:85.52937316894531\n",
            "2341/3000 train_loss: 18.641483306884766 test_loss:84.0232925415039\n",
            "2342/3000 train_loss: 24.524030685424805 test_loss:81.90066528320312\n",
            "2343/3000 train_loss: 22.225099563598633 test_loss:81.12992858886719\n",
            "2344/3000 train_loss: 20.650978088378906 test_loss:88.10435485839844\n",
            "2345/3000 train_loss: 15.962077140808105 test_loss:83.40216827392578\n",
            "2346/3000 train_loss: 14.346268653869629 test_loss:84.79451751708984\n",
            "2347/3000 train_loss: 16.02607536315918 test_loss:83.98309326171875\n",
            "2348/3000 train_loss: 18.40532875061035 test_loss:84.2747802734375\n",
            "2349/3000 train_loss: 19.02057456970215 test_loss:94.97830963134766\n",
            "2350/3000 train_loss: 16.33796501159668 test_loss:81.80007934570312\n",
            "2351/3000 train_loss: 18.70992660522461 test_loss:81.70897674560547\n",
            "2352/3000 train_loss: 23.987213134765625 test_loss:90.82080078125\n",
            "2353/3000 train_loss: 19.698226928710938 test_loss:89.42967224121094\n",
            "2354/3000 train_loss: 18.346923828125 test_loss:84.37715148925781\n",
            "2355/3000 train_loss: 21.181732177734375 test_loss:84.62960052490234\n",
            "2356/3000 train_loss: 16.864830017089844 test_loss:95.74539184570312\n",
            "2357/3000 train_loss: 18.27349853515625 test_loss:83.87628173828125\n",
            "2358/3000 train_loss: 21.851869583129883 test_loss:80.91264343261719\n",
            "2359/3000 train_loss: 18.873064041137695 test_loss:81.06675720214844\n",
            "2360/3000 train_loss: 17.363679885864258 test_loss:92.23399353027344\n",
            "2361/3000 train_loss: 16.38572120666504 test_loss:89.50603485107422\n",
            "2362/3000 train_loss: 15.729595184326172 test_loss:87.34922790527344\n",
            "2363/3000 train_loss: 14.788200378417969 test_loss:80.22541809082031\n",
            "2364/3000 train_loss: 16.779157638549805 test_loss:90.0205078125\n",
            "2365/3000 train_loss: 16.89580726623535 test_loss:92.87786865234375\n",
            "2366/3000 train_loss: 17.398521423339844 test_loss:89.20756530761719\n",
            "2367/3000 train_loss: 15.534088134765625 test_loss:84.70481872558594\n",
            "2368/3000 train_loss: 16.044172286987305 test_loss:89.23763275146484\n",
            "2369/3000 train_loss: 18.492929458618164 test_loss:94.69221496582031\n",
            "2370/3000 train_loss: 18.160171508789062 test_loss:77.95500183105469\n",
            "2371/3000 train_loss: 19.72905921936035 test_loss:89.14956665039062\n",
            "2372/3000 train_loss: 15.937376976013184 test_loss:87.49916076660156\n",
            "2373/3000 train_loss: 29.54922866821289 test_loss:88.75025939941406\n",
            "2374/3000 train_loss: 19.173532485961914 test_loss:93.16703796386719\n",
            "2375/3000 train_loss: 19.225841522216797 test_loss:93.48906707763672\n",
            "2376/3000 train_loss: 16.87395477294922 test_loss:91.36506652832031\n",
            "2377/3000 train_loss: 15.5674467086792 test_loss:91.17178344726562\n",
            "2378/3000 train_loss: 13.956439971923828 test_loss:87.77438354492188\n",
            "2379/3000 train_loss: 15.752939224243164 test_loss:86.49241638183594\n",
            "2380/3000 train_loss: 26.139202117919922 test_loss:92.90670776367188\n",
            "2381/3000 train_loss: 20.451202392578125 test_loss:86.74970245361328\n",
            "2382/3000 train_loss: 17.565135955810547 test_loss:88.73341369628906\n",
            "2383/3000 train_loss: 17.443227767944336 test_loss:89.74356079101562\n",
            "2384/3000 train_loss: 16.754669189453125 test_loss:83.81306457519531\n",
            "2385/3000 train_loss: 16.51450538635254 test_loss:93.91363525390625\n",
            "2386/3000 train_loss: 17.396053314208984 test_loss:81.98058319091797\n",
            "2387/3000 train_loss: 16.36582374572754 test_loss:87.2978515625\n",
            "2388/3000 train_loss: 20.963422775268555 test_loss:92.85792541503906\n",
            "2389/3000 train_loss: 15.800514221191406 test_loss:89.68624877929688\n",
            "2390/3000 train_loss: 20.682340621948242 test_loss:95.29685974121094\n",
            "2391/3000 train_loss: 14.939347267150879 test_loss:87.2476806640625\n",
            "2392/3000 train_loss: 16.92924690246582 test_loss:88.39593505859375\n",
            "2393/3000 train_loss: 15.01411247253418 test_loss:98.97197723388672\n",
            "2394/3000 train_loss: 14.651494979858398 test_loss:91.12889099121094\n",
            "2395/3000 train_loss: 18.68238067626953 test_loss:90.6614761352539\n",
            "2396/3000 train_loss: 19.869998931884766 test_loss:94.42991638183594\n",
            "2397/3000 train_loss: 18.602630615234375 test_loss:81.22683715820312\n",
            "2398/3000 train_loss: 17.41298484802246 test_loss:97.05217742919922\n",
            "2399/3000 train_loss: 21.459793090820312 test_loss:87.83794403076172\n",
            "2400/3000 train_loss: 17.292387008666992 test_loss:82.5358657836914\n",
            "2401/3000 train_loss: 19.49980926513672 test_loss:82.03118133544922\n",
            "2402/3000 train_loss: 18.434282302856445 test_loss:93.14207458496094\n",
            "2403/3000 train_loss: 16.08175277709961 test_loss:84.37757873535156\n",
            "2404/3000 train_loss: 17.687448501586914 test_loss:89.64627838134766\n",
            "2405/3000 train_loss: 17.37749671936035 test_loss:88.58736419677734\n",
            "2406/3000 train_loss: 15.423344612121582 test_loss:87.16431427001953\n",
            "2407/3000 train_loss: 16.05841064453125 test_loss:79.68466186523438\n",
            "2408/3000 train_loss: 18.04172706604004 test_loss:82.59065246582031\n",
            "2409/3000 train_loss: 17.369468688964844 test_loss:93.82234954833984\n",
            "2410/3000 train_loss: 14.98304557800293 test_loss:87.3613052368164\n",
            "2411/3000 train_loss: 17.582687377929688 test_loss:90.02565002441406\n",
            "2412/3000 train_loss: 22.61812973022461 test_loss:88.48823547363281\n",
            "2413/3000 train_loss: 15.211039543151855 test_loss:82.50627136230469\n",
            "2414/3000 train_loss: 19.510658264160156 test_loss:83.73767852783203\n",
            "2415/3000 train_loss: 15.547711372375488 test_loss:89.59428405761719\n",
            "2416/3000 train_loss: 15.768023490905762 test_loss:90.58161926269531\n",
            "2417/3000 train_loss: 19.51585578918457 test_loss:100.96739959716797\n",
            "2418/3000 train_loss: 17.988039016723633 test_loss:84.63863372802734\n",
            "2419/3000 train_loss: 15.160699844360352 test_loss:89.16958618164062\n",
            "2420/3000 train_loss: 15.309377670288086 test_loss:87.21098327636719\n",
            "2421/3000 train_loss: 15.635101318359375 test_loss:79.92863464355469\n",
            "2422/3000 train_loss: 15.682711601257324 test_loss:92.01604461669922\n",
            "2423/3000 train_loss: 18.87202262878418 test_loss:98.79762268066406\n",
            "2424/3000 train_loss: 14.974395751953125 test_loss:83.33705139160156\n",
            "2425/3000 train_loss: 15.889200210571289 test_loss:86.85256958007812\n",
            "2426/3000 train_loss: 18.389686584472656 test_loss:96.22113800048828\n",
            "2427/3000 train_loss: 16.269575119018555 test_loss:82.7576904296875\n",
            "2428/3000 train_loss: 15.85280704498291 test_loss:88.4495849609375\n",
            "2429/3000 train_loss: 16.908071517944336 test_loss:85.54149627685547\n",
            "2430/3000 train_loss: 17.3415584564209 test_loss:82.28461456298828\n",
            "2431/3000 train_loss: 16.635570526123047 test_loss:86.40040588378906\n",
            "2432/3000 train_loss: 16.43235969543457 test_loss:85.38356018066406\n",
            "2433/3000 train_loss: 13.43125057220459 test_loss:82.37028503417969\n",
            "2434/3000 train_loss: 20.095836639404297 test_loss:81.34459686279297\n",
            "2435/3000 train_loss: 17.893083572387695 test_loss:79.22379302978516\n",
            "2436/3000 train_loss: 26.975269317626953 test_loss:86.77302551269531\n",
            "2437/3000 train_loss: 19.39737892150879 test_loss:89.57339477539062\n",
            "2438/3000 train_loss: 17.484031677246094 test_loss:77.77131652832031\n",
            "2439/3000 train_loss: 19.483545303344727 test_loss:91.85749816894531\n",
            "2440/3000 train_loss: 17.43345832824707 test_loss:87.01968383789062\n",
            "2441/3000 train_loss: 20.199970245361328 test_loss:83.01530456542969\n",
            "2442/3000 train_loss: 20.270097732543945 test_loss:81.2418212890625\n",
            "2443/3000 train_loss: 19.14362335205078 test_loss:91.85438537597656\n",
            "2444/3000 train_loss: 19.412128448486328 test_loss:93.341064453125\n",
            "2445/3000 train_loss: 17.25139617919922 test_loss:86.73780822753906\n",
            "2446/3000 train_loss: 14.836630821228027 test_loss:88.06334686279297\n",
            "2447/3000 train_loss: 18.667137145996094 test_loss:88.36897277832031\n",
            "2448/3000 train_loss: 17.5158634185791 test_loss:80.63907623291016\n",
            "2449/3000 train_loss: 21.471363067626953 test_loss:87.95429229736328\n",
            "2450/3000 train_loss: 15.572936058044434 test_loss:77.11074829101562\n",
            "2451/3000 train_loss: 17.583362579345703 test_loss:96.05241394042969\n",
            "2452/3000 train_loss: 18.057687759399414 test_loss:84.39167785644531\n",
            "2453/3000 train_loss: 16.682483673095703 test_loss:86.90121459960938\n",
            "2454/3000 train_loss: 15.77579116821289 test_loss:83.27272033691406\n",
            "2455/3000 train_loss: 15.519177436828613 test_loss:84.76800537109375\n",
            "2456/3000 train_loss: 18.126728057861328 test_loss:98.05323791503906\n",
            "2457/3000 train_loss: 19.51817512512207 test_loss:80.95703125\n",
            "2458/3000 train_loss: 14.625753402709961 test_loss:88.91559600830078\n",
            "2459/3000 train_loss: 17.772977828979492 test_loss:87.15704345703125\n",
            "2460/3000 train_loss: 17.461694717407227 test_loss:77.60134887695312\n",
            "2461/3000 train_loss: 15.077110290527344 test_loss:87.60698699951172\n",
            "2462/3000 train_loss: 15.342389106750488 test_loss:81.3089370727539\n",
            "2463/3000 train_loss: 19.77170181274414 test_loss:83.366455078125\n",
            "2464/3000 train_loss: 17.28441047668457 test_loss:85.7616958618164\n",
            "2465/3000 train_loss: 15.552852630615234 test_loss:88.29408264160156\n",
            "2466/3000 train_loss: 16.52543067932129 test_loss:87.5347900390625\n",
            "2467/3000 train_loss: 22.928600311279297 test_loss:84.21241760253906\n",
            "2468/3000 train_loss: 14.76602554321289 test_loss:91.03689575195312\n",
            "2469/3000 train_loss: 14.655396461486816 test_loss:85.31883239746094\n",
            "2470/3000 train_loss: 16.743507385253906 test_loss:83.37239837646484\n",
            "2471/3000 train_loss: 15.136199951171875 test_loss:81.52522277832031\n",
            "2472/3000 train_loss: 14.82216739654541 test_loss:83.70965576171875\n",
            "2473/3000 train_loss: 18.826786041259766 test_loss:92.57362365722656\n",
            "2474/3000 train_loss: 15.853525161743164 test_loss:84.9947280883789\n",
            "2475/3000 train_loss: 15.303674697875977 test_loss:80.18060302734375\n",
            "2476/3000 train_loss: 18.5572566986084 test_loss:77.44596862792969\n",
            "2477/3000 train_loss: 16.01860237121582 test_loss:81.84142303466797\n",
            "2478/3000 train_loss: 21.488603591918945 test_loss:88.7720718383789\n",
            "2479/3000 train_loss: 15.681745529174805 test_loss:83.47157287597656\n",
            "2480/3000 train_loss: 15.224740982055664 test_loss:83.30545043945312\n",
            "2481/3000 train_loss: 16.241497039794922 test_loss:87.73797607421875\n",
            "2482/3000 train_loss: 15.364387512207031 test_loss:84.69080352783203\n",
            "2483/3000 train_loss: 16.08426284790039 test_loss:82.91020202636719\n",
            "2484/3000 train_loss: 18.65753173828125 test_loss:77.72877502441406\n",
            "2485/3000 train_loss: 17.61362648010254 test_loss:84.84235382080078\n",
            "2486/3000 train_loss: 25.119598388671875 test_loss:93.9269027709961\n",
            "2487/3000 train_loss: 21.815916061401367 test_loss:85.65695190429688\n",
            "2488/3000 train_loss: 16.54342269897461 test_loss:87.94285583496094\n",
            "2489/3000 train_loss: 16.437707901000977 test_loss:84.65950012207031\n",
            "2490/3000 train_loss: 24.517375946044922 test_loss:80.2158432006836\n",
            "2491/3000 train_loss: 17.08679962158203 test_loss:89.08428955078125\n",
            "2492/3000 train_loss: 16.03849983215332 test_loss:79.60431671142578\n",
            "2493/3000 train_loss: 17.07010841369629 test_loss:83.80120086669922\n",
            "2494/3000 train_loss: 15.282478332519531 test_loss:92.21204376220703\n",
            "2495/3000 train_loss: 18.158721923828125 test_loss:87.91195678710938\n",
            "2496/3000 train_loss: 16.643949508666992 test_loss:84.44004821777344\n",
            "2497/3000 train_loss: 17.87551498413086 test_loss:83.01150512695312\n",
            "2498/3000 train_loss: 17.870187759399414 test_loss:88.86298370361328\n",
            "2499/3000 train_loss: 15.49145221710205 test_loss:86.80774688720703\n",
            "2500/3000 train_loss: 15.902547836303711 test_loss:85.32209014892578\n",
            "2501/3000 train_loss: 16.994089126586914 test_loss:84.71563720703125\n",
            "2502/3000 train_loss: 13.931721687316895 test_loss:86.04701232910156\n",
            "2503/3000 train_loss: 16.707000732421875 test_loss:85.42111206054688\n",
            "2504/3000 train_loss: 13.966334342956543 test_loss:82.98521423339844\n",
            "2505/3000 train_loss: 16.606107711791992 test_loss:82.81985473632812\n",
            "2506/3000 train_loss: 12.909431457519531 test_loss:76.48099517822266\n",
            "2507/3000 train_loss: 15.188911437988281 test_loss:84.88947296142578\n",
            "2508/3000 train_loss: 17.976308822631836 test_loss:76.22006225585938\n",
            "2509/3000 train_loss: 13.816832542419434 test_loss:81.04438781738281\n",
            "2510/3000 train_loss: 19.207372665405273 test_loss:82.4200439453125\n",
            "2511/3000 train_loss: 22.35585594177246 test_loss:76.83677673339844\n",
            "2512/3000 train_loss: 19.440616607666016 test_loss:86.3633041381836\n",
            "2513/3000 train_loss: 18.259431838989258 test_loss:87.75733947753906\n",
            "2514/3000 train_loss: 16.80280303955078 test_loss:86.57376098632812\n",
            "2515/3000 train_loss: 15.490901947021484 test_loss:85.38629913330078\n",
            "2516/3000 train_loss: 15.453248977661133 test_loss:76.77627563476562\n",
            "2517/3000 train_loss: 18.881607055664062 test_loss:84.01870727539062\n",
            "2518/3000 train_loss: 17.259511947631836 test_loss:96.12409210205078\n",
            "2519/3000 train_loss: 16.474611282348633 test_loss:81.67124938964844\n",
            "2520/3000 train_loss: 16.562114715576172 test_loss:85.58297729492188\n",
            "2521/3000 train_loss: 15.615771293640137 test_loss:88.90318298339844\n",
            "2522/3000 train_loss: 20.37010383605957 test_loss:84.36138916015625\n",
            "2523/3000 train_loss: 15.480836868286133 test_loss:81.56748962402344\n",
            "2524/3000 train_loss: 14.404035568237305 test_loss:84.83932495117188\n",
            "2525/3000 train_loss: 15.84348201751709 test_loss:86.53255462646484\n",
            "2526/3000 train_loss: 15.478585243225098 test_loss:80.68009948730469\n",
            "2527/3000 train_loss: 16.858444213867188 test_loss:76.02119445800781\n",
            "2528/3000 train_loss: 18.878253936767578 test_loss:88.43885803222656\n",
            "2529/3000 train_loss: 14.09660530090332 test_loss:80.67010498046875\n",
            "2530/3000 train_loss: 16.83104133605957 test_loss:76.05846405029297\n",
            "2531/3000 train_loss: 19.367835998535156 test_loss:81.64158630371094\n",
            "2532/3000 train_loss: 16.061717987060547 test_loss:82.43840026855469\n",
            "2533/3000 train_loss: 14.725931167602539 test_loss:79.06840515136719\n",
            "2534/3000 train_loss: 15.487573623657227 test_loss:85.19257354736328\n",
            "2535/3000 train_loss: 17.368270874023438 test_loss:85.659912109375\n",
            "2536/3000 train_loss: 19.006750106811523 test_loss:85.07356262207031\n",
            "2537/3000 train_loss: 13.989418983459473 test_loss:81.49617767333984\n",
            "2538/3000 train_loss: 20.477819442749023 test_loss:85.08394622802734\n",
            "2539/3000 train_loss: 13.340032577514648 test_loss:80.98886108398438\n",
            "2540/3000 train_loss: 14.53161334991455 test_loss:78.43394470214844\n",
            "2541/3000 train_loss: 12.465330123901367 test_loss:83.58378601074219\n",
            "2542/3000 train_loss: 16.719802856445312 test_loss:82.77023315429688\n",
            "2543/3000 train_loss: 16.809986114501953 test_loss:86.5424575805664\n",
            "2544/3000 train_loss: 14.734408378601074 test_loss:82.32054138183594\n",
            "2545/3000 train_loss: 18.845003128051758 test_loss:77.7420883178711\n",
            "2546/3000 train_loss: 14.348285675048828 test_loss:81.63619995117188\n",
            "2547/3000 train_loss: 19.49563217163086 test_loss:88.65835571289062\n",
            "2548/3000 train_loss: 19.565467834472656 test_loss:74.78777313232422\n",
            "2549/3000 train_loss: 19.068578720092773 test_loss:76.37350463867188\n",
            "2550/3000 train_loss: 16.678647994995117 test_loss:83.8673095703125\n",
            "2551/3000 train_loss: 20.355712890625 test_loss:73.46891021728516\n",
            "2552/3000 train_loss: 20.595666885375977 test_loss:77.17321014404297\n",
            "2553/3000 train_loss: 20.014812469482422 test_loss:84.62678527832031\n",
            "2554/3000 train_loss: 17.399707794189453 test_loss:79.84989166259766\n",
            "2555/3000 train_loss: 18.61631965637207 test_loss:76.28852844238281\n",
            "2556/3000 train_loss: 16.110918045043945 test_loss:84.09124755859375\n",
            "2557/3000 train_loss: 13.851948738098145 test_loss:75.76124572753906\n",
            "2558/3000 train_loss: 17.17123794555664 test_loss:78.20169830322266\n",
            "2559/3000 train_loss: 14.238550186157227 test_loss:92.84113311767578\n",
            "2560/3000 train_loss: 13.755107879638672 test_loss:75.35084533691406\n",
            "2561/3000 train_loss: 18.189067840576172 test_loss:94.04508972167969\n",
            "2562/3000 train_loss: 19.104528427124023 test_loss:81.33828735351562\n",
            "2563/3000 train_loss: 15.934871673583984 test_loss:73.60968780517578\n",
            "2564/3000 train_loss: 16.895275115966797 test_loss:82.92835998535156\n",
            "2565/3000 train_loss: 15.454018592834473 test_loss:81.45532989501953\n",
            "2566/3000 train_loss: 15.842116355895996 test_loss:78.4698715209961\n",
            "2567/3000 train_loss: 16.441513061523438 test_loss:81.17022705078125\n",
            "2568/3000 train_loss: 18.608186721801758 test_loss:80.1759033203125\n",
            "2569/3000 train_loss: 13.559945106506348 test_loss:75.56058502197266\n",
            "2570/3000 train_loss: 17.13414764404297 test_loss:80.07862091064453\n",
            "2571/3000 train_loss: 16.691631317138672 test_loss:88.81639099121094\n",
            "2572/3000 train_loss: 16.790889739990234 test_loss:84.4429702758789\n",
            "2573/3000 train_loss: 16.53726577758789 test_loss:83.8184585571289\n",
            "2574/3000 train_loss: 15.172994613647461 test_loss:75.69947052001953\n",
            "2575/3000 train_loss: 17.6630859375 test_loss:77.17948913574219\n",
            "2576/3000 train_loss: 13.885595321655273 test_loss:75.58872985839844\n",
            "2577/3000 train_loss: 19.911962509155273 test_loss:78.65483856201172\n",
            "2578/3000 train_loss: 15.702665328979492 test_loss:76.88682556152344\n",
            "2579/3000 train_loss: 17.891080856323242 test_loss:78.3868408203125\n",
            "2580/3000 train_loss: 13.295055389404297 test_loss:76.40654754638672\n",
            "2581/3000 train_loss: 16.122365951538086 test_loss:93.08857727050781\n",
            "2582/3000 train_loss: 16.72523307800293 test_loss:77.34513854980469\n",
            "2583/3000 train_loss: 17.018444061279297 test_loss:84.76173400878906\n",
            "2584/3000 train_loss: 14.903708457946777 test_loss:81.56892395019531\n",
            "2585/3000 train_loss: 14.40071964263916 test_loss:77.10980987548828\n",
            "2586/3000 train_loss: 17.972618103027344 test_loss:78.58931732177734\n",
            "2587/3000 train_loss: 15.965149879455566 test_loss:80.25697326660156\n",
            "2588/3000 train_loss: 19.959325790405273 test_loss:84.43206024169922\n",
            "2589/3000 train_loss: 15.333642959594727 test_loss:86.86662292480469\n",
            "2590/3000 train_loss: 14.263952255249023 test_loss:85.41159057617188\n",
            "2591/3000 train_loss: 15.516244888305664 test_loss:85.618408203125\n",
            "2592/3000 train_loss: 18.059738159179688 test_loss:86.51434326171875\n",
            "2593/3000 train_loss: 15.522926330566406 test_loss:80.82573699951172\n",
            "2594/3000 train_loss: 14.629838943481445 test_loss:80.34339904785156\n",
            "2595/3000 train_loss: 14.212785720825195 test_loss:88.31096649169922\n",
            "2596/3000 train_loss: 13.088808059692383 test_loss:80.27668762207031\n",
            "2597/3000 train_loss: 15.927449226379395 test_loss:84.72547149658203\n",
            "2598/3000 train_loss: 13.140868186950684 test_loss:79.88200378417969\n",
            "2599/3000 train_loss: 14.940183639526367 test_loss:84.76118469238281\n",
            "2600/3000 train_loss: 15.351336479187012 test_loss:73.24523162841797\n",
            "2601/3000 train_loss: 15.674566268920898 test_loss:82.13011169433594\n",
            "2602/3000 train_loss: 18.255157470703125 test_loss:81.7097396850586\n",
            "2603/3000 train_loss: 16.576393127441406 test_loss:80.04399108886719\n",
            "2604/3000 train_loss: 18.999616622924805 test_loss:81.55393981933594\n",
            "2605/3000 train_loss: 16.28912925720215 test_loss:78.35293579101562\n",
            "2606/3000 train_loss: 16.868539810180664 test_loss:85.39368438720703\n",
            "2607/3000 train_loss: 20.489181518554688 test_loss:89.48995971679688\n",
            "2608/3000 train_loss: 15.84225845336914 test_loss:83.23847961425781\n",
            "2609/3000 train_loss: 13.657498359680176 test_loss:79.13607788085938\n",
            "2610/3000 train_loss: 15.293930053710938 test_loss:83.3025894165039\n",
            "2611/3000 train_loss: 15.682413101196289 test_loss:84.00767517089844\n",
            "2612/3000 train_loss: 16.16531753540039 test_loss:91.09073638916016\n",
            "2613/3000 train_loss: 16.537609100341797 test_loss:83.81712341308594\n",
            "2614/3000 train_loss: 14.71761417388916 test_loss:86.01060485839844\n",
            "2615/3000 train_loss: 16.764719009399414 test_loss:80.70065307617188\n",
            "2616/3000 train_loss: 17.278457641601562 test_loss:85.03059387207031\n",
            "2617/3000 train_loss: 15.366143226623535 test_loss:81.3567886352539\n",
            "2618/3000 train_loss: 15.224225044250488 test_loss:81.36763000488281\n",
            "2619/3000 train_loss: 15.037270545959473 test_loss:84.13888549804688\n",
            "2620/3000 train_loss: 16.138446807861328 test_loss:77.52239990234375\n",
            "2621/3000 train_loss: 15.124076843261719 test_loss:83.43216705322266\n",
            "2622/3000 train_loss: 11.582697868347168 test_loss:84.93036651611328\n",
            "2623/3000 train_loss: 14.671388626098633 test_loss:87.85487365722656\n",
            "2624/3000 train_loss: 14.825307846069336 test_loss:75.99513244628906\n",
            "2625/3000 train_loss: 16.51301383972168 test_loss:74.5443344116211\n",
            "2626/3000 train_loss: 14.163962364196777 test_loss:86.57071685791016\n",
            "2627/3000 train_loss: 15.780137062072754 test_loss:78.2369155883789\n",
            "2628/3000 train_loss: 13.756385803222656 test_loss:79.13191223144531\n",
            "2629/3000 train_loss: 18.23590850830078 test_loss:76.80065155029297\n",
            "2630/3000 train_loss: 16.74764060974121 test_loss:88.29345703125\n",
            "2631/3000 train_loss: 11.749059677124023 test_loss:77.91950225830078\n",
            "2632/3000 train_loss: 15.54903793334961 test_loss:86.94232177734375\n",
            "2633/3000 train_loss: 17.657255172729492 test_loss:88.83975982666016\n",
            "2634/3000 train_loss: 13.910548210144043 test_loss:79.1808853149414\n",
            "2635/3000 train_loss: 15.429905891418457 test_loss:83.5982666015625\n",
            "2636/3000 train_loss: 15.531416893005371 test_loss:81.73906707763672\n",
            "2637/3000 train_loss: 16.766847610473633 test_loss:82.11258697509766\n",
            "2638/3000 train_loss: 15.89260482788086 test_loss:88.2449951171875\n",
            "2639/3000 train_loss: 16.66954803466797 test_loss:82.81251525878906\n",
            "2640/3000 train_loss: 16.125234603881836 test_loss:85.7443618774414\n",
            "2641/3000 train_loss: 20.73694610595703 test_loss:85.94481658935547\n",
            "2642/3000 train_loss: 14.929082870483398 test_loss:74.1556167602539\n",
            "2643/3000 train_loss: 15.009328842163086 test_loss:84.90084075927734\n",
            "2644/3000 train_loss: 20.098392486572266 test_loss:88.75883483886719\n",
            "2645/3000 train_loss: 13.89704418182373 test_loss:80.94792175292969\n",
            "2646/3000 train_loss: 16.708515167236328 test_loss:77.82182312011719\n",
            "2647/3000 train_loss: 15.156632423400879 test_loss:84.4745101928711\n",
            "2648/3000 train_loss: 15.650534629821777 test_loss:80.31346130371094\n",
            "2649/3000 train_loss: 17.801496505737305 test_loss:82.25369262695312\n",
            "2650/3000 train_loss: 13.500143051147461 test_loss:82.42501068115234\n",
            "2651/3000 train_loss: 14.762370109558105 test_loss:86.6506576538086\n",
            "2652/3000 train_loss: 18.43239402770996 test_loss:90.32389831542969\n",
            "2653/3000 train_loss: 14.94833755493164 test_loss:80.79359436035156\n",
            "2654/3000 train_loss: 13.248480796813965 test_loss:85.84187316894531\n",
            "2655/3000 train_loss: 12.997465133666992 test_loss:84.602294921875\n",
            "2656/3000 train_loss: 15.150738716125488 test_loss:82.36595153808594\n",
            "2657/3000 train_loss: 14.451723098754883 test_loss:77.2461166381836\n",
            "2658/3000 train_loss: 13.585893630981445 test_loss:77.74334716796875\n",
            "2659/3000 train_loss: 16.65838623046875 test_loss:80.3412857055664\n",
            "2660/3000 train_loss: 13.784395217895508 test_loss:82.80656433105469\n",
            "2661/3000 train_loss: 15.816237449645996 test_loss:82.17874145507812\n",
            "2662/3000 train_loss: 16.941301345825195 test_loss:76.22171020507812\n",
            "2663/3000 train_loss: 12.942155838012695 test_loss:84.1832046508789\n",
            "2664/3000 train_loss: 17.466520309448242 test_loss:81.80682373046875\n",
            "2665/3000 train_loss: 15.701384544372559 test_loss:79.48699951171875\n",
            "2666/3000 train_loss: 14.284402847290039 test_loss:74.06597137451172\n",
            "2667/3000 train_loss: 16.943035125732422 test_loss:89.1017837524414\n",
            "2668/3000 train_loss: 14.653196334838867 test_loss:83.99510192871094\n",
            "2669/3000 train_loss: 13.454902648925781 test_loss:75.01094055175781\n",
            "2670/3000 train_loss: 15.748087882995605 test_loss:82.43960571289062\n",
            "2671/3000 train_loss: 12.584372520446777 test_loss:80.74047088623047\n",
            "2672/3000 train_loss: 22.48948860168457 test_loss:79.644775390625\n",
            "2673/3000 train_loss: 16.105905532836914 test_loss:90.82551574707031\n",
            "2674/3000 train_loss: 15.846956253051758 test_loss:88.09646606445312\n",
            "2675/3000 train_loss: 13.6056489944458 test_loss:87.08697509765625\n",
            "2676/3000 train_loss: 14.6522798538208 test_loss:91.18228149414062\n",
            "2677/3000 train_loss: 12.52635669708252 test_loss:79.90327453613281\n",
            "2678/3000 train_loss: 14.466026306152344 test_loss:77.95555114746094\n",
            "2679/3000 train_loss: 16.020051956176758 test_loss:93.1636962890625\n",
            "2680/3000 train_loss: 21.06450843811035 test_loss:83.91256713867188\n",
            "2681/3000 train_loss: 13.348288536071777 test_loss:80.90797424316406\n",
            "2682/3000 train_loss: 15.83210563659668 test_loss:85.40892791748047\n",
            "2683/3000 train_loss: 17.37733268737793 test_loss:77.17901611328125\n",
            "2684/3000 train_loss: 19.45440101623535 test_loss:81.05443572998047\n",
            "2685/3000 train_loss: 16.336240768432617 test_loss:91.23436737060547\n",
            "2686/3000 train_loss: 12.873796463012695 test_loss:91.58712768554688\n",
            "2687/3000 train_loss: 14.68138599395752 test_loss:94.73074340820312\n",
            "2688/3000 train_loss: 14.072932243347168 test_loss:83.02411651611328\n",
            "2689/3000 train_loss: 13.937957763671875 test_loss:82.0810317993164\n",
            "2690/3000 train_loss: 12.118062973022461 test_loss:82.66302490234375\n",
            "2691/3000 train_loss: 16.052490234375 test_loss:81.31497192382812\n",
            "2692/3000 train_loss: 14.80256175994873 test_loss:81.9716796875\n",
            "2693/3000 train_loss: 15.009282112121582 test_loss:83.35123443603516\n",
            "2694/3000 train_loss: 15.019941329956055 test_loss:79.15870666503906\n",
            "2695/3000 train_loss: 13.264961242675781 test_loss:80.48255920410156\n",
            "2696/3000 train_loss: 15.32763671875 test_loss:81.05654907226562\n",
            "2697/3000 train_loss: 12.58325481414795 test_loss:87.10480499267578\n",
            "2698/3000 train_loss: 15.140496253967285 test_loss:77.90323638916016\n",
            "2699/3000 train_loss: 15.069857597351074 test_loss:90.02821350097656\n",
            "2700/3000 train_loss: 18.058147430419922 test_loss:77.73532104492188\n",
            "2701/3000 train_loss: 13.411772727966309 test_loss:80.38633728027344\n",
            "2702/3000 train_loss: 14.835867881774902 test_loss:87.35939025878906\n",
            "2703/3000 train_loss: 15.205973625183105 test_loss:75.494384765625\n",
            "2704/3000 train_loss: 18.202409744262695 test_loss:80.31786346435547\n",
            "2705/3000 train_loss: 14.56856918334961 test_loss:83.55704498291016\n",
            "2706/3000 train_loss: 14.196723937988281 test_loss:85.22120666503906\n",
            "2707/3000 train_loss: 15.271246910095215 test_loss:85.50367736816406\n",
            "2708/3000 train_loss: 13.241477966308594 test_loss:72.49755859375\n",
            "2709/3000 train_loss: 17.09346580505371 test_loss:81.8878173828125\n",
            "2710/3000 train_loss: 18.594738006591797 test_loss:78.313720703125\n",
            "2711/3000 train_loss: 13.143136024475098 test_loss:87.82917785644531\n",
            "2712/3000 train_loss: 14.160266876220703 test_loss:81.23544311523438\n",
            "2713/3000 train_loss: 17.845069885253906 test_loss:81.39546203613281\n",
            "2714/3000 train_loss: 15.61789321899414 test_loss:71.99142456054688\n",
            "2715/3000 train_loss: 13.752134323120117 test_loss:80.48845672607422\n",
            "2716/3000 train_loss: 14.616410255432129 test_loss:86.08531188964844\n",
            "2717/3000 train_loss: 16.1030330657959 test_loss:82.5199203491211\n",
            "2718/3000 train_loss: 20.791362762451172 test_loss:81.79855346679688\n",
            "2719/3000 train_loss: 14.388856887817383 test_loss:79.23812103271484\n",
            "2720/3000 train_loss: 16.488811492919922 test_loss:80.8875732421875\n",
            "2721/3000 train_loss: 15.431960105895996 test_loss:84.4049072265625\n",
            "2722/3000 train_loss: 15.993459701538086 test_loss:75.317138671875\n",
            "2723/3000 train_loss: 13.108558654785156 test_loss:79.9285888671875\n",
            "2724/3000 train_loss: 13.506629943847656 test_loss:84.70480346679688\n",
            "2725/3000 train_loss: 14.168296813964844 test_loss:78.0857925415039\n",
            "2726/3000 train_loss: 14.650372505187988 test_loss:80.84253692626953\n",
            "2727/3000 train_loss: 14.835166931152344 test_loss:84.5224609375\n",
            "2728/3000 train_loss: 15.75271987915039 test_loss:87.9659423828125\n",
            "2729/3000 train_loss: 15.00353717803955 test_loss:82.63203430175781\n",
            "2730/3000 train_loss: 15.098884582519531 test_loss:84.54551696777344\n",
            "2731/3000 train_loss: 14.897905349731445 test_loss:81.4996337890625\n",
            "2732/3000 train_loss: 16.331445693969727 test_loss:95.41706848144531\n",
            "2733/3000 train_loss: 14.150410652160645 test_loss:77.59256744384766\n",
            "2734/3000 train_loss: 16.45037269592285 test_loss:79.06394958496094\n",
            "2735/3000 train_loss: 15.579848289489746 test_loss:87.17251586914062\n",
            "2736/3000 train_loss: 15.401182174682617 test_loss:87.65342712402344\n",
            "2737/3000 train_loss: 15.697678565979004 test_loss:80.27047729492188\n",
            "2738/3000 train_loss: 26.5340633392334 test_loss:86.57223510742188\n",
            "2739/3000 train_loss: 17.860836029052734 test_loss:85.08588409423828\n",
            "2740/3000 train_loss: 15.671449661254883 test_loss:78.99134826660156\n",
            "2741/3000 train_loss: 18.81715202331543 test_loss:83.56085205078125\n",
            "2742/3000 train_loss: 11.563602447509766 test_loss:91.50038146972656\n",
            "2743/3000 train_loss: 16.959440231323242 test_loss:83.06890106201172\n",
            "2744/3000 train_loss: 18.10651397705078 test_loss:82.0009765625\n",
            "2745/3000 train_loss: 16.234786987304688 test_loss:77.78468322753906\n",
            "2746/3000 train_loss: 15.005546569824219 test_loss:78.05665588378906\n",
            "2747/3000 train_loss: 13.806946754455566 test_loss:93.05992889404297\n",
            "2748/3000 train_loss: 16.145198822021484 test_loss:79.2846450805664\n",
            "2749/3000 train_loss: 17.064414978027344 test_loss:75.89979553222656\n",
            "2750/3000 train_loss: 12.535710334777832 test_loss:89.27804565429688\n",
            "2751/3000 train_loss: 12.512164115905762 test_loss:85.48521423339844\n",
            "2752/3000 train_loss: 12.880287170410156 test_loss:89.88369750976562\n",
            "2753/3000 train_loss: 14.174246788024902 test_loss:83.60657501220703\n",
            "2754/3000 train_loss: 13.128710746765137 test_loss:78.0789794921875\n",
            "2755/3000 train_loss: 14.355892181396484 test_loss:89.6620101928711\n",
            "2756/3000 train_loss: 14.527017593383789 test_loss:83.39427947998047\n",
            "2757/3000 train_loss: 16.259750366210938 test_loss:81.83685302734375\n",
            "2758/3000 train_loss: 18.67535972595215 test_loss:79.9000015258789\n",
            "2759/3000 train_loss: 14.603271484375 test_loss:95.64250183105469\n",
            "2760/3000 train_loss: 15.27033805847168 test_loss:88.63233947753906\n",
            "2761/3000 train_loss: 15.919986724853516 test_loss:87.93504333496094\n",
            "2762/3000 train_loss: 14.497323989868164 test_loss:85.74859619140625\n",
            "2763/3000 train_loss: 14.226065635681152 test_loss:90.75942993164062\n",
            "2764/3000 train_loss: 14.817184448242188 test_loss:79.880615234375\n",
            "2765/3000 train_loss: 14.699918746948242 test_loss:83.1207275390625\n",
            "2766/3000 train_loss: 13.643539428710938 test_loss:99.14215087890625\n",
            "2767/3000 train_loss: 14.663290977478027 test_loss:82.48169708251953\n",
            "2768/3000 train_loss: 14.274054527282715 test_loss:86.56178283691406\n",
            "2769/3000 train_loss: 13.989082336425781 test_loss:80.57655334472656\n",
            "2770/3000 train_loss: 17.417247772216797 test_loss:79.93978881835938\n",
            "2771/3000 train_loss: 16.906583786010742 test_loss:74.59693908691406\n",
            "2772/3000 train_loss: 13.85811996459961 test_loss:77.12129211425781\n",
            "2773/3000 train_loss: 14.103906631469727 test_loss:79.04563903808594\n",
            "2774/3000 train_loss: 13.382534980773926 test_loss:90.24295806884766\n",
            "2775/3000 train_loss: 14.829880714416504 test_loss:78.0268325805664\n",
            "2776/3000 train_loss: 15.987916946411133 test_loss:74.27873992919922\n",
            "2777/3000 train_loss: 13.471281051635742 test_loss:78.18317413330078\n",
            "2778/3000 train_loss: 14.343031883239746 test_loss:80.98323059082031\n",
            "2779/3000 train_loss: 20.35299301147461 test_loss:83.89111328125\n",
            "2780/3000 train_loss: 13.269463539123535 test_loss:86.68414306640625\n",
            "2781/3000 train_loss: 13.168089866638184 test_loss:76.70533752441406\n",
            "2782/3000 train_loss: 13.218276023864746 test_loss:76.65579986572266\n",
            "2783/3000 train_loss: 16.070377349853516 test_loss:78.44242858886719\n",
            "2784/3000 train_loss: 13.396384239196777 test_loss:75.59262084960938\n",
            "2785/3000 train_loss: 16.974586486816406 test_loss:86.22283935546875\n",
            "2786/3000 train_loss: 17.891368865966797 test_loss:76.21337890625\n",
            "2787/3000 train_loss: 14.430741310119629 test_loss:83.02772521972656\n",
            "2788/3000 train_loss: 18.087318420410156 test_loss:81.15591430664062\n",
            "2789/3000 train_loss: 15.23369026184082 test_loss:86.04947662353516\n",
            "2790/3000 train_loss: 15.167293548583984 test_loss:92.30500793457031\n",
            "2791/3000 train_loss: 15.842951774597168 test_loss:83.06657409667969\n",
            "2792/3000 train_loss: 19.87978744506836 test_loss:83.81732177734375\n",
            "2793/3000 train_loss: 12.130575180053711 test_loss:86.5284423828125\n",
            "2794/3000 train_loss: 17.226295471191406 test_loss:86.3701171875\n",
            "2795/3000 train_loss: 14.420921325683594 test_loss:79.55335998535156\n",
            "2796/3000 train_loss: 15.128307342529297 test_loss:85.74211120605469\n",
            "2797/3000 train_loss: 15.306668281555176 test_loss:82.31480407714844\n",
            "2798/3000 train_loss: 16.39780616760254 test_loss:89.28988647460938\n",
            "2799/3000 train_loss: 13.918274879455566 test_loss:87.87031555175781\n",
            "2800/3000 train_loss: 16.524009704589844 test_loss:80.67430877685547\n",
            "2801/3000 train_loss: 14.200864791870117 test_loss:75.59451293945312\n",
            "2802/3000 train_loss: 13.635974884033203 test_loss:79.31525421142578\n",
            "2803/3000 train_loss: 14.564234733581543 test_loss:98.93997192382812\n",
            "2804/3000 train_loss: 14.14391040802002 test_loss:88.62643432617188\n",
            "2805/3000 train_loss: 12.684537887573242 test_loss:77.12305450439453\n",
            "2806/3000 train_loss: 14.900484085083008 test_loss:93.48361206054688\n",
            "2807/3000 train_loss: 13.718233108520508 test_loss:78.96372985839844\n",
            "2808/3000 train_loss: 13.373432159423828 test_loss:79.41851806640625\n",
            "2809/3000 train_loss: 14.905248641967773 test_loss:91.6014175415039\n",
            "2810/3000 train_loss: 18.509366989135742 test_loss:90.08348083496094\n",
            "2811/3000 train_loss: 13.139250755310059 test_loss:76.2947998046875\n",
            "2812/3000 train_loss: 16.669265747070312 test_loss:82.57316589355469\n",
            "2813/3000 train_loss: 14.37607479095459 test_loss:91.96002197265625\n",
            "2814/3000 train_loss: 15.111123085021973 test_loss:83.76524353027344\n",
            "2815/3000 train_loss: 15.021718978881836 test_loss:76.35516357421875\n",
            "2816/3000 train_loss: 11.40542984008789 test_loss:78.27082824707031\n",
            "2817/3000 train_loss: 13.063216209411621 test_loss:79.27549743652344\n",
            "2818/3000 train_loss: 20.803503036499023 test_loss:80.19257354736328\n",
            "2819/3000 train_loss: 12.895575523376465 test_loss:82.81849670410156\n",
            "2820/3000 train_loss: 11.603410720825195 test_loss:79.1238784790039\n",
            "2821/3000 train_loss: 11.93791389465332 test_loss:81.76459503173828\n",
            "2822/3000 train_loss: 12.282174110412598 test_loss:82.76932525634766\n",
            "2823/3000 train_loss: 19.676998138427734 test_loss:74.82398223876953\n",
            "2824/3000 train_loss: 17.763565063476562 test_loss:83.37427520751953\n",
            "2825/3000 train_loss: 17.61465835571289 test_loss:84.87925720214844\n",
            "2826/3000 train_loss: 13.35839557647705 test_loss:79.52886962890625\n",
            "2827/3000 train_loss: 13.76976203918457 test_loss:81.75375366210938\n",
            "2828/3000 train_loss: 12.023468017578125 test_loss:79.392333984375\n",
            "2829/3000 train_loss: 18.106891632080078 test_loss:79.96305084228516\n",
            "2830/3000 train_loss: 15.354616165161133 test_loss:88.67486572265625\n",
            "2831/3000 train_loss: 14.157318115234375 test_loss:79.47561645507812\n",
            "2832/3000 train_loss: 15.29364013671875 test_loss:87.54304504394531\n",
            "2833/3000 train_loss: 18.94435691833496 test_loss:88.01211547851562\n",
            "2834/3000 train_loss: 13.610061645507812 test_loss:79.07172393798828\n",
            "2835/3000 train_loss: 17.120710372924805 test_loss:75.33192443847656\n",
            "2836/3000 train_loss: 15.490471839904785 test_loss:82.41361999511719\n",
            "2837/3000 train_loss: 13.803825378417969 test_loss:78.0325698852539\n",
            "2838/3000 train_loss: 13.921207427978516 test_loss:82.29817199707031\n",
            "2839/3000 train_loss: 15.030599594116211 test_loss:89.45955657958984\n",
            "2840/3000 train_loss: 13.882280349731445 test_loss:83.4541015625\n",
            "2841/3000 train_loss: 13.803109169006348 test_loss:79.28819274902344\n",
            "2842/3000 train_loss: 16.34832191467285 test_loss:84.50285339355469\n",
            "2843/3000 train_loss: 12.128095626831055 test_loss:88.82737731933594\n",
            "2844/3000 train_loss: 14.062577247619629 test_loss:87.75759887695312\n",
            "2845/3000 train_loss: 15.429117202758789 test_loss:79.82235717773438\n",
            "2846/3000 train_loss: 15.933623313903809 test_loss:83.99583435058594\n",
            "2847/3000 train_loss: 16.506908416748047 test_loss:80.44220733642578\n",
            "2848/3000 train_loss: 13.053881645202637 test_loss:85.54998779296875\n",
            "2849/3000 train_loss: 14.884230613708496 test_loss:79.31417083740234\n",
            "2850/3000 train_loss: 17.414581298828125 test_loss:80.48477172851562\n",
            "2851/3000 train_loss: 14.387938499450684 test_loss:85.26924133300781\n",
            "2852/3000 train_loss: 15.101301193237305 test_loss:101.22891235351562\n",
            "2853/3000 train_loss: 18.475276947021484 test_loss:88.72899627685547\n",
            "2854/3000 train_loss: 15.767322540283203 test_loss:89.21464538574219\n",
            "2855/3000 train_loss: 15.398073196411133 test_loss:81.87808227539062\n",
            "2856/3000 train_loss: 13.392457962036133 test_loss:78.57832336425781\n",
            "2857/3000 train_loss: 12.558417320251465 test_loss:86.75727844238281\n",
            "2858/3000 train_loss: 15.353888511657715 test_loss:85.06086730957031\n",
            "2859/3000 train_loss: 18.987812042236328 test_loss:74.6727066040039\n",
            "2860/3000 train_loss: 16.63848114013672 test_loss:92.5977783203125\n",
            "2861/3000 train_loss: 15.612567901611328 test_loss:84.10969543457031\n",
            "2862/3000 train_loss: 14.656437873840332 test_loss:79.74716186523438\n",
            "2863/3000 train_loss: 14.779838562011719 test_loss:90.98532104492188\n",
            "2864/3000 train_loss: 13.393587112426758 test_loss:75.03962707519531\n",
            "2865/3000 train_loss: 12.556767463684082 test_loss:76.05592346191406\n",
            "2866/3000 train_loss: 13.678512573242188 test_loss:72.16847229003906\n",
            "2867/3000 train_loss: 18.732072830200195 test_loss:79.73763275146484\n",
            "2868/3000 train_loss: 16.889724731445312 test_loss:78.73287963867188\n",
            "2869/3000 train_loss: 18.145471572875977 test_loss:89.73548126220703\n",
            "2870/3000 train_loss: 27.699905395507812 test_loss:89.32911682128906\n",
            "2871/3000 train_loss: 17.48322296142578 test_loss:88.76639556884766\n",
            "2872/3000 train_loss: 24.718355178833008 test_loss:88.35066986083984\n",
            "2873/3000 train_loss: 15.65092658996582 test_loss:82.10228729248047\n",
            "2874/3000 train_loss: 14.466432571411133 test_loss:75.95109558105469\n",
            "2875/3000 train_loss: 20.264894485473633 test_loss:88.75784301757812\n",
            "2876/3000 train_loss: 17.3995418548584 test_loss:85.50192260742188\n",
            "2877/3000 train_loss: 20.101402282714844 test_loss:74.77073669433594\n",
            "2878/3000 train_loss: 12.389184951782227 test_loss:78.94283294677734\n",
            "2879/3000 train_loss: 19.184812545776367 test_loss:97.23419952392578\n",
            "2880/3000 train_loss: 20.492359161376953 test_loss:81.56745147705078\n",
            "2881/3000 train_loss: 14.020237922668457 test_loss:81.73082733154297\n",
            "2882/3000 train_loss: 15.116382598876953 test_loss:85.5984115600586\n",
            "2883/3000 train_loss: 13.953267097473145 test_loss:83.15745544433594\n",
            "2884/3000 train_loss: 15.030449867248535 test_loss:81.67463684082031\n",
            "2885/3000 train_loss: 14.758971214294434 test_loss:80.3804931640625\n",
            "2886/3000 train_loss: 15.697742462158203 test_loss:85.25471496582031\n",
            "2887/3000 train_loss: 11.551543235778809 test_loss:86.57275390625\n",
            "2888/3000 train_loss: 11.656120300292969 test_loss:86.85879516601562\n",
            "2889/3000 train_loss: 12.854532241821289 test_loss:81.22991943359375\n",
            "2890/3000 train_loss: 14.771485328674316 test_loss:79.8525390625\n",
            "2891/3000 train_loss: 13.185543060302734 test_loss:90.22738647460938\n",
            "2892/3000 train_loss: 14.199422836303711 test_loss:105.56266784667969\n",
            "2893/3000 train_loss: 18.33817481994629 test_loss:97.34385681152344\n",
            "2894/3000 train_loss: 13.285901069641113 test_loss:81.75804138183594\n",
            "2895/3000 train_loss: 12.864317893981934 test_loss:83.60201263427734\n",
            "2896/3000 train_loss: 11.724791526794434 test_loss:84.79666137695312\n",
            "2897/3000 train_loss: 18.279569625854492 test_loss:83.94300842285156\n",
            "2898/3000 train_loss: 12.719137191772461 test_loss:81.94738006591797\n",
            "2899/3000 train_loss: 13.640022277832031 test_loss:94.30189514160156\n",
            "2900/3000 train_loss: 13.768309593200684 test_loss:91.94779205322266\n",
            "2901/3000 train_loss: 13.168681144714355 test_loss:83.90886688232422\n",
            "2902/3000 train_loss: 13.875919342041016 test_loss:90.65217590332031\n",
            "2903/3000 train_loss: 12.366785049438477 test_loss:77.94438171386719\n",
            "2904/3000 train_loss: 12.378870010375977 test_loss:88.11720275878906\n",
            "2905/3000 train_loss: 12.49467945098877 test_loss:82.44276428222656\n",
            "2906/3000 train_loss: 11.32833480834961 test_loss:87.0013198852539\n",
            "2907/3000 train_loss: 13.347716331481934 test_loss:84.07159423828125\n",
            "2908/3000 train_loss: 20.561100006103516 test_loss:85.10381317138672\n",
            "2909/3000 train_loss: 17.294099807739258 test_loss:82.20133972167969\n",
            "2910/3000 train_loss: 14.928321838378906 test_loss:77.90532684326172\n",
            "2911/3000 train_loss: 14.373015403747559 test_loss:89.01800537109375\n",
            "2912/3000 train_loss: 12.108022689819336 test_loss:86.23728942871094\n",
            "2913/3000 train_loss: 16.157358169555664 test_loss:76.73163604736328\n",
            "2914/3000 train_loss: 16.050373077392578 test_loss:85.56919860839844\n",
            "2915/3000 train_loss: 14.503962516784668 test_loss:83.84065246582031\n",
            "2916/3000 train_loss: 11.863951683044434 test_loss:77.93528747558594\n",
            "2917/3000 train_loss: 13.040940284729004 test_loss:77.96387481689453\n",
            "2918/3000 train_loss: 14.403946876525879 test_loss:78.2826919555664\n",
            "2919/3000 train_loss: 12.133752822875977 test_loss:83.60359954833984\n",
            "2920/3000 train_loss: 16.8564510345459 test_loss:86.96809387207031\n",
            "2921/3000 train_loss: 17.261180877685547 test_loss:83.45602416992188\n",
            "2922/3000 train_loss: 13.840417861938477 test_loss:81.17163848876953\n",
            "2923/3000 train_loss: 11.598651885986328 test_loss:81.49140930175781\n",
            "2924/3000 train_loss: 13.05152416229248 test_loss:79.72317504882812\n",
            "2925/3000 train_loss: 13.862975120544434 test_loss:84.96168518066406\n",
            "2926/3000 train_loss: 14.854390144348145 test_loss:88.78886413574219\n",
            "2927/3000 train_loss: 14.436957359313965 test_loss:90.10539245605469\n",
            "2928/3000 train_loss: 13.268354415893555 test_loss:79.9170150756836\n",
            "2929/3000 train_loss: 13.134482383728027 test_loss:86.846923828125\n",
            "2930/3000 train_loss: 12.67354965209961 test_loss:82.05671691894531\n",
            "2931/3000 train_loss: 11.259554862976074 test_loss:81.48727416992188\n",
            "2932/3000 train_loss: 13.910045623779297 test_loss:83.62010192871094\n",
            "2933/3000 train_loss: 12.62110710144043 test_loss:86.87821197509766\n",
            "2934/3000 train_loss: 11.192421913146973 test_loss:84.55125427246094\n",
            "2935/3000 train_loss: 14.760432243347168 test_loss:83.47589111328125\n",
            "2936/3000 train_loss: 14.199196815490723 test_loss:84.7393798828125\n",
            "2937/3000 train_loss: 13.289807319641113 test_loss:79.24981689453125\n",
            "2938/3000 train_loss: 14.520503044128418 test_loss:83.67535400390625\n",
            "2939/3000 train_loss: 13.227362632751465 test_loss:86.06306457519531\n",
            "2940/3000 train_loss: 12.294755935668945 test_loss:87.45479583740234\n",
            "2941/3000 train_loss: 14.876501083374023 test_loss:80.22822570800781\n",
            "2942/3000 train_loss: 14.629226684570312 test_loss:79.35231018066406\n",
            "2943/3000 train_loss: 14.708183288574219 test_loss:81.50764465332031\n",
            "2944/3000 train_loss: 23.48748779296875 test_loss:80.26313781738281\n",
            "2945/3000 train_loss: 14.335783958435059 test_loss:93.30075073242188\n",
            "2946/3000 train_loss: 18.380699157714844 test_loss:86.36593627929688\n",
            "2947/3000 train_loss: 15.056952476501465 test_loss:88.56236267089844\n",
            "2948/3000 train_loss: 12.24791145324707 test_loss:91.44367980957031\n",
            "2949/3000 train_loss: 12.31615924835205 test_loss:89.62429809570312\n",
            "2950/3000 train_loss: 13.739704132080078 test_loss:82.42137145996094\n",
            "2951/3000 train_loss: 14.741303443908691 test_loss:82.78205108642578\n",
            "2952/3000 train_loss: 14.385970115661621 test_loss:82.32157897949219\n",
            "2953/3000 train_loss: 16.464611053466797 test_loss:87.29083251953125\n",
            "2954/3000 train_loss: 14.26724624633789 test_loss:94.50072479248047\n",
            "2955/3000 train_loss: 15.171734809875488 test_loss:84.40511322021484\n",
            "2956/3000 train_loss: 15.245630264282227 test_loss:78.95011138916016\n",
            "2957/3000 train_loss: 16.66105842590332 test_loss:94.76409912109375\n",
            "2958/3000 train_loss: 16.041240692138672 test_loss:93.12643432617188\n",
            "2959/3000 train_loss: 15.093734741210938 test_loss:80.88018798828125\n",
            "2960/3000 train_loss: 12.699437141418457 test_loss:92.10154724121094\n",
            "2961/3000 train_loss: 12.395818710327148 test_loss:79.24476623535156\n",
            "2962/3000 train_loss: 11.156426429748535 test_loss:91.43028259277344\n",
            "2963/3000 train_loss: 14.29986572265625 test_loss:88.96305084228516\n",
            "2964/3000 train_loss: 11.646060943603516 test_loss:84.10321044921875\n",
            "2965/3000 train_loss: 17.925865173339844 test_loss:87.77812194824219\n",
            "2966/3000 train_loss: 12.51467227935791 test_loss:83.63775634765625\n",
            "2967/3000 train_loss: 13.735660552978516 test_loss:76.12543487548828\n",
            "2968/3000 train_loss: 19.7330265045166 test_loss:104.41234588623047\n",
            "2969/3000 train_loss: 14.653879165649414 test_loss:80.88143920898438\n",
            "2970/3000 train_loss: 15.631694793701172 test_loss:78.15034484863281\n",
            "2971/3000 train_loss: 11.655710220336914 test_loss:87.36343383789062\n",
            "2972/3000 train_loss: 16.219209671020508 test_loss:81.42053985595703\n",
            "2973/3000 train_loss: 11.917350769042969 test_loss:84.04399108886719\n",
            "2974/3000 train_loss: 12.458233833312988 test_loss:76.45349884033203\n",
            "2975/3000 train_loss: 14.173681259155273 test_loss:83.21968841552734\n",
            "2976/3000 train_loss: 12.256290435791016 test_loss:87.7718734741211\n",
            "2977/3000 train_loss: 12.278732299804688 test_loss:85.20172882080078\n",
            "2978/3000 train_loss: 11.823533058166504 test_loss:82.27473449707031\n",
            "2979/3000 train_loss: 15.224957466125488 test_loss:97.58893585205078\n",
            "2980/3000 train_loss: 13.634546279907227 test_loss:84.89938354492188\n",
            "2981/3000 train_loss: 11.925638198852539 test_loss:87.58486938476562\n",
            "2982/3000 train_loss: 13.757275581359863 test_loss:83.69371032714844\n",
            "2983/3000 train_loss: 13.311901092529297 test_loss:79.51288604736328\n",
            "2984/3000 train_loss: 12.158935546875 test_loss:81.81714630126953\n",
            "2985/3000 train_loss: 12.222063064575195 test_loss:81.95162963867188\n",
            "2986/3000 train_loss: 16.24970245361328 test_loss:87.70065307617188\n",
            "2987/3000 train_loss: 13.971517562866211 test_loss:84.99496459960938\n",
            "2988/3000 train_loss: 11.872872352600098 test_loss:87.89750671386719\n",
            "2989/3000 train_loss: 12.657575607299805 test_loss:89.32601928710938\n",
            "2990/3000 train_loss: 14.100112915039062 test_loss:85.96380615234375\n",
            "2991/3000 train_loss: 15.026041984558105 test_loss:82.5160903930664\n",
            "2992/3000 train_loss: 14.585153579711914 test_loss:84.93917083740234\n",
            "2993/3000 train_loss: 15.560877799987793 test_loss:82.7843246459961\n",
            "2994/3000 train_loss: 15.473461151123047 test_loss:76.39779663085938\n",
            "2995/3000 train_loss: 15.178094863891602 test_loss:77.52706146240234\n",
            "2996/3000 train_loss: 15.490717887878418 test_loss:76.46798706054688\n",
            "2997/3000 train_loss: 17.412866592407227 test_loss:82.43521118164062\n",
            "2998/3000 train_loss: 16.0395450592041 test_loss:90.535888671875\n",
            "2999/3000 train_loss: 12.310354232788086 test_loss:82.09896850585938\n",
            "3000/3000 train_loss: 15.684478759765625 test_loss:79.81733703613281\n"
          ]
        }
      ],
      "source": [
        "losses = train(model = unet, optimizer = optimizer, criterion=criterion, data_tr=train_data,\n",
        "               data_val = test_data, scheduler = scheduler,device = device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "6Ew7_F0-q7aL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b012b26-d68a-4323-bc6b-ebf3a1ca072d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(79.8173)"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ],
      "source": [
        "avg_loss = 0\n",
        "# unet.eval()\n",
        "# unet.train()\n",
        "preds = []\n",
        "i = 0\n",
        "test_anomaly_losses = []\n",
        "test_normal_losses = []\n",
        "test_losses = []\n",
        "test_real = y_test.tolist()\n",
        "for batch in test_data:\n",
        "  with torch.no_grad():\n",
        "    # unet.train()\n",
        "    predictions = unet(batch.to(device)).cpu()\n",
        "    preds.append(predictions)\n",
        "  loss = criterion(predictions, batch.cpu())\n",
        "  for j in range(len(predictions)):\n",
        "    if int(y_test[i]) == 1:\n",
        "      test_anomaly_losses.append(float(criterion(predictions[j], batch[j])))\n",
        "    else:\n",
        "      test_normal_losses.append(float(criterion(predictions[j], batch[j])))\n",
        "    i += 1\n",
        "    test_losses.append(criterion(predictions[j], batch[j]))\n",
        "  # print(loss)\n",
        "  # print(loss)\n",
        "  avg_loss += loss / len(test_data)\n",
        "# avg_loss\n",
        "\n",
        "avg_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "VpDKorrRso9o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1115336e-c17a-4aee-cc77-42456557bea3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(156.26770414728108, 14.697947600228446)"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ],
      "source": [
        "sum(test_anomaly_losses)/len(test_anomaly_losses) , sum(test_normal_losses)/len(test_normal_losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "cJE0-57Qts3E"
      },
      "outputs": [],
      "source": [
        "# torch.save(unet, \"unet_fan2_2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LEvbZKYuh7J",
        "outputId": "6a058698-44a4-4c47-a05c-3143d533de0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9511904761904761\n"
          ]
        }
      ],
      "source": [
        "results = []\n",
        "vals = np.arange(40, 900, 0.5).tolist()\n",
        "for threshold in vals:\n",
        "  preds = []\n",
        "  for j in range(len(test_losses)):\n",
        "    if test_losses[j] > threshold:\n",
        "      preds.append(1)\n",
        "    else:\n",
        "      preds.append(0)\n",
        "  \n",
        "  results.append(roc_auc_score(test_real,preds))\n",
        "\n",
        "print(max(results))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "W4H4vpFX35yK"
      },
      "outputs": [],
      "source": [
        "def get_logmelspectrogram(waveform):\n",
        "    melspec = librosa.feature.melspectrogram(y=waveform.numpy(), hop_length=250, n_mels = 304)\n",
        "\n",
        "    logmelspec = librosa.power_to_db(melspec)\n",
        "\n",
        "    return logmelspec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vo1-S_qcuUZR"
      },
      "outputs": [],
      "source": [
        "# train_logmelspecs, test_logmelspecs = mean_logmelspecs(df_train), mean_logmelspecs(df_test)\n",
        "train_data1 = []\n",
        "for wave in df_train:\n",
        "  train_data1.append(get_logmelspectrogram(wave)[0])\n",
        "\n",
        "test_data1 = []\n",
        "for wave in df_test:\n",
        "  test_data1.append(get_logmelspectrogram(wave)[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGd9oI5IEVMx",
        "outputId": "73a79d4e-9e50-4b29-e7ea-55ee60a08c22"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-30-68ec04120629>:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  train_data1 = torch.FloatTensor(train_data1)\n"
          ]
        }
      ],
      "source": [
        "train_data1 = torch.FloatTensor(train_data1)\n",
        "test_data1 = torch.FloatTensor(test_data1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMOi9331OVb4"
      },
      "outputs": [],
      "source": [
        "train_logs = DataLoader(train_data1.reshape(916*304,641),batch_size = 304)\n",
        "test_logs = DataLoader(test_data1.reshape(459*304,641),batch_size = 304)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9rD6tuI1rfe"
      },
      "outputs": [],
      "source": [
        "unet1 = UNet_FC(in_features=641).to(device)\n",
        "optimizer1 = Adam(params = unet1.parameters(), lr = 10e-3)\n",
        "# optimizer = Adam(params = unet.parameters())\n",
        "criterion1 = nn.MSELoss()\n",
        "scheduler1 = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer1, factor=0.5, \n",
        "                                                       min_lr=10e-4, mode = 'min',\n",
        "                                                       patience = 30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qr6k85ma3ftD",
        "outputId": "67174f45-79f9-41f1-8959-a0c75a2b17fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 train_loss: 21.703369140625 test_loss:17.446470260620117\n"
          ]
        }
      ],
      "source": [
        "losses = train(model = unet1, optimizer = optimizer1, criterion=criterion1, data_tr=train_logs,\n",
        "               data_val = test_logs, device = device, epochs = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrPbpKgSPx7v",
        "outputId": "a7bd10e8-e2bd-4579-8211-d6eaaa879711"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(17.4465)"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "avg_loss = 0\n",
        "# unet.eval()\n",
        "# unet.train()\n",
        "preds = []\n",
        "i = 0\n",
        "test_anomaly_losses = []\n",
        "test_normal_losses = []\n",
        "test_losses = []\n",
        "test_real = y_test.tolist()\n",
        "for batch in test_logs:\n",
        "  with torch.no_grad():\n",
        "    # unet.train()\n",
        "    predictions = unet1(batch.to(device)).cpu()\n",
        "    preds.append(predictions)\n",
        "    loss = criterion(predictions, batch.cpu())\n",
        "    test_losses.append(loss)\n",
        "    if int(y_test[i]) == 1:\n",
        "      test_anomaly_losses.append(loss)\n",
        "    else:\n",
        "      test_normal_losses.append(loss)\n",
        "    i += 1\n",
        "  # print(loss)\n",
        "  # print(loss)\n",
        "  avg_loss += loss / len(test_logs)\n",
        "# avg_loss\n",
        "\n",
        "avg_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Z5Z1XYFN_x2",
        "outputId": "9d767817-5525-443f-db72-2588edb4cfbc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor(18.0466), tensor(15.2920))"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sum(test_anomaly_losses)/len(test_anomaly_losses) , sum(test_normal_losses)/len(test_normal_losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "er74WfG7P_B1",
        "outputId": "884ef8f3-8bfe-4d71-bc01-534f5d5a0a03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.5875626740947075\n"
          ]
        }
      ],
      "source": [
        "results = []\n",
        "vals = np.arange(10, 21, 0.1).tolist()\n",
        "for threshold in vals:\n",
        "  preds = []\n",
        "  for j in range(len(test_losses)):\n",
        "    if test_losses[j] > threshold:\n",
        "      preds.append(1)\n",
        "    else:\n",
        "      preds.append(0)\n",
        "  \n",
        "  results.append(roc_auc_score(test_real,preds))\n",
        "\n",
        "print(max(results))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QaSSqG8SbAw2"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}