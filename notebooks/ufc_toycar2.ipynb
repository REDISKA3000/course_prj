{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "9SStKf4G0V5H"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "import torchaudio\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from skimage.util import img_as_ubyte\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import io\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_XtxbKLZq5KX",
    "outputId": "0c6de58c-f736-440b-85ce-a6ad2ae77c29"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CYxHegIM0Z4i",
    "outputId": "8b35e1ba-3d36-4aca-c569-e94fbad47a14"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive/\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "h9DATQwS0ivD"
   },
   "outputs": [],
   "source": [
    "class MimiiDataset(Dataset):\n",
    "    def __init__(self,audio_dir, n_fft = 1024, win_length = 1024,\n",
    "                 hop_length = 512,power = 2,n_mels = 128,pad_mode = 'reflect',\n",
    "                 sr = 16000,center = True,norm = None):\n",
    "      \n",
    "        super(MimiiDataset, self).__init__()\n",
    "        self.audio_dir = audio_dir\n",
    "        self.n_mels = n_mels\n",
    "        self.n_fft = n_fft\n",
    "        self.win_length = win_length\n",
    "        self.hop_length = hop_length\n",
    "        self.power = power\n",
    "        self.pad_mode = pad_mode\n",
    "        self.sr = sr\n",
    "        self.center = center\n",
    "        self.norm = norm\n",
    "\n",
    "    def get_files(self):\n",
    "       return self.train_files, self.test_files\n",
    "    \n",
    "    def get_data(self,device, id):\n",
    "        \n",
    "        self.train_files, self.train_labels = self._train_file_list(device, id)\n",
    "        self.test_files, self.test_labels = self._test_file_list(device, id)\n",
    "        \n",
    "        self.train_data = self.get_audios(self.train_files)\n",
    "        self.test_data = self.get_audios(self.test_files)\n",
    "        \n",
    "        return self.train_data, self.test_data, self.train_labels, self.test_labels\n",
    "    \n",
    "    def _train_file_list(self, device, id):\n",
    "        query = os.path.abspath(\n",
    "            f\"{self.audio_dir}/{device}/train/normal_id_0{id}*.wav\"\n",
    "        )\n",
    "        train_normal_files = sorted(glob.glob(query))\n",
    "        train_normal_labels = np.zeros(len(train_normal_files))\n",
    "        \n",
    "        query = os.path.abspath(\n",
    "                f\"{self.audio_dir}/{device}/train/anomaly_id_0{id}*.wav\"\n",
    "            )\n",
    "        train_anomaly_files = sorted(glob.glob(query))\n",
    "        train_anomaly_labels = np.ones(len(train_anomaly_files))\n",
    "        \n",
    "        train_file_list = np.concatenate((train_normal_files, train_anomaly_files), axis=0)\n",
    "        train_labels = np.concatenate((train_normal_labels, train_anomaly_labels), axis=0)\n",
    "        \n",
    "        return train_file_list, train_labels\n",
    "    \n",
    "    def _test_file_list(self, device, id):     \n",
    "        query = os.path.abspath(\n",
    "            f\"{self.audio_dir}/{device}/test/normal_id_0{id}*.wav\"\n",
    "            )\n",
    "        test_normal_files = sorted(glob.glob(query))\n",
    "        test_normal_labels = np.zeros(len(test_normal_files))\n",
    "        \n",
    "        query = os.path.abspath(\n",
    "            f\"{self.audio_dir}/{device}/test/anomaly_id_0{id}*.wav\"\n",
    "            )\n",
    "        test_anomaly_files = sorted(glob.glob(query))\n",
    "        test_anomaly_labels = np.ones(len(test_anomaly_files))\n",
    "        \n",
    "        test_file_list = np.concatenate((test_normal_files, \n",
    "                                          test_anomaly_files), axis=0)\n",
    "        test_labels = np.concatenate((test_normal_labels,\n",
    "                                      test_anomaly_labels), axis=0)\n",
    "          \n",
    "        return test_file_list, test_labels\n",
    "\n",
    "    def normalize(self,tensor):\n",
    "        tensor_minusmean = tensor - tensor.mean()\n",
    "        return tensor_minusmean/np.absolute(tensor_minusmean).max()\n",
    "\n",
    "    def make0min(self,tensornd):\n",
    "        tensor = tensornd.numpy()\n",
    "        res = np.where(tensor == 0, 1E-19 , tensor)\n",
    "        return torch.from_numpy(res)\n",
    "\n",
    "    def spectrogrameToImage(self,specgram):\n",
    "        # specgram = torchaudio.transforms.MelSpectrogram(n_fft=1024, win_length=1024, \n",
    "        #                                                 hop_length=512, power=2, \n",
    "        #                                                 normalized=True, n_mels=128)(waveform )\n",
    "        specgram= self.make0min(specgram)\n",
    "        specgram = specgram.log2()[0,:,:].numpy()\n",
    "        \n",
    "        tr2image = transforms.Compose([transforms.ToPILImage()])\n",
    "\n",
    "        specgram= self.normalize(specgram)\n",
    "        # specgram = img_as_ubyte(specgram)\n",
    "        specgramImage = tr2image(specgram)\n",
    "        return specgramImage\n",
    "\n",
    "    def get_logmelspectrogram(self, waveform):\n",
    "        melspec = librosa.feature.melspectrogram(\n",
    "          n_fft=self.n_fft, win_length=self.win_length, hop_length=self.hop_length,\n",
    "          power=self.power,n_mels=self.n_mels,pad_mode=self.pad_mode,sr=self.sr,\n",
    "          center=self.center,norm=self.norm,htk=True,\n",
    "          y = waveform.numpy()\n",
    "        )\n",
    "\n",
    "        logmelspec = librosa.power_to_db(melspec)\n",
    "\n",
    "        return logmelspec\n",
    "\n",
    "    def get_melspectrogram(self,waveform):\n",
    "        melspec = librosa.feature.melspectrogram(\n",
    "            n_fft=self.n_fft, win_length=self.win_length, hop_length=self.hop_length,\n",
    "            power=self.power,n_mels=self.n_mels,pad_mode=self.pad_mode,sr=self.sr,\n",
    "            center=self.center,norm=self.norm,htk=True,\n",
    "            y = waveform.numpy()\n",
    "        )\n",
    "\n",
    "        return melspec\n",
    "    \n",
    "    def get_mfcc(self,waveform):\n",
    "        mfcc = librosa.feature.mfcc(    \n",
    "            n_fft=self.n_fft, win_length=self.win_length, \n",
    "            hop_length=self.hop_length,pad_mode=self.pad_mode,sr=self.sr,\n",
    "            center=self.center,norm=self.norm,n_mfcc=40,\n",
    "            y = waveform.numpy()\n",
    "        )\n",
    "\n",
    "        return mfcc\n",
    "\n",
    "    def get_chroma_stft(self,waveform):\n",
    "        stft = librosa.feature.chroma_stft(\n",
    "            n_fft=self.n_fft, win_length=self.win_length, \n",
    "            hop_length=self.hop_length,pad_mode=self.pad_mode,sr=self.sr,\n",
    "            center=self.center,norm=self.norm,n_chroma=12,\n",
    "            y=waveform.numpy()\n",
    "        )\n",
    "\n",
    "        return stft\n",
    "\n",
    "    def get_spectral_contrast(self,waveform):\n",
    "        spec_contrast = librosa.feature.spectral_contrast(    \n",
    "            n_fft=self.n_fft, win_length=self.win_length,center=self.center,\n",
    "            hop_length=self.hop_length,pad_mode=self.pad_mode,sr=self.sr,\n",
    "            y = waveform.numpy()\n",
    "        )\n",
    "\n",
    "        return spec_contrast\n",
    "    \n",
    "    def get_tonnetz(self,waveform):\n",
    "        harmonic = librosa.effects.harmonic(waveform.numpy())\n",
    "        tonnetz = librosa.feature.tonnetz(y=harmonic,sr=self.sr)\n",
    "\n",
    "        return tonnetz\n",
    "\n",
    "    def get_audios(self, file_list):\n",
    "        data = []\n",
    "        for i in range(len(file_list)):\n",
    "          y, sr = torchaudio.load(file_list[i])  \n",
    "          data.append(y)\n",
    "\n",
    "        return data\n",
    "    def _derive_data(self, file_list):\n",
    "        train_data = []\n",
    "        test_data = []\n",
    "        train_mode = True\n",
    "        for file_list in [self.train_files, self.test_files]:\n",
    "          tr2tensor = transforms.Compose([transforms.PILToTensor()])\n",
    "          data = []\n",
    "          for j in range(len(file_list)):\n",
    "            y, sr = torchaudio.load(file_list[j])  \n",
    "            spec = self.get_melspectrogram(y)\n",
    "            spec = self.spectrogrameToImage(spec)\n",
    "            spec = spec.convert('RGB')\n",
    "            vectors = tr2tensor(spec)\n",
    "            if train_mode:     \n",
    "              train_data.append(vectors)\n",
    "            else:\n",
    "              test_data.append(vectors)\n",
    "            \n",
    "          train_mode = False\n",
    "                \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "S96soeIc0o13"
   },
   "outputs": [],
   "source": [
    "dataset = MimiiDataset('/content/drive/MyDrive/mimii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "Gn2zdn92doi1"
   },
   "outputs": [],
   "source": [
    "_, _, y_train, y_test = dataset.get_data('ToyCar', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "SgjpeWy_RV1C"
   },
   "outputs": [],
   "source": [
    "train_mixed_f = torch.load('/content/drive/MyDrive/mixed_features/train_mf_toycar2.pt')\n",
    "test_mixed_f = torch.load('/content/drive/MyDrive/mixed_features/test_mf_toycar2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "jWMPVGu1qiEq"
   },
   "outputs": [],
   "source": [
    "train_data = DataLoader(train_mixed_f, batch_size=64, shuffle = True)\n",
    "test_data = DataLoader(test_mixed_f, batch_size = 64, shuffle= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "vNTBTRe6qnBq"
   },
   "outputs": [],
   "source": [
    "class UNet_FC(nn.Module):\n",
    "\n",
    "  def __init__(self, in_features):\n",
    "    super().__init__()\n",
    "    \n",
    "    self.bn = nn.BatchNorm1d(128)\n",
    "    self.relu = nn.ReLU()\n",
    "\n",
    "    self.fc0 = nn.Linear(in_features=in_features,out_features=in_features)\n",
    "\n",
    "    # encoder\n",
    "    self.fc1 = nn.Linear(in_features=in_features, out_features=128)\n",
    "    self.fc2 = nn.Linear(in_features=128,out_features=128)\n",
    "    self.fc3 = nn.Linear(in_features=128, out_features=128)\n",
    "    self.fc4 = nn.Linear(in_features=128, out_features=128)\n",
    "    self.fc5 = nn.Linear(in_features=128, out_features=8)\n",
    "\n",
    "    # decoder\n",
    "    self.fc6 = nn.Linear(in_features=8, out_features=128)\n",
    "    self.fc7 = nn.Linear(in_features=128*2, out_features=128)\n",
    "    self.fc8 = nn.Linear(in_features=128*2, out_features=128)\n",
    "    self.fc9 = nn.Linear(in_features=128*2, out_features=128)\n",
    "\n",
    "    self.out = nn.Linear(in_features=128*2, out_features=in_features)\n",
    "\n",
    "  def forward(self, x):\n",
    "    input = self.fc0(x)\n",
    "\n",
    "    x1 = self.relu(self.bn(self.fc1(input)))\n",
    "    x2 = self.relu(self.bn(self.fc2(x1)))\n",
    "    x3 = self.relu(self.bn(self.fc3(x2)))\n",
    "    x4 = self.relu(self.bn(self.fc4(x3)))\n",
    "    x5 = self.relu(self.fc5(x4))\n",
    "\n",
    "    xy = [x5, x4, x3, x2, x1]\n",
    "\n",
    "    x6 = self.relu(self.fc6(xy[0]))\n",
    "    con1 = torch.cat((x6,xy[1]), 1) \n",
    "    x7 = self.relu(self.bn(self.fc7(con1)))\n",
    "    con2 = torch.cat((x7,xy[2]), 1)\n",
    "    x8 = self.relu(self.bn(self.fc8(con2)))\n",
    "    con3 = torch.cat((x8,xy[3]), 1)\n",
    "    x9 = self.relu(self.bn(self.fc9(con3)))\n",
    "    con4 = torch.cat((x9,xy[4]), 1)\n",
    "\n",
    "    x10 = self.out(con4)\n",
    "\n",
    "    return x10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "ZfgcBtQ3qn5l"
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, data_tr, data_val, scheduler = None,\n",
    "          epochs = 3000, device = 'cpu'):\n",
    "    # X_val, Y_val = next(iter(data_val))\n",
    "    losses = []\n",
    "    prev_avg_loss = 100000\n",
    "    for epoch in range(epochs):\n",
    "        train_avg_loss = 0\n",
    "        test_avg_loss = 0\n",
    "        # model.train()  # train mode\n",
    "        for batch in data_tr:\n",
    "          # data to device\n",
    "          batch = batch.to(device)\n",
    "          # set parameter gradients to zero\n",
    "          optimizer.zero_grad()\n",
    "          # forward\n",
    "          # print(Y_batch.shape)\n",
    "          predictions = model(batch)\n",
    "          loss = criterion(predictions, batch)\n",
    "          loss.backward() # backward-pass\n",
    "          optimizer.step()  # update weights\n",
    "          # calculate loss to show the user\n",
    "          if scheduler:\n",
    "            scheduler.step(loss)\n",
    "          train_avg_loss += loss / len(data_tr)\n",
    "\n",
    "        # model.eval()\n",
    "        for batch in data_val:\n",
    "          with torch.no_grad():\n",
    "            preds = model(batch.to(device)).cpu()\n",
    "            loss = criterion(preds,batch)\n",
    "            test_avg_loss += loss / len(data_val)\n",
    "                    \n",
    "        losses.append(train_avg_loss.item())\n",
    "        # if (epoch+1)%50 == 0:\n",
    "        print(\"{}/{} train_loss: {} test_loss:{}\".format(epoch+1, epochs, train_avg_loss, test_avg_loss))\n",
    "        # if test_avg_loss < 70:\n",
    "        #   break\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "ptkVTF55quOL"
   },
   "outputs": [],
   "source": [
    "unet = UNet_FC(in_features=193).to(device)\n",
    "optimizer = Adam(params = unet.parameters(), lr = 10e-3)\n",
    "# optimizer = Adam(params = unet.parameters())\n",
    "criterion = nn.MSELoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, \n",
    "                                                       min_lr=10e-4, mode = 'min',\n",
    "                                                       patience = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QkfmYl9oXhcB",
    "outputId": "ad12863f-56e0-4e9a-c386-ee080fe9fc02"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1/3000 train_loss: 286259.96875 test_loss:277565.25\n",
      "2/3000 train_loss: 282303.375 test_loss:272648.4375\n",
      "3/3000 train_loss: 276440.0625 test_loss:265730.78125\n",
      "4/3000 train_loss: 268514.90625 test_loss:256738.25\n",
      "5/3000 train_loss: 258145.4375 test_loss:245131.203125\n",
      "6/3000 train_loss: 245080.46875 test_loss:230924.6875\n",
      "7/3000 train_loss: 229352.015625 test_loss:213966.03125\n",
      "8/3000 train_loss: 211374.28125 test_loss:195899.90625\n",
      "9/3000 train_loss: 192580.8125 test_loss:177029.71875\n",
      "10/3000 train_loss: 173449.6875 test_loss:158060.4375\n",
      "11/3000 train_loss: 153803.703125 test_loss:139190.421875\n",
      "12/3000 train_loss: 135116.171875 test_loss:120883.8515625\n",
      "13/3000 train_loss: 116991.171875 test_loss:103890.15625\n",
      "14/3000 train_loss: 99609.90625 test_loss:87058.1875\n",
      "15/3000 train_loss: 82449.6328125 test_loss:70230.703125\n",
      "16/3000 train_loss: 65954.359375 test_loss:55294.5859375\n",
      "17/3000 train_loss: 52091.4453125 test_loss:43244.44921875\n",
      "18/3000 train_loss: 40323.6640625 test_loss:32550.51953125\n",
      "19/3000 train_loss: 30315.544921875 test_loss:23927.2421875\n",
      "20/3000 train_loss: 22971.34375 test_loss:21495.619140625\n",
      "21/3000 train_loss: 17278.798828125 test_loss:12322.9921875\n",
      "22/3000 train_loss: 11258.783203125 test_loss:7937.17822265625\n",
      "23/3000 train_loss: 7483.3408203125 test_loss:5119.505859375\n",
      "24/3000 train_loss: 4905.830078125 test_loss:3265.57861328125\n",
      "25/3000 train_loss: 3076.41796875 test_loss:2110.96533203125\n",
      "26/3000 train_loss: 2020.5093994140625 test_loss:1379.15234375\n",
      "27/3000 train_loss: 1297.54638671875 test_loss:876.1354370117188\n",
      "28/3000 train_loss: 917.6461181640625 test_loss:592.3128662109375\n",
      "29/3000 train_loss: 628.3401489257812 test_loss:502.7370300292969\n",
      "30/3000 train_loss: 418.86761474609375 test_loss:412.0229187011719\n",
      "31/3000 train_loss: 358.0301513671875 test_loss:367.16510009765625\n",
      "32/3000 train_loss: 346.0891418457031 test_loss:345.68060302734375\n",
      "33/3000 train_loss: 273.8702697753906 test_loss:333.9544677734375\n",
      "34/3000 train_loss: 233.80393981933594 test_loss:351.0164794921875\n",
      "35/3000 train_loss: 240.00369262695312 test_loss:335.533935546875\n",
      "36/3000 train_loss: 231.83641052246094 test_loss:317.3578796386719\n",
      "37/3000 train_loss: 221.43597412109375 test_loss:338.38677978515625\n",
      "38/3000 train_loss: 223.1126708984375 test_loss:320.97906494140625\n",
      "39/3000 train_loss: 209.29592895507812 test_loss:320.51397705078125\n",
      "40/3000 train_loss: 202.013916015625 test_loss:316.81488037109375\n",
      "41/3000 train_loss: 232.721435546875 test_loss:322.0128479003906\n",
      "42/3000 train_loss: 219.80661010742188 test_loss:317.9590759277344\n",
      "43/3000 train_loss: 196.5849609375 test_loss:325.1103515625\n",
      "44/3000 train_loss: 244.05364990234375 test_loss:319.12286376953125\n",
      "45/3000 train_loss: 213.0984649658203 test_loss:324.77142333984375\n",
      "46/3000 train_loss: 228.9568634033203 test_loss:321.7420349121094\n",
      "47/3000 train_loss: 187.64430236816406 test_loss:324.69207763671875\n",
      "48/3000 train_loss: 209.517578125 test_loss:323.2816162109375\n",
      "49/3000 train_loss: 197.0996856689453 test_loss:323.49078369140625\n",
      "50/3000 train_loss: 179.14501953125 test_loss:321.5356140136719\n",
      "51/3000 train_loss: 201.4308319091797 test_loss:320.005615234375\n",
      "52/3000 train_loss: 180.58843994140625 test_loss:327.708740234375\n",
      "53/3000 train_loss: 178.8979949951172 test_loss:319.26129150390625\n",
      "54/3000 train_loss: 168.07437133789062 test_loss:320.7018737792969\n",
      "55/3000 train_loss: 201.8348388671875 test_loss:326.15582275390625\n",
      "56/3000 train_loss: 168.1799774169922 test_loss:318.41986083984375\n",
      "57/3000 train_loss: 187.75143432617188 test_loss:319.382568359375\n",
      "58/3000 train_loss: 181.48068237304688 test_loss:317.0914001464844\n",
      "59/3000 train_loss: 208.12411499023438 test_loss:321.6755065917969\n",
      "60/3000 train_loss: 184.5755615234375 test_loss:315.38629150390625\n",
      "61/3000 train_loss: 193.42918395996094 test_loss:317.3949279785156\n",
      "62/3000 train_loss: 177.25643920898438 test_loss:319.01568603515625\n",
      "63/3000 train_loss: 172.5236358642578 test_loss:312.72662353515625\n",
      "64/3000 train_loss: 182.0225372314453 test_loss:320.58392333984375\n",
      "65/3000 train_loss: 190.1216583251953 test_loss:314.3038330078125\n",
      "66/3000 train_loss: 183.75204467773438 test_loss:320.1071472167969\n",
      "67/3000 train_loss: 168.8459930419922 test_loss:309.929931640625\n",
      "68/3000 train_loss: 175.12451171875 test_loss:316.9924621582031\n",
      "69/3000 train_loss: 182.21397399902344 test_loss:312.46624755859375\n",
      "70/3000 train_loss: 170.26431274414062 test_loss:312.07464599609375\n",
      "71/3000 train_loss: 192.75106811523438 test_loss:314.4559020996094\n",
      "72/3000 train_loss: 174.30859375 test_loss:310.6015319824219\n",
      "73/3000 train_loss: 186.41146850585938 test_loss:308.3904113769531\n",
      "74/3000 train_loss: 184.77603149414062 test_loss:311.044921875\n",
      "75/3000 train_loss: 165.50137329101562 test_loss:313.81060791015625\n",
      "76/3000 train_loss: 181.73207092285156 test_loss:308.05078125\n",
      "77/3000 train_loss: 191.2892608642578 test_loss:311.7285461425781\n",
      "78/3000 train_loss: 210.3011016845703 test_loss:319.45037841796875\n",
      "79/3000 train_loss: 175.47021484375 test_loss:306.9684143066406\n",
      "80/3000 train_loss: 196.22885131835938 test_loss:306.48248291015625\n",
      "81/3000 train_loss: 166.8350372314453 test_loss:310.9754638671875\n",
      "82/3000 train_loss: 161.24755859375 test_loss:305.0147705078125\n",
      "83/3000 train_loss: 170.44065856933594 test_loss:311.2469177246094\n",
      "84/3000 train_loss: 161.902587890625 test_loss:305.0177001953125\n",
      "85/3000 train_loss: 187.18084716796875 test_loss:304.26483154296875\n",
      "86/3000 train_loss: 164.98513793945312 test_loss:309.08245849609375\n",
      "87/3000 train_loss: 165.39141845703125 test_loss:305.7357177734375\n",
      "88/3000 train_loss: 222.90097045898438 test_loss:309.58099365234375\n",
      "89/3000 train_loss: 180.70782470703125 test_loss:304.4764404296875\n",
      "90/3000 train_loss: 162.58299255371094 test_loss:304.51214599609375\n",
      "91/3000 train_loss: 163.35647583007812 test_loss:304.0030822753906\n",
      "92/3000 train_loss: 174.0895538330078 test_loss:301.038818359375\n",
      "93/3000 train_loss: 169.0555877685547 test_loss:304.0956115722656\n",
      "94/3000 train_loss: 156.5150146484375 test_loss:299.3560791015625\n",
      "95/3000 train_loss: 181.04095458984375 test_loss:306.1502685546875\n",
      "96/3000 train_loss: 200.1145477294922 test_loss:300.3428955078125\n",
      "97/3000 train_loss: 174.3671112060547 test_loss:305.9214172363281\n",
      "98/3000 train_loss: 176.63714599609375 test_loss:303.97052001953125\n",
      "99/3000 train_loss: 160.2607421875 test_loss:300.8773193359375\n",
      "100/3000 train_loss: 205.32977294921875 test_loss:300.0511779785156\n",
      "101/3000 train_loss: 160.02752685546875 test_loss:303.3766174316406\n",
      "102/3000 train_loss: 179.9110565185547 test_loss:299.3822326660156\n",
      "103/3000 train_loss: 164.64834594726562 test_loss:296.13623046875\n",
      "104/3000 train_loss: 170.70626831054688 test_loss:293.3684387207031\n",
      "105/3000 train_loss: 163.65313720703125 test_loss:296.11749267578125\n",
      "106/3000 train_loss: 164.0347137451172 test_loss:301.89599609375\n",
      "107/3000 train_loss: 173.63587951660156 test_loss:294.722900390625\n",
      "108/3000 train_loss: 155.14801025390625 test_loss:293.09637451171875\n",
      "109/3000 train_loss: 159.99388122558594 test_loss:293.1928405761719\n",
      "110/3000 train_loss: 156.85516357421875 test_loss:294.6641540527344\n",
      "111/3000 train_loss: 165.9230499267578 test_loss:293.4940490722656\n",
      "112/3000 train_loss: 175.06983947753906 test_loss:297.46875\n",
      "113/3000 train_loss: 178.17727661132812 test_loss:293.26177978515625\n",
      "114/3000 train_loss: 160.9894256591797 test_loss:294.7656555175781\n",
      "115/3000 train_loss: 173.929931640625 test_loss:292.8330383300781\n",
      "116/3000 train_loss: 179.14453125 test_loss:296.5108947753906\n",
      "117/3000 train_loss: 189.74072265625 test_loss:296.0550231933594\n",
      "118/3000 train_loss: 165.41360473632812 test_loss:295.9423828125\n",
      "119/3000 train_loss: 165.68309020996094 test_loss:295.3214111328125\n",
      "120/3000 train_loss: 177.10691833496094 test_loss:291.8631591796875\n",
      "121/3000 train_loss: 192.4847869873047 test_loss:302.8675842285156\n",
      "122/3000 train_loss: 158.0293731689453 test_loss:298.628173828125\n",
      "123/3000 train_loss: 176.08987426757812 test_loss:295.67938232421875\n",
      "124/3000 train_loss: 168.8621826171875 test_loss:292.3534240722656\n",
      "125/3000 train_loss: 150.5122528076172 test_loss:294.4217529296875\n",
      "126/3000 train_loss: 158.525634765625 test_loss:295.01251220703125\n",
      "127/3000 train_loss: 156.21197509765625 test_loss:289.8451232910156\n",
      "128/3000 train_loss: 178.20921325683594 test_loss:289.6719055175781\n",
      "129/3000 train_loss: 169.816650390625 test_loss:287.564453125\n",
      "130/3000 train_loss: 151.79457092285156 test_loss:286.4551086425781\n",
      "131/3000 train_loss: 156.29100036621094 test_loss:287.49151611328125\n",
      "132/3000 train_loss: 151.5765380859375 test_loss:285.7478332519531\n",
      "133/3000 train_loss: 157.10064697265625 test_loss:281.9883117675781\n",
      "134/3000 train_loss: 163.3371124267578 test_loss:287.0832214355469\n",
      "135/3000 train_loss: 147.9397430419922 test_loss:290.28155517578125\n",
      "136/3000 train_loss: 183.37850952148438 test_loss:285.6292419433594\n",
      "137/3000 train_loss: 150.39483642578125 test_loss:288.0216979980469\n",
      "138/3000 train_loss: 159.585693359375 test_loss:286.18902587890625\n",
      "139/3000 train_loss: 145.1283721923828 test_loss:283.26544189453125\n",
      "140/3000 train_loss: 145.85142517089844 test_loss:286.9513854980469\n",
      "141/3000 train_loss: 166.03561401367188 test_loss:286.51593017578125\n",
      "142/3000 train_loss: 154.90545654296875 test_loss:283.3770446777344\n",
      "143/3000 train_loss: 142.31747436523438 test_loss:280.2340393066406\n",
      "144/3000 train_loss: 163.62840270996094 test_loss:282.1165771484375\n",
      "145/3000 train_loss: 145.05796813964844 test_loss:283.5500793457031\n",
      "146/3000 train_loss: 148.42567443847656 test_loss:283.9958801269531\n",
      "147/3000 train_loss: 155.08651733398438 test_loss:275.6946105957031\n",
      "148/3000 train_loss: 161.62942504882812 test_loss:284.4631042480469\n",
      "149/3000 train_loss: 164.8675994873047 test_loss:284.778564453125\n",
      "150/3000 train_loss: 160.34747314453125 test_loss:281.3425598144531\n",
      "151/3000 train_loss: 148.6964569091797 test_loss:283.15753173828125\n",
      "152/3000 train_loss: 172.7677459716797 test_loss:281.699462890625\n",
      "153/3000 train_loss: 178.6064910888672 test_loss:279.7523193359375\n",
      "154/3000 train_loss: 146.91842651367188 test_loss:285.7097473144531\n",
      "155/3000 train_loss: 157.46275329589844 test_loss:278.7146301269531\n",
      "156/3000 train_loss: 149.09768676757812 test_loss:280.7615966796875\n",
      "157/3000 train_loss: 139.56817626953125 test_loss:277.06597900390625\n",
      "158/3000 train_loss: 153.87388610839844 test_loss:280.2086181640625\n",
      "159/3000 train_loss: 164.69496154785156 test_loss:283.51324462890625\n",
      "160/3000 train_loss: 152.66790771484375 test_loss:283.85699462890625\n",
      "161/3000 train_loss: 142.78057861328125 test_loss:278.3332214355469\n",
      "162/3000 train_loss: 143.49728393554688 test_loss:277.15789794921875\n",
      "163/3000 train_loss: 138.72264099121094 test_loss:273.40667724609375\n",
      "164/3000 train_loss: 147.53323364257812 test_loss:274.709716796875\n",
      "165/3000 train_loss: 171.3249053955078 test_loss:276.40380859375\n",
      "166/3000 train_loss: 142.13893127441406 test_loss:277.873779296875\n",
      "167/3000 train_loss: 157.1956787109375 test_loss:276.6292724609375\n",
      "168/3000 train_loss: 140.40548706054688 test_loss:272.1580810546875\n",
      "169/3000 train_loss: 158.55050659179688 test_loss:275.8956604003906\n",
      "170/3000 train_loss: 140.93472290039062 test_loss:271.26324462890625\n",
      "171/3000 train_loss: 134.759033203125 test_loss:268.9985656738281\n",
      "172/3000 train_loss: 147.1523895263672 test_loss:274.0040588378906\n",
      "173/3000 train_loss: 138.85797119140625 test_loss:276.3431396484375\n",
      "174/3000 train_loss: 149.91412353515625 test_loss:276.01470947265625\n",
      "175/3000 train_loss: 129.48744201660156 test_loss:271.8095703125\n",
      "176/3000 train_loss: 162.74978637695312 test_loss:274.2896423339844\n",
      "177/3000 train_loss: 170.78244018554688 test_loss:280.78582763671875\n",
      "178/3000 train_loss: 143.82176208496094 test_loss:278.6461181640625\n",
      "179/3000 train_loss: 146.45440673828125 test_loss:273.30877685546875\n",
      "180/3000 train_loss: 150.5823516845703 test_loss:273.3562316894531\n",
      "181/3000 train_loss: 143.66236877441406 test_loss:272.04864501953125\n",
      "182/3000 train_loss: 147.61114501953125 test_loss:274.08380126953125\n",
      "183/3000 train_loss: 158.83502197265625 test_loss:276.1852111816406\n",
      "184/3000 train_loss: 154.4519500732422 test_loss:273.8121337890625\n",
      "185/3000 train_loss: 142.12062072753906 test_loss:270.13934326171875\n",
      "186/3000 train_loss: 146.6392059326172 test_loss:274.7916259765625\n",
      "187/3000 train_loss: 193.91366577148438 test_loss:271.18377685546875\n",
      "188/3000 train_loss: 142.83042907714844 test_loss:271.0552978515625\n",
      "189/3000 train_loss: 135.19540405273438 test_loss:270.8011779785156\n",
      "190/3000 train_loss: 361.0338439941406 test_loss:272.880126953125\n",
      "191/3000 train_loss: 222.57449340820312 test_loss:306.2527770996094\n",
      "192/3000 train_loss: 185.73641967773438 test_loss:275.65087890625\n",
      "193/3000 train_loss: 161.3271942138672 test_loss:275.59295654296875\n",
      "194/3000 train_loss: 150.01463317871094 test_loss:265.67156982421875\n",
      "195/3000 train_loss: 137.7518310546875 test_loss:268.6611328125\n",
      "196/3000 train_loss: 152.67672729492188 test_loss:271.00439453125\n",
      "197/3000 train_loss: 163.96002197265625 test_loss:274.3328857421875\n",
      "198/3000 train_loss: 158.26361083984375 test_loss:271.1687316894531\n",
      "199/3000 train_loss: 145.05020141601562 test_loss:281.1929016113281\n",
      "200/3000 train_loss: 150.34829711914062 test_loss:270.0818176269531\n",
      "201/3000 train_loss: 147.04766845703125 test_loss:270.3279724121094\n",
      "202/3000 train_loss: 137.55052185058594 test_loss:269.8267822265625\n",
      "203/3000 train_loss: 132.25930786132812 test_loss:267.61602783203125\n",
      "204/3000 train_loss: 152.55442810058594 test_loss:273.4530334472656\n",
      "205/3000 train_loss: 144.31771850585938 test_loss:265.6698303222656\n",
      "206/3000 train_loss: 150.64634704589844 test_loss:270.5142517089844\n",
      "207/3000 train_loss: 139.65098571777344 test_loss:262.0965270996094\n",
      "208/3000 train_loss: 143.71754455566406 test_loss:265.6920471191406\n",
      "209/3000 train_loss: 150.84185791015625 test_loss:264.0205078125\n",
      "210/3000 train_loss: 127.65656280517578 test_loss:272.78369140625\n",
      "211/3000 train_loss: 129.5158233642578 test_loss:262.06878662109375\n",
      "212/3000 train_loss: 129.5067138671875 test_loss:261.1021728515625\n",
      "213/3000 train_loss: 124.30776977539062 test_loss:257.6014404296875\n",
      "214/3000 train_loss: 131.27684020996094 test_loss:259.4503173828125\n",
      "215/3000 train_loss: 139.75335693359375 test_loss:259.7806701660156\n",
      "216/3000 train_loss: 132.66075134277344 test_loss:264.8859558105469\n",
      "217/3000 train_loss: 145.28285217285156 test_loss:269.18804931640625\n",
      "218/3000 train_loss: 138.68885803222656 test_loss:261.81610107421875\n",
      "219/3000 train_loss: 140.2079620361328 test_loss:265.52081298828125\n",
      "220/3000 train_loss: 147.87164306640625 test_loss:265.6647033691406\n",
      "221/3000 train_loss: 127.12664031982422 test_loss:268.4052734375\n",
      "222/3000 train_loss: 128.5960693359375 test_loss:264.5381164550781\n",
      "223/3000 train_loss: 143.31903076171875 test_loss:262.0687255859375\n",
      "224/3000 train_loss: 136.79812622070312 test_loss:256.51837158203125\n",
      "225/3000 train_loss: 128.60243225097656 test_loss:257.06573486328125\n",
      "226/3000 train_loss: 140.72523498535156 test_loss:261.1758728027344\n",
      "227/3000 train_loss: 128.5473175048828 test_loss:255.44883728027344\n",
      "228/3000 train_loss: 146.66957092285156 test_loss:259.0594177246094\n",
      "229/3000 train_loss: 133.1913299560547 test_loss:262.4927978515625\n",
      "230/3000 train_loss: 128.07666015625 test_loss:260.750244140625\n",
      "231/3000 train_loss: 133.131103515625 test_loss:259.896240234375\n",
      "232/3000 train_loss: 127.88059997558594 test_loss:260.1116638183594\n",
      "233/3000 train_loss: 121.6411361694336 test_loss:260.1692810058594\n",
      "234/3000 train_loss: 118.81288146972656 test_loss:255.30435180664062\n",
      "235/3000 train_loss: 128.37754821777344 test_loss:256.5616149902344\n",
      "236/3000 train_loss: 124.27164459228516 test_loss:255.85964965820312\n",
      "237/3000 train_loss: 122.23600769042969 test_loss:255.1927490234375\n",
      "238/3000 train_loss: 124.54766845703125 test_loss:251.6743927001953\n",
      "239/3000 train_loss: 124.79058837890625 test_loss:254.53810119628906\n",
      "240/3000 train_loss: 140.626708984375 test_loss:252.99957275390625\n",
      "241/3000 train_loss: 132.19281005859375 test_loss:255.7543487548828\n",
      "242/3000 train_loss: 124.06275939941406 test_loss:252.19363403320312\n",
      "243/3000 train_loss: 140.97679138183594 test_loss:249.9267120361328\n",
      "244/3000 train_loss: 128.81521606445312 test_loss:256.4764404296875\n",
      "245/3000 train_loss: 135.5540313720703 test_loss:260.9347839355469\n",
      "246/3000 train_loss: 120.2967529296875 test_loss:258.13702392578125\n",
      "247/3000 train_loss: 133.013916015625 test_loss:253.11557006835938\n",
      "248/3000 train_loss: 133.37828063964844 test_loss:249.8957977294922\n",
      "249/3000 train_loss: 132.3380584716797 test_loss:260.68243408203125\n",
      "250/3000 train_loss: 137.4291534423828 test_loss:256.0322570800781\n",
      "251/3000 train_loss: 119.26976013183594 test_loss:259.29779052734375\n",
      "252/3000 train_loss: 113.8099365234375 test_loss:250.2275390625\n",
      "253/3000 train_loss: 120.73741149902344 test_loss:244.3685302734375\n",
      "254/3000 train_loss: 131.8941650390625 test_loss:248.4259796142578\n",
      "255/3000 train_loss: 116.0076904296875 test_loss:249.36239624023438\n",
      "256/3000 train_loss: 118.92823791503906 test_loss:244.58535766601562\n",
      "257/3000 train_loss: 120.9809341430664 test_loss:253.87628173828125\n",
      "258/3000 train_loss: 116.38980102539062 test_loss:250.77671813964844\n",
      "259/3000 train_loss: 121.8624038696289 test_loss:249.37237548828125\n",
      "260/3000 train_loss: 115.25347900390625 test_loss:247.70709228515625\n",
      "261/3000 train_loss: 126.96237182617188 test_loss:248.33560180664062\n",
      "262/3000 train_loss: 150.95213317871094 test_loss:260.5157165527344\n",
      "263/3000 train_loss: 115.91861724853516 test_loss:247.1031036376953\n",
      "264/3000 train_loss: 111.2938461303711 test_loss:251.03016662597656\n",
      "265/3000 train_loss: 109.88699340820312 test_loss:245.50904846191406\n",
      "266/3000 train_loss: 125.76075744628906 test_loss:245.04217529296875\n",
      "267/3000 train_loss: 119.20610046386719 test_loss:243.80142211914062\n",
      "268/3000 train_loss: 118.87064361572266 test_loss:245.87899780273438\n",
      "269/3000 train_loss: 119.93994140625 test_loss:246.8595428466797\n",
      "270/3000 train_loss: 132.61659240722656 test_loss:243.7218017578125\n",
      "271/3000 train_loss: 138.91978454589844 test_loss:251.72006225585938\n",
      "272/3000 train_loss: 110.71186065673828 test_loss:249.44210815429688\n",
      "273/3000 train_loss: 122.83020782470703 test_loss:246.0718994140625\n",
      "274/3000 train_loss: 122.80293273925781 test_loss:242.28726196289062\n",
      "275/3000 train_loss: 130.54776000976562 test_loss:253.8292694091797\n",
      "276/3000 train_loss: 114.83740997314453 test_loss:260.893310546875\n",
      "277/3000 train_loss: 152.41815185546875 test_loss:254.65330505371094\n",
      "278/3000 train_loss: 106.05451965332031 test_loss:247.2497100830078\n",
      "279/3000 train_loss: 108.9518051147461 test_loss:245.26516723632812\n",
      "280/3000 train_loss: 109.17060089111328 test_loss:238.03909301757812\n",
      "281/3000 train_loss: 110.18400573730469 test_loss:239.19952392578125\n",
      "282/3000 train_loss: 107.01991271972656 test_loss:238.4560546875\n",
      "283/3000 train_loss: 120.12181091308594 test_loss:248.2469024658203\n",
      "284/3000 train_loss: 112.54782104492188 test_loss:240.1343994140625\n",
      "285/3000 train_loss: 123.22557067871094 test_loss:243.89749145507812\n",
      "286/3000 train_loss: 120.36988067626953 test_loss:250.17999267578125\n",
      "287/3000 train_loss: 111.78292083740234 test_loss:254.5546417236328\n",
      "288/3000 train_loss: 120.19892883300781 test_loss:252.08848571777344\n",
      "289/3000 train_loss: 114.2707748413086 test_loss:252.8952178955078\n",
      "290/3000 train_loss: 105.91146850585938 test_loss:246.4361114501953\n",
      "291/3000 train_loss: 113.33018493652344 test_loss:242.51210021972656\n",
      "292/3000 train_loss: 119.97976684570312 test_loss:242.87062072753906\n",
      "293/3000 train_loss: 112.33902740478516 test_loss:246.34727478027344\n",
      "294/3000 train_loss: 109.2133560180664 test_loss:236.22100830078125\n",
      "295/3000 train_loss: 105.71115112304688 test_loss:239.51535034179688\n",
      "296/3000 train_loss: 99.59490966796875 test_loss:246.16152954101562\n",
      "297/3000 train_loss: 97.44595336914062 test_loss:237.3408203125\n",
      "298/3000 train_loss: 114.68739318847656 test_loss:237.89996337890625\n",
      "299/3000 train_loss: 101.1590576171875 test_loss:237.69003295898438\n",
      "300/3000 train_loss: 118.6999282836914 test_loss:240.01846313476562\n",
      "301/3000 train_loss: 104.22914123535156 test_loss:236.04647827148438\n",
      "302/3000 train_loss: 106.45957946777344 test_loss:247.42276000976562\n",
      "303/3000 train_loss: 103.00389099121094 test_loss:251.97097778320312\n",
      "304/3000 train_loss: 105.52678680419922 test_loss:236.34805297851562\n",
      "305/3000 train_loss: 165.3871612548828 test_loss:240.26104736328125\n",
      "306/3000 train_loss: 117.58831787109375 test_loss:245.4078369140625\n",
      "307/3000 train_loss: 112.75822448730469 test_loss:238.18984985351562\n",
      "308/3000 train_loss: 102.51412200927734 test_loss:232.40328979492188\n",
      "309/3000 train_loss: 109.72904205322266 test_loss:240.0514373779297\n",
      "310/3000 train_loss: 111.13799285888672 test_loss:234.9530029296875\n",
      "311/3000 train_loss: 99.05616760253906 test_loss:231.99859619140625\n",
      "312/3000 train_loss: 106.69320678710938 test_loss:227.644775390625\n",
      "313/3000 train_loss: 104.72944641113281 test_loss:229.6502685546875\n",
      "314/3000 train_loss: 116.78770446777344 test_loss:228.63308715820312\n",
      "315/3000 train_loss: 100.47496032714844 test_loss:223.505126953125\n",
      "316/3000 train_loss: 103.9989013671875 test_loss:224.2384033203125\n",
      "317/3000 train_loss: 102.59088134765625 test_loss:222.01092529296875\n",
      "318/3000 train_loss: 103.45233154296875 test_loss:235.69235229492188\n",
      "319/3000 train_loss: 98.7212905883789 test_loss:229.71832275390625\n",
      "320/3000 train_loss: 94.36101531982422 test_loss:220.58535766601562\n",
      "321/3000 train_loss: 99.09259796142578 test_loss:222.1537322998047\n",
      "322/3000 train_loss: 95.47715759277344 test_loss:221.38760375976562\n",
      "323/3000 train_loss: 99.11866760253906 test_loss:230.218017578125\n",
      "324/3000 train_loss: 104.9215087890625 test_loss:225.84979248046875\n",
      "325/3000 train_loss: 116.93258666992188 test_loss:233.07904052734375\n",
      "326/3000 train_loss: 108.87105560302734 test_loss:222.37420654296875\n",
      "327/3000 train_loss: 101.82979583740234 test_loss:213.7547149658203\n",
      "328/3000 train_loss: 109.54975128173828 test_loss:223.97479248046875\n",
      "329/3000 train_loss: 106.20598602294922 test_loss:234.89894104003906\n",
      "330/3000 train_loss: 88.62389373779297 test_loss:217.3761749267578\n",
      "331/3000 train_loss: 97.9346923828125 test_loss:222.64874267578125\n",
      "332/3000 train_loss: 92.38986206054688 test_loss:219.5810546875\n",
      "333/3000 train_loss: 84.49826049804688 test_loss:210.3957061767578\n",
      "334/3000 train_loss: 96.93270111083984 test_loss:215.33294677734375\n",
      "335/3000 train_loss: 97.858642578125 test_loss:216.64154052734375\n",
      "336/3000 train_loss: 100.06896209716797 test_loss:214.95339965820312\n",
      "337/3000 train_loss: 87.64390563964844 test_loss:208.75942993164062\n",
      "338/3000 train_loss: 96.92826080322266 test_loss:213.9053955078125\n",
      "339/3000 train_loss: 87.77918243408203 test_loss:214.50350952148438\n",
      "340/3000 train_loss: 89.89552307128906 test_loss:212.45773315429688\n",
      "341/3000 train_loss: 92.51366424560547 test_loss:213.43453979492188\n",
      "342/3000 train_loss: 93.60730743408203 test_loss:223.48863220214844\n",
      "343/3000 train_loss: 103.19212341308594 test_loss:214.32827758789062\n",
      "344/3000 train_loss: 84.88923645019531 test_loss:200.4122772216797\n",
      "345/3000 train_loss: 102.597412109375 test_loss:207.9339141845703\n",
      "346/3000 train_loss: 98.97882843017578 test_loss:210.84561157226562\n",
      "347/3000 train_loss: 104.2629165649414 test_loss:206.48764038085938\n",
      "348/3000 train_loss: 80.97445678710938 test_loss:206.94772338867188\n",
      "349/3000 train_loss: 93.5172348022461 test_loss:209.9091796875\n",
      "350/3000 train_loss: 95.10498046875 test_loss:216.30816650390625\n",
      "351/3000 train_loss: 93.86825561523438 test_loss:215.20028686523438\n",
      "352/3000 train_loss: 94.59410095214844 test_loss:203.9412841796875\n",
      "353/3000 train_loss: 89.52751159667969 test_loss:204.6871337890625\n",
      "354/3000 train_loss: 90.85488891601562 test_loss:203.69175720214844\n",
      "355/3000 train_loss: 90.06871795654297 test_loss:213.59085083007812\n",
      "356/3000 train_loss: 80.06936645507812 test_loss:220.35916137695312\n",
      "357/3000 train_loss: 90.93022155761719 test_loss:201.5511016845703\n",
      "358/3000 train_loss: 86.55941009521484 test_loss:200.3940887451172\n",
      "359/3000 train_loss: 88.04727172851562 test_loss:199.19859313964844\n",
      "360/3000 train_loss: 87.19873046875 test_loss:199.9393768310547\n",
      "361/3000 train_loss: 91.26319885253906 test_loss:201.9516143798828\n",
      "362/3000 train_loss: 78.96560668945312 test_loss:199.20230102539062\n",
      "363/3000 train_loss: 87.13105010986328 test_loss:213.27407836914062\n",
      "364/3000 train_loss: 84.13702392578125 test_loss:197.25128173828125\n",
      "365/3000 train_loss: 91.43622589111328 test_loss:210.6736602783203\n",
      "366/3000 train_loss: 88.03521728515625 test_loss:199.3013916015625\n",
      "367/3000 train_loss: 79.36664581298828 test_loss:199.39273071289062\n",
      "368/3000 train_loss: 88.2933578491211 test_loss:207.8197021484375\n",
      "369/3000 train_loss: 90.07832336425781 test_loss:210.35665893554688\n",
      "370/3000 train_loss: 89.14924621582031 test_loss:204.89593505859375\n",
      "371/3000 train_loss: 84.65897369384766 test_loss:197.56790161132812\n",
      "372/3000 train_loss: 99.67156219482422 test_loss:201.47816467285156\n",
      "373/3000 train_loss: 81.91414642333984 test_loss:194.443603515625\n",
      "374/3000 train_loss: 88.72882080078125 test_loss:212.08836364746094\n",
      "375/3000 train_loss: 76.5660171508789 test_loss:203.36517333984375\n",
      "376/3000 train_loss: 107.70724487304688 test_loss:201.34298706054688\n",
      "377/3000 train_loss: 96.93360900878906 test_loss:204.6282501220703\n",
      "378/3000 train_loss: 85.8619384765625 test_loss:217.44696044921875\n",
      "379/3000 train_loss: 81.79183959960938 test_loss:201.18536376953125\n",
      "380/3000 train_loss: 83.42338562011719 test_loss:199.42672729492188\n",
      "381/3000 train_loss: 76.76666259765625 test_loss:203.16212463378906\n",
      "382/3000 train_loss: 81.07914733886719 test_loss:198.31707763671875\n",
      "383/3000 train_loss: 71.95329284667969 test_loss:204.74539184570312\n",
      "384/3000 train_loss: 98.46980285644531 test_loss:202.148681640625\n",
      "385/3000 train_loss: 81.74028778076172 test_loss:200.8499298095703\n",
      "386/3000 train_loss: 85.84391784667969 test_loss:203.52630615234375\n",
      "387/3000 train_loss: 74.78904724121094 test_loss:191.60638427734375\n",
      "388/3000 train_loss: 117.48542022705078 test_loss:205.4867401123047\n",
      "389/3000 train_loss: 82.9419174194336 test_loss:212.1436767578125\n",
      "390/3000 train_loss: 75.9131851196289 test_loss:199.45704650878906\n",
      "391/3000 train_loss: 82.4486312866211 test_loss:199.86822509765625\n",
      "392/3000 train_loss: 86.3886489868164 test_loss:207.55694580078125\n",
      "393/3000 train_loss: 76.4203109741211 test_loss:209.54737854003906\n",
      "394/3000 train_loss: 81.47840118408203 test_loss:197.27464294433594\n",
      "395/3000 train_loss: 102.19954681396484 test_loss:199.84454345703125\n",
      "396/3000 train_loss: 77.37464904785156 test_loss:194.72592163085938\n",
      "397/3000 train_loss: 80.55818939208984 test_loss:201.1450653076172\n",
      "398/3000 train_loss: 74.62747192382812 test_loss:197.48609924316406\n",
      "399/3000 train_loss: 86.54674530029297 test_loss:204.7548828125\n",
      "400/3000 train_loss: 115.12907409667969 test_loss:201.93978881835938\n",
      "401/3000 train_loss: 83.0981674194336 test_loss:204.5993194580078\n",
      "402/3000 train_loss: 77.56721496582031 test_loss:201.18807983398438\n",
      "403/3000 train_loss: 77.82430267333984 test_loss:192.29083251953125\n",
      "404/3000 train_loss: 88.53079223632812 test_loss:207.53880310058594\n",
      "405/3000 train_loss: 73.60509490966797 test_loss:191.66558837890625\n",
      "406/3000 train_loss: 84.6545181274414 test_loss:192.3546600341797\n",
      "407/3000 train_loss: 79.59358215332031 test_loss:188.11962890625\n",
      "408/3000 train_loss: 85.24237823486328 test_loss:200.12979125976562\n",
      "409/3000 train_loss: 75.84577941894531 test_loss:187.488037109375\n",
      "410/3000 train_loss: 100.6183853149414 test_loss:190.58160400390625\n",
      "411/3000 train_loss: 74.63502502441406 test_loss:199.09774780273438\n",
      "412/3000 train_loss: 83.66146087646484 test_loss:195.336669921875\n",
      "413/3000 train_loss: 71.40096282958984 test_loss:192.72853088378906\n",
      "414/3000 train_loss: 89.61253356933594 test_loss:197.81887817382812\n",
      "415/3000 train_loss: 71.8510971069336 test_loss:191.41586303710938\n",
      "416/3000 train_loss: 87.81468963623047 test_loss:191.26112365722656\n",
      "417/3000 train_loss: 79.20915222167969 test_loss:197.25262451171875\n",
      "418/3000 train_loss: 78.67379760742188 test_loss:185.49453735351562\n",
      "419/3000 train_loss: 72.30719757080078 test_loss:187.4136199951172\n",
      "420/3000 train_loss: 71.66690063476562 test_loss:186.43698120117188\n",
      "421/3000 train_loss: 68.26863098144531 test_loss:183.93812561035156\n",
      "422/3000 train_loss: 76.83843231201172 test_loss:201.79978942871094\n",
      "423/3000 train_loss: 72.01984405517578 test_loss:186.52294921875\n",
      "424/3000 train_loss: 79.7263412475586 test_loss:188.40480041503906\n",
      "425/3000 train_loss: 80.50591278076172 test_loss:187.32769775390625\n",
      "426/3000 train_loss: 81.0654525756836 test_loss:192.23464965820312\n",
      "427/3000 train_loss: 72.90457153320312 test_loss:193.5707550048828\n",
      "428/3000 train_loss: 70.54129791259766 test_loss:185.57443237304688\n",
      "429/3000 train_loss: 76.94487762451172 test_loss:185.11001586914062\n",
      "430/3000 train_loss: 71.0613021850586 test_loss:181.70750427246094\n",
      "431/3000 train_loss: 70.33357238769531 test_loss:188.16873168945312\n",
      "432/3000 train_loss: 70.64974975585938 test_loss:190.0426483154297\n",
      "433/3000 train_loss: 74.15999603271484 test_loss:182.8248291015625\n",
      "434/3000 train_loss: 77.01779174804688 test_loss:188.88888549804688\n",
      "435/3000 train_loss: 85.62413024902344 test_loss:190.77716064453125\n",
      "436/3000 train_loss: 74.33978271484375 test_loss:190.6637420654297\n",
      "437/3000 train_loss: 79.01981353759766 test_loss:189.35162353515625\n",
      "438/3000 train_loss: 73.4184341430664 test_loss:176.42703247070312\n",
      "439/3000 train_loss: 77.85201263427734 test_loss:185.31716918945312\n",
      "440/3000 train_loss: 70.1026611328125 test_loss:183.97610473632812\n",
      "441/3000 train_loss: 79.08847045898438 test_loss:200.16543579101562\n",
      "442/3000 train_loss: 83.79315185546875 test_loss:187.8062286376953\n",
      "443/3000 train_loss: 76.37969207763672 test_loss:170.89678955078125\n",
      "444/3000 train_loss: 73.64454650878906 test_loss:185.73110961914062\n",
      "445/3000 train_loss: 70.07234191894531 test_loss:171.18592834472656\n",
      "446/3000 train_loss: 76.20048522949219 test_loss:192.24945068359375\n",
      "447/3000 train_loss: 66.25298309326172 test_loss:178.4389190673828\n",
      "448/3000 train_loss: 72.54788208007812 test_loss:179.86767578125\n",
      "449/3000 train_loss: 71.00614929199219 test_loss:173.98370361328125\n",
      "450/3000 train_loss: 65.30880737304688 test_loss:188.02001953125\n",
      "451/3000 train_loss: 78.57901763916016 test_loss:176.2586669921875\n",
      "452/3000 train_loss: 71.34635925292969 test_loss:175.59657287597656\n",
      "453/3000 train_loss: 68.7430191040039 test_loss:192.967529296875\n",
      "454/3000 train_loss: 91.63018035888672 test_loss:192.27548217773438\n",
      "455/3000 train_loss: 74.08189392089844 test_loss:177.95858764648438\n",
      "456/3000 train_loss: 76.88641357421875 test_loss:178.11245727539062\n",
      "457/3000 train_loss: 80.7413330078125 test_loss:179.29949951171875\n",
      "458/3000 train_loss: 68.99097442626953 test_loss:182.40748596191406\n",
      "459/3000 train_loss: 70.84709167480469 test_loss:172.71974182128906\n",
      "460/3000 train_loss: 67.05583190917969 test_loss:175.58486938476562\n",
      "461/3000 train_loss: 73.30965423583984 test_loss:191.3992919921875\n",
      "462/3000 train_loss: 75.88551330566406 test_loss:177.3238067626953\n",
      "463/3000 train_loss: 64.23584747314453 test_loss:170.465087890625\n",
      "464/3000 train_loss: 71.19886779785156 test_loss:181.52413940429688\n",
      "465/3000 train_loss: 67.2815933227539 test_loss:172.0115966796875\n",
      "466/3000 train_loss: 66.58219909667969 test_loss:174.25991821289062\n",
      "467/3000 train_loss: 63.3305549621582 test_loss:172.6937255859375\n",
      "468/3000 train_loss: 74.50106048583984 test_loss:168.34571838378906\n",
      "469/3000 train_loss: 68.04129028320312 test_loss:178.1129150390625\n",
      "470/3000 train_loss: 66.697509765625 test_loss:171.98516845703125\n",
      "471/3000 train_loss: 70.68894958496094 test_loss:173.97268676757812\n",
      "472/3000 train_loss: 67.57523345947266 test_loss:170.74815368652344\n",
      "473/3000 train_loss: 73.43074035644531 test_loss:166.7039031982422\n",
      "474/3000 train_loss: 73.87543487548828 test_loss:174.7898406982422\n",
      "475/3000 train_loss: 69.8252944946289 test_loss:172.2508087158203\n",
      "476/3000 train_loss: 64.73330688476562 test_loss:173.92738342285156\n",
      "477/3000 train_loss: 63.376922607421875 test_loss:180.70545959472656\n",
      "478/3000 train_loss: 74.55362701416016 test_loss:175.6402587890625\n",
      "479/3000 train_loss: 66.70404815673828 test_loss:169.84486389160156\n",
      "480/3000 train_loss: 67.33367156982422 test_loss:178.05604553222656\n",
      "481/3000 train_loss: 62.316078186035156 test_loss:174.5935821533203\n",
      "482/3000 train_loss: 62.028038024902344 test_loss:165.72467041015625\n",
      "483/3000 train_loss: 62.021366119384766 test_loss:173.38067626953125\n",
      "484/3000 train_loss: 62.555015563964844 test_loss:164.22177124023438\n",
      "485/3000 train_loss: 76.19520568847656 test_loss:179.7869110107422\n",
      "486/3000 train_loss: 63.15005111694336 test_loss:164.08721923828125\n",
      "487/3000 train_loss: 65.66319274902344 test_loss:177.9866485595703\n",
      "488/3000 train_loss: 65.9925765991211 test_loss:167.18685913085938\n",
      "489/3000 train_loss: 66.95372009277344 test_loss:163.66226196289062\n",
      "490/3000 train_loss: 61.923274993896484 test_loss:165.353271484375\n",
      "491/3000 train_loss: 58.00692367553711 test_loss:161.28680419921875\n",
      "492/3000 train_loss: 65.76155853271484 test_loss:175.8555145263672\n",
      "493/3000 train_loss: 60.247196197509766 test_loss:163.121826171875\n",
      "494/3000 train_loss: 90.40433502197266 test_loss:177.86309814453125\n",
      "495/3000 train_loss: 72.25260162353516 test_loss:173.12152099609375\n",
      "496/3000 train_loss: 58.94672775268555 test_loss:170.3881378173828\n",
      "497/3000 train_loss: 69.039306640625 test_loss:167.33607482910156\n",
      "498/3000 train_loss: 62.178138732910156 test_loss:165.39877319335938\n",
      "499/3000 train_loss: 61.56040573120117 test_loss:182.49046325683594\n",
      "500/3000 train_loss: 70.83985900878906 test_loss:165.53955078125\n",
      "501/3000 train_loss: 61.44479751586914 test_loss:178.37091064453125\n",
      "502/3000 train_loss: 61.51682662963867 test_loss:173.58950805664062\n",
      "503/3000 train_loss: 64.44175720214844 test_loss:177.79452514648438\n",
      "504/3000 train_loss: 69.2537841796875 test_loss:170.411376953125\n",
      "505/3000 train_loss: 74.47046661376953 test_loss:170.783935546875\n",
      "506/3000 train_loss: 82.73738098144531 test_loss:180.39381408691406\n",
      "507/3000 train_loss: 65.9247055053711 test_loss:167.78933715820312\n",
      "508/3000 train_loss: 62.58732604980469 test_loss:170.771728515625\n",
      "509/3000 train_loss: 57.83614730834961 test_loss:172.0110321044922\n",
      "510/3000 train_loss: 64.97032165527344 test_loss:161.25392150878906\n",
      "511/3000 train_loss: 62.066619873046875 test_loss:176.10243225097656\n",
      "512/3000 train_loss: 64.9489517211914 test_loss:167.01695251464844\n",
      "513/3000 train_loss: 78.7895736694336 test_loss:178.1110382080078\n",
      "514/3000 train_loss: 67.8954086303711 test_loss:165.30133056640625\n",
      "515/3000 train_loss: 65.52828216552734 test_loss:170.36007690429688\n",
      "516/3000 train_loss: 62.8197135925293 test_loss:175.62722778320312\n",
      "517/3000 train_loss: 66.34432983398438 test_loss:169.04879760742188\n",
      "518/3000 train_loss: 59.669944763183594 test_loss:158.6558380126953\n",
      "519/3000 train_loss: 70.26930236816406 test_loss:162.30960083007812\n",
      "520/3000 train_loss: 55.20953369140625 test_loss:170.97174072265625\n",
      "521/3000 train_loss: 75.41565704345703 test_loss:162.689697265625\n",
      "522/3000 train_loss: 55.98509216308594 test_loss:153.0232696533203\n",
      "523/3000 train_loss: 51.478275299072266 test_loss:153.60874938964844\n",
      "524/3000 train_loss: 55.58898162841797 test_loss:159.80691528320312\n",
      "525/3000 train_loss: 57.357765197753906 test_loss:156.39273071289062\n",
      "526/3000 train_loss: 65.59540557861328 test_loss:172.82302856445312\n",
      "527/3000 train_loss: 71.45367431640625 test_loss:159.37301635742188\n",
      "528/3000 train_loss: 62.944252014160156 test_loss:159.32376098632812\n",
      "529/3000 train_loss: 57.40106201171875 test_loss:165.9351806640625\n",
      "530/3000 train_loss: 59.54637145996094 test_loss:155.46493530273438\n",
      "531/3000 train_loss: 60.38907241821289 test_loss:165.9759063720703\n",
      "532/3000 train_loss: 59.019046783447266 test_loss:169.45681762695312\n",
      "533/3000 train_loss: 65.75251007080078 test_loss:160.84442138671875\n",
      "534/3000 train_loss: 69.52548217773438 test_loss:159.0814971923828\n",
      "535/3000 train_loss: 59.98976135253906 test_loss:155.69239807128906\n",
      "536/3000 train_loss: 56.54072952270508 test_loss:171.1687774658203\n",
      "537/3000 train_loss: 60.09066390991211 test_loss:157.82228088378906\n",
      "538/3000 train_loss: 57.70656204223633 test_loss:160.72894287109375\n",
      "539/3000 train_loss: 57.583595275878906 test_loss:163.34060668945312\n",
      "540/3000 train_loss: 61.12358856201172 test_loss:164.8785400390625\n",
      "541/3000 train_loss: 64.75614929199219 test_loss:168.08966064453125\n",
      "542/3000 train_loss: 55.3326301574707 test_loss:153.27581787109375\n",
      "543/3000 train_loss: 57.83819580078125 test_loss:161.88522338867188\n",
      "544/3000 train_loss: 61.488216400146484 test_loss:171.0015869140625\n",
      "545/3000 train_loss: 67.1491928100586 test_loss:171.49462890625\n",
      "546/3000 train_loss: 60.429710388183594 test_loss:162.10769653320312\n",
      "547/3000 train_loss: 61.296234130859375 test_loss:166.78564453125\n",
      "548/3000 train_loss: 60.70844268798828 test_loss:159.33377075195312\n",
      "549/3000 train_loss: 60.35617446899414 test_loss:164.5433349609375\n",
      "550/3000 train_loss: 66.01012420654297 test_loss:168.89382934570312\n",
      "551/3000 train_loss: 58.23520278930664 test_loss:161.04364013671875\n",
      "552/3000 train_loss: 53.84832000732422 test_loss:153.08668518066406\n",
      "553/3000 train_loss: 62.411865234375 test_loss:160.92922973632812\n",
      "554/3000 train_loss: 52.334930419921875 test_loss:158.9720458984375\n",
      "555/3000 train_loss: 64.95531463623047 test_loss:172.89767456054688\n",
      "556/3000 train_loss: 59.79561233520508 test_loss:160.4189453125\n",
      "557/3000 train_loss: 53.690711975097656 test_loss:163.3673095703125\n",
      "558/3000 train_loss: 63.64227294921875 test_loss:154.3655242919922\n",
      "559/3000 train_loss: 54.54652404785156 test_loss:166.05345153808594\n",
      "560/3000 train_loss: 55.3954963684082 test_loss:163.21160888671875\n",
      "561/3000 train_loss: 57.48646926879883 test_loss:159.5057830810547\n",
      "562/3000 train_loss: 57.698734283447266 test_loss:154.55911254882812\n",
      "563/3000 train_loss: 56.43644332885742 test_loss:156.0448455810547\n",
      "564/3000 train_loss: 62.49199676513672 test_loss:160.879638671875\n",
      "565/3000 train_loss: 57.01737976074219 test_loss:153.1934814453125\n",
      "566/3000 train_loss: 62.209983825683594 test_loss:163.94529724121094\n",
      "567/3000 train_loss: 65.33040618896484 test_loss:160.16812133789062\n",
      "568/3000 train_loss: 55.72520446777344 test_loss:156.9158477783203\n",
      "569/3000 train_loss: 53.0274772644043 test_loss:168.04502868652344\n",
      "570/3000 train_loss: 56.32242965698242 test_loss:159.26382446289062\n",
      "571/3000 train_loss: 54.497222900390625 test_loss:161.1618194580078\n",
      "572/3000 train_loss: 66.33589172363281 test_loss:161.7646942138672\n",
      "573/3000 train_loss: 54.834007263183594 test_loss:164.110595703125\n",
      "574/3000 train_loss: 57.762516021728516 test_loss:156.50167846679688\n",
      "575/3000 train_loss: 56.751434326171875 test_loss:159.43910217285156\n",
      "576/3000 train_loss: 58.05202102661133 test_loss:154.79531860351562\n",
      "577/3000 train_loss: 58.740936279296875 test_loss:167.73529052734375\n",
      "578/3000 train_loss: 57.32198715209961 test_loss:163.1881103515625\n",
      "579/3000 train_loss: 57.747344970703125 test_loss:155.57135009765625\n",
      "580/3000 train_loss: 65.0191650390625 test_loss:164.6017608642578\n",
      "581/3000 train_loss: 57.98240280151367 test_loss:148.61331176757812\n",
      "582/3000 train_loss: 53.258941650390625 test_loss:159.13821411132812\n",
      "583/3000 train_loss: 59.502254486083984 test_loss:143.925537109375\n",
      "584/3000 train_loss: 49.70415496826172 test_loss:152.27220153808594\n",
      "585/3000 train_loss: 46.105472564697266 test_loss:150.08045959472656\n",
      "586/3000 train_loss: 71.50517272949219 test_loss:161.61793518066406\n",
      "587/3000 train_loss: 59.401031494140625 test_loss:150.06117248535156\n",
      "588/3000 train_loss: 90.60601043701172 test_loss:182.16537475585938\n",
      "589/3000 train_loss: 67.0888442993164 test_loss:159.2967529296875\n",
      "590/3000 train_loss: 61.92280960083008 test_loss:150.69976806640625\n",
      "591/3000 train_loss: 61.12021255493164 test_loss:149.613037109375\n",
      "592/3000 train_loss: 47.20819854736328 test_loss:153.35244750976562\n",
      "593/3000 train_loss: 53.91661071777344 test_loss:151.64413452148438\n",
      "594/3000 train_loss: 50.51899719238281 test_loss:152.37164306640625\n",
      "595/3000 train_loss: 68.51331329345703 test_loss:171.87808227539062\n",
      "596/3000 train_loss: 49.66415786743164 test_loss:146.79046630859375\n",
      "597/3000 train_loss: 54.68284606933594 test_loss:149.8466339111328\n",
      "598/3000 train_loss: 65.50884246826172 test_loss:164.73956298828125\n",
      "599/3000 train_loss: 52.85073471069336 test_loss:146.24273681640625\n",
      "600/3000 train_loss: 56.04846954345703 test_loss:162.2469024658203\n",
      "601/3000 train_loss: 55.040836334228516 test_loss:144.9651336669922\n",
      "602/3000 train_loss: 56.53923797607422 test_loss:159.62744140625\n",
      "603/3000 train_loss: 48.806461334228516 test_loss:151.2742919921875\n",
      "604/3000 train_loss: 64.98762512207031 test_loss:165.0255126953125\n",
      "605/3000 train_loss: 51.77034378051758 test_loss:147.384521484375\n",
      "606/3000 train_loss: 50.813873291015625 test_loss:157.4130859375\n",
      "607/3000 train_loss: 54.31339645385742 test_loss:148.62075805664062\n",
      "608/3000 train_loss: 50.0140495300293 test_loss:156.32662963867188\n",
      "609/3000 train_loss: 60.4918098449707 test_loss:156.6634521484375\n",
      "610/3000 train_loss: 52.840755462646484 test_loss:150.53713989257812\n",
      "611/3000 train_loss: 51.42412567138672 test_loss:151.36607360839844\n",
      "612/3000 train_loss: 54.15815734863281 test_loss:160.1879425048828\n",
      "613/3000 train_loss: 50.65763473510742 test_loss:156.44046020507812\n",
      "614/3000 train_loss: 51.064422607421875 test_loss:157.5875244140625\n",
      "615/3000 train_loss: 63.846534729003906 test_loss:165.73345947265625\n",
      "616/3000 train_loss: 50.76434326171875 test_loss:143.81756591796875\n",
      "617/3000 train_loss: 58.61923599243164 test_loss:167.57803344726562\n",
      "618/3000 train_loss: 55.63188171386719 test_loss:148.04959106445312\n",
      "619/3000 train_loss: 53.60415267944336 test_loss:151.56610107421875\n",
      "620/3000 train_loss: 49.343910217285156 test_loss:148.7368927001953\n",
      "621/3000 train_loss: 52.51725387573242 test_loss:166.3516845703125\n",
      "622/3000 train_loss: 52.429107666015625 test_loss:157.7270965576172\n",
      "623/3000 train_loss: 54.04121780395508 test_loss:150.89305114746094\n",
      "624/3000 train_loss: 51.008583068847656 test_loss:162.84353637695312\n",
      "625/3000 train_loss: 51.91267776489258 test_loss:154.93295288085938\n",
      "626/3000 train_loss: 60.49305725097656 test_loss:160.04635620117188\n",
      "627/3000 train_loss: 52.238895416259766 test_loss:144.1588134765625\n",
      "628/3000 train_loss: 61.751914978027344 test_loss:161.3347930908203\n",
      "629/3000 train_loss: 54.42007064819336 test_loss:157.14483642578125\n",
      "630/3000 train_loss: 52.7383918762207 test_loss:155.47653198242188\n",
      "631/3000 train_loss: 53.347991943359375 test_loss:158.0074920654297\n",
      "632/3000 train_loss: 55.4166145324707 test_loss:158.30828857421875\n",
      "633/3000 train_loss: 48.30352783203125 test_loss:154.40106201171875\n",
      "634/3000 train_loss: 53.12334060668945 test_loss:159.04550170898438\n",
      "635/3000 train_loss: 50.04286193847656 test_loss:147.18310546875\n",
      "636/3000 train_loss: 42.62078094482422 test_loss:157.21559143066406\n",
      "637/3000 train_loss: 76.98226928710938 test_loss:158.75192260742188\n",
      "638/3000 train_loss: 55.0223503112793 test_loss:158.76193237304688\n",
      "639/3000 train_loss: 50.41868209838867 test_loss:163.46180725097656\n",
      "640/3000 train_loss: 49.395286560058594 test_loss:155.4981689453125\n",
      "641/3000 train_loss: 54.516414642333984 test_loss:160.50741577148438\n",
      "642/3000 train_loss: 51.71352005004883 test_loss:155.2996063232422\n",
      "643/3000 train_loss: 50.594566345214844 test_loss:149.82614135742188\n",
      "644/3000 train_loss: 46.37095642089844 test_loss:158.44168090820312\n",
      "645/3000 train_loss: 50.05010986328125 test_loss:147.9454803466797\n",
      "646/3000 train_loss: 47.713592529296875 test_loss:155.7985076904297\n",
      "647/3000 train_loss: 64.00096893310547 test_loss:146.37777709960938\n",
      "648/3000 train_loss: 49.79102325439453 test_loss:157.78363037109375\n",
      "649/3000 train_loss: 50.524871826171875 test_loss:148.1817169189453\n",
      "650/3000 train_loss: 53.42133712768555 test_loss:162.2615966796875\n",
      "651/3000 train_loss: 47.959434509277344 test_loss:148.44886779785156\n",
      "652/3000 train_loss: 51.01866912841797 test_loss:152.2197265625\n",
      "653/3000 train_loss: 48.3013801574707 test_loss:156.09945678710938\n",
      "654/3000 train_loss: 49.928871154785156 test_loss:164.10409545898438\n",
      "655/3000 train_loss: 48.6164665222168 test_loss:148.94834899902344\n",
      "656/3000 train_loss: 48.895694732666016 test_loss:160.77365112304688\n",
      "657/3000 train_loss: 48.37474822998047 test_loss:146.27987670898438\n",
      "658/3000 train_loss: 78.24409484863281 test_loss:161.70779418945312\n",
      "659/3000 train_loss: 50.84526062011719 test_loss:147.7698516845703\n",
      "660/3000 train_loss: 52.743804931640625 test_loss:154.8240966796875\n",
      "661/3000 train_loss: 55.900245666503906 test_loss:156.90696716308594\n",
      "662/3000 train_loss: 54.00352478027344 test_loss:155.2981414794922\n",
      "663/3000 train_loss: 51.460575103759766 test_loss:144.73486328125\n",
      "664/3000 train_loss: 51.05263900756836 test_loss:154.1124267578125\n",
      "665/3000 train_loss: 48.83710479736328 test_loss:146.84954833984375\n",
      "666/3000 train_loss: 58.0756721496582 test_loss:154.98345947265625\n",
      "667/3000 train_loss: 48.693485260009766 test_loss:139.19430541992188\n",
      "668/3000 train_loss: 49.29813003540039 test_loss:156.45797729492188\n",
      "669/3000 train_loss: 56.26026916503906 test_loss:151.57073974609375\n",
      "670/3000 train_loss: 49.60319900512695 test_loss:160.576171875\n",
      "671/3000 train_loss: 43.59654998779297 test_loss:140.91104125976562\n",
      "672/3000 train_loss: 50.52973556518555 test_loss:157.2057647705078\n",
      "673/3000 train_loss: 59.050514221191406 test_loss:144.00291442871094\n",
      "674/3000 train_loss: 50.24089050292969 test_loss:158.2647705078125\n",
      "675/3000 train_loss: 56.25406265258789 test_loss:157.6131591796875\n",
      "676/3000 train_loss: 53.369815826416016 test_loss:152.76150512695312\n",
      "677/3000 train_loss: 46.35789489746094 test_loss:151.06063842773438\n",
      "678/3000 train_loss: 47.613250732421875 test_loss:152.8013916015625\n",
      "679/3000 train_loss: 50.09187316894531 test_loss:152.47012329101562\n",
      "680/3000 train_loss: 50.29529571533203 test_loss:142.25738525390625\n",
      "681/3000 train_loss: 46.02288818359375 test_loss:158.04718017578125\n",
      "682/3000 train_loss: 46.86760711669922 test_loss:145.9967041015625\n",
      "683/3000 train_loss: 75.39480590820312 test_loss:152.34732055664062\n",
      "684/3000 train_loss: 53.58522415161133 test_loss:155.33203125\n",
      "685/3000 train_loss: 54.38848114013672 test_loss:150.01791381835938\n",
      "686/3000 train_loss: 53.35990905761719 test_loss:148.66082763671875\n",
      "687/3000 train_loss: 48.8424072265625 test_loss:152.97494506835938\n",
      "688/3000 train_loss: 49.73800277709961 test_loss:145.7460479736328\n",
      "689/3000 train_loss: 45.87104415893555 test_loss:148.18313598632812\n",
      "690/3000 train_loss: 44.72625732421875 test_loss:147.9386749267578\n",
      "691/3000 train_loss: 47.69325637817383 test_loss:139.73379516601562\n",
      "692/3000 train_loss: 45.036277770996094 test_loss:137.35208129882812\n",
      "693/3000 train_loss: 43.404083251953125 test_loss:149.5605010986328\n",
      "694/3000 train_loss: 45.789859771728516 test_loss:146.25958251953125\n",
      "695/3000 train_loss: 52.0880012512207 test_loss:139.83010864257812\n",
      "696/3000 train_loss: 43.11589813232422 test_loss:144.53277587890625\n",
      "697/3000 train_loss: 43.41010665893555 test_loss:137.6492919921875\n",
      "698/3000 train_loss: 45.786075592041016 test_loss:153.28091430664062\n",
      "699/3000 train_loss: 47.207664489746094 test_loss:151.70663452148438\n",
      "700/3000 train_loss: 46.16791534423828 test_loss:140.84957885742188\n",
      "701/3000 train_loss: 47.16697692871094 test_loss:164.89959716796875\n",
      "702/3000 train_loss: 43.951637268066406 test_loss:138.29774475097656\n",
      "703/3000 train_loss: 46.33842086791992 test_loss:161.75900268554688\n",
      "704/3000 train_loss: 49.773101806640625 test_loss:148.14889526367188\n",
      "705/3000 train_loss: 44.81950759887695 test_loss:143.4512939453125\n",
      "706/3000 train_loss: 46.76353073120117 test_loss:147.93817138671875\n",
      "707/3000 train_loss: 40.156768798828125 test_loss:150.00086975097656\n",
      "708/3000 train_loss: 41.74937438964844 test_loss:143.35726928710938\n",
      "709/3000 train_loss: 44.17296600341797 test_loss:147.10923767089844\n",
      "710/3000 train_loss: 42.73073196411133 test_loss:143.70220947265625\n",
      "711/3000 train_loss: 44.937339782714844 test_loss:139.95419311523438\n",
      "712/3000 train_loss: 49.94264221191406 test_loss:149.66964721679688\n",
      "713/3000 train_loss: 42.72496795654297 test_loss:145.40383911132812\n",
      "714/3000 train_loss: 48.64374923706055 test_loss:149.84848022460938\n",
      "715/3000 train_loss: 44.66363525390625 test_loss:138.50311279296875\n",
      "716/3000 train_loss: 44.73723602294922 test_loss:141.31350708007812\n",
      "717/3000 train_loss: 45.31065368652344 test_loss:144.44439697265625\n",
      "718/3000 train_loss: 48.13742446899414 test_loss:144.99856567382812\n",
      "719/3000 train_loss: 42.22964096069336 test_loss:136.40272521972656\n",
      "720/3000 train_loss: 41.19229507446289 test_loss:144.9838409423828\n",
      "721/3000 train_loss: 40.92735290527344 test_loss:139.9788360595703\n",
      "722/3000 train_loss: 47.15351104736328 test_loss:137.66432189941406\n",
      "723/3000 train_loss: 38.922119140625 test_loss:152.82638549804688\n",
      "724/3000 train_loss: 56.37456130981445 test_loss:144.85166931152344\n",
      "725/3000 train_loss: 47.3548469543457 test_loss:145.78256225585938\n",
      "726/3000 train_loss: 48.0445442199707 test_loss:155.08236694335938\n",
      "727/3000 train_loss: 42.88145446777344 test_loss:138.8626251220703\n",
      "728/3000 train_loss: 42.8632926940918 test_loss:144.64581298828125\n",
      "729/3000 train_loss: 45.222965240478516 test_loss:147.07977294921875\n",
      "730/3000 train_loss: 42.28010559082031 test_loss:143.0917205810547\n",
      "731/3000 train_loss: 42.882999420166016 test_loss:143.904541015625\n",
      "732/3000 train_loss: 42.5145378112793 test_loss:143.16140747070312\n",
      "733/3000 train_loss: 45.71461486816406 test_loss:138.19561767578125\n",
      "734/3000 train_loss: 43.30628204345703 test_loss:151.10235595703125\n",
      "735/3000 train_loss: 42.55926513671875 test_loss:145.79605102539062\n",
      "736/3000 train_loss: 48.00349044799805 test_loss:142.1540985107422\n",
      "737/3000 train_loss: 44.841793060302734 test_loss:148.35279846191406\n",
      "738/3000 train_loss: 46.236717224121094 test_loss:140.7074737548828\n",
      "739/3000 train_loss: 44.10242462158203 test_loss:145.23260498046875\n",
      "740/3000 train_loss: 58.553958892822266 test_loss:157.1881561279297\n",
      "741/3000 train_loss: 49.87907409667969 test_loss:139.57568359375\n",
      "742/3000 train_loss: 50.54436492919922 test_loss:158.08551025390625\n",
      "743/3000 train_loss: 52.49821090698242 test_loss:130.83151245117188\n",
      "744/3000 train_loss: 41.059120178222656 test_loss:141.18402099609375\n",
      "745/3000 train_loss: 41.74142837524414 test_loss:141.30758666992188\n",
      "746/3000 train_loss: 45.954769134521484 test_loss:137.93441772460938\n",
      "747/3000 train_loss: 42.94730758666992 test_loss:145.0462188720703\n",
      "748/3000 train_loss: 40.80543518066406 test_loss:139.23855590820312\n",
      "749/3000 train_loss: 42.66870880126953 test_loss:135.0077362060547\n",
      "750/3000 train_loss: 47.62413787841797 test_loss:135.8967742919922\n",
      "751/3000 train_loss: 104.85903930664062 test_loss:156.96434020996094\n",
      "752/3000 train_loss: 48.471500396728516 test_loss:145.05819702148438\n",
      "753/3000 train_loss: 39.182151794433594 test_loss:136.05677795410156\n",
      "754/3000 train_loss: 42.56012725830078 test_loss:142.46273803710938\n",
      "755/3000 train_loss: 53.85726547241211 test_loss:139.37698364257812\n",
      "756/3000 train_loss: 41.42546844482422 test_loss:141.25149536132812\n",
      "757/3000 train_loss: 38.309776306152344 test_loss:139.17221069335938\n",
      "758/3000 train_loss: 45.04912567138672 test_loss:149.04925537109375\n",
      "759/3000 train_loss: 44.830101013183594 test_loss:137.46981811523438\n",
      "760/3000 train_loss: 48.050010681152344 test_loss:144.5745849609375\n",
      "761/3000 train_loss: 43.277530670166016 test_loss:141.138916015625\n",
      "762/3000 train_loss: 44.8695182800293 test_loss:141.32699584960938\n",
      "763/3000 train_loss: 45.39570999145508 test_loss:138.5620574951172\n",
      "764/3000 train_loss: 48.18772506713867 test_loss:139.37408447265625\n",
      "765/3000 train_loss: 43.572898864746094 test_loss:137.19334411621094\n",
      "766/3000 train_loss: 40.161861419677734 test_loss:142.81666564941406\n",
      "767/3000 train_loss: 41.27963638305664 test_loss:144.61611938476562\n",
      "768/3000 train_loss: 38.817440032958984 test_loss:129.90721130371094\n",
      "769/3000 train_loss: 43.8243408203125 test_loss:151.75279235839844\n",
      "770/3000 train_loss: 51.38637161254883 test_loss:140.57521057128906\n",
      "771/3000 train_loss: 49.9371452331543 test_loss:140.00022888183594\n",
      "772/3000 train_loss: 54.53948211669922 test_loss:140.9627227783203\n",
      "773/3000 train_loss: 38.600704193115234 test_loss:136.02284240722656\n",
      "774/3000 train_loss: 46.266048431396484 test_loss:142.7140655517578\n",
      "775/3000 train_loss: 35.98721694946289 test_loss:131.73822021484375\n",
      "776/3000 train_loss: 47.15712356567383 test_loss:141.0079345703125\n",
      "777/3000 train_loss: 42.21461486816406 test_loss:140.99021911621094\n",
      "778/3000 train_loss: 64.68489074707031 test_loss:136.96041870117188\n",
      "779/3000 train_loss: 45.414554595947266 test_loss:129.87489318847656\n",
      "780/3000 train_loss: 42.16615295410156 test_loss:139.69717407226562\n",
      "781/3000 train_loss: 35.8568115234375 test_loss:133.6769256591797\n",
      "782/3000 train_loss: 38.99363708496094 test_loss:129.35589599609375\n",
      "783/3000 train_loss: 39.76780700683594 test_loss:145.81964111328125\n",
      "784/3000 train_loss: 40.54549026489258 test_loss:135.40492248535156\n",
      "785/3000 train_loss: 43.23398971557617 test_loss:141.95651245117188\n",
      "786/3000 train_loss: 40.22930145263672 test_loss:137.3159942626953\n",
      "787/3000 train_loss: 38.00074768066406 test_loss:127.48072814941406\n",
      "788/3000 train_loss: 47.00436782836914 test_loss:134.51560974121094\n",
      "789/3000 train_loss: 46.7427864074707 test_loss:138.5608673095703\n",
      "790/3000 train_loss: 39.64750289916992 test_loss:132.51573181152344\n",
      "791/3000 train_loss: 40.37357711791992 test_loss:136.0022735595703\n",
      "792/3000 train_loss: 38.530025482177734 test_loss:133.69921875\n",
      "793/3000 train_loss: 46.280426025390625 test_loss:132.115966796875\n",
      "794/3000 train_loss: 40.77893829345703 test_loss:131.93927001953125\n",
      "795/3000 train_loss: 42.120277404785156 test_loss:140.9564666748047\n",
      "796/3000 train_loss: 47.36503601074219 test_loss:142.64297485351562\n",
      "797/3000 train_loss: 44.11027908325195 test_loss:139.55572509765625\n",
      "798/3000 train_loss: 39.069091796875 test_loss:133.31060791015625\n",
      "799/3000 train_loss: 41.817115783691406 test_loss:131.96998596191406\n",
      "800/3000 train_loss: 40.32265853881836 test_loss:143.60650634765625\n",
      "801/3000 train_loss: 39.248592376708984 test_loss:131.26962280273438\n",
      "802/3000 train_loss: 53.22074890136719 test_loss:146.14697265625\n",
      "803/3000 train_loss: 38.46990203857422 test_loss:130.08831787109375\n",
      "804/3000 train_loss: 38.16216278076172 test_loss:133.24114990234375\n",
      "805/3000 train_loss: 49.96957778930664 test_loss:151.38021850585938\n",
      "806/3000 train_loss: 39.118988037109375 test_loss:125.92024993896484\n",
      "807/3000 train_loss: 38.9577751159668 test_loss:141.36717224121094\n",
      "808/3000 train_loss: 39.71598815917969 test_loss:125.85940551757812\n",
      "809/3000 train_loss: 44.035003662109375 test_loss:142.76504516601562\n",
      "810/3000 train_loss: 39.76338577270508 test_loss:133.43289184570312\n",
      "811/3000 train_loss: 40.388206481933594 test_loss:138.15089416503906\n",
      "812/3000 train_loss: 45.42103958129883 test_loss:132.4292755126953\n",
      "813/3000 train_loss: 48.18730926513672 test_loss:137.0142822265625\n",
      "814/3000 train_loss: 42.26008605957031 test_loss:133.18533325195312\n",
      "815/3000 train_loss: 40.32634353637695 test_loss:139.94483947753906\n",
      "816/3000 train_loss: 35.485687255859375 test_loss:131.28033447265625\n",
      "817/3000 train_loss: 39.7588996887207 test_loss:139.48231506347656\n",
      "818/3000 train_loss: 38.231868743896484 test_loss:132.04673767089844\n",
      "819/3000 train_loss: 41.416831970214844 test_loss:129.9358673095703\n",
      "820/3000 train_loss: 43.17319869995117 test_loss:136.99765014648438\n",
      "821/3000 train_loss: 45.136680603027344 test_loss:128.2583770751953\n",
      "822/3000 train_loss: 42.46073913574219 test_loss:133.6817169189453\n",
      "823/3000 train_loss: 41.50789260864258 test_loss:145.16146850585938\n",
      "824/3000 train_loss: 43.72993469238281 test_loss:132.65155029296875\n",
      "825/3000 train_loss: 42.92586898803711 test_loss:129.22344970703125\n",
      "826/3000 train_loss: 48.539493560791016 test_loss:135.2432403564453\n",
      "827/3000 train_loss: 44.80356216430664 test_loss:131.88839721679688\n",
      "828/3000 train_loss: 40.42100524902344 test_loss:133.07350158691406\n",
      "829/3000 train_loss: 39.91075134277344 test_loss:135.6063690185547\n",
      "830/3000 train_loss: 43.05730438232422 test_loss:128.55007934570312\n",
      "831/3000 train_loss: 41.58317184448242 test_loss:140.38189697265625\n",
      "832/3000 train_loss: 40.29029083251953 test_loss:135.93374633789062\n",
      "833/3000 train_loss: 39.012603759765625 test_loss:132.8065643310547\n",
      "834/3000 train_loss: 39.90654754638672 test_loss:136.69415283203125\n",
      "835/3000 train_loss: 36.794246673583984 test_loss:129.88653564453125\n",
      "836/3000 train_loss: 38.79412078857422 test_loss:135.30873107910156\n",
      "837/3000 train_loss: 43.149147033691406 test_loss:125.30736541748047\n",
      "838/3000 train_loss: 38.384212493896484 test_loss:137.28294372558594\n",
      "839/3000 train_loss: 36.90007019042969 test_loss:133.50302124023438\n",
      "840/3000 train_loss: 37.6850700378418 test_loss:137.08920288085938\n",
      "841/3000 train_loss: 36.69527053833008 test_loss:132.76853942871094\n",
      "842/3000 train_loss: 39.408294677734375 test_loss:127.42998504638672\n",
      "843/3000 train_loss: 37.946128845214844 test_loss:132.31727600097656\n",
      "844/3000 train_loss: 37.53756332397461 test_loss:123.81163024902344\n",
      "845/3000 train_loss: 48.094058990478516 test_loss:136.48069763183594\n",
      "846/3000 train_loss: 36.72090530395508 test_loss:125.86634826660156\n",
      "847/3000 train_loss: 37.763736724853516 test_loss:137.23789978027344\n",
      "848/3000 train_loss: 46.133155822753906 test_loss:126.88923645019531\n",
      "849/3000 train_loss: 37.47039794921875 test_loss:135.3460693359375\n",
      "850/3000 train_loss: 37.537391662597656 test_loss:130.37838745117188\n",
      "851/3000 train_loss: 37.87526321411133 test_loss:134.15884399414062\n",
      "852/3000 train_loss: 43.531349182128906 test_loss:129.19676208496094\n",
      "853/3000 train_loss: 41.70466232299805 test_loss:130.6317901611328\n",
      "854/3000 train_loss: 40.4058837890625 test_loss:134.6742706298828\n",
      "855/3000 train_loss: 39.56264114379883 test_loss:126.04179382324219\n",
      "856/3000 train_loss: 53.150794982910156 test_loss:135.06094360351562\n",
      "857/3000 train_loss: 40.905601501464844 test_loss:131.55072021484375\n",
      "858/3000 train_loss: 46.401527404785156 test_loss:128.63772583007812\n",
      "859/3000 train_loss: 41.698516845703125 test_loss:128.9656524658203\n",
      "860/3000 train_loss: 41.81623458862305 test_loss:144.43801879882812\n",
      "861/3000 train_loss: 43.70755386352539 test_loss:133.0006561279297\n",
      "862/3000 train_loss: 35.87763214111328 test_loss:131.54348754882812\n",
      "863/3000 train_loss: 49.310001373291016 test_loss:145.95721435546875\n",
      "864/3000 train_loss: 37.61884307861328 test_loss:127.5797119140625\n",
      "865/3000 train_loss: 38.88433074951172 test_loss:129.01499938964844\n",
      "866/3000 train_loss: 42.99021911621094 test_loss:146.06292724609375\n",
      "867/3000 train_loss: 39.26182556152344 test_loss:128.73007202148438\n",
      "868/3000 train_loss: 35.90030288696289 test_loss:135.4079132080078\n",
      "869/3000 train_loss: 35.83027267456055 test_loss:127.68807983398438\n",
      "870/3000 train_loss: 36.154685974121094 test_loss:133.59754943847656\n",
      "871/3000 train_loss: 37.36188507080078 test_loss:131.38087463378906\n",
      "872/3000 train_loss: 39.70930099487305 test_loss:130.26234436035156\n",
      "873/3000 train_loss: 38.41897964477539 test_loss:129.66397094726562\n",
      "874/3000 train_loss: 41.136558532714844 test_loss:126.1583251953125\n",
      "875/3000 train_loss: 39.55038070678711 test_loss:124.71963500976562\n",
      "876/3000 train_loss: 36.1838493347168 test_loss:132.8795623779297\n",
      "877/3000 train_loss: 37.87455368041992 test_loss:123.27011108398438\n",
      "878/3000 train_loss: 35.45306396484375 test_loss:130.5386962890625\n",
      "879/3000 train_loss: 35.551429748535156 test_loss:131.38075256347656\n",
      "880/3000 train_loss: 39.745059967041016 test_loss:127.35734558105469\n",
      "881/3000 train_loss: 37.18082809448242 test_loss:128.27549743652344\n",
      "882/3000 train_loss: 35.022056579589844 test_loss:131.70245361328125\n",
      "883/3000 train_loss: 38.40627670288086 test_loss:125.54578399658203\n",
      "884/3000 train_loss: 34.01986312866211 test_loss:133.60218811035156\n",
      "885/3000 train_loss: 36.679378509521484 test_loss:124.27241516113281\n",
      "886/3000 train_loss: 42.55707550048828 test_loss:140.6856689453125\n",
      "887/3000 train_loss: 40.032997131347656 test_loss:124.3885498046875\n",
      "888/3000 train_loss: 42.916725158691406 test_loss:133.10287475585938\n",
      "889/3000 train_loss: 44.33666229248047 test_loss:122.7476577758789\n",
      "890/3000 train_loss: 37.576385498046875 test_loss:139.47828674316406\n",
      "891/3000 train_loss: 34.14443588256836 test_loss:128.56539916992188\n",
      "892/3000 train_loss: 41.375022888183594 test_loss:136.6407470703125\n",
      "893/3000 train_loss: 39.56898498535156 test_loss:131.04429626464844\n",
      "894/3000 train_loss: 36.66456985473633 test_loss:134.73770141601562\n",
      "895/3000 train_loss: 38.005489349365234 test_loss:141.08839416503906\n",
      "896/3000 train_loss: 42.39326858520508 test_loss:133.25047302246094\n",
      "897/3000 train_loss: 35.47555160522461 test_loss:135.29208374023438\n",
      "898/3000 train_loss: 41.297969818115234 test_loss:129.5388946533203\n",
      "899/3000 train_loss: 39.782073974609375 test_loss:143.46685791015625\n",
      "900/3000 train_loss: 41.3139533996582 test_loss:141.44728088378906\n",
      "901/3000 train_loss: 38.853759765625 test_loss:136.879150390625\n",
      "902/3000 train_loss: 37.97397232055664 test_loss:128.95883178710938\n",
      "903/3000 train_loss: 36.99607849121094 test_loss:130.39193725585938\n",
      "904/3000 train_loss: 36.56601333618164 test_loss:137.51177978515625\n",
      "905/3000 train_loss: 37.12189865112305 test_loss:127.37101745605469\n",
      "906/3000 train_loss: 35.523128509521484 test_loss:135.55157470703125\n",
      "907/3000 train_loss: 36.24895477294922 test_loss:127.974609375\n",
      "908/3000 train_loss: 33.45270538330078 test_loss:127.14435577392578\n",
      "909/3000 train_loss: 41.416236877441406 test_loss:118.95857238769531\n",
      "910/3000 train_loss: 39.33317184448242 test_loss:130.65444946289062\n",
      "911/3000 train_loss: 36.68341064453125 test_loss:123.74813079833984\n",
      "912/3000 train_loss: 44.821556091308594 test_loss:125.42250061035156\n",
      "913/3000 train_loss: 35.06330108642578 test_loss:122.51539611816406\n",
      "914/3000 train_loss: 34.9873046875 test_loss:143.28944396972656\n",
      "915/3000 train_loss: 37.357669830322266 test_loss:126.38734436035156\n",
      "916/3000 train_loss: 36.124717712402344 test_loss:137.28076171875\n",
      "917/3000 train_loss: 41.334228515625 test_loss:120.70245361328125\n",
      "918/3000 train_loss: 35.643951416015625 test_loss:129.91290283203125\n",
      "919/3000 train_loss: 35.34354782104492 test_loss:122.49493408203125\n",
      "920/3000 train_loss: 38.219703674316406 test_loss:123.91072845458984\n",
      "921/3000 train_loss: 36.2309684753418 test_loss:125.66349029541016\n",
      "922/3000 train_loss: 34.71305847167969 test_loss:131.29043579101562\n",
      "923/3000 train_loss: 35.424232482910156 test_loss:120.61957550048828\n",
      "924/3000 train_loss: 36.664878845214844 test_loss:120.1213150024414\n",
      "925/3000 train_loss: 36.18878936767578 test_loss:126.0575180053711\n",
      "926/3000 train_loss: 35.64537048339844 test_loss:121.25985717773438\n",
      "927/3000 train_loss: 39.2296028137207 test_loss:130.09983825683594\n",
      "928/3000 train_loss: 32.364070892333984 test_loss:129.07691955566406\n",
      "929/3000 train_loss: 37.56597900390625 test_loss:129.06346130371094\n",
      "930/3000 train_loss: 45.44344711303711 test_loss:132.25180053710938\n",
      "931/3000 train_loss: 37.385597229003906 test_loss:126.02877807617188\n",
      "932/3000 train_loss: 52.46275329589844 test_loss:122.62000274658203\n",
      "933/3000 train_loss: 37.462032318115234 test_loss:140.59617614746094\n",
      "934/3000 train_loss: 31.82729148864746 test_loss:125.11361694335938\n",
      "935/3000 train_loss: 32.56261444091797 test_loss:124.11593627929688\n",
      "936/3000 train_loss: 40.19247055053711 test_loss:128.68455505371094\n",
      "937/3000 train_loss: 33.129150390625 test_loss:125.41449737548828\n",
      "938/3000 train_loss: 40.76567840576172 test_loss:140.09767150878906\n",
      "939/3000 train_loss: 37.02598571777344 test_loss:127.50935363769531\n",
      "940/3000 train_loss: 41.65872573852539 test_loss:123.73723602294922\n",
      "941/3000 train_loss: 40.524009704589844 test_loss:126.26698303222656\n",
      "942/3000 train_loss: 34.29790496826172 test_loss:124.50553131103516\n",
      "943/3000 train_loss: 33.54466247558594 test_loss:124.37672424316406\n",
      "944/3000 train_loss: 29.940841674804688 test_loss:120.92083740234375\n",
      "945/3000 train_loss: 32.776222229003906 test_loss:123.13655853271484\n",
      "946/3000 train_loss: 34.715965270996094 test_loss:126.46881866455078\n",
      "947/3000 train_loss: 40.97731399536133 test_loss:130.66940307617188\n",
      "948/3000 train_loss: 37.294189453125 test_loss:128.02467346191406\n",
      "949/3000 train_loss: 33.186866760253906 test_loss:125.40115356445312\n",
      "950/3000 train_loss: 43.401397705078125 test_loss:123.874755859375\n",
      "951/3000 train_loss: 37.17569351196289 test_loss:135.13331604003906\n",
      "952/3000 train_loss: 34.58070373535156 test_loss:125.69259643554688\n",
      "953/3000 train_loss: 33.67665100097656 test_loss:134.6869659423828\n",
      "954/3000 train_loss: 34.11802291870117 test_loss:126.57665252685547\n",
      "955/3000 train_loss: 48.15513229370117 test_loss:124.82102966308594\n",
      "956/3000 train_loss: 33.88723373413086 test_loss:129.31854248046875\n",
      "957/3000 train_loss: 35.03690719604492 test_loss:123.2696762084961\n",
      "958/3000 train_loss: 32.84294128417969 test_loss:122.39423370361328\n",
      "959/3000 train_loss: 34.64900588989258 test_loss:132.3045654296875\n",
      "960/3000 train_loss: 35.349571228027344 test_loss:131.07388305664062\n",
      "961/3000 train_loss: 32.430137634277344 test_loss:119.67413330078125\n",
      "962/3000 train_loss: 32.555809020996094 test_loss:131.64993286132812\n",
      "963/3000 train_loss: 30.963090896606445 test_loss:123.44013214111328\n",
      "964/3000 train_loss: 42.12996292114258 test_loss:128.65296936035156\n",
      "965/3000 train_loss: 34.5855598449707 test_loss:128.10011291503906\n",
      "966/3000 train_loss: 39.76405334472656 test_loss:124.02223205566406\n",
      "967/3000 train_loss: 34.39979934692383 test_loss:132.39572143554688\n",
      "968/3000 train_loss: 33.2791862487793 test_loss:130.95166015625\n",
      "969/3000 train_loss: 37.32737350463867 test_loss:126.22282409667969\n",
      "970/3000 train_loss: 36.97749710083008 test_loss:133.1036376953125\n",
      "971/3000 train_loss: 30.126060485839844 test_loss:126.98434448242188\n",
      "972/3000 train_loss: 33.3374137878418 test_loss:128.43624877929688\n",
      "973/3000 train_loss: 32.30261993408203 test_loss:134.36386108398438\n",
      "974/3000 train_loss: 35.443092346191406 test_loss:125.61907958984375\n",
      "975/3000 train_loss: 33.70670700073242 test_loss:127.13815307617188\n",
      "976/3000 train_loss: 34.51560592651367 test_loss:121.79224395751953\n",
      "977/3000 train_loss: 37.174964904785156 test_loss:130.2982177734375\n",
      "978/3000 train_loss: 31.388681411743164 test_loss:125.40416717529297\n",
      "979/3000 train_loss: 36.76788330078125 test_loss:129.06021118164062\n",
      "980/3000 train_loss: 33.650726318359375 test_loss:120.31373596191406\n",
      "981/3000 train_loss: 34.00840759277344 test_loss:127.78549194335938\n",
      "982/3000 train_loss: 33.87213897705078 test_loss:123.3036117553711\n",
      "983/3000 train_loss: 36.1385498046875 test_loss:119.33069610595703\n",
      "984/3000 train_loss: 32.73934555053711 test_loss:123.07030487060547\n",
      "985/3000 train_loss: 33.64170455932617 test_loss:122.46366882324219\n",
      "986/3000 train_loss: 36.79270553588867 test_loss:123.60002136230469\n",
      "987/3000 train_loss: 33.61714172363281 test_loss:131.510009765625\n",
      "988/3000 train_loss: 35.13576126098633 test_loss:118.50459289550781\n",
      "989/3000 train_loss: 32.60737609863281 test_loss:128.7611846923828\n",
      "990/3000 train_loss: 42.297630310058594 test_loss:133.16293334960938\n",
      "991/3000 train_loss: 36.09157943725586 test_loss:126.5595703125\n",
      "992/3000 train_loss: 33.44076156616211 test_loss:136.05519104003906\n",
      "993/3000 train_loss: 35.1976318359375 test_loss:131.1386260986328\n",
      "994/3000 train_loss: 30.22478675842285 test_loss:120.04803466796875\n",
      "995/3000 train_loss: 31.856523513793945 test_loss:127.8621597290039\n",
      "996/3000 train_loss: 30.718875885009766 test_loss:131.28021240234375\n",
      "997/3000 train_loss: 33.366825103759766 test_loss:129.48434448242188\n",
      "998/3000 train_loss: 31.65198516845703 test_loss:132.98690795898438\n",
      "999/3000 train_loss: 29.468421936035156 test_loss:122.88296508789062\n",
      "1000/3000 train_loss: 36.37712097167969 test_loss:133.47549438476562\n",
      "1001/3000 train_loss: 33.13099670410156 test_loss:132.83287048339844\n",
      "1002/3000 train_loss: 35.63468933105469 test_loss:124.34895324707031\n",
      "1003/3000 train_loss: 30.837738037109375 test_loss:125.95677185058594\n",
      "1004/3000 train_loss: 35.26704025268555 test_loss:127.41609954833984\n",
      "1005/3000 train_loss: 34.49578094482422 test_loss:122.1673583984375\n",
      "1006/3000 train_loss: 43.21268081665039 test_loss:120.71951293945312\n",
      "1007/3000 train_loss: 34.05508041381836 test_loss:128.06454467773438\n",
      "1008/3000 train_loss: 34.628814697265625 test_loss:128.20631408691406\n",
      "1009/3000 train_loss: 36.80033493041992 test_loss:123.03761291503906\n",
      "1010/3000 train_loss: 34.476097106933594 test_loss:131.12158203125\n",
      "1011/3000 train_loss: 30.525278091430664 test_loss:129.37783813476562\n",
      "1012/3000 train_loss: 35.60487365722656 test_loss:129.64163208007812\n",
      "1013/3000 train_loss: 36.03250503540039 test_loss:131.24472045898438\n",
      "1014/3000 train_loss: 35.6541748046875 test_loss:122.13048553466797\n",
      "1015/3000 train_loss: 34.535972595214844 test_loss:123.9491195678711\n",
      "1016/3000 train_loss: 42.577640533447266 test_loss:131.11099243164062\n",
      "1017/3000 train_loss: 34.29907989501953 test_loss:134.46192932128906\n",
      "1018/3000 train_loss: 32.07743453979492 test_loss:123.44599914550781\n",
      "1019/3000 train_loss: 37.622596740722656 test_loss:129.97071838378906\n",
      "1020/3000 train_loss: 30.380157470703125 test_loss:130.0625762939453\n",
      "1021/3000 train_loss: 28.731979370117188 test_loss:128.23312377929688\n",
      "1022/3000 train_loss: 31.18543243408203 test_loss:130.56834411621094\n",
      "1023/3000 train_loss: 37.59075164794922 test_loss:131.68458557128906\n",
      "1024/3000 train_loss: 31.9856014251709 test_loss:133.58401489257812\n",
      "1025/3000 train_loss: 31.750581741333008 test_loss:134.69320678710938\n",
      "1026/3000 train_loss: 30.348430633544922 test_loss:130.71908569335938\n",
      "1027/3000 train_loss: 34.35676574707031 test_loss:141.33197021484375\n",
      "1028/3000 train_loss: 34.620941162109375 test_loss:127.18098449707031\n",
      "1029/3000 train_loss: 32.68974304199219 test_loss:135.26820373535156\n",
      "1030/3000 train_loss: 31.51457405090332 test_loss:126.79279327392578\n",
      "1031/3000 train_loss: 38.221683502197266 test_loss:124.85491943359375\n",
      "1032/3000 train_loss: 30.61522674560547 test_loss:126.67191314697266\n",
      "1033/3000 train_loss: 32.818790435791016 test_loss:126.86186218261719\n",
      "1034/3000 train_loss: 28.320117950439453 test_loss:128.3065643310547\n",
      "1035/3000 train_loss: 29.80270767211914 test_loss:124.81212615966797\n",
      "1036/3000 train_loss: 31.707862854003906 test_loss:123.16270446777344\n",
      "1037/3000 train_loss: 33.15314865112305 test_loss:130.71719360351562\n",
      "1038/3000 train_loss: 36.226375579833984 test_loss:137.69476318359375\n",
      "1039/3000 train_loss: 33.36054992675781 test_loss:126.53514099121094\n",
      "1040/3000 train_loss: 28.091323852539062 test_loss:122.24259948730469\n",
      "1041/3000 train_loss: 32.83256912231445 test_loss:127.66924285888672\n",
      "1042/3000 train_loss: 33.207435607910156 test_loss:125.31665802001953\n",
      "1043/3000 train_loss: 33.370880126953125 test_loss:129.87989807128906\n",
      "1044/3000 train_loss: 34.504119873046875 test_loss:131.6455841064453\n",
      "1045/3000 train_loss: 35.311649322509766 test_loss:118.54118347167969\n",
      "1046/3000 train_loss: 35.29090881347656 test_loss:133.3303985595703\n",
      "1047/3000 train_loss: 36.91653060913086 test_loss:133.72857666015625\n",
      "1048/3000 train_loss: 32.305885314941406 test_loss:119.63975524902344\n",
      "1049/3000 train_loss: 30.96269416809082 test_loss:130.25730895996094\n",
      "1050/3000 train_loss: 37.85145950317383 test_loss:121.19007110595703\n",
      "1051/3000 train_loss: 29.99266815185547 test_loss:123.80015563964844\n",
      "1052/3000 train_loss: 34.31422805786133 test_loss:127.40446472167969\n",
      "1053/3000 train_loss: 30.65500259399414 test_loss:120.39175415039062\n",
      "1054/3000 train_loss: 33.18242263793945 test_loss:125.38775634765625\n",
      "1055/3000 train_loss: 32.267086029052734 test_loss:125.93511962890625\n",
      "1056/3000 train_loss: 32.409610748291016 test_loss:116.4508056640625\n",
      "1057/3000 train_loss: 34.32962417602539 test_loss:126.16999053955078\n",
      "1058/3000 train_loss: 30.403060913085938 test_loss:130.08782958984375\n",
      "1059/3000 train_loss: 34.8758544921875 test_loss:116.73988342285156\n",
      "1060/3000 train_loss: 34.756561279296875 test_loss:136.39187622070312\n",
      "1061/3000 train_loss: 34.818115234375 test_loss:127.76441955566406\n",
      "1062/3000 train_loss: 32.16023635864258 test_loss:137.06643676757812\n",
      "1063/3000 train_loss: 35.05741500854492 test_loss:128.60870361328125\n",
      "1064/3000 train_loss: 29.072711944580078 test_loss:128.76129150390625\n",
      "1065/3000 train_loss: 35.38685607910156 test_loss:124.95948791503906\n",
      "1066/3000 train_loss: 32.914634704589844 test_loss:127.82444763183594\n",
      "1067/3000 train_loss: 36.9417724609375 test_loss:133.38023376464844\n",
      "1068/3000 train_loss: 31.497039794921875 test_loss:126.21296691894531\n",
      "1069/3000 train_loss: 34.810516357421875 test_loss:121.77574920654297\n",
      "1070/3000 train_loss: 36.82636642456055 test_loss:124.64042663574219\n",
      "1071/3000 train_loss: 32.45531463623047 test_loss:128.96173095703125\n",
      "1072/3000 train_loss: 34.624290466308594 test_loss:124.54997253417969\n",
      "1073/3000 train_loss: 32.25510787963867 test_loss:129.17935180664062\n",
      "1074/3000 train_loss: 32.78664779663086 test_loss:121.43870544433594\n",
      "1075/3000 train_loss: 36.81938552856445 test_loss:125.9118423461914\n",
      "1076/3000 train_loss: 36.222347259521484 test_loss:130.96986389160156\n",
      "1077/3000 train_loss: 35.621402740478516 test_loss:126.60482025146484\n",
      "1078/3000 train_loss: 33.86427307128906 test_loss:120.73709106445312\n",
      "1079/3000 train_loss: 36.019710540771484 test_loss:137.2578125\n",
      "1080/3000 train_loss: 32.41262435913086 test_loss:116.19171905517578\n",
      "1081/3000 train_loss: 30.438364028930664 test_loss:126.05143737792969\n",
      "1082/3000 train_loss: 30.77545928955078 test_loss:121.02082061767578\n",
      "1083/3000 train_loss: 34.078365325927734 test_loss:127.69515991210938\n",
      "1084/3000 train_loss: 34.16910171508789 test_loss:125.66199493408203\n",
      "1085/3000 train_loss: 36.505279541015625 test_loss:120.69649505615234\n",
      "1086/3000 train_loss: 38.05125427246094 test_loss:120.49526977539062\n",
      "1087/3000 train_loss: 36.221473693847656 test_loss:121.25421905517578\n",
      "1088/3000 train_loss: 33.42129898071289 test_loss:119.83212280273438\n",
      "1089/3000 train_loss: 33.5600471496582 test_loss:117.14833068847656\n",
      "1090/3000 train_loss: 34.32279968261719 test_loss:111.25953674316406\n",
      "1091/3000 train_loss: 39.823638916015625 test_loss:130.34295654296875\n",
      "1092/3000 train_loss: 33.1713752746582 test_loss:114.06509399414062\n",
      "1093/3000 train_loss: 33.194759368896484 test_loss:128.76980590820312\n",
      "1094/3000 train_loss: 40.529911041259766 test_loss:125.09233093261719\n",
      "1095/3000 train_loss: 34.499603271484375 test_loss:125.47065734863281\n",
      "1096/3000 train_loss: 33.226226806640625 test_loss:114.94412231445312\n",
      "1097/3000 train_loss: 32.308555603027344 test_loss:125.13031005859375\n",
      "1098/3000 train_loss: 29.246021270751953 test_loss:116.59256744384766\n",
      "1099/3000 train_loss: 28.313098907470703 test_loss:119.53886413574219\n",
      "1100/3000 train_loss: 34.16341781616211 test_loss:121.74111938476562\n",
      "1101/3000 train_loss: 30.218040466308594 test_loss:119.8373031616211\n",
      "1102/3000 train_loss: 29.569271087646484 test_loss:118.96792602539062\n",
      "1103/3000 train_loss: 30.030805587768555 test_loss:120.93365478515625\n",
      "1104/3000 train_loss: 29.97309684753418 test_loss:117.36909484863281\n",
      "1105/3000 train_loss: 28.758079528808594 test_loss:119.70524597167969\n",
      "1106/3000 train_loss: 36.08274841308594 test_loss:117.71607208251953\n",
      "1107/3000 train_loss: 30.485305786132812 test_loss:118.23063659667969\n",
      "1108/3000 train_loss: 35.30930709838867 test_loss:130.97340393066406\n",
      "1109/3000 train_loss: 43.80207824707031 test_loss:124.15315246582031\n",
      "1110/3000 train_loss: 33.934486389160156 test_loss:128.42091369628906\n",
      "1111/3000 train_loss: 31.948671340942383 test_loss:115.98231506347656\n",
      "1112/3000 train_loss: 30.405927658081055 test_loss:126.38758850097656\n",
      "1113/3000 train_loss: 33.72883987426758 test_loss:120.76914978027344\n",
      "1114/3000 train_loss: 33.35395050048828 test_loss:119.23601531982422\n",
      "1115/3000 train_loss: 29.44612693786621 test_loss:122.72254943847656\n",
      "1116/3000 train_loss: 29.87120819091797 test_loss:128.061279296875\n",
      "1117/3000 train_loss: 31.648448944091797 test_loss:114.85367584228516\n",
      "1118/3000 train_loss: 29.223196029663086 test_loss:120.83827209472656\n",
      "1119/3000 train_loss: 26.65839958190918 test_loss:127.20042419433594\n",
      "1120/3000 train_loss: 33.21720504760742 test_loss:117.8084487915039\n",
      "1121/3000 train_loss: 33.758140563964844 test_loss:117.73043823242188\n",
      "1122/3000 train_loss: 27.78498077392578 test_loss:118.91899108886719\n",
      "1123/3000 train_loss: 28.551128387451172 test_loss:118.20040893554688\n",
      "1124/3000 train_loss: 34.002593994140625 test_loss:125.62565612792969\n",
      "1125/3000 train_loss: 39.279354095458984 test_loss:112.42033386230469\n",
      "1126/3000 train_loss: 29.275230407714844 test_loss:122.326416015625\n",
      "1127/3000 train_loss: 35.602989196777344 test_loss:111.81982421875\n",
      "1128/3000 train_loss: 31.411497116088867 test_loss:124.48269653320312\n",
      "1129/3000 train_loss: 28.552257537841797 test_loss:114.02351379394531\n",
      "1130/3000 train_loss: 39.64954376220703 test_loss:115.74478149414062\n",
      "1131/3000 train_loss: 28.111764907836914 test_loss:120.29776763916016\n",
      "1132/3000 train_loss: 31.346240997314453 test_loss:124.10348510742188\n",
      "1133/3000 train_loss: 28.590808868408203 test_loss:116.1488265991211\n",
      "1134/3000 train_loss: 30.179903030395508 test_loss:126.35774993896484\n",
      "1135/3000 train_loss: 42.01228713989258 test_loss:120.20561981201172\n",
      "1136/3000 train_loss: 37.375205993652344 test_loss:133.47354125976562\n",
      "1137/3000 train_loss: 33.82101058959961 test_loss:120.9905014038086\n",
      "1138/3000 train_loss: 31.5438289642334 test_loss:122.38428497314453\n",
      "1139/3000 train_loss: 29.996618270874023 test_loss:125.58218383789062\n",
      "1140/3000 train_loss: 34.692684173583984 test_loss:121.83863830566406\n",
      "1141/3000 train_loss: 29.02705192565918 test_loss:117.80123901367188\n",
      "1142/3000 train_loss: 33.43416213989258 test_loss:119.05070495605469\n",
      "1143/3000 train_loss: 29.18301010131836 test_loss:125.25300598144531\n",
      "1144/3000 train_loss: 28.296846389770508 test_loss:120.6064224243164\n",
      "1145/3000 train_loss: 29.47621726989746 test_loss:124.9600830078125\n",
      "1146/3000 train_loss: 31.703784942626953 test_loss:121.61544799804688\n",
      "1147/3000 train_loss: 32.50225830078125 test_loss:128.57705688476562\n",
      "1148/3000 train_loss: 32.712947845458984 test_loss:115.96484375\n",
      "1149/3000 train_loss: 33.08692169189453 test_loss:122.13011169433594\n",
      "1150/3000 train_loss: 31.067859649658203 test_loss:129.74501037597656\n",
      "1151/3000 train_loss: 39.886295318603516 test_loss:127.9561767578125\n",
      "1152/3000 train_loss: 31.651308059692383 test_loss:128.71775817871094\n",
      "1153/3000 train_loss: 29.308435440063477 test_loss:128.33505249023438\n",
      "1154/3000 train_loss: 29.990550994873047 test_loss:122.2725830078125\n",
      "1155/3000 train_loss: 29.481779098510742 test_loss:123.12977600097656\n",
      "1156/3000 train_loss: 30.164518356323242 test_loss:123.31405639648438\n",
      "1157/3000 train_loss: 31.19639778137207 test_loss:119.3387451171875\n",
      "1158/3000 train_loss: 30.177005767822266 test_loss:119.60812377929688\n",
      "1159/3000 train_loss: 28.903079986572266 test_loss:120.04806518554688\n",
      "1160/3000 train_loss: 30.068796157836914 test_loss:118.88772583007812\n",
      "1161/3000 train_loss: 29.181928634643555 test_loss:123.62532043457031\n",
      "1162/3000 train_loss: 29.938125610351562 test_loss:117.71319580078125\n",
      "1163/3000 train_loss: 27.036100387573242 test_loss:126.39495849609375\n",
      "1164/3000 train_loss: 33.30000305175781 test_loss:113.48179626464844\n",
      "1165/3000 train_loss: 30.14767837524414 test_loss:121.60194396972656\n",
      "1166/3000 train_loss: 31.68453025817871 test_loss:115.66231536865234\n",
      "1167/3000 train_loss: 28.139734268188477 test_loss:120.43608093261719\n",
      "1168/3000 train_loss: 29.905250549316406 test_loss:108.86549377441406\n",
      "1169/3000 train_loss: 26.742935180664062 test_loss:116.78935241699219\n",
      "1170/3000 train_loss: 27.745948791503906 test_loss:115.69233703613281\n",
      "1171/3000 train_loss: 27.0767822265625 test_loss:115.46177673339844\n",
      "1172/3000 train_loss: 30.674091339111328 test_loss:116.70256042480469\n",
      "1173/3000 train_loss: 27.926925659179688 test_loss:117.02592468261719\n",
      "1174/3000 train_loss: 29.20900535583496 test_loss:112.29702758789062\n",
      "1175/3000 train_loss: 31.715364456176758 test_loss:125.06562042236328\n",
      "1176/3000 train_loss: 29.354001998901367 test_loss:118.32100677490234\n",
      "1177/3000 train_loss: 29.6870059967041 test_loss:123.19862365722656\n",
      "1178/3000 train_loss: 31.83871078491211 test_loss:120.9551010131836\n",
      "1179/3000 train_loss: 27.30341339111328 test_loss:115.32191467285156\n",
      "1180/3000 train_loss: 29.686153411865234 test_loss:128.7295684814453\n",
      "1181/3000 train_loss: 29.636157989501953 test_loss:113.39545440673828\n",
      "1182/3000 train_loss: 30.455623626708984 test_loss:126.77593994140625\n",
      "1183/3000 train_loss: 31.266382217407227 test_loss:119.6646728515625\n",
      "1184/3000 train_loss: 28.603322982788086 test_loss:116.57832336425781\n",
      "1185/3000 train_loss: 30.142475128173828 test_loss:128.13302612304688\n",
      "1186/3000 train_loss: 29.757295608520508 test_loss:118.52032470703125\n",
      "1187/3000 train_loss: 26.96640968322754 test_loss:131.19049072265625\n",
      "1188/3000 train_loss: 33.54682159423828 test_loss:111.89820098876953\n",
      "1189/3000 train_loss: 31.241994857788086 test_loss:122.11434173583984\n",
      "1190/3000 train_loss: 28.0035343170166 test_loss:125.00657653808594\n",
      "1191/3000 train_loss: 36.147647857666016 test_loss:129.5802764892578\n",
      "1192/3000 train_loss: 32.79770278930664 test_loss:135.18630981445312\n",
      "1193/3000 train_loss: 39.33977508544922 test_loss:123.69548034667969\n",
      "1194/3000 train_loss: 29.89362335205078 test_loss:119.67919921875\n",
      "1195/3000 train_loss: 26.263452529907227 test_loss:122.15220642089844\n",
      "1196/3000 train_loss: 28.09917640686035 test_loss:121.67620849609375\n",
      "1197/3000 train_loss: 33.98408126831055 test_loss:133.80101013183594\n",
      "1198/3000 train_loss: 31.489404678344727 test_loss:124.29090881347656\n",
      "1199/3000 train_loss: 27.169160842895508 test_loss:121.31067657470703\n",
      "1200/3000 train_loss: 30.419124603271484 test_loss:119.11962127685547\n",
      "1201/3000 train_loss: 27.4382266998291 test_loss:126.3083724975586\n",
      "1202/3000 train_loss: 30.5391902923584 test_loss:120.79649353027344\n",
      "1203/3000 train_loss: 30.862367630004883 test_loss:127.6285400390625\n",
      "1204/3000 train_loss: 33.78919219970703 test_loss:113.42337036132812\n",
      "1205/3000 train_loss: 29.831605911254883 test_loss:126.52379608154297\n",
      "1206/3000 train_loss: 27.093915939331055 test_loss:121.43492126464844\n",
      "1207/3000 train_loss: 30.18671226501465 test_loss:138.9451904296875\n",
      "1208/3000 train_loss: 30.44782829284668 test_loss:111.22509765625\n",
      "1209/3000 train_loss: 28.420446395874023 test_loss:137.81005859375\n",
      "1210/3000 train_loss: 32.2214469909668 test_loss:122.94564056396484\n",
      "1211/3000 train_loss: 27.91607666015625 test_loss:118.79405975341797\n",
      "1212/3000 train_loss: 29.57339096069336 test_loss:115.49497985839844\n",
      "1213/3000 train_loss: 33.406436920166016 test_loss:119.58130645751953\n",
      "1214/3000 train_loss: 30.18366813659668 test_loss:115.10122680664062\n",
      "1215/3000 train_loss: 28.484474182128906 test_loss:132.3640899658203\n",
      "1216/3000 train_loss: 28.917003631591797 test_loss:124.20506286621094\n",
      "1217/3000 train_loss: 28.462265014648438 test_loss:120.44746398925781\n",
      "1218/3000 train_loss: 26.610767364501953 test_loss:121.56236267089844\n",
      "1219/3000 train_loss: 26.648290634155273 test_loss:119.83905029296875\n",
      "1220/3000 train_loss: 28.278005599975586 test_loss:124.7762680053711\n",
      "1221/3000 train_loss: 38.2886962890625 test_loss:131.17800903320312\n",
      "1222/3000 train_loss: 32.01905822753906 test_loss:117.6143798828125\n",
      "1223/3000 train_loss: 27.234708786010742 test_loss:126.05026245117188\n",
      "1224/3000 train_loss: 26.562259674072266 test_loss:119.49469757080078\n",
      "1225/3000 train_loss: 28.1629581451416 test_loss:117.90322875976562\n",
      "1226/3000 train_loss: 28.300748825073242 test_loss:118.00572204589844\n",
      "1227/3000 train_loss: 28.042667388916016 test_loss:120.45814514160156\n",
      "1228/3000 train_loss: 30.156982421875 test_loss:116.29582977294922\n",
      "1229/3000 train_loss: 24.83928108215332 test_loss:121.61420440673828\n",
      "1230/3000 train_loss: 27.552644729614258 test_loss:114.6358642578125\n",
      "1231/3000 train_loss: 25.02235984802246 test_loss:115.67573547363281\n",
      "1232/3000 train_loss: 27.149362564086914 test_loss:116.96293640136719\n",
      "1233/3000 train_loss: 31.272432327270508 test_loss:117.69696044921875\n",
      "1234/3000 train_loss: 28.84623908996582 test_loss:112.78321075439453\n",
      "1235/3000 train_loss: 25.58095359802246 test_loss:113.49537658691406\n",
      "1236/3000 train_loss: 25.32495880126953 test_loss:113.74372863769531\n",
      "1237/3000 train_loss: 29.570735931396484 test_loss:124.7674331665039\n",
      "1238/3000 train_loss: 28.909080505371094 test_loss:117.24198913574219\n",
      "1239/3000 train_loss: 32.12355041503906 test_loss:107.44674682617188\n",
      "1240/3000 train_loss: 36.04235076904297 test_loss:128.84609985351562\n",
      "1241/3000 train_loss: 28.0732479095459 test_loss:111.72593688964844\n",
      "1242/3000 train_loss: 26.233675003051758 test_loss:123.18739318847656\n",
      "1243/3000 train_loss: 28.6171932220459 test_loss:118.52718353271484\n",
      "1244/3000 train_loss: 29.988082885742188 test_loss:120.75479125976562\n",
      "1245/3000 train_loss: 25.70442771911621 test_loss:111.69047546386719\n",
      "1246/3000 train_loss: 28.161848068237305 test_loss:117.50398254394531\n",
      "1247/3000 train_loss: 30.919614791870117 test_loss:112.79142761230469\n",
      "1248/3000 train_loss: 26.448707580566406 test_loss:121.9051284790039\n",
      "1249/3000 train_loss: 30.41299057006836 test_loss:111.54348754882812\n",
      "1250/3000 train_loss: 25.475133895874023 test_loss:119.10874938964844\n",
      "1251/3000 train_loss: 28.932334899902344 test_loss:108.70279693603516\n",
      "1252/3000 train_loss: 25.424945831298828 test_loss:123.06820678710938\n",
      "1253/3000 train_loss: 28.395965576171875 test_loss:108.2130126953125\n",
      "1254/3000 train_loss: 26.851943969726562 test_loss:126.04424285888672\n",
      "1255/3000 train_loss: 24.149417877197266 test_loss:113.6422119140625\n",
      "1256/3000 train_loss: 26.171201705932617 test_loss:130.08746337890625\n",
      "1257/3000 train_loss: 29.479076385498047 test_loss:114.8965072631836\n",
      "1258/3000 train_loss: 25.19504737854004 test_loss:114.03639221191406\n",
      "1259/3000 train_loss: 27.297466278076172 test_loss:112.83447265625\n",
      "1260/3000 train_loss: 37.5964469909668 test_loss:114.22193908691406\n",
      "1261/3000 train_loss: 30.454391479492188 test_loss:118.9898910522461\n",
      "1262/3000 train_loss: 30.38534927368164 test_loss:108.6068115234375\n",
      "1263/3000 train_loss: 27.126056671142578 test_loss:117.35348510742188\n",
      "1264/3000 train_loss: 25.687767028808594 test_loss:121.15251159667969\n",
      "1265/3000 train_loss: 26.246912002563477 test_loss:116.39361572265625\n",
      "1266/3000 train_loss: 28.883352279663086 test_loss:119.06073760986328\n",
      "1267/3000 train_loss: 26.242870330810547 test_loss:126.0737533569336\n",
      "1268/3000 train_loss: 37.470672607421875 test_loss:134.16845703125\n",
      "1269/3000 train_loss: 35.71071243286133 test_loss:109.1774673461914\n",
      "1270/3000 train_loss: 30.73539161682129 test_loss:111.7391128540039\n",
      "1271/3000 train_loss: 27.597431182861328 test_loss:109.3135986328125\n",
      "1272/3000 train_loss: 27.6033878326416 test_loss:128.16390991210938\n",
      "1273/3000 train_loss: 30.896316528320312 test_loss:115.24858093261719\n",
      "1274/3000 train_loss: 27.46099853515625 test_loss:117.59642791748047\n",
      "1275/3000 train_loss: 26.2740421295166 test_loss:114.16659545898438\n",
      "1276/3000 train_loss: 25.67403221130371 test_loss:114.21505737304688\n",
      "1277/3000 train_loss: 28.064790725708008 test_loss:110.3285140991211\n",
      "1278/3000 train_loss: 28.329280853271484 test_loss:108.85301971435547\n",
      "1279/3000 train_loss: 27.91343116760254 test_loss:116.30294799804688\n",
      "1280/3000 train_loss: 28.52459716796875 test_loss:113.29460144042969\n",
      "1281/3000 train_loss: 27.976831436157227 test_loss:115.76422119140625\n",
      "1282/3000 train_loss: 26.30936622619629 test_loss:112.9083251953125\n",
      "1283/3000 train_loss: 26.48495101928711 test_loss:122.10462951660156\n",
      "1284/3000 train_loss: 26.47258949279785 test_loss:106.316162109375\n",
      "1285/3000 train_loss: 26.027578353881836 test_loss:113.72477722167969\n",
      "1286/3000 train_loss: 26.221872329711914 test_loss:103.86949157714844\n",
      "1287/3000 train_loss: 26.77192497253418 test_loss:123.2245101928711\n",
      "1288/3000 train_loss: 28.407888412475586 test_loss:105.3590087890625\n",
      "1289/3000 train_loss: 24.721628189086914 test_loss:113.68299865722656\n",
      "1290/3000 train_loss: 26.580440521240234 test_loss:111.18182373046875\n",
      "1291/3000 train_loss: 26.484699249267578 test_loss:114.7403793334961\n",
      "1292/3000 train_loss: 27.651208877563477 test_loss:108.87576293945312\n",
      "1293/3000 train_loss: 24.2387752532959 test_loss:105.57179260253906\n",
      "1294/3000 train_loss: 27.30150032043457 test_loss:122.52293395996094\n",
      "1295/3000 train_loss: 28.930208206176758 test_loss:111.41874694824219\n",
      "1296/3000 train_loss: 25.13888931274414 test_loss:109.58991241455078\n",
      "1297/3000 train_loss: 25.22758674621582 test_loss:111.96073913574219\n",
      "1298/3000 train_loss: 27.936126708984375 test_loss:114.53715515136719\n",
      "1299/3000 train_loss: 31.175716400146484 test_loss:113.00125122070312\n",
      "1300/3000 train_loss: 25.509248733520508 test_loss:115.5434341430664\n",
      "1301/3000 train_loss: 25.283031463623047 test_loss:117.33961486816406\n",
      "1302/3000 train_loss: 28.818754196166992 test_loss:121.94944763183594\n",
      "1303/3000 train_loss: 25.930984497070312 test_loss:103.88182830810547\n",
      "1304/3000 train_loss: 27.93006706237793 test_loss:116.75443267822266\n",
      "1305/3000 train_loss: 25.064830780029297 test_loss:102.012939453125\n",
      "1306/3000 train_loss: 32.41191864013672 test_loss:117.07238006591797\n",
      "1307/3000 train_loss: 24.766572952270508 test_loss:115.54364013671875\n",
      "1308/3000 train_loss: 28.9480037689209 test_loss:107.69068908691406\n",
      "1309/3000 train_loss: 25.879915237426758 test_loss:118.42725372314453\n",
      "1310/3000 train_loss: 24.713943481445312 test_loss:104.35767364501953\n",
      "1311/3000 train_loss: 24.34895133972168 test_loss:118.83149719238281\n",
      "1312/3000 train_loss: 25.33382225036621 test_loss:111.96206665039062\n",
      "1313/3000 train_loss: 27.845523834228516 test_loss:115.43194580078125\n",
      "1314/3000 train_loss: 23.201047897338867 test_loss:112.03033447265625\n",
      "1315/3000 train_loss: 26.317739486694336 test_loss:115.79243469238281\n",
      "1316/3000 train_loss: 26.53318977355957 test_loss:116.889404296875\n",
      "1317/3000 train_loss: 30.66781234741211 test_loss:113.18120574951172\n",
      "1318/3000 train_loss: 27.04403305053711 test_loss:111.82571411132812\n",
      "1319/3000 train_loss: 35.539154052734375 test_loss:115.58078002929688\n",
      "1320/3000 train_loss: 25.10980987548828 test_loss:116.74034881591797\n",
      "1321/3000 train_loss: 26.26375961303711 test_loss:112.41632843017578\n",
      "1322/3000 train_loss: 25.63759422302246 test_loss:111.51713562011719\n",
      "1323/3000 train_loss: 29.412151336669922 test_loss:107.77867126464844\n",
      "1324/3000 train_loss: 26.40123176574707 test_loss:108.65638732910156\n",
      "1325/3000 train_loss: 23.89672088623047 test_loss:106.64453125\n",
      "1326/3000 train_loss: 25.202030181884766 test_loss:109.9626235961914\n",
      "1327/3000 train_loss: 30.221363067626953 test_loss:104.13351440429688\n",
      "1328/3000 train_loss: 27.95067024230957 test_loss:124.756591796875\n",
      "1329/3000 train_loss: 27.697233200073242 test_loss:109.15802001953125\n",
      "1330/3000 train_loss: 27.477495193481445 test_loss:104.63856506347656\n",
      "1331/3000 train_loss: 25.778772354125977 test_loss:115.36741638183594\n",
      "1332/3000 train_loss: 24.353553771972656 test_loss:108.91850280761719\n",
      "1333/3000 train_loss: 23.182279586791992 test_loss:111.96932220458984\n",
      "1334/3000 train_loss: 29.74164390563965 test_loss:103.934814453125\n",
      "1335/3000 train_loss: 25.031818389892578 test_loss:110.472900390625\n",
      "1336/3000 train_loss: 27.432140350341797 test_loss:109.82266998291016\n",
      "1337/3000 train_loss: 25.19921875 test_loss:115.92864990234375\n",
      "1338/3000 train_loss: 28.645618438720703 test_loss:110.40428924560547\n",
      "1339/3000 train_loss: 24.346920013427734 test_loss:109.2226791381836\n",
      "1340/3000 train_loss: 24.120363235473633 test_loss:110.54750061035156\n",
      "1341/3000 train_loss: 22.57368278503418 test_loss:110.88397216796875\n",
      "1342/3000 train_loss: 31.637052536010742 test_loss:114.70767974853516\n",
      "1343/3000 train_loss: 29.421415328979492 test_loss:104.68629455566406\n",
      "1344/3000 train_loss: 31.059789657592773 test_loss:123.05359649658203\n",
      "1345/3000 train_loss: 28.63067626953125 test_loss:109.51364135742188\n",
      "1346/3000 train_loss: 26.162841796875 test_loss:107.84996795654297\n",
      "1347/3000 train_loss: 24.804487228393555 test_loss:105.48321533203125\n",
      "1348/3000 train_loss: 24.890731811523438 test_loss:109.92242431640625\n",
      "1349/3000 train_loss: 24.152769088745117 test_loss:112.83296203613281\n",
      "1350/3000 train_loss: 24.882532119750977 test_loss:106.57176208496094\n",
      "1351/3000 train_loss: 27.365636825561523 test_loss:108.03433227539062\n",
      "1352/3000 train_loss: 24.735790252685547 test_loss:108.88398742675781\n",
      "1353/3000 train_loss: 23.91067123413086 test_loss:105.27468872070312\n",
      "1354/3000 train_loss: 25.781986236572266 test_loss:110.7760009765625\n",
      "1355/3000 train_loss: 24.627233505249023 test_loss:107.72825622558594\n",
      "1356/3000 train_loss: 23.730459213256836 test_loss:102.98023986816406\n",
      "1357/3000 train_loss: 24.498992919921875 test_loss:109.99632263183594\n",
      "1358/3000 train_loss: 25.884418487548828 test_loss:108.35362243652344\n",
      "1359/3000 train_loss: 26.37541389465332 test_loss:107.81216430664062\n",
      "1360/3000 train_loss: 23.4653377532959 test_loss:116.1042709350586\n",
      "1361/3000 train_loss: 24.785079956054688 test_loss:103.17178344726562\n",
      "1362/3000 train_loss: 24.919347763061523 test_loss:119.64019775390625\n",
      "1363/3000 train_loss: 27.566709518432617 test_loss:103.18312072753906\n",
      "1364/3000 train_loss: 27.338319778442383 test_loss:117.19690704345703\n",
      "1365/3000 train_loss: 25.9078311920166 test_loss:107.02925109863281\n",
      "1366/3000 train_loss: 26.086641311645508 test_loss:102.19335174560547\n",
      "1367/3000 train_loss: 27.88947296142578 test_loss:103.58464050292969\n",
      "1368/3000 train_loss: 24.415075302124023 test_loss:113.01658630371094\n",
      "1369/3000 train_loss: 27.438167572021484 test_loss:112.27174377441406\n",
      "1370/3000 train_loss: 21.191913604736328 test_loss:107.93296813964844\n",
      "1371/3000 train_loss: 24.67391014099121 test_loss:109.51002502441406\n",
      "1372/3000 train_loss: 28.822574615478516 test_loss:113.6552963256836\n",
      "1373/3000 train_loss: 27.881956100463867 test_loss:110.58557891845703\n",
      "1374/3000 train_loss: 24.08823013305664 test_loss:111.54273223876953\n",
      "1375/3000 train_loss: 24.3684024810791 test_loss:109.10297393798828\n",
      "1376/3000 train_loss: 25.977169036865234 test_loss:106.41450500488281\n",
      "1377/3000 train_loss: 25.948083877563477 test_loss:104.4526138305664\n",
      "1378/3000 train_loss: 28.077350616455078 test_loss:111.60798645019531\n",
      "1379/3000 train_loss: 31.301162719726562 test_loss:107.20762634277344\n",
      "1380/3000 train_loss: 30.921480178833008 test_loss:107.73631286621094\n",
      "1381/3000 train_loss: 23.762556076049805 test_loss:105.73541259765625\n",
      "1382/3000 train_loss: 26.492759704589844 test_loss:103.36077880859375\n",
      "1383/3000 train_loss: 27.105106353759766 test_loss:109.74565124511719\n",
      "1384/3000 train_loss: 25.43610954284668 test_loss:109.87838745117188\n",
      "1385/3000 train_loss: 26.071950912475586 test_loss:108.72271728515625\n",
      "1386/3000 train_loss: 26.635351181030273 test_loss:104.77371215820312\n",
      "1387/3000 train_loss: 27.450942993164062 test_loss:107.03582763671875\n",
      "1388/3000 train_loss: 23.504329681396484 test_loss:112.92646789550781\n",
      "1389/3000 train_loss: 25.88469123840332 test_loss:105.75857543945312\n",
      "1390/3000 train_loss: 25.95590591430664 test_loss:109.11858367919922\n",
      "1391/3000 train_loss: 24.096050262451172 test_loss:104.64075469970703\n",
      "1392/3000 train_loss: 30.314958572387695 test_loss:109.00592041015625\n",
      "1393/3000 train_loss: 27.915035247802734 test_loss:101.94888305664062\n",
      "1394/3000 train_loss: 29.64877700805664 test_loss:102.10774993896484\n",
      "1395/3000 train_loss: 25.9824275970459 test_loss:104.97740173339844\n",
      "1396/3000 train_loss: 24.875267028808594 test_loss:105.4024658203125\n",
      "1397/3000 train_loss: 23.23697853088379 test_loss:111.46905517578125\n",
      "1398/3000 train_loss: 22.85012435913086 test_loss:104.90103912353516\n",
      "1399/3000 train_loss: 26.31703758239746 test_loss:103.8398208618164\n",
      "1400/3000 train_loss: 24.330520629882812 test_loss:97.92424011230469\n",
      "1401/3000 train_loss: 29.1800594329834 test_loss:108.1832504272461\n",
      "1402/3000 train_loss: 23.9440975189209 test_loss:101.09325408935547\n",
      "1403/3000 train_loss: 23.681236267089844 test_loss:106.07582092285156\n",
      "1404/3000 train_loss: 25.817699432373047 test_loss:107.88507843017578\n",
      "1405/3000 train_loss: 26.562164306640625 test_loss:102.310546875\n",
      "1406/3000 train_loss: 24.722610473632812 test_loss:99.23873138427734\n",
      "1407/3000 train_loss: 32.02027893066406 test_loss:105.64596557617188\n",
      "1408/3000 train_loss: 26.461610794067383 test_loss:113.36511993408203\n",
      "1409/3000 train_loss: 26.953989028930664 test_loss:104.05945587158203\n",
      "1410/3000 train_loss: 26.705787658691406 test_loss:101.41309356689453\n",
      "1411/3000 train_loss: 24.622920989990234 test_loss:112.99771118164062\n",
      "1412/3000 train_loss: 28.519474029541016 test_loss:106.33490753173828\n",
      "1413/3000 train_loss: 22.411218643188477 test_loss:102.6736068725586\n",
      "1414/3000 train_loss: 24.787174224853516 test_loss:105.330810546875\n",
      "1415/3000 train_loss: 26.70318031311035 test_loss:103.71592712402344\n",
      "1416/3000 train_loss: 24.88312339782715 test_loss:105.03968048095703\n",
      "1417/3000 train_loss: 25.006309509277344 test_loss:111.08200073242188\n",
      "1418/3000 train_loss: 22.13088607788086 test_loss:102.51742553710938\n",
      "1419/3000 train_loss: 25.543352127075195 test_loss:107.36892700195312\n",
      "1420/3000 train_loss: 25.439998626708984 test_loss:105.24745178222656\n",
      "1421/3000 train_loss: 20.67314338684082 test_loss:105.06787109375\n",
      "1422/3000 train_loss: 24.13614273071289 test_loss:104.28475952148438\n",
      "1423/3000 train_loss: 27.764699935913086 test_loss:103.37367248535156\n",
      "1424/3000 train_loss: 21.940364837646484 test_loss:104.3772964477539\n",
      "1425/3000 train_loss: 22.525039672851562 test_loss:99.06999206542969\n",
      "1426/3000 train_loss: 23.191158294677734 test_loss:107.86149597167969\n",
      "1427/3000 train_loss: 22.141700744628906 test_loss:106.34779357910156\n",
      "1428/3000 train_loss: 24.98055076599121 test_loss:106.02531433105469\n",
      "1429/3000 train_loss: 26.017210006713867 test_loss:108.44969177246094\n",
      "1430/3000 train_loss: 29.260656356811523 test_loss:101.4071044921875\n",
      "1431/3000 train_loss: 23.40566635131836 test_loss:105.55287170410156\n",
      "1432/3000 train_loss: 25.68425178527832 test_loss:100.36544036865234\n",
      "1433/3000 train_loss: 25.85573959350586 test_loss:106.72614288330078\n",
      "1434/3000 train_loss: 26.32278060913086 test_loss:111.50843811035156\n",
      "1435/3000 train_loss: 29.01970100402832 test_loss:103.55549621582031\n",
      "1436/3000 train_loss: 25.769533157348633 test_loss:104.43518829345703\n",
      "1437/3000 train_loss: 23.45281410217285 test_loss:116.60394287109375\n",
      "1438/3000 train_loss: 27.148517608642578 test_loss:106.59052276611328\n",
      "1439/3000 train_loss: 24.849712371826172 test_loss:104.09237670898438\n",
      "1440/3000 train_loss: 26.92843246459961 test_loss:107.47171020507812\n",
      "1441/3000 train_loss: 26.142986297607422 test_loss:106.56550598144531\n",
      "1442/3000 train_loss: 22.37202262878418 test_loss:102.41831970214844\n",
      "1443/3000 train_loss: 29.568220138549805 test_loss:102.0977554321289\n",
      "1444/3000 train_loss: 26.503896713256836 test_loss:108.38119506835938\n",
      "1445/3000 train_loss: 27.020612716674805 test_loss:104.44352722167969\n",
      "1446/3000 train_loss: 27.934396743774414 test_loss:110.99891662597656\n",
      "1447/3000 train_loss: 21.952476501464844 test_loss:105.33654022216797\n",
      "1448/3000 train_loss: 27.30657958984375 test_loss:102.18209075927734\n",
      "1449/3000 train_loss: 30.225515365600586 test_loss:107.79058837890625\n",
      "1450/3000 train_loss: 32.75782775878906 test_loss:109.9421157836914\n",
      "1451/3000 train_loss: 27.034421920776367 test_loss:98.86383056640625\n",
      "1452/3000 train_loss: 26.767297744750977 test_loss:104.271728515625\n",
      "1453/3000 train_loss: 23.0405216217041 test_loss:101.16650390625\n",
      "1454/3000 train_loss: 26.55158042907715 test_loss:106.63897705078125\n",
      "1455/3000 train_loss: 25.50950813293457 test_loss:99.17528533935547\n",
      "1456/3000 train_loss: 25.483552932739258 test_loss:96.88936614990234\n",
      "1457/3000 train_loss: 23.572853088378906 test_loss:104.4595718383789\n",
      "1458/3000 train_loss: 24.631826400756836 test_loss:97.28927612304688\n",
      "1459/3000 train_loss: 24.35881233215332 test_loss:103.98348999023438\n",
      "1460/3000 train_loss: 20.98815155029297 test_loss:100.33525085449219\n",
      "1461/3000 train_loss: 24.095735549926758 test_loss:101.99581146240234\n",
      "1462/3000 train_loss: 26.572277069091797 test_loss:99.98839569091797\n",
      "1463/3000 train_loss: 23.013362884521484 test_loss:106.93318176269531\n",
      "1464/3000 train_loss: 30.542884826660156 test_loss:108.40835571289062\n",
      "1465/3000 train_loss: 24.949478149414062 test_loss:97.27516174316406\n",
      "1466/3000 train_loss: 25.819679260253906 test_loss:105.82705688476562\n",
      "1467/3000 train_loss: 29.597349166870117 test_loss:105.2281723022461\n",
      "1468/3000 train_loss: 24.30786895751953 test_loss:105.39973449707031\n",
      "1469/3000 train_loss: 23.24180030822754 test_loss:101.4263916015625\n",
      "1470/3000 train_loss: 24.422937393188477 test_loss:96.716064453125\n",
      "1471/3000 train_loss: 25.03457260131836 test_loss:99.94087982177734\n",
      "1472/3000 train_loss: 24.30982780456543 test_loss:99.99141693115234\n",
      "1473/3000 train_loss: 28.0752010345459 test_loss:104.7488021850586\n",
      "1474/3000 train_loss: 22.150287628173828 test_loss:103.779052734375\n",
      "1475/3000 train_loss: 21.5329532623291 test_loss:100.64292907714844\n",
      "1476/3000 train_loss: 23.272865295410156 test_loss:107.01632690429688\n",
      "1477/3000 train_loss: 24.898788452148438 test_loss:99.5445556640625\n",
      "1478/3000 train_loss: 34.63173294067383 test_loss:105.98749542236328\n",
      "1479/3000 train_loss: 23.72623062133789 test_loss:105.71511840820312\n",
      "1480/3000 train_loss: 23.352434158325195 test_loss:97.90379333496094\n",
      "1481/3000 train_loss: 23.245410919189453 test_loss:105.16716766357422\n",
      "1482/3000 train_loss: 20.884986877441406 test_loss:101.71477508544922\n",
      "1483/3000 train_loss: 26.986289978027344 test_loss:99.77172088623047\n",
      "1484/3000 train_loss: 23.210527420043945 test_loss:100.4910888671875\n",
      "1485/3000 train_loss: 21.461618423461914 test_loss:100.64484405517578\n",
      "1486/3000 train_loss: 23.44623374938965 test_loss:106.27447509765625\n",
      "1487/3000 train_loss: 24.387731552124023 test_loss:109.4394302368164\n",
      "1488/3000 train_loss: 24.288192749023438 test_loss:99.20211791992188\n",
      "1489/3000 train_loss: 25.77720832824707 test_loss:103.40231323242188\n",
      "1490/3000 train_loss: 22.591732025146484 test_loss:101.25371551513672\n",
      "1491/3000 train_loss: 23.234397888183594 test_loss:102.5748291015625\n",
      "1492/3000 train_loss: 25.810041427612305 test_loss:106.26608276367188\n",
      "1493/3000 train_loss: 29.04904556274414 test_loss:102.57888793945312\n",
      "1494/3000 train_loss: 23.66377067565918 test_loss:111.32313537597656\n",
      "1495/3000 train_loss: 24.53884506225586 test_loss:104.32614135742188\n",
      "1496/3000 train_loss: 23.18238639831543 test_loss:101.19999694824219\n",
      "1497/3000 train_loss: 22.406923294067383 test_loss:97.10873413085938\n",
      "1498/3000 train_loss: 23.59642791748047 test_loss:100.00874328613281\n",
      "1499/3000 train_loss: 24.938188552856445 test_loss:104.69061279296875\n",
      "1500/3000 train_loss: 25.546554565429688 test_loss:107.8565673828125\n",
      "1501/3000 train_loss: 24.78745460510254 test_loss:102.05976104736328\n",
      "1502/3000 train_loss: 26.02890968322754 test_loss:109.98037719726562\n",
      "1503/3000 train_loss: 23.247743606567383 test_loss:97.63246154785156\n",
      "1504/3000 train_loss: 21.710285186767578 test_loss:106.27604675292969\n",
      "1505/3000 train_loss: 24.356224060058594 test_loss:101.23127746582031\n",
      "1506/3000 train_loss: 24.56340980529785 test_loss:98.74177551269531\n",
      "1507/3000 train_loss: 28.839750289916992 test_loss:103.24575805664062\n",
      "1508/3000 train_loss: 23.84469985961914 test_loss:101.67160034179688\n",
      "1509/3000 train_loss: 23.77297019958496 test_loss:98.06071472167969\n",
      "1510/3000 train_loss: 27.82379913330078 test_loss:116.43923950195312\n",
      "1511/3000 train_loss: 23.21944236755371 test_loss:100.61723327636719\n",
      "1512/3000 train_loss: 23.92220115661621 test_loss:96.06395721435547\n",
      "1513/3000 train_loss: 22.023962020874023 test_loss:97.6684799194336\n",
      "1514/3000 train_loss: 26.798606872558594 test_loss:104.32139587402344\n",
      "1515/3000 train_loss: 24.045244216918945 test_loss:95.46932983398438\n",
      "1516/3000 train_loss: 20.614301681518555 test_loss:100.23075866699219\n",
      "1517/3000 train_loss: 26.324264526367188 test_loss:97.15443420410156\n",
      "1518/3000 train_loss: 22.024982452392578 test_loss:103.14118957519531\n",
      "1519/3000 train_loss: 21.545940399169922 test_loss:97.4399642944336\n",
      "1520/3000 train_loss: 21.707748413085938 test_loss:108.91648864746094\n",
      "1521/3000 train_loss: 22.89012908935547 test_loss:99.28237915039062\n",
      "1522/3000 train_loss: 22.635976791381836 test_loss:109.32865142822266\n",
      "1523/3000 train_loss: 26.011260986328125 test_loss:109.01644134521484\n",
      "1524/3000 train_loss: 26.41252326965332 test_loss:103.41613006591797\n",
      "1525/3000 train_loss: 24.230161666870117 test_loss:107.68122100830078\n",
      "1526/3000 train_loss: 32.990333557128906 test_loss:99.25853729248047\n",
      "1527/3000 train_loss: 24.381824493408203 test_loss:108.77650451660156\n",
      "1528/3000 train_loss: 23.454389572143555 test_loss:99.79879760742188\n",
      "1529/3000 train_loss: 20.59368896484375 test_loss:105.39900970458984\n",
      "1530/3000 train_loss: 27.71665382385254 test_loss:108.16754150390625\n",
      "1531/3000 train_loss: 25.04503631591797 test_loss:100.81946563720703\n",
      "1532/3000 train_loss: 22.987817764282227 test_loss:107.85655212402344\n",
      "1533/3000 train_loss: 22.43074607849121 test_loss:102.69099426269531\n",
      "1534/3000 train_loss: 23.575180053710938 test_loss:116.25318145751953\n",
      "1535/3000 train_loss: 23.069358825683594 test_loss:96.57852935791016\n",
      "1536/3000 train_loss: 23.22623062133789 test_loss:111.62521362304688\n",
      "1537/3000 train_loss: 23.28298568725586 test_loss:108.4797134399414\n",
      "1538/3000 train_loss: 30.526138305664062 test_loss:107.77993774414062\n",
      "1539/3000 train_loss: 20.742084503173828 test_loss:105.48391723632812\n",
      "1540/3000 train_loss: 22.153268814086914 test_loss:106.16920471191406\n",
      "1541/3000 train_loss: 20.469636917114258 test_loss:108.27320861816406\n",
      "1542/3000 train_loss: 22.81146240234375 test_loss:103.04129028320312\n",
      "1543/3000 train_loss: 23.9403018951416 test_loss:115.3660659790039\n",
      "1544/3000 train_loss: 20.662193298339844 test_loss:102.27207946777344\n",
      "1545/3000 train_loss: 24.01212501525879 test_loss:102.96168518066406\n",
      "1546/3000 train_loss: 20.635278701782227 test_loss:103.76126098632812\n",
      "1547/3000 train_loss: 21.82256507873535 test_loss:104.12962341308594\n",
      "1548/3000 train_loss: 21.706188201904297 test_loss:107.44577026367188\n",
      "1549/3000 train_loss: 26.529865264892578 test_loss:110.5090103149414\n",
      "1550/3000 train_loss: 28.3487491607666 test_loss:108.3212661743164\n",
      "1551/3000 train_loss: 21.27141761779785 test_loss:102.05489349365234\n",
      "1552/3000 train_loss: 21.119234085083008 test_loss:120.6264419555664\n",
      "1553/3000 train_loss: 25.01384162902832 test_loss:107.3852767944336\n",
      "1554/3000 train_loss: 25.2368106842041 test_loss:106.19093322753906\n",
      "1555/3000 train_loss: 26.364757537841797 test_loss:109.76444244384766\n",
      "1556/3000 train_loss: 21.533586502075195 test_loss:108.32685852050781\n",
      "1557/3000 train_loss: 24.011024475097656 test_loss:98.89120483398438\n",
      "1558/3000 train_loss: 24.005573272705078 test_loss:102.9686050415039\n",
      "1559/3000 train_loss: 20.67954444885254 test_loss:101.69754791259766\n",
      "1560/3000 train_loss: 22.23678970336914 test_loss:105.27018737792969\n",
      "1561/3000 train_loss: 24.465248107910156 test_loss:107.32376098632812\n",
      "1562/3000 train_loss: 21.340620040893555 test_loss:105.79376983642578\n",
      "1563/3000 train_loss: 22.513402938842773 test_loss:107.11445617675781\n",
      "1564/3000 train_loss: 25.331584930419922 test_loss:105.677734375\n",
      "1565/3000 train_loss: 20.602947235107422 test_loss:99.57499694824219\n",
      "1566/3000 train_loss: 24.254209518432617 test_loss:104.96613311767578\n",
      "1567/3000 train_loss: 23.92006492614746 test_loss:105.00694274902344\n",
      "1568/3000 train_loss: 20.95387077331543 test_loss:105.40444946289062\n",
      "1569/3000 train_loss: 22.89564323425293 test_loss:103.09322357177734\n",
      "1570/3000 train_loss: 19.637998580932617 test_loss:98.89154815673828\n",
      "1571/3000 train_loss: 20.23960304260254 test_loss:107.12809753417969\n",
      "1572/3000 train_loss: 18.724567413330078 test_loss:105.40907287597656\n",
      "1573/3000 train_loss: 23.898408889770508 test_loss:103.91938781738281\n",
      "1574/3000 train_loss: 21.952329635620117 test_loss:99.05101013183594\n",
      "1575/3000 train_loss: 19.573266983032227 test_loss:102.9699478149414\n",
      "1576/3000 train_loss: 20.261507034301758 test_loss:108.69921112060547\n",
      "1577/3000 train_loss: 22.430810928344727 test_loss:104.81209564208984\n",
      "1578/3000 train_loss: 23.201580047607422 test_loss:98.12384033203125\n",
      "1579/3000 train_loss: 23.738325119018555 test_loss:102.00634765625\n",
      "1580/3000 train_loss: 19.569704055786133 test_loss:101.69847106933594\n",
      "1581/3000 train_loss: 21.567102432250977 test_loss:103.7232666015625\n",
      "1582/3000 train_loss: 23.034833908081055 test_loss:100.8341064453125\n",
      "1583/3000 train_loss: 20.310466766357422 test_loss:103.23974609375\n",
      "1584/3000 train_loss: 21.940567016601562 test_loss:97.85983276367188\n",
      "1585/3000 train_loss: 21.212692260742188 test_loss:96.55430603027344\n",
      "1586/3000 train_loss: 19.274309158325195 test_loss:107.20278930664062\n",
      "1587/3000 train_loss: 23.096553802490234 test_loss:103.1545181274414\n",
      "1588/3000 train_loss: 25.687562942504883 test_loss:102.09123992919922\n",
      "1589/3000 train_loss: 30.03081512451172 test_loss:101.63875579833984\n",
      "1590/3000 train_loss: 23.003705978393555 test_loss:107.83023071289062\n",
      "1591/3000 train_loss: 20.38606071472168 test_loss:108.89096069335938\n",
      "1592/3000 train_loss: 21.848060607910156 test_loss:97.31330871582031\n",
      "1593/3000 train_loss: 22.26889419555664 test_loss:103.79179382324219\n",
      "1594/3000 train_loss: 20.471046447753906 test_loss:102.4172134399414\n",
      "1595/3000 train_loss: 22.354454040527344 test_loss:99.06086730957031\n",
      "1596/3000 train_loss: 22.91440773010254 test_loss:95.88174438476562\n",
      "1597/3000 train_loss: 25.281597137451172 test_loss:105.91184997558594\n",
      "1598/3000 train_loss: 23.86786460876465 test_loss:98.55748748779297\n",
      "1599/3000 train_loss: 21.990346908569336 test_loss:98.12113952636719\n",
      "1600/3000 train_loss: 23.732654571533203 test_loss:112.26216125488281\n",
      "1601/3000 train_loss: 20.69672393798828 test_loss:100.43321990966797\n",
      "1602/3000 train_loss: 24.90142250061035 test_loss:105.69026947021484\n",
      "1603/3000 train_loss: 22.371339797973633 test_loss:115.4092025756836\n",
      "1604/3000 train_loss: 25.2436580657959 test_loss:102.19133758544922\n",
      "1605/3000 train_loss: 21.33123779296875 test_loss:107.7226333618164\n",
      "1606/3000 train_loss: 22.1763916015625 test_loss:106.69522094726562\n",
      "1607/3000 train_loss: 20.61612892150879 test_loss:101.43011474609375\n",
      "1608/3000 train_loss: 25.342079162597656 test_loss:104.26286315917969\n",
      "1609/3000 train_loss: 21.195350646972656 test_loss:104.10028076171875\n",
      "1610/3000 train_loss: 24.33783531188965 test_loss:108.97356414794922\n",
      "1611/3000 train_loss: 23.4549617767334 test_loss:105.54261779785156\n",
      "1612/3000 train_loss: 22.92386245727539 test_loss:106.8099365234375\n",
      "1613/3000 train_loss: 24.782875061035156 test_loss:113.48947143554688\n",
      "1614/3000 train_loss: 21.07027816772461 test_loss:98.66693115234375\n",
      "1615/3000 train_loss: 21.989927291870117 test_loss:96.37483215332031\n",
      "1616/3000 train_loss: 24.827287673950195 test_loss:102.99407196044922\n",
      "1617/3000 train_loss: 25.41507911682129 test_loss:122.40814208984375\n",
      "1618/3000 train_loss: 24.97696876525879 test_loss:95.598388671875\n",
      "1619/3000 train_loss: 21.054838180541992 test_loss:104.37722778320312\n",
      "1620/3000 train_loss: 19.6021728515625 test_loss:99.49491882324219\n",
      "1621/3000 train_loss: 20.536561965942383 test_loss:96.99243927001953\n",
      "1622/3000 train_loss: 21.61391258239746 test_loss:98.2044906616211\n",
      "1623/3000 train_loss: 21.027067184448242 test_loss:99.08570861816406\n",
      "1624/3000 train_loss: 19.15772247314453 test_loss:95.63851928710938\n",
      "1625/3000 train_loss: 22.547901153564453 test_loss:95.45841979980469\n",
      "1626/3000 train_loss: 20.29826545715332 test_loss:99.33653259277344\n",
      "1627/3000 train_loss: 20.90812873840332 test_loss:103.30204772949219\n",
      "1628/3000 train_loss: 25.89229965209961 test_loss:101.84002685546875\n",
      "1629/3000 train_loss: 24.768213272094727 test_loss:99.38604736328125\n",
      "1630/3000 train_loss: 21.96393585205078 test_loss:100.25485229492188\n",
      "1631/3000 train_loss: 22.246824264526367 test_loss:102.9795913696289\n",
      "1632/3000 train_loss: 22.890483856201172 test_loss:99.32965850830078\n",
      "1633/3000 train_loss: 26.51775360107422 test_loss:99.7750244140625\n",
      "1634/3000 train_loss: 21.279644012451172 test_loss:107.58073425292969\n",
      "1635/3000 train_loss: 24.297727584838867 test_loss:101.36090087890625\n",
      "1636/3000 train_loss: 20.8601016998291 test_loss:97.93939971923828\n",
      "1637/3000 train_loss: 21.30812644958496 test_loss:99.06297302246094\n",
      "1638/3000 train_loss: 20.427940368652344 test_loss:107.33810424804688\n",
      "1639/3000 train_loss: 20.32035255432129 test_loss:100.10598754882812\n",
      "1640/3000 train_loss: 18.324546813964844 test_loss:106.38153076171875\n",
      "1641/3000 train_loss: 22.658321380615234 test_loss:101.13755798339844\n",
      "1642/3000 train_loss: 21.082475662231445 test_loss:103.603759765625\n",
      "1643/3000 train_loss: 21.751434326171875 test_loss:97.91138458251953\n",
      "1644/3000 train_loss: 20.261632919311523 test_loss:98.74055480957031\n",
      "1645/3000 train_loss: 23.421463012695312 test_loss:97.39167785644531\n",
      "1646/3000 train_loss: 20.604591369628906 test_loss:96.4515380859375\n",
      "1647/3000 train_loss: 20.633014678955078 test_loss:101.25208282470703\n",
      "1648/3000 train_loss: 25.27620506286621 test_loss:96.91058349609375\n",
      "1649/3000 train_loss: 23.906089782714844 test_loss:103.20402526855469\n",
      "1650/3000 train_loss: 24.285934448242188 test_loss:104.95630645751953\n",
      "1651/3000 train_loss: 23.62235450744629 test_loss:107.84039306640625\n",
      "1652/3000 train_loss: 18.987886428833008 test_loss:99.89982604980469\n",
      "1653/3000 train_loss: 23.89601707458496 test_loss:103.42524719238281\n",
      "1654/3000 train_loss: 18.913169860839844 test_loss:95.16697692871094\n",
      "1655/3000 train_loss: 18.043867111206055 test_loss:99.94750213623047\n",
      "1656/3000 train_loss: 20.861785888671875 test_loss:94.8112564086914\n",
      "1657/3000 train_loss: 23.526832580566406 test_loss:98.42098236083984\n",
      "1658/3000 train_loss: 19.321887969970703 test_loss:97.3905258178711\n",
      "1659/3000 train_loss: 24.825279235839844 test_loss:99.72331237792969\n",
      "1660/3000 train_loss: 22.705434799194336 test_loss:92.6923828125\n",
      "1661/3000 train_loss: 20.716838836669922 test_loss:99.01300811767578\n",
      "1662/3000 train_loss: 25.59269142150879 test_loss:104.22703552246094\n",
      "1663/3000 train_loss: 25.893375396728516 test_loss:110.13156127929688\n",
      "1664/3000 train_loss: 19.981351852416992 test_loss:95.77197265625\n",
      "1665/3000 train_loss: 20.981197357177734 test_loss:93.04515075683594\n",
      "1666/3000 train_loss: 21.705110549926758 test_loss:105.81950378417969\n",
      "1667/3000 train_loss: 22.11798095703125 test_loss:101.12736511230469\n",
      "1668/3000 train_loss: 20.917369842529297 test_loss:103.17845153808594\n",
      "1669/3000 train_loss: 21.376907348632812 test_loss:103.15643310546875\n",
      "1670/3000 train_loss: 20.881439208984375 test_loss:101.2664794921875\n",
      "1671/3000 train_loss: 21.26439666748047 test_loss:102.92057800292969\n",
      "1672/3000 train_loss: 20.728715896606445 test_loss:94.40393829345703\n",
      "1673/3000 train_loss: 21.140308380126953 test_loss:95.61628723144531\n",
      "1674/3000 train_loss: 19.20899772644043 test_loss:110.0910873413086\n",
      "1675/3000 train_loss: 22.651317596435547 test_loss:100.94290161132812\n",
      "1676/3000 train_loss: 21.691909790039062 test_loss:97.4400405883789\n",
      "1677/3000 train_loss: 20.918968200683594 test_loss:100.24806213378906\n",
      "1678/3000 train_loss: 21.212644577026367 test_loss:98.34561920166016\n",
      "1679/3000 train_loss: 24.069076538085938 test_loss:104.58830261230469\n",
      "1680/3000 train_loss: 23.50893783569336 test_loss:98.83538818359375\n",
      "1681/3000 train_loss: 22.119022369384766 test_loss:103.94387817382812\n",
      "1682/3000 train_loss: 24.4068546295166 test_loss:97.91120910644531\n",
      "1683/3000 train_loss: 20.261751174926758 test_loss:98.05278778076172\n",
      "1684/3000 train_loss: 18.993595123291016 test_loss:96.31884765625\n",
      "1685/3000 train_loss: 24.831317901611328 test_loss:99.09077453613281\n",
      "1686/3000 train_loss: 19.54024314880371 test_loss:111.4749755859375\n",
      "1687/3000 train_loss: 21.950504302978516 test_loss:101.94755554199219\n",
      "1688/3000 train_loss: 24.556995391845703 test_loss:96.07682800292969\n",
      "1689/3000 train_loss: 20.614164352416992 test_loss:103.27569580078125\n",
      "1690/3000 train_loss: 19.840469360351562 test_loss:102.40986633300781\n",
      "1691/3000 train_loss: 20.81668472290039 test_loss:103.69014739990234\n",
      "1692/3000 train_loss: 18.251527786254883 test_loss:97.26663208007812\n",
      "1693/3000 train_loss: 19.71986198425293 test_loss:106.36160278320312\n",
      "1694/3000 train_loss: 19.35269546508789 test_loss:96.01002502441406\n",
      "1695/3000 train_loss: 19.187477111816406 test_loss:100.84819030761719\n",
      "1696/3000 train_loss: 21.831523895263672 test_loss:93.74990844726562\n",
      "1697/3000 train_loss: 19.340606689453125 test_loss:111.64225769042969\n",
      "1698/3000 train_loss: 27.17829132080078 test_loss:96.94623565673828\n",
      "1699/3000 train_loss: 19.78105354309082 test_loss:105.31883239746094\n",
      "1700/3000 train_loss: 20.42491340637207 test_loss:94.22530364990234\n",
      "1701/3000 train_loss: 23.124107360839844 test_loss:97.72249603271484\n",
      "1702/3000 train_loss: 21.372007369995117 test_loss:106.02561950683594\n",
      "1703/3000 train_loss: 23.66738510131836 test_loss:93.82977294921875\n",
      "1704/3000 train_loss: 18.6682071685791 test_loss:103.9873046875\n",
      "1705/3000 train_loss: 18.908658981323242 test_loss:94.7970199584961\n",
      "1706/3000 train_loss: 21.238447189331055 test_loss:99.47664642333984\n",
      "1707/3000 train_loss: 18.586122512817383 test_loss:91.16702270507812\n",
      "1708/3000 train_loss: 20.555816650390625 test_loss:101.429931640625\n",
      "1709/3000 train_loss: 20.886938095092773 test_loss:96.70513916015625\n",
      "1710/3000 train_loss: 23.06670379638672 test_loss:97.82344055175781\n",
      "1711/3000 train_loss: 19.180923461914062 test_loss:93.36333465576172\n",
      "1712/3000 train_loss: 19.70220947265625 test_loss:97.78496551513672\n",
      "1713/3000 train_loss: 18.638124465942383 test_loss:93.03779602050781\n",
      "1714/3000 train_loss: 20.632734298706055 test_loss:91.88823699951172\n",
      "1715/3000 train_loss: 20.162294387817383 test_loss:93.8772201538086\n",
      "1716/3000 train_loss: 20.556636810302734 test_loss:92.56698608398438\n",
      "1717/3000 train_loss: 22.84476089477539 test_loss:103.62581634521484\n",
      "1718/3000 train_loss: 21.650102615356445 test_loss:104.06700134277344\n",
      "1719/3000 train_loss: 27.0640869140625 test_loss:96.92115020751953\n",
      "1720/3000 train_loss: 23.198698043823242 test_loss:103.3041000366211\n",
      "1721/3000 train_loss: 23.279138565063477 test_loss:101.13526916503906\n",
      "1722/3000 train_loss: 21.26662254333496 test_loss:97.47215270996094\n",
      "1723/3000 train_loss: 20.567747116088867 test_loss:99.76734161376953\n",
      "1724/3000 train_loss: 18.9360408782959 test_loss:95.94776916503906\n",
      "1725/3000 train_loss: 21.223037719726562 test_loss:104.22608947753906\n",
      "1726/3000 train_loss: 17.590118408203125 test_loss:96.54728698730469\n",
      "1727/3000 train_loss: 17.06564712524414 test_loss:96.2748031616211\n",
      "1728/3000 train_loss: 20.768959045410156 test_loss:102.90867614746094\n",
      "1729/3000 train_loss: 21.507932662963867 test_loss:99.58951568603516\n",
      "1730/3000 train_loss: 17.803937911987305 test_loss:95.29161071777344\n",
      "1731/3000 train_loss: 21.241348266601562 test_loss:101.32048034667969\n",
      "1732/3000 train_loss: 18.729900360107422 test_loss:97.28083801269531\n",
      "1733/3000 train_loss: 23.034221649169922 test_loss:96.41429901123047\n",
      "1734/3000 train_loss: 21.56753921508789 test_loss:95.23672485351562\n",
      "1735/3000 train_loss: 20.232362747192383 test_loss:95.21844482421875\n",
      "1736/3000 train_loss: 20.16952896118164 test_loss:91.83953857421875\n",
      "1737/3000 train_loss: 19.746135711669922 test_loss:99.27584838867188\n",
      "1738/3000 train_loss: 22.117877960205078 test_loss:96.75785827636719\n",
      "1739/3000 train_loss: 23.116243362426758 test_loss:92.15172576904297\n",
      "1740/3000 train_loss: 21.034664154052734 test_loss:98.68817138671875\n",
      "1741/3000 train_loss: 21.68642807006836 test_loss:101.37056732177734\n",
      "1742/3000 train_loss: 20.621681213378906 test_loss:96.77641296386719\n",
      "1743/3000 train_loss: 22.103055953979492 test_loss:95.84407043457031\n",
      "1744/3000 train_loss: 19.80396270751953 test_loss:96.27957916259766\n",
      "1745/3000 train_loss: 22.804641723632812 test_loss:94.5478744506836\n",
      "1746/3000 train_loss: 24.43033218383789 test_loss:96.51568603515625\n",
      "1747/3000 train_loss: 20.24814224243164 test_loss:102.13732147216797\n",
      "1748/3000 train_loss: 19.36020851135254 test_loss:91.31118774414062\n",
      "1749/3000 train_loss: 23.310190200805664 test_loss:99.80950927734375\n",
      "1750/3000 train_loss: 22.287921905517578 test_loss:87.40444946289062\n",
      "1751/3000 train_loss: 19.411840438842773 test_loss:94.95616912841797\n",
      "1752/3000 train_loss: 20.04673957824707 test_loss:95.31855010986328\n",
      "1753/3000 train_loss: 20.947471618652344 test_loss:89.73345947265625\n",
      "1754/3000 train_loss: 23.596912384033203 test_loss:101.0079574584961\n",
      "1755/3000 train_loss: 21.292049407958984 test_loss:90.27053833007812\n",
      "1756/3000 train_loss: 19.9848690032959 test_loss:97.79988098144531\n",
      "1757/3000 train_loss: 20.02028465270996 test_loss:95.74578857421875\n",
      "1758/3000 train_loss: 23.170690536499023 test_loss:89.19868469238281\n",
      "1759/3000 train_loss: 20.41299057006836 test_loss:90.57870483398438\n",
      "1760/3000 train_loss: 23.28384780883789 test_loss:88.89033508300781\n",
      "1761/3000 train_loss: 20.229721069335938 test_loss:93.4947509765625\n",
      "1762/3000 train_loss: 17.61527442932129 test_loss:92.03211975097656\n",
      "1763/3000 train_loss: 22.39507293701172 test_loss:92.14741516113281\n",
      "1764/3000 train_loss: 22.083065032958984 test_loss:92.51628112792969\n",
      "1765/3000 train_loss: 17.82021713256836 test_loss:97.31002807617188\n",
      "1766/3000 train_loss: 18.310861587524414 test_loss:90.62602233886719\n",
      "1767/3000 train_loss: 19.406919479370117 test_loss:91.36782836914062\n",
      "1768/3000 train_loss: 20.508682250976562 test_loss:106.35839080810547\n",
      "1769/3000 train_loss: 20.626968383789062 test_loss:93.80870056152344\n",
      "1770/3000 train_loss: 17.819900512695312 test_loss:94.56620025634766\n",
      "1771/3000 train_loss: 18.091882705688477 test_loss:90.29833984375\n",
      "1772/3000 train_loss: 23.341899871826172 test_loss:103.34603881835938\n",
      "1773/3000 train_loss: 19.325637817382812 test_loss:92.8561019897461\n",
      "1774/3000 train_loss: 21.21759605407715 test_loss:87.97044372558594\n",
      "1775/3000 train_loss: 23.774173736572266 test_loss:95.723388671875\n",
      "1776/3000 train_loss: 20.49239730834961 test_loss:90.8128433227539\n",
      "1777/3000 train_loss: 19.618677139282227 test_loss:96.94206237792969\n",
      "1778/3000 train_loss: 21.177640914916992 test_loss:91.67743682861328\n",
      "1779/3000 train_loss: 20.212188720703125 test_loss:99.0341567993164\n",
      "1780/3000 train_loss: 23.119688034057617 test_loss:100.95263671875\n",
      "1781/3000 train_loss: 21.779701232910156 test_loss:90.73762512207031\n",
      "1782/3000 train_loss: 17.29561424255371 test_loss:94.20946502685547\n",
      "1783/3000 train_loss: 26.527462005615234 test_loss:96.81665802001953\n",
      "1784/3000 train_loss: 20.630971908569336 test_loss:94.08613586425781\n",
      "1785/3000 train_loss: 23.92832374572754 test_loss:110.82150268554688\n",
      "1786/3000 train_loss: 21.063806533813477 test_loss:93.15576171875\n",
      "1787/3000 train_loss: 19.038145065307617 test_loss:101.21963500976562\n",
      "1788/3000 train_loss: 20.491647720336914 test_loss:95.17375946044922\n",
      "1789/3000 train_loss: 19.03927993774414 test_loss:93.87891387939453\n",
      "1790/3000 train_loss: 19.439586639404297 test_loss:97.71337127685547\n",
      "1791/3000 train_loss: 23.558738708496094 test_loss:94.90043640136719\n",
      "1792/3000 train_loss: 20.81692123413086 test_loss:96.59163665771484\n",
      "1793/3000 train_loss: 25.3138427734375 test_loss:102.27519989013672\n",
      "1794/3000 train_loss: 17.98029327392578 test_loss:94.5315933227539\n",
      "1795/3000 train_loss: 22.076353073120117 test_loss:96.792236328125\n",
      "1796/3000 train_loss: 21.329967498779297 test_loss:93.2099838256836\n",
      "1797/3000 train_loss: 23.609235763549805 test_loss:94.135498046875\n",
      "1798/3000 train_loss: 20.6947021484375 test_loss:96.39966583251953\n",
      "1799/3000 train_loss: 19.758678436279297 test_loss:88.85292053222656\n",
      "1800/3000 train_loss: 20.346696853637695 test_loss:98.81855773925781\n",
      "1801/3000 train_loss: 22.217632293701172 test_loss:92.41520690917969\n",
      "1802/3000 train_loss: 23.963119506835938 test_loss:104.85944366455078\n",
      "1803/3000 train_loss: 18.031972885131836 test_loss:93.12744140625\n",
      "1804/3000 train_loss: 21.096332550048828 test_loss:94.41950988769531\n",
      "1805/3000 train_loss: 21.251283645629883 test_loss:94.87847900390625\n",
      "1806/3000 train_loss: 19.319400787353516 test_loss:96.59195709228516\n",
      "1807/3000 train_loss: 20.514493942260742 test_loss:98.28683471679688\n",
      "1808/3000 train_loss: 19.316898345947266 test_loss:97.56996154785156\n",
      "1809/3000 train_loss: 17.942716598510742 test_loss:90.55852508544922\n",
      "1810/3000 train_loss: 18.76874542236328 test_loss:97.29641723632812\n",
      "1811/3000 train_loss: 22.339914321899414 test_loss:92.04237365722656\n",
      "1812/3000 train_loss: 18.53968620300293 test_loss:95.68475341796875\n",
      "1813/3000 train_loss: 20.933605194091797 test_loss:93.37792205810547\n",
      "1814/3000 train_loss: 21.63204002380371 test_loss:95.152099609375\n",
      "1815/3000 train_loss: 18.24121856689453 test_loss:89.99095153808594\n",
      "1816/3000 train_loss: 19.176076889038086 test_loss:94.74002075195312\n",
      "1817/3000 train_loss: 18.426292419433594 test_loss:94.99736022949219\n",
      "1818/3000 train_loss: 17.091184616088867 test_loss:95.54914855957031\n",
      "1819/3000 train_loss: 19.163522720336914 test_loss:88.05062866210938\n",
      "1820/3000 train_loss: 29.30430030822754 test_loss:112.3167724609375\n",
      "1821/3000 train_loss: 20.932392120361328 test_loss:90.60908508300781\n",
      "1822/3000 train_loss: 17.99310302734375 test_loss:96.22747039794922\n",
      "1823/3000 train_loss: 23.50688362121582 test_loss:100.96806335449219\n",
      "1824/3000 train_loss: 22.00335121154785 test_loss:89.50457763671875\n",
      "1825/3000 train_loss: 18.497451782226562 test_loss:90.51278686523438\n",
      "1826/3000 train_loss: 22.090938568115234 test_loss:101.920166015625\n",
      "1827/3000 train_loss: 21.611080169677734 test_loss:92.00135803222656\n",
      "1828/3000 train_loss: 21.572486877441406 test_loss:90.96774291992188\n",
      "1829/3000 train_loss: 22.39381980895996 test_loss:100.27029418945312\n",
      "1830/3000 train_loss: 23.396596908569336 test_loss:102.8043212890625\n",
      "1831/3000 train_loss: 22.45067596435547 test_loss:96.12876892089844\n",
      "1832/3000 train_loss: 19.916545867919922 test_loss:95.98973846435547\n",
      "1833/3000 train_loss: 23.320423126220703 test_loss:97.49359893798828\n",
      "1834/3000 train_loss: 21.64149284362793 test_loss:98.61100769042969\n",
      "1835/3000 train_loss: 20.188127517700195 test_loss:93.37094116210938\n",
      "1836/3000 train_loss: 21.573232650756836 test_loss:96.39889526367188\n",
      "1837/3000 train_loss: 21.323450088500977 test_loss:94.15750885009766\n",
      "1838/3000 train_loss: 19.466615676879883 test_loss:96.40510559082031\n",
      "1839/3000 train_loss: 17.914318084716797 test_loss:96.65223693847656\n",
      "1840/3000 train_loss: 17.3726863861084 test_loss:94.47724151611328\n",
      "1841/3000 train_loss: 18.931379318237305 test_loss:99.94779968261719\n",
      "1842/3000 train_loss: 17.1903133392334 test_loss:89.63975524902344\n",
      "1843/3000 train_loss: 20.80926513671875 test_loss:98.44232940673828\n",
      "1844/3000 train_loss: 21.449087142944336 test_loss:93.59321594238281\n",
      "1845/3000 train_loss: 18.457841873168945 test_loss:94.38187408447266\n",
      "1846/3000 train_loss: 24.041725158691406 test_loss:97.1938705444336\n",
      "1847/3000 train_loss: 18.495647430419922 test_loss:90.62764739990234\n",
      "1848/3000 train_loss: 19.673789978027344 test_loss:92.98770141601562\n",
      "1849/3000 train_loss: 19.596179962158203 test_loss:90.48855590820312\n",
      "1850/3000 train_loss: 19.91191291809082 test_loss:93.4673843383789\n",
      "1851/3000 train_loss: 22.425281524658203 test_loss:93.85609436035156\n",
      "1852/3000 train_loss: 20.036903381347656 test_loss:92.56401062011719\n",
      "1853/3000 train_loss: 16.354713439941406 test_loss:88.80624389648438\n",
      "1854/3000 train_loss: 18.056711196899414 test_loss:93.72531127929688\n",
      "1855/3000 train_loss: 20.398839950561523 test_loss:90.1012191772461\n",
      "1856/3000 train_loss: 16.89527130126953 test_loss:86.79832458496094\n",
      "1857/3000 train_loss: 16.940303802490234 test_loss:86.49191284179688\n",
      "1858/3000 train_loss: 17.063800811767578 test_loss:89.62258911132812\n",
      "1859/3000 train_loss: 21.07170295715332 test_loss:85.10862731933594\n",
      "1860/3000 train_loss: 18.92326545715332 test_loss:90.83918762207031\n",
      "1861/3000 train_loss: 18.672801971435547 test_loss:93.14321899414062\n",
      "1862/3000 train_loss: 19.83926773071289 test_loss:100.80651092529297\n",
      "1863/3000 train_loss: 19.92893409729004 test_loss:86.77085876464844\n",
      "1864/3000 train_loss: 19.85724639892578 test_loss:87.49960327148438\n",
      "1865/3000 train_loss: 20.68910026550293 test_loss:86.94236755371094\n",
      "1866/3000 train_loss: 21.869421005249023 test_loss:85.61895751953125\n",
      "1867/3000 train_loss: 18.05825424194336 test_loss:97.65013122558594\n",
      "1868/3000 train_loss: 23.283920288085938 test_loss:88.67341613769531\n",
      "1869/3000 train_loss: 21.797592163085938 test_loss:95.93138885498047\n",
      "1870/3000 train_loss: 19.617116928100586 test_loss:92.32366943359375\n",
      "1871/3000 train_loss: 17.807655334472656 test_loss:93.24333190917969\n",
      "1872/3000 train_loss: 20.13677215576172 test_loss:103.26905822753906\n",
      "1873/3000 train_loss: 19.66796875 test_loss:86.78111267089844\n",
      "1874/3000 train_loss: 21.86980438232422 test_loss:92.06290435791016\n",
      "1875/3000 train_loss: 21.041595458984375 test_loss:87.37991333007812\n",
      "1876/3000 train_loss: 17.466154098510742 test_loss:87.64462280273438\n",
      "1877/3000 train_loss: 17.66848373413086 test_loss:90.73088073730469\n",
      "1878/3000 train_loss: 17.13604736328125 test_loss:89.87423706054688\n",
      "1879/3000 train_loss: 19.542434692382812 test_loss:90.01405334472656\n",
      "1880/3000 train_loss: 19.05318260192871 test_loss:94.5042724609375\n",
      "1881/3000 train_loss: 18.01112174987793 test_loss:86.78012084960938\n",
      "1882/3000 train_loss: 22.010835647583008 test_loss:90.10940551757812\n",
      "1883/3000 train_loss: 18.086706161499023 test_loss:95.90979766845703\n",
      "1884/3000 train_loss: 18.504135131835938 test_loss:92.48975372314453\n",
      "1885/3000 train_loss: 21.318042755126953 test_loss:96.759521484375\n",
      "1886/3000 train_loss: 15.929702758789062 test_loss:94.7524185180664\n",
      "1887/3000 train_loss: 19.30683135986328 test_loss:93.80184936523438\n",
      "1888/3000 train_loss: 18.41401481628418 test_loss:88.7452392578125\n",
      "1889/3000 train_loss: 19.426942825317383 test_loss:94.52322387695312\n",
      "1890/3000 train_loss: 19.67884063720703 test_loss:91.53712463378906\n",
      "1891/3000 train_loss: 23.955480575561523 test_loss:92.52581787109375\n",
      "1892/3000 train_loss: 21.78455352783203 test_loss:90.4999008178711\n",
      "1893/3000 train_loss: 17.55357551574707 test_loss:93.44490814208984\n",
      "1894/3000 train_loss: 20.994834899902344 test_loss:96.88349151611328\n",
      "1895/3000 train_loss: 20.119144439697266 test_loss:92.98118591308594\n",
      "1896/3000 train_loss: 19.25289535522461 test_loss:89.40240478515625\n",
      "1897/3000 train_loss: 20.609561920166016 test_loss:86.44989776611328\n",
      "1898/3000 train_loss: 20.74578857421875 test_loss:91.38897705078125\n",
      "1899/3000 train_loss: 17.638837814331055 test_loss:88.71280670166016\n",
      "1900/3000 train_loss: 25.27002716064453 test_loss:97.29624938964844\n",
      "1901/3000 train_loss: 19.156705856323242 test_loss:86.78214263916016\n",
      "1902/3000 train_loss: 22.0213680267334 test_loss:86.8186264038086\n",
      "1903/3000 train_loss: 18.17714500427246 test_loss:87.24327850341797\n",
      "1904/3000 train_loss: 18.51995849609375 test_loss:86.92529296875\n",
      "1905/3000 train_loss: 18.186445236206055 test_loss:82.17034912109375\n",
      "1906/3000 train_loss: 20.02587127685547 test_loss:95.2932357788086\n",
      "1907/3000 train_loss: 19.843671798706055 test_loss:93.23033142089844\n",
      "1908/3000 train_loss: 18.538923263549805 test_loss:84.7854232788086\n",
      "1909/3000 train_loss: 17.639951705932617 test_loss:87.84455871582031\n",
      "1910/3000 train_loss: 18.60429573059082 test_loss:90.00943756103516\n",
      "1911/3000 train_loss: 16.778472900390625 test_loss:88.59962463378906\n",
      "1912/3000 train_loss: 17.004549026489258 test_loss:88.08995819091797\n",
      "1913/3000 train_loss: 18.993953704833984 test_loss:97.37550354003906\n",
      "1914/3000 train_loss: 19.24321174621582 test_loss:91.04627227783203\n",
      "1915/3000 train_loss: 18.367328643798828 test_loss:87.92118835449219\n",
      "1916/3000 train_loss: 16.57511329650879 test_loss:91.661865234375\n",
      "1917/3000 train_loss: 17.2412109375 test_loss:84.98086547851562\n",
      "1918/3000 train_loss: 19.15138816833496 test_loss:89.52503967285156\n",
      "1919/3000 train_loss: 19.20478057861328 test_loss:95.98355865478516\n",
      "1920/3000 train_loss: 19.47247314453125 test_loss:81.96507263183594\n",
      "1921/3000 train_loss: 19.06029510498047 test_loss:87.37098693847656\n",
      "1922/3000 train_loss: 18.380624771118164 test_loss:92.16976165771484\n",
      "1923/3000 train_loss: 16.674880981445312 test_loss:89.3534927368164\n",
      "1924/3000 train_loss: 20.32862663269043 test_loss:87.26400756835938\n",
      "1925/3000 train_loss: 19.528270721435547 test_loss:87.576416015625\n",
      "1926/3000 train_loss: 19.42081069946289 test_loss:90.35249328613281\n",
      "1927/3000 train_loss: 17.918771743774414 test_loss:89.4163589477539\n",
      "1928/3000 train_loss: 18.480554580688477 test_loss:89.29483795166016\n",
      "1929/3000 train_loss: 22.642175674438477 test_loss:95.97249603271484\n",
      "1930/3000 train_loss: 19.130212783813477 test_loss:89.06333923339844\n",
      "1931/3000 train_loss: 20.59646987915039 test_loss:94.00965118408203\n",
      "1932/3000 train_loss: 18.74641990661621 test_loss:93.33362579345703\n",
      "1933/3000 train_loss: 16.453975677490234 test_loss:91.20834350585938\n",
      "1934/3000 train_loss: 18.16339874267578 test_loss:91.22496795654297\n",
      "1935/3000 train_loss: 21.73578643798828 test_loss:97.9489517211914\n",
      "1936/3000 train_loss: 18.21001434326172 test_loss:93.53240966796875\n",
      "1937/3000 train_loss: 19.778284072875977 test_loss:93.30265808105469\n",
      "1938/3000 train_loss: 20.865535736083984 test_loss:89.9754409790039\n",
      "1939/3000 train_loss: 23.318391799926758 test_loss:97.78337097167969\n",
      "1940/3000 train_loss: 20.60803985595703 test_loss:96.15725708007812\n",
      "1941/3000 train_loss: 23.102750778198242 test_loss:93.68475341796875\n",
      "1942/3000 train_loss: 18.40316390991211 test_loss:96.94351959228516\n",
      "1943/3000 train_loss: 17.95928192138672 test_loss:93.6043701171875\n",
      "1944/3000 train_loss: 17.969621658325195 test_loss:88.56063842773438\n",
      "1945/3000 train_loss: 16.243818283081055 test_loss:93.6707992553711\n",
      "1946/3000 train_loss: 16.630905151367188 test_loss:89.99699401855469\n",
      "1947/3000 train_loss: 22.28209114074707 test_loss:98.11392211914062\n",
      "1948/3000 train_loss: 27.279993057250977 test_loss:93.93684387207031\n",
      "1949/3000 train_loss: 21.74492645263672 test_loss:93.27546691894531\n",
      "1950/3000 train_loss: 18.364925384521484 test_loss:97.52991485595703\n",
      "1951/3000 train_loss: 18.083911895751953 test_loss:92.09629821777344\n",
      "1952/3000 train_loss: 19.123687744140625 test_loss:94.58392333984375\n",
      "1953/3000 train_loss: 19.366294860839844 test_loss:101.25918579101562\n",
      "1954/3000 train_loss: 20.481473922729492 test_loss:94.72549438476562\n",
      "1955/3000 train_loss: 18.400718688964844 test_loss:96.98585510253906\n",
      "1956/3000 train_loss: 16.81475257873535 test_loss:92.52978515625\n",
      "1957/3000 train_loss: 19.062101364135742 test_loss:104.11296081542969\n",
      "1958/3000 train_loss: 18.710710525512695 test_loss:96.70794677734375\n",
      "1959/3000 train_loss: 16.2724552154541 test_loss:89.04752349853516\n",
      "1960/3000 train_loss: 17.521060943603516 test_loss:99.76605224609375\n",
      "1961/3000 train_loss: 19.01908302307129 test_loss:96.10911560058594\n",
      "1962/3000 train_loss: 19.12355613708496 test_loss:88.50288391113281\n",
      "1963/3000 train_loss: 19.106002807617188 test_loss:92.20775604248047\n",
      "1964/3000 train_loss: 16.97860336303711 test_loss:91.26435852050781\n",
      "1965/3000 train_loss: 19.32475471496582 test_loss:91.558837890625\n",
      "1966/3000 train_loss: 18.94160270690918 test_loss:96.78895568847656\n",
      "1967/3000 train_loss: 17.300418853759766 test_loss:91.0008316040039\n",
      "1968/3000 train_loss: 20.04374122619629 test_loss:97.239013671875\n",
      "1969/3000 train_loss: 19.281986236572266 test_loss:88.0818099975586\n",
      "1970/3000 train_loss: 17.996410369873047 test_loss:94.29045104980469\n",
      "1971/3000 train_loss: 18.044645309448242 test_loss:96.9010009765625\n",
      "1972/3000 train_loss: 20.669513702392578 test_loss:91.65130615234375\n",
      "1973/3000 train_loss: 19.38040542602539 test_loss:94.56694793701172\n",
      "1974/3000 train_loss: 16.376014709472656 test_loss:87.03993225097656\n",
      "1975/3000 train_loss: 17.028779983520508 test_loss:95.03527069091797\n",
      "1976/3000 train_loss: 18.099645614624023 test_loss:89.68772888183594\n",
      "1977/3000 train_loss: 17.647092819213867 test_loss:96.70382690429688\n",
      "1978/3000 train_loss: 16.367753982543945 test_loss:92.35052490234375\n",
      "1979/3000 train_loss: 17.76377296447754 test_loss:85.35154724121094\n",
      "1980/3000 train_loss: 18.22953987121582 test_loss:91.87188720703125\n",
      "1981/3000 train_loss: 16.564701080322266 test_loss:89.43910217285156\n",
      "1982/3000 train_loss: 19.127521514892578 test_loss:89.8315200805664\n",
      "1983/3000 train_loss: 36.91925048828125 test_loss:103.71891784667969\n",
      "1984/3000 train_loss: 37.027099609375 test_loss:121.26105499267578\n",
      "1985/3000 train_loss: 22.014942169189453 test_loss:99.91204071044922\n",
      "1986/3000 train_loss: 20.61031150817871 test_loss:101.53755950927734\n",
      "1987/3000 train_loss: 17.37798309326172 test_loss:96.29039764404297\n",
      "1988/3000 train_loss: 17.38203239440918 test_loss:104.10581970214844\n",
      "1989/3000 train_loss: 19.435733795166016 test_loss:93.73513793945312\n",
      "1990/3000 train_loss: 17.565650939941406 test_loss:95.51065063476562\n",
      "1991/3000 train_loss: 16.719226837158203 test_loss:91.3681640625\n",
      "1992/3000 train_loss: 15.421748161315918 test_loss:89.0029296875\n",
      "1993/3000 train_loss: 18.49486541748047 test_loss:88.57608032226562\n",
      "1994/3000 train_loss: 17.141414642333984 test_loss:93.91388702392578\n",
      "1995/3000 train_loss: 17.37164878845215 test_loss:89.24362182617188\n",
      "1996/3000 train_loss: 15.561766624450684 test_loss:88.88688659667969\n",
      "1997/3000 train_loss: 17.16966438293457 test_loss:87.96731567382812\n",
      "1998/3000 train_loss: 19.121479034423828 test_loss:88.99401092529297\n",
      "1999/3000 train_loss: 17.502918243408203 test_loss:90.57777404785156\n",
      "2000/3000 train_loss: 20.929128646850586 test_loss:93.89657592773438\n",
      "2001/3000 train_loss: 17.81791877746582 test_loss:89.41675567626953\n",
      "2002/3000 train_loss: 15.663640975952148 test_loss:89.53404998779297\n",
      "2003/3000 train_loss: 17.148906707763672 test_loss:97.0859603881836\n",
      "2004/3000 train_loss: 15.818146705627441 test_loss:92.55741882324219\n",
      "2005/3000 train_loss: 19.158401489257812 test_loss:89.98046875\n",
      "2006/3000 train_loss: 22.229570388793945 test_loss:93.64932250976562\n",
      "2007/3000 train_loss: 19.78653335571289 test_loss:89.36317443847656\n",
      "2008/3000 train_loss: 18.86646842956543 test_loss:97.24405670166016\n",
      "2009/3000 train_loss: 17.50804328918457 test_loss:89.94253540039062\n",
      "2010/3000 train_loss: 16.93379020690918 test_loss:85.34959411621094\n",
      "2011/3000 train_loss: 15.773297309875488 test_loss:92.0613784790039\n",
      "2012/3000 train_loss: 16.883291244506836 test_loss:85.5317153930664\n",
      "2013/3000 train_loss: 16.471847534179688 test_loss:92.99336242675781\n",
      "2014/3000 train_loss: 16.17384147644043 test_loss:88.0263671875\n",
      "2015/3000 train_loss: 16.839277267456055 test_loss:87.99378967285156\n",
      "2016/3000 train_loss: 17.95135498046875 test_loss:91.07148742675781\n",
      "2017/3000 train_loss: 16.076961517333984 test_loss:88.10773468017578\n",
      "2018/3000 train_loss: 17.92315673828125 test_loss:90.25669860839844\n",
      "2019/3000 train_loss: 18.34895896911621 test_loss:85.94676208496094\n",
      "2020/3000 train_loss: 18.60655975341797 test_loss:90.17678833007812\n",
      "2021/3000 train_loss: 16.839643478393555 test_loss:84.36904907226562\n",
      "2022/3000 train_loss: 19.006027221679688 test_loss:88.14583587646484\n",
      "2023/3000 train_loss: 20.13843536376953 test_loss:97.42411804199219\n",
      "2024/3000 train_loss: 16.301584243774414 test_loss:84.94412231445312\n",
      "2025/3000 train_loss: 17.464801788330078 test_loss:87.55180358886719\n",
      "2026/3000 train_loss: 19.308610916137695 test_loss:86.79679107666016\n",
      "2027/3000 train_loss: 21.018638610839844 test_loss:85.54280090332031\n",
      "2028/3000 train_loss: 20.79372215270996 test_loss:85.50648498535156\n",
      "2029/3000 train_loss: 15.700122833251953 test_loss:86.5677490234375\n",
      "2030/3000 train_loss: 19.159046173095703 test_loss:99.13607025146484\n",
      "2031/3000 train_loss: 16.911535263061523 test_loss:91.24938201904297\n",
      "2032/3000 train_loss: 18.600208282470703 test_loss:87.91902160644531\n",
      "2033/3000 train_loss: 16.998899459838867 test_loss:91.04756927490234\n",
      "2034/3000 train_loss: 18.990379333496094 test_loss:93.4343490600586\n",
      "2035/3000 train_loss: 16.493473052978516 test_loss:83.33297729492188\n",
      "2036/3000 train_loss: 18.0318603515625 test_loss:95.64224243164062\n",
      "2037/3000 train_loss: 17.396461486816406 test_loss:87.48299407958984\n",
      "2038/3000 train_loss: 19.581680297851562 test_loss:85.63662719726562\n",
      "2039/3000 train_loss: 23.5318660736084 test_loss:90.54129028320312\n",
      "2040/3000 train_loss: 16.909814834594727 test_loss:88.88082885742188\n",
      "2041/3000 train_loss: 16.950176239013672 test_loss:82.72288513183594\n",
      "2042/3000 train_loss: 17.031906127929688 test_loss:86.34837341308594\n",
      "2043/3000 train_loss: 16.854705810546875 test_loss:81.47393035888672\n",
      "2044/3000 train_loss: 19.4144344329834 test_loss:90.64710235595703\n",
      "2045/3000 train_loss: 14.930535316467285 test_loss:84.67379760742188\n",
      "2046/3000 train_loss: 14.963241577148438 test_loss:83.82157897949219\n",
      "2047/3000 train_loss: 16.94135856628418 test_loss:79.95015716552734\n",
      "2048/3000 train_loss: 16.41050910949707 test_loss:87.39213562011719\n",
      "2049/3000 train_loss: 17.780439376831055 test_loss:97.15255737304688\n",
      "2050/3000 train_loss: 16.184885025024414 test_loss:88.05165100097656\n",
      "2051/3000 train_loss: 19.36970329284668 test_loss:90.39923095703125\n",
      "2052/3000 train_loss: 19.45166778564453 test_loss:89.59410095214844\n",
      "2053/3000 train_loss: 18.13641357421875 test_loss:90.26480102539062\n",
      "2054/3000 train_loss: 22.842012405395508 test_loss:91.12301635742188\n",
      "2055/3000 train_loss: 15.983893394470215 test_loss:84.77838134765625\n",
      "2056/3000 train_loss: 17.107276916503906 test_loss:90.5256576538086\n",
      "2057/3000 train_loss: 18.369157791137695 test_loss:87.920166015625\n",
      "2058/3000 train_loss: 14.337258338928223 test_loss:88.73332214355469\n",
      "2059/3000 train_loss: 16.710906982421875 test_loss:93.11727142333984\n",
      "2060/3000 train_loss: 17.48426628112793 test_loss:87.95518493652344\n",
      "2061/3000 train_loss: 16.286056518554688 test_loss:89.41897583007812\n",
      "2062/3000 train_loss: 20.94184112548828 test_loss:94.393310546875\n",
      "2063/3000 train_loss: 17.007022857666016 test_loss:90.12864685058594\n",
      "2064/3000 train_loss: 20.22075843811035 test_loss:83.93246459960938\n",
      "2065/3000 train_loss: 16.532203674316406 test_loss:88.12112426757812\n",
      "2066/3000 train_loss: 18.95969581604004 test_loss:93.46697998046875\n",
      "2067/3000 train_loss: 17.77772331237793 test_loss:84.0401382446289\n",
      "2068/3000 train_loss: 17.833457946777344 test_loss:90.59576416015625\n",
      "2069/3000 train_loss: 16.102384567260742 test_loss:85.69424438476562\n",
      "2070/3000 train_loss: 16.731170654296875 test_loss:94.98979187011719\n",
      "2071/3000 train_loss: 17.03604507446289 test_loss:85.10176849365234\n",
      "2072/3000 train_loss: 19.816600799560547 test_loss:86.20738220214844\n",
      "2073/3000 train_loss: 16.029666900634766 test_loss:93.81844329833984\n",
      "2074/3000 train_loss: 15.3878755569458 test_loss:85.53343200683594\n",
      "2075/3000 train_loss: 16.206071853637695 test_loss:89.28880310058594\n",
      "2076/3000 train_loss: 16.886323928833008 test_loss:90.61781311035156\n",
      "2077/3000 train_loss: 14.379739761352539 test_loss:81.3822021484375\n",
      "2078/3000 train_loss: 17.327346801757812 test_loss:87.07630920410156\n",
      "2079/3000 train_loss: 17.206518173217773 test_loss:89.27205657958984\n",
      "2080/3000 train_loss: 19.432435989379883 test_loss:87.81535339355469\n",
      "2081/3000 train_loss: 16.629884719848633 test_loss:89.34454345703125\n",
      "2082/3000 train_loss: 18.53985023498535 test_loss:86.63995361328125\n",
      "2083/3000 train_loss: 25.350095748901367 test_loss:95.58483123779297\n",
      "2084/3000 train_loss: 19.568830490112305 test_loss:106.71916198730469\n",
      "2085/3000 train_loss: 18.86421012878418 test_loss:93.52549743652344\n",
      "2086/3000 train_loss: 18.709047317504883 test_loss:89.2389907836914\n",
      "2087/3000 train_loss: 16.209867477416992 test_loss:97.16130065917969\n",
      "2088/3000 train_loss: 17.319534301757812 test_loss:91.6930923461914\n",
      "2089/3000 train_loss: 14.283867835998535 test_loss:89.62486267089844\n",
      "2090/3000 train_loss: 14.537565231323242 test_loss:93.34030151367188\n",
      "2091/3000 train_loss: 16.42042350769043 test_loss:92.17721557617188\n",
      "2092/3000 train_loss: 14.921828269958496 test_loss:89.83348083496094\n",
      "2093/3000 train_loss: 17.40693473815918 test_loss:88.13656616210938\n",
      "2094/3000 train_loss: 15.740903854370117 test_loss:91.26734161376953\n",
      "2095/3000 train_loss: 14.76292896270752 test_loss:85.2860107421875\n",
      "2096/3000 train_loss: 16.104286193847656 test_loss:87.18949890136719\n",
      "2097/3000 train_loss: 16.005645751953125 test_loss:89.79246520996094\n",
      "2098/3000 train_loss: 19.930458068847656 test_loss:86.65470886230469\n",
      "2099/3000 train_loss: 16.98291778564453 test_loss:86.57295227050781\n",
      "2100/3000 train_loss: 15.55439567565918 test_loss:87.58820343017578\n",
      "2101/3000 train_loss: 17.848979949951172 test_loss:85.3846664428711\n",
      "2102/3000 train_loss: 14.908224105834961 test_loss:84.19099426269531\n",
      "2103/3000 train_loss: 17.911230087280273 test_loss:85.3017349243164\n",
      "2104/3000 train_loss: 17.160804748535156 test_loss:89.14515686035156\n",
      "2105/3000 train_loss: 15.692780494689941 test_loss:85.5814437866211\n",
      "2106/3000 train_loss: 18.36488151550293 test_loss:84.71380615234375\n",
      "2107/3000 train_loss: 19.75948715209961 test_loss:85.40386199951172\n",
      "2108/3000 train_loss: 16.661527633666992 test_loss:89.65767669677734\n",
      "2109/3000 train_loss: 14.855713844299316 test_loss:88.26690673828125\n",
      "2110/3000 train_loss: 14.316461563110352 test_loss:88.69795227050781\n",
      "2111/3000 train_loss: 15.477582931518555 test_loss:91.11969757080078\n",
      "2112/3000 train_loss: 16.395870208740234 test_loss:87.943603515625\n",
      "2113/3000 train_loss: 19.468982696533203 test_loss:90.21978759765625\n",
      "2114/3000 train_loss: 18.924570083618164 test_loss:87.91680908203125\n",
      "2115/3000 train_loss: 18.20049476623535 test_loss:88.15585327148438\n",
      "2116/3000 train_loss: 21.411149978637695 test_loss:100.88653564453125\n",
      "2117/3000 train_loss: 19.912212371826172 test_loss:93.18836212158203\n",
      "2118/3000 train_loss: 22.769773483276367 test_loss:92.9411849975586\n",
      "2119/3000 train_loss: 15.904593467712402 test_loss:89.56331634521484\n",
      "2120/3000 train_loss: 15.924592018127441 test_loss:93.27156066894531\n",
      "2121/3000 train_loss: 15.56137752532959 test_loss:90.906982421875\n",
      "2122/3000 train_loss: 18.23967170715332 test_loss:86.93968200683594\n",
      "2123/3000 train_loss: 20.33977699279785 test_loss:96.975341796875\n",
      "2124/3000 train_loss: 16.49173355102539 test_loss:91.52230834960938\n",
      "2125/3000 train_loss: 19.000263214111328 test_loss:92.81009674072266\n",
      "2126/3000 train_loss: 18.04799461364746 test_loss:88.47608947753906\n",
      "2127/3000 train_loss: 16.594106674194336 test_loss:96.69292449951172\n",
      "2128/3000 train_loss: 15.545994758605957 test_loss:91.5491943359375\n",
      "2129/3000 train_loss: 17.796850204467773 test_loss:95.34089660644531\n",
      "2130/3000 train_loss: 14.97164535522461 test_loss:86.70982360839844\n",
      "2131/3000 train_loss: 15.936749458312988 test_loss:90.545166015625\n",
      "2132/3000 train_loss: 15.781997680664062 test_loss:85.11256408691406\n",
      "2133/3000 train_loss: 15.384310722351074 test_loss:97.06376647949219\n",
      "2134/3000 train_loss: 16.720932006835938 test_loss:85.97071838378906\n",
      "2135/3000 train_loss: 16.599605560302734 test_loss:94.85531616210938\n",
      "2136/3000 train_loss: 16.4667911529541 test_loss:87.71327209472656\n",
      "2137/3000 train_loss: 17.59705352783203 test_loss:87.0687484741211\n",
      "2138/3000 train_loss: 15.657684326171875 test_loss:89.8382797241211\n",
      "2139/3000 train_loss: 17.695940017700195 test_loss:95.02653503417969\n",
      "2140/3000 train_loss: 15.638612747192383 test_loss:81.60077667236328\n",
      "2141/3000 train_loss: 16.26239013671875 test_loss:90.71517944335938\n",
      "2142/3000 train_loss: 16.338071823120117 test_loss:84.80154418945312\n",
      "2143/3000 train_loss: 15.041034698486328 test_loss:88.91726684570312\n",
      "2144/3000 train_loss: 14.588732719421387 test_loss:86.81240844726562\n",
      "2145/3000 train_loss: 16.03530502319336 test_loss:83.06489562988281\n",
      "2146/3000 train_loss: 19.945003509521484 test_loss:88.09489440917969\n",
      "2147/3000 train_loss: 17.25029754638672 test_loss:89.77488708496094\n",
      "2148/3000 train_loss: 17.08852767944336 test_loss:90.78797912597656\n",
      "2149/3000 train_loss: 16.850107192993164 test_loss:91.09587860107422\n",
      "2150/3000 train_loss: 15.439948081970215 test_loss:90.80862426757812\n",
      "2151/3000 train_loss: 17.598636627197266 test_loss:100.09252166748047\n",
      "2152/3000 train_loss: 18.9993839263916 test_loss:91.46507263183594\n",
      "2153/3000 train_loss: 16.155052185058594 test_loss:92.02151489257812\n",
      "2154/3000 train_loss: 15.821882247924805 test_loss:85.01222229003906\n",
      "2155/3000 train_loss: 14.786022186279297 test_loss:91.78779602050781\n",
      "2156/3000 train_loss: 16.119516372680664 test_loss:91.51042175292969\n",
      "2157/3000 train_loss: 17.884788513183594 test_loss:89.89208221435547\n",
      "2158/3000 train_loss: 13.252903938293457 test_loss:90.85134887695312\n",
      "2159/3000 train_loss: 16.670257568359375 test_loss:86.11482238769531\n",
      "2160/3000 train_loss: 15.687504768371582 test_loss:83.46112060546875\n",
      "2161/3000 train_loss: 17.232219696044922 test_loss:87.99287414550781\n",
      "2162/3000 train_loss: 16.20868682861328 test_loss:92.05548095703125\n",
      "2163/3000 train_loss: 16.9487361907959 test_loss:93.32425689697266\n",
      "2164/3000 train_loss: 17.789234161376953 test_loss:87.69076538085938\n",
      "2165/3000 train_loss: 17.779470443725586 test_loss:85.37293243408203\n",
      "2166/3000 train_loss: 17.644123077392578 test_loss:94.5660400390625\n",
      "2167/3000 train_loss: 18.273225784301758 test_loss:94.79468536376953\n",
      "2168/3000 train_loss: 18.55536460876465 test_loss:91.13813018798828\n",
      "2169/3000 train_loss: 18.30174446105957 test_loss:85.52569580078125\n",
      "2170/3000 train_loss: 17.37405014038086 test_loss:96.21905517578125\n",
      "2171/3000 train_loss: 18.001441955566406 test_loss:82.47025299072266\n",
      "2172/3000 train_loss: 17.089969635009766 test_loss:84.54855346679688\n",
      "2173/3000 train_loss: 15.519704818725586 test_loss:92.04926300048828\n",
      "2174/3000 train_loss: 17.127347946166992 test_loss:87.88780212402344\n",
      "2175/3000 train_loss: 18.987781524658203 test_loss:90.9271240234375\n",
      "2176/3000 train_loss: 16.991846084594727 test_loss:84.05142211914062\n",
      "2177/3000 train_loss: 17.385269165039062 test_loss:84.82722473144531\n",
      "2178/3000 train_loss: 18.195022583007812 test_loss:91.34844970703125\n",
      "2179/3000 train_loss: 16.579452514648438 test_loss:81.13343811035156\n",
      "2180/3000 train_loss: 14.776285171508789 test_loss:83.95680236816406\n",
      "2181/3000 train_loss: 16.825214385986328 test_loss:88.13019561767578\n",
      "2182/3000 train_loss: 15.735238075256348 test_loss:92.1847915649414\n",
      "2183/3000 train_loss: 17.776975631713867 test_loss:91.92343139648438\n",
      "2184/3000 train_loss: 16.940710067749023 test_loss:95.49017333984375\n",
      "2185/3000 train_loss: 17.04497528076172 test_loss:92.55841064453125\n",
      "2186/3000 train_loss: 18.098772048950195 test_loss:86.67733764648438\n",
      "2187/3000 train_loss: 17.49047088623047 test_loss:92.71343994140625\n",
      "2188/3000 train_loss: 17.55389976501465 test_loss:85.36167907714844\n",
      "2189/3000 train_loss: 14.082724571228027 test_loss:88.95802307128906\n",
      "2190/3000 train_loss: 13.5047025680542 test_loss:87.02045440673828\n",
      "2191/3000 train_loss: 16.724050521850586 test_loss:96.2831039428711\n",
      "2192/3000 train_loss: 17.480670928955078 test_loss:87.03092956542969\n",
      "2193/3000 train_loss: 14.980734825134277 test_loss:85.73521423339844\n",
      "2194/3000 train_loss: 15.1898832321167 test_loss:92.67473602294922\n",
      "2195/3000 train_loss: 16.304119110107422 test_loss:83.44003295898438\n",
      "2196/3000 train_loss: 16.71596336364746 test_loss:86.05335235595703\n",
      "2197/3000 train_loss: 15.934924125671387 test_loss:91.30439758300781\n",
      "2198/3000 train_loss: 18.310359954833984 test_loss:83.58497619628906\n",
      "2199/3000 train_loss: 14.347230911254883 test_loss:85.4411849975586\n",
      "2200/3000 train_loss: 16.19069480895996 test_loss:82.7023696899414\n",
      "2201/3000 train_loss: 13.71200180053711 test_loss:91.39248657226562\n",
      "2202/3000 train_loss: 14.210756301879883 test_loss:82.50904846191406\n",
      "2203/3000 train_loss: 16.877351760864258 test_loss:85.83161926269531\n",
      "2204/3000 train_loss: 19.92816734313965 test_loss:82.50982666015625\n",
      "2205/3000 train_loss: 16.452566146850586 test_loss:86.89036560058594\n",
      "2206/3000 train_loss: 17.308935165405273 test_loss:83.04044342041016\n",
      "2207/3000 train_loss: 18.271013259887695 test_loss:89.44832611083984\n",
      "2208/3000 train_loss: 16.582807540893555 test_loss:81.09765625\n",
      "2209/3000 train_loss: 17.221141815185547 test_loss:86.8129653930664\n",
      "2210/3000 train_loss: 13.822558403015137 test_loss:87.11772918701172\n",
      "2211/3000 train_loss: 18.418067932128906 test_loss:88.15345001220703\n",
      "2212/3000 train_loss: 15.31700611114502 test_loss:81.27295684814453\n",
      "2213/3000 train_loss: 19.39703369140625 test_loss:87.24217224121094\n",
      "2214/3000 train_loss: 17.06051254272461 test_loss:90.0576171875\n",
      "2215/3000 train_loss: 17.73680305480957 test_loss:83.97596740722656\n",
      "2216/3000 train_loss: 15.672450065612793 test_loss:87.95916748046875\n",
      "2217/3000 train_loss: 15.493000030517578 test_loss:95.99722290039062\n",
      "2218/3000 train_loss: 16.510875701904297 test_loss:87.85924530029297\n",
      "2219/3000 train_loss: 15.14897632598877 test_loss:91.24996185302734\n",
      "2220/3000 train_loss: 21.41046714782715 test_loss:93.49774169921875\n",
      "2221/3000 train_loss: 23.912973403930664 test_loss:94.73222351074219\n",
      "2222/3000 train_loss: 17.72909164428711 test_loss:87.15849304199219\n",
      "2223/3000 train_loss: 18.89249038696289 test_loss:96.28358459472656\n",
      "2224/3000 train_loss: 16.193313598632812 test_loss:90.48834991455078\n",
      "2225/3000 train_loss: 18.859025955200195 test_loss:94.5454330444336\n",
      "2226/3000 train_loss: 14.876253128051758 test_loss:89.65718078613281\n",
      "2227/3000 train_loss: 16.483610153198242 test_loss:89.98625946044922\n",
      "2228/3000 train_loss: 15.858352661132812 test_loss:98.34850311279297\n",
      "2229/3000 train_loss: 16.00885581970215 test_loss:87.32200622558594\n",
      "2230/3000 train_loss: 18.727859497070312 test_loss:84.23603820800781\n",
      "2231/3000 train_loss: 19.364030838012695 test_loss:91.15528869628906\n",
      "2232/3000 train_loss: 17.213640213012695 test_loss:88.22154998779297\n",
      "2233/3000 train_loss: 12.990578651428223 test_loss:95.27413940429688\n",
      "2234/3000 train_loss: 14.287558555603027 test_loss:91.39968872070312\n",
      "2235/3000 train_loss: 13.777647972106934 test_loss:86.34765625\n",
      "2236/3000 train_loss: 14.377349853515625 test_loss:84.70407104492188\n",
      "2237/3000 train_loss: 17.840089797973633 test_loss:84.14897918701172\n",
      "2238/3000 train_loss: 14.652018547058105 test_loss:83.55096435546875\n",
      "2239/3000 train_loss: 14.617640495300293 test_loss:86.41307067871094\n",
      "2240/3000 train_loss: 22.194366455078125 test_loss:90.68473052978516\n",
      "2241/3000 train_loss: 16.154155731201172 test_loss:89.70420837402344\n",
      "2242/3000 train_loss: 21.90302085876465 test_loss:90.38180541992188\n",
      "2243/3000 train_loss: 18.38082504272461 test_loss:93.97859191894531\n",
      "2244/3000 train_loss: 16.1799259185791 test_loss:89.3679428100586\n",
      "2245/3000 train_loss: 17.19803237915039 test_loss:86.498291015625\n",
      "2246/3000 train_loss: 15.504979133605957 test_loss:94.37183380126953\n",
      "2247/3000 train_loss: 15.743441581726074 test_loss:89.42991638183594\n",
      "2248/3000 train_loss: 15.861300468444824 test_loss:86.68319702148438\n",
      "2249/3000 train_loss: 15.733413696289062 test_loss:86.59038543701172\n",
      "2250/3000 train_loss: 14.078003883361816 test_loss:88.82109069824219\n",
      "2251/3000 train_loss: 13.298652648925781 test_loss:87.28713989257812\n",
      "2252/3000 train_loss: 17.165210723876953 test_loss:90.28948974609375\n",
      "2253/3000 train_loss: 13.825770378112793 test_loss:90.47242736816406\n",
      "2254/3000 train_loss: 14.194013595581055 test_loss:87.37911987304688\n",
      "2255/3000 train_loss: 14.495514869689941 test_loss:84.29312133789062\n",
      "2256/3000 train_loss: 17.464019775390625 test_loss:86.79469299316406\n",
      "2257/3000 train_loss: 14.64401626586914 test_loss:88.82635498046875\n",
      "2258/3000 train_loss: 14.887645721435547 test_loss:85.20919799804688\n",
      "2259/3000 train_loss: 14.449466705322266 test_loss:87.71141052246094\n",
      "2260/3000 train_loss: 14.399946212768555 test_loss:84.99227905273438\n",
      "2261/3000 train_loss: 13.180038452148438 test_loss:88.17658996582031\n",
      "2262/3000 train_loss: 13.28905963897705 test_loss:89.57710266113281\n",
      "2263/3000 train_loss: 14.251255989074707 test_loss:84.88035583496094\n",
      "2264/3000 train_loss: 18.34938621520996 test_loss:92.2398452758789\n",
      "2265/3000 train_loss: 18.329463958740234 test_loss:89.95488739013672\n",
      "2266/3000 train_loss: 16.413894653320312 test_loss:89.28661346435547\n",
      "2267/3000 train_loss: 19.316814422607422 test_loss:95.18406677246094\n",
      "2268/3000 train_loss: 19.66429901123047 test_loss:83.36604309082031\n",
      "2269/3000 train_loss: 14.98141860961914 test_loss:93.26630401611328\n",
      "2270/3000 train_loss: 15.778200149536133 test_loss:89.00497436523438\n",
      "2271/3000 train_loss: 16.944446563720703 test_loss:83.6751708984375\n",
      "2272/3000 train_loss: 15.739806175231934 test_loss:88.22621154785156\n",
      "2273/3000 train_loss: 18.87800407409668 test_loss:85.35111999511719\n",
      "2274/3000 train_loss: 15.23046588897705 test_loss:83.59115600585938\n",
      "2275/3000 train_loss: 16.46815299987793 test_loss:84.079345703125\n",
      "2276/3000 train_loss: 15.576775550842285 test_loss:93.40040588378906\n",
      "2277/3000 train_loss: 16.271236419677734 test_loss:82.34346008300781\n",
      "2278/3000 train_loss: 13.872079849243164 test_loss:82.6102294921875\n",
      "2279/3000 train_loss: 16.386638641357422 test_loss:90.469970703125\n",
      "2280/3000 train_loss: 12.552595138549805 test_loss:82.78376770019531\n",
      "2281/3000 train_loss: 14.684276580810547 test_loss:87.78106689453125\n",
      "2282/3000 train_loss: 15.412304878234863 test_loss:84.67771911621094\n",
      "2283/3000 train_loss: 15.443607330322266 test_loss:86.00959777832031\n",
      "2284/3000 train_loss: 15.223867416381836 test_loss:88.53382873535156\n",
      "2285/3000 train_loss: 13.681997299194336 test_loss:87.85802459716797\n",
      "2286/3000 train_loss: 16.53421401977539 test_loss:87.34406280517578\n",
      "2287/3000 train_loss: 16.402223587036133 test_loss:96.67696380615234\n",
      "2288/3000 train_loss: 16.609106063842773 test_loss:85.47309112548828\n",
      "2289/3000 train_loss: 14.447089195251465 test_loss:91.9209976196289\n",
      "2290/3000 train_loss: 14.451526641845703 test_loss:84.62928771972656\n",
      "2291/3000 train_loss: 14.995952606201172 test_loss:84.88114929199219\n",
      "2292/3000 train_loss: 15.376407623291016 test_loss:86.59561920166016\n",
      "2293/3000 train_loss: 12.779581069946289 test_loss:84.69119262695312\n",
      "2294/3000 train_loss: 13.10952091217041 test_loss:86.35614013671875\n",
      "2295/3000 train_loss: 14.800992965698242 test_loss:89.85581970214844\n",
      "2296/3000 train_loss: 16.101835250854492 test_loss:89.91171264648438\n",
      "2297/3000 train_loss: 14.689618110656738 test_loss:84.39031219482422\n",
      "2298/3000 train_loss: 14.500179290771484 test_loss:86.84242248535156\n",
      "2299/3000 train_loss: 15.983755111694336 test_loss:83.28877258300781\n",
      "2300/3000 train_loss: 13.476672172546387 test_loss:82.06875610351562\n",
      "2301/3000 train_loss: 15.358901023864746 test_loss:93.906494140625\n",
      "2302/3000 train_loss: 14.413928031921387 test_loss:91.94093322753906\n",
      "2303/3000 train_loss: 15.08416748046875 test_loss:87.54379272460938\n",
      "2304/3000 train_loss: 29.012813568115234 test_loss:88.45423889160156\n",
      "2305/3000 train_loss: 15.351387977600098 test_loss:90.6296157836914\n",
      "2306/3000 train_loss: 15.74957275390625 test_loss:87.64561462402344\n",
      "2307/3000 train_loss: 14.839285850524902 test_loss:82.94073486328125\n",
      "2308/3000 train_loss: 17.311092376708984 test_loss:88.87944030761719\n",
      "2309/3000 train_loss: 14.594481468200684 test_loss:90.98130798339844\n",
      "2310/3000 train_loss: 15.37426471710205 test_loss:90.01052856445312\n",
      "2311/3000 train_loss: 15.474722862243652 test_loss:100.08189392089844\n",
      "2312/3000 train_loss: 18.734224319458008 test_loss:92.54103088378906\n",
      "2313/3000 train_loss: 17.670541763305664 test_loss:79.57107543945312\n",
      "2314/3000 train_loss: 16.500534057617188 test_loss:82.5226821899414\n",
      "2315/3000 train_loss: 16.442068099975586 test_loss:95.37648010253906\n",
      "2316/3000 train_loss: 17.503992080688477 test_loss:84.25286102294922\n",
      "2317/3000 train_loss: 15.85720157623291 test_loss:79.10823059082031\n",
      "2318/3000 train_loss: 17.966842651367188 test_loss:80.2242431640625\n",
      "2319/3000 train_loss: 17.838056564331055 test_loss:84.36934661865234\n",
      "2320/3000 train_loss: 15.189410209655762 test_loss:90.4097900390625\n",
      "2321/3000 train_loss: 16.636032104492188 test_loss:93.78801727294922\n",
      "2322/3000 train_loss: 13.32102108001709 test_loss:84.63395690917969\n",
      "2323/3000 train_loss: 17.545793533325195 test_loss:89.8533935546875\n",
      "2324/3000 train_loss: 15.67812728881836 test_loss:90.55320739746094\n",
      "2325/3000 train_loss: 16.06396484375 test_loss:81.72833251953125\n",
      "2326/3000 train_loss: 14.408246040344238 test_loss:86.03446960449219\n",
      "2327/3000 train_loss: 14.17888069152832 test_loss:81.6116943359375\n",
      "2328/3000 train_loss: 14.915029525756836 test_loss:86.21493530273438\n",
      "2329/3000 train_loss: 13.2545804977417 test_loss:92.24148559570312\n",
      "2330/3000 train_loss: 15.480781555175781 test_loss:88.3143310546875\n",
      "2331/3000 train_loss: 15.933170318603516 test_loss:82.77497863769531\n",
      "2332/3000 train_loss: 15.231451988220215 test_loss:82.90699768066406\n",
      "2333/3000 train_loss: 14.932658195495605 test_loss:88.39155578613281\n",
      "2334/3000 train_loss: 15.349026679992676 test_loss:93.68700408935547\n",
      "2335/3000 train_loss: 18.53203773498535 test_loss:83.10933685302734\n",
      "2336/3000 train_loss: 17.190881729125977 test_loss:95.42347717285156\n",
      "2337/3000 train_loss: 17.86223030090332 test_loss:87.0531005859375\n",
      "2338/3000 train_loss: 14.233054161071777 test_loss:92.56726837158203\n",
      "2339/3000 train_loss: 13.302361488342285 test_loss:91.72232055664062\n",
      "2340/3000 train_loss: 14.131614685058594 test_loss:85.29948425292969\n",
      "2341/3000 train_loss: 13.654059410095215 test_loss:84.4848403930664\n",
      "2342/3000 train_loss: 15.82113265991211 test_loss:94.53292846679688\n",
      "2343/3000 train_loss: 14.691060066223145 test_loss:86.39311218261719\n",
      "2344/3000 train_loss: 21.033769607543945 test_loss:88.56376647949219\n",
      "2345/3000 train_loss: 16.627443313598633 test_loss:89.1852798461914\n",
      "2346/3000 train_loss: 15.406070709228516 test_loss:86.76751708984375\n",
      "2347/3000 train_loss: 13.582627296447754 test_loss:88.97903442382812\n",
      "2348/3000 train_loss: 13.456357955932617 test_loss:85.30690002441406\n",
      "2349/3000 train_loss: 15.146761894226074 test_loss:85.42581176757812\n",
      "2350/3000 train_loss: 17.597902297973633 test_loss:82.6427230834961\n",
      "2351/3000 train_loss: 15.190306663513184 test_loss:85.77854919433594\n",
      "2352/3000 train_loss: 17.01760482788086 test_loss:87.86799621582031\n",
      "2353/3000 train_loss: 15.506768226623535 test_loss:81.88089752197266\n",
      "2354/3000 train_loss: 13.069339752197266 test_loss:92.31930541992188\n",
      "2355/3000 train_loss: 13.531088829040527 test_loss:83.43162536621094\n",
      "2356/3000 train_loss: 13.99266529083252 test_loss:87.55538940429688\n",
      "2357/3000 train_loss: 13.306962013244629 test_loss:89.72452545166016\n",
      "2358/3000 train_loss: 13.509833335876465 test_loss:84.01510620117188\n",
      "2359/3000 train_loss: 15.543235778808594 test_loss:85.1498031616211\n",
      "2360/3000 train_loss: 17.189159393310547 test_loss:84.67374420166016\n",
      "2361/3000 train_loss: 13.85004711151123 test_loss:96.65538024902344\n",
      "2362/3000 train_loss: 18.174766540527344 test_loss:80.27214050292969\n",
      "2363/3000 train_loss: 15.131386756896973 test_loss:82.61380767822266\n",
      "2364/3000 train_loss: 16.44736099243164 test_loss:87.08317565917969\n",
      "2365/3000 train_loss: 14.166662216186523 test_loss:87.36106872558594\n",
      "2366/3000 train_loss: 14.554656982421875 test_loss:92.36882781982422\n",
      "2367/3000 train_loss: 17.622318267822266 test_loss:85.56330871582031\n",
      "2368/3000 train_loss: 14.872821807861328 test_loss:83.55157470703125\n",
      "2369/3000 train_loss: 15.343579292297363 test_loss:89.5126953125\n",
      "2370/3000 train_loss: 15.456660270690918 test_loss:89.33100891113281\n",
      "2371/3000 train_loss: 12.787458419799805 test_loss:83.84809875488281\n",
      "2372/3000 train_loss: 13.266905784606934 test_loss:85.6353759765625\n",
      "2373/3000 train_loss: 14.76205062866211 test_loss:88.7684555053711\n",
      "2374/3000 train_loss: 12.092024803161621 test_loss:87.61345672607422\n",
      "2375/3000 train_loss: 15.011894226074219 test_loss:92.76774597167969\n",
      "2376/3000 train_loss: 17.819547653198242 test_loss:89.04217529296875\n",
      "2377/3000 train_loss: 16.023178100585938 test_loss:89.91092681884766\n",
      "2378/3000 train_loss: 18.637611389160156 test_loss:88.62542724609375\n",
      "2379/3000 train_loss: 17.456371307373047 test_loss:93.48976135253906\n",
      "2380/3000 train_loss: 14.76045036315918 test_loss:91.67713165283203\n",
      "2381/3000 train_loss: 15.5236234664917 test_loss:88.87257385253906\n",
      "2382/3000 train_loss: 15.770999908447266 test_loss:85.99044799804688\n",
      "2383/3000 train_loss: 14.400022506713867 test_loss:88.93521881103516\n",
      "2384/3000 train_loss: 17.510074615478516 test_loss:92.37565612792969\n",
      "2385/3000 train_loss: 17.430177688598633 test_loss:79.90798950195312\n",
      "2386/3000 train_loss: 15.639443397521973 test_loss:92.18842315673828\n",
      "2387/3000 train_loss: 16.837650299072266 test_loss:91.5296859741211\n",
      "2388/3000 train_loss: 16.936899185180664 test_loss:90.46830749511719\n",
      "2389/3000 train_loss: 13.58874225616455 test_loss:87.09505462646484\n",
      "2390/3000 train_loss: 13.996967315673828 test_loss:80.03398895263672\n",
      "2391/3000 train_loss: 13.26452350616455 test_loss:85.63023376464844\n",
      "2392/3000 train_loss: 12.994040489196777 test_loss:84.53836059570312\n",
      "2393/3000 train_loss: 14.639928817749023 test_loss:79.87815856933594\n",
      "2394/3000 train_loss: 12.495868682861328 test_loss:86.30621337890625\n",
      "2395/3000 train_loss: 12.054603576660156 test_loss:81.0521469116211\n",
      "2396/3000 train_loss: 14.647090911865234 test_loss:87.26495361328125\n",
      "2397/3000 train_loss: 16.889726638793945 test_loss:78.64031982421875\n",
      "2398/3000 train_loss: 16.073497772216797 test_loss:86.76986694335938\n",
      "2399/3000 train_loss: 13.919242858886719 test_loss:83.47918701171875\n",
      "2400/3000 train_loss: 13.449118614196777 test_loss:90.72452545166016\n",
      "2401/3000 train_loss: 16.324857711791992 test_loss:84.43502807617188\n",
      "2402/3000 train_loss: 17.49026107788086 test_loss:81.59013366699219\n",
      "2403/3000 train_loss: 12.899956703186035 test_loss:82.77886199951172\n",
      "2404/3000 train_loss: 11.62191104888916 test_loss:85.55027770996094\n",
      "2405/3000 train_loss: 15.003393173217773 test_loss:80.64773559570312\n",
      "2406/3000 train_loss: 14.57550048828125 test_loss:90.99903869628906\n",
      "2407/3000 train_loss: 14.369316101074219 test_loss:80.51734924316406\n",
      "2408/3000 train_loss: 13.591154098510742 test_loss:84.94779205322266\n",
      "2409/3000 train_loss: 17.62592124938965 test_loss:91.16014099121094\n",
      "2410/3000 train_loss: 15.844029426574707 test_loss:87.04283905029297\n",
      "2411/3000 train_loss: 16.980993270874023 test_loss:77.36780548095703\n",
      "2412/3000 train_loss: 15.395785331726074 test_loss:82.40963745117188\n",
      "2413/3000 train_loss: 18.317577362060547 test_loss:85.61245727539062\n",
      "2414/3000 train_loss: 15.422627449035645 test_loss:81.48866271972656\n",
      "2415/3000 train_loss: 15.113277435302734 test_loss:85.22130584716797\n",
      "2416/3000 train_loss: 16.910091400146484 test_loss:82.00328063964844\n",
      "2417/3000 train_loss: 15.943942070007324 test_loss:85.59507751464844\n",
      "2418/3000 train_loss: 15.689234733581543 test_loss:82.900146484375\n",
      "2419/3000 train_loss: 13.159626007080078 test_loss:85.59586334228516\n",
      "2420/3000 train_loss: 12.82723617553711 test_loss:83.15193939208984\n",
      "2421/3000 train_loss: 12.38829231262207 test_loss:82.69989013671875\n",
      "2422/3000 train_loss: 16.48049545288086 test_loss:81.58900451660156\n",
      "2423/3000 train_loss: 14.629101753234863 test_loss:84.99468994140625\n",
      "2424/3000 train_loss: 15.11642837524414 test_loss:92.45135498046875\n",
      "2425/3000 train_loss: 16.452423095703125 test_loss:81.52420806884766\n",
      "2426/3000 train_loss: 13.810479164123535 test_loss:79.94064331054688\n",
      "2427/3000 train_loss: 15.161062240600586 test_loss:81.67611694335938\n",
      "2428/3000 train_loss: 14.055962562561035 test_loss:82.64502716064453\n",
      "2429/3000 train_loss: 13.450530052185059 test_loss:78.42060852050781\n",
      "2430/3000 train_loss: 14.164169311523438 test_loss:84.85792541503906\n",
      "2431/3000 train_loss: 19.793212890625 test_loss:93.20962524414062\n",
      "2432/3000 train_loss: 20.023122787475586 test_loss:80.03137969970703\n",
      "2433/3000 train_loss: 15.267606735229492 test_loss:80.96109008789062\n",
      "2434/3000 train_loss: 14.560559272766113 test_loss:93.02403259277344\n",
      "2435/3000 train_loss: 13.47233772277832 test_loss:91.09127044677734\n",
      "2436/3000 train_loss: 16.823423385620117 test_loss:79.59273529052734\n",
      "2437/3000 train_loss: 14.907240867614746 test_loss:84.96609497070312\n",
      "2438/3000 train_loss: 13.321207046508789 test_loss:85.19708251953125\n",
      "2439/3000 train_loss: 12.909430503845215 test_loss:82.8479232788086\n",
      "2440/3000 train_loss: 13.985320091247559 test_loss:84.31460571289062\n",
      "2441/3000 train_loss: 18.383642196655273 test_loss:77.33483123779297\n",
      "2442/3000 train_loss: 11.78017520904541 test_loss:83.90799713134766\n",
      "2443/3000 train_loss: 15.054388999938965 test_loss:86.37693786621094\n",
      "2444/3000 train_loss: 16.146495819091797 test_loss:76.78150939941406\n",
      "2445/3000 train_loss: 13.360133171081543 test_loss:77.4465560913086\n",
      "2446/3000 train_loss: 12.7576322555542 test_loss:89.13334655761719\n",
      "2447/3000 train_loss: 16.46474838256836 test_loss:88.80867767333984\n",
      "2448/3000 train_loss: 16.961078643798828 test_loss:79.79916381835938\n",
      "2449/3000 train_loss: 16.36090850830078 test_loss:81.25975799560547\n",
      "2450/3000 train_loss: 14.685711860656738 test_loss:86.90804290771484\n",
      "2451/3000 train_loss: 15.628438949584961 test_loss:81.4515609741211\n",
      "2452/3000 train_loss: 14.993766784667969 test_loss:88.78573608398438\n",
      "2453/3000 train_loss: 21.09174156188965 test_loss:86.22630310058594\n",
      "2454/3000 train_loss: 15.552388191223145 test_loss:83.26736450195312\n",
      "2455/3000 train_loss: 13.453579902648926 test_loss:85.54071807861328\n",
      "2456/3000 train_loss: 13.74045467376709 test_loss:91.50579833984375\n",
      "2457/3000 train_loss: 13.364715576171875 test_loss:80.32612609863281\n",
      "2458/3000 train_loss: 13.79969310760498 test_loss:80.54493713378906\n",
      "2459/3000 train_loss: 13.908918380737305 test_loss:84.15084838867188\n",
      "2460/3000 train_loss: 13.888932228088379 test_loss:81.50108337402344\n",
      "2461/3000 train_loss: 15.356887817382812 test_loss:83.53352355957031\n",
      "2462/3000 train_loss: 14.624202728271484 test_loss:91.91731262207031\n",
      "2463/3000 train_loss: 14.25976848602295 test_loss:82.11859130859375\n",
      "2464/3000 train_loss: 16.494476318359375 test_loss:81.596435546875\n",
      "2465/3000 train_loss: 13.547199249267578 test_loss:88.66598510742188\n",
      "2466/3000 train_loss: 20.777740478515625 test_loss:82.40953063964844\n",
      "2467/3000 train_loss: 14.578457832336426 test_loss:89.65531921386719\n",
      "2468/3000 train_loss: 16.697267532348633 test_loss:83.81713104248047\n",
      "2469/3000 train_loss: 15.585290908813477 test_loss:82.43588256835938\n",
      "2470/3000 train_loss: 13.561793327331543 test_loss:82.5616455078125\n",
      "2471/3000 train_loss: 20.484569549560547 test_loss:91.33383178710938\n",
      "2472/3000 train_loss: 14.108051300048828 test_loss:84.69691467285156\n",
      "2473/3000 train_loss: 17.862442016601562 test_loss:76.7367935180664\n",
      "2474/3000 train_loss: 14.258940696716309 test_loss:79.7817611694336\n",
      "2475/3000 train_loss: 15.223600387573242 test_loss:82.45437622070312\n",
      "2476/3000 train_loss: 14.943460464477539 test_loss:84.0493392944336\n",
      "2477/3000 train_loss: 12.2817964553833 test_loss:84.0391845703125\n",
      "2478/3000 train_loss: 13.204565048217773 test_loss:81.42948913574219\n",
      "2479/3000 train_loss: 14.23292350769043 test_loss:78.86579895019531\n",
      "2480/3000 train_loss: 18.841510772705078 test_loss:79.43061828613281\n",
      "2481/3000 train_loss: 17.707260131835938 test_loss:84.78153991699219\n",
      "2482/3000 train_loss: 12.111102104187012 test_loss:79.7719497680664\n",
      "2483/3000 train_loss: 14.427739143371582 test_loss:82.08695983886719\n",
      "2484/3000 train_loss: 13.146986961364746 test_loss:87.98716735839844\n",
      "2485/3000 train_loss: 13.987465858459473 test_loss:78.41860961914062\n",
      "2486/3000 train_loss: 17.194650650024414 test_loss:87.09040832519531\n",
      "2487/3000 train_loss: 12.650829315185547 test_loss:83.67323303222656\n",
      "2488/3000 train_loss: 13.20224380493164 test_loss:77.30055236816406\n",
      "2489/3000 train_loss: 14.658778190612793 test_loss:82.45927429199219\n",
      "2490/3000 train_loss: 17.96577262878418 test_loss:86.64871215820312\n",
      "2491/3000 train_loss: 15.050708770751953 test_loss:83.94956970214844\n",
      "2492/3000 train_loss: 16.632678985595703 test_loss:88.06958770751953\n",
      "2493/3000 train_loss: 14.358702659606934 test_loss:85.09498596191406\n",
      "2494/3000 train_loss: 15.709641456604004 test_loss:84.37876892089844\n",
      "2495/3000 train_loss: 14.553255081176758 test_loss:82.46480560302734\n",
      "2496/3000 train_loss: 16.513629913330078 test_loss:88.13329315185547\n",
      "2497/3000 train_loss: 13.263639450073242 test_loss:76.78164672851562\n",
      "2498/3000 train_loss: 16.937135696411133 test_loss:82.73634338378906\n",
      "2499/3000 train_loss: 15.234495162963867 test_loss:86.43400573730469\n",
      "2500/3000 train_loss: 14.336658477783203 test_loss:83.95498657226562\n",
      "2501/3000 train_loss: 12.998355865478516 test_loss:79.88909912109375\n",
      "2502/3000 train_loss: 13.290956497192383 test_loss:82.0370101928711\n",
      "2503/3000 train_loss: 17.27505874633789 test_loss:90.07487487792969\n",
      "2504/3000 train_loss: 14.752351760864258 test_loss:91.51968383789062\n",
      "2505/3000 train_loss: 16.600669860839844 test_loss:82.28472137451172\n",
      "2506/3000 train_loss: 13.084732055664062 test_loss:79.86866760253906\n",
      "2507/3000 train_loss: 15.802574157714844 test_loss:82.58763885498047\n",
      "2508/3000 train_loss: 14.786577224731445 test_loss:85.52800750732422\n",
      "2509/3000 train_loss: 17.398183822631836 test_loss:87.23300170898438\n",
      "2510/3000 train_loss: 13.992805480957031 test_loss:79.98499298095703\n",
      "2511/3000 train_loss: 15.018941879272461 test_loss:75.36205291748047\n",
      "2512/3000 train_loss: 13.224165916442871 test_loss:89.97295379638672\n",
      "2513/3000 train_loss: 14.60120677947998 test_loss:78.63508605957031\n",
      "2514/3000 train_loss: 14.110115051269531 test_loss:86.4742431640625\n",
      "2515/3000 train_loss: 15.072742462158203 test_loss:82.79267883300781\n",
      "2516/3000 train_loss: 12.828201293945312 test_loss:78.68374633789062\n",
      "2517/3000 train_loss: 12.992339134216309 test_loss:85.91769409179688\n",
      "2518/3000 train_loss: 15.062551498413086 test_loss:83.84446716308594\n",
      "2519/3000 train_loss: 13.557536125183105 test_loss:79.44174194335938\n",
      "2520/3000 train_loss: 13.157618522644043 test_loss:85.38804626464844\n",
      "2521/3000 train_loss: 16.857641220092773 test_loss:85.17926788330078\n",
      "2522/3000 train_loss: 12.63676929473877 test_loss:77.28954315185547\n",
      "2523/3000 train_loss: 13.656119346618652 test_loss:84.90888214111328\n",
      "2524/3000 train_loss: 14.097840309143066 test_loss:78.54280853271484\n",
      "2525/3000 train_loss: 13.258380889892578 test_loss:79.03216552734375\n",
      "2526/3000 train_loss: 14.074033737182617 test_loss:81.32920837402344\n",
      "2527/3000 train_loss: 15.039220809936523 test_loss:80.97601318359375\n",
      "2528/3000 train_loss: 15.250471115112305 test_loss:79.97493743896484\n",
      "2529/3000 train_loss: 16.415624618530273 test_loss:86.27365112304688\n",
      "2530/3000 train_loss: 14.058637619018555 test_loss:81.91436004638672\n",
      "2531/3000 train_loss: 11.659323692321777 test_loss:81.80229187011719\n",
      "2532/3000 train_loss: 12.772089958190918 test_loss:83.90642547607422\n",
      "2533/3000 train_loss: 12.667860984802246 test_loss:81.69248962402344\n",
      "2534/3000 train_loss: 12.482002258300781 test_loss:85.21492767333984\n",
      "2535/3000 train_loss: 12.087045669555664 test_loss:78.61565399169922\n",
      "2536/3000 train_loss: 14.674766540527344 test_loss:80.9466552734375\n",
      "2537/3000 train_loss: 15.223920822143555 test_loss:81.46015167236328\n",
      "2538/3000 train_loss: 14.985105514526367 test_loss:80.95947265625\n",
      "2539/3000 train_loss: 14.449942588806152 test_loss:80.31278228759766\n",
      "2540/3000 train_loss: 14.117925643920898 test_loss:85.5393295288086\n",
      "2541/3000 train_loss: 24.579641342163086 test_loss:99.63883209228516\n",
      "2542/3000 train_loss: 17.819128036499023 test_loss:82.510986328125\n",
      "2543/3000 train_loss: 18.21173858642578 test_loss:79.86761474609375\n",
      "2544/3000 train_loss: 12.924919128417969 test_loss:86.12734985351562\n",
      "2545/3000 train_loss: 12.080405235290527 test_loss:83.03130340576172\n",
      "2546/3000 train_loss: 14.692852020263672 test_loss:81.43946075439453\n",
      "2547/3000 train_loss: 14.167803764343262 test_loss:81.8270492553711\n",
      "2548/3000 train_loss: 13.6532621383667 test_loss:79.07699584960938\n",
      "2549/3000 train_loss: 13.725244522094727 test_loss:81.43696594238281\n",
      "2550/3000 train_loss: 13.618412017822266 test_loss:82.49535369873047\n",
      "2551/3000 train_loss: 15.381231307983398 test_loss:80.90261840820312\n",
      "2552/3000 train_loss: 14.730903625488281 test_loss:86.38695526123047\n",
      "2553/3000 train_loss: 12.252745628356934 test_loss:81.90116119384766\n",
      "2554/3000 train_loss: 13.201616287231445 test_loss:77.90792846679688\n",
      "2555/3000 train_loss: 14.956409454345703 test_loss:87.76423645019531\n",
      "2556/3000 train_loss: 15.234184265136719 test_loss:82.53016662597656\n",
      "2557/3000 train_loss: 14.062430381774902 test_loss:78.62837219238281\n",
      "2558/3000 train_loss: 14.226800918579102 test_loss:78.18919372558594\n",
      "2559/3000 train_loss: 16.39674186706543 test_loss:77.73606872558594\n",
      "2560/3000 train_loss: 16.718032836914062 test_loss:97.09040832519531\n",
      "2561/3000 train_loss: 17.930585861206055 test_loss:83.92369079589844\n",
      "2562/3000 train_loss: 13.008411407470703 test_loss:86.20675659179688\n",
      "2563/3000 train_loss: 11.199146270751953 test_loss:80.75245666503906\n",
      "2564/3000 train_loss: 11.63798713684082 test_loss:86.39266204833984\n",
      "2565/3000 train_loss: 15.581441879272461 test_loss:82.50889587402344\n",
      "2566/3000 train_loss: 16.541126251220703 test_loss:90.37875366210938\n",
      "2567/3000 train_loss: 14.4191255569458 test_loss:88.99115753173828\n",
      "2568/3000 train_loss: 13.744685173034668 test_loss:82.03439331054688\n",
      "2569/3000 train_loss: 14.712373733520508 test_loss:83.6779556274414\n",
      "2570/3000 train_loss: 12.42531681060791 test_loss:86.71805572509766\n",
      "2571/3000 train_loss: 10.703237533569336 test_loss:83.3916244506836\n",
      "2572/3000 train_loss: 13.560569763183594 test_loss:90.8155746459961\n",
      "2573/3000 train_loss: 13.227017402648926 test_loss:80.4730224609375\n",
      "2574/3000 train_loss: 12.849569320678711 test_loss:80.40734100341797\n",
      "2575/3000 train_loss: 13.525629997253418 test_loss:86.948974609375\n",
      "2576/3000 train_loss: 12.487079620361328 test_loss:87.7160873413086\n",
      "2577/3000 train_loss: 12.78783130645752 test_loss:80.27804565429688\n",
      "2578/3000 train_loss: 11.839329719543457 test_loss:82.70750427246094\n",
      "2579/3000 train_loss: 12.693628311157227 test_loss:79.506103515625\n",
      "2580/3000 train_loss: 16.570568084716797 test_loss:91.00289154052734\n",
      "2581/3000 train_loss: 15.946020126342773 test_loss:87.61476135253906\n",
      "2582/3000 train_loss: 12.804753303527832 test_loss:79.73736572265625\n",
      "2583/3000 train_loss: 14.750770568847656 test_loss:85.65168762207031\n",
      "2584/3000 train_loss: 13.489961624145508 test_loss:83.2785873413086\n",
      "2585/3000 train_loss: 16.19670867919922 test_loss:76.39822387695312\n",
      "2586/3000 train_loss: 17.6512451171875 test_loss:87.99327087402344\n",
      "2587/3000 train_loss: 14.36340045928955 test_loss:91.28218841552734\n",
      "2588/3000 train_loss: 16.37063217163086 test_loss:81.06978607177734\n",
      "2589/3000 train_loss: 11.80647087097168 test_loss:89.73783111572266\n",
      "2590/3000 train_loss: 12.425055503845215 test_loss:78.67684936523438\n",
      "2591/3000 train_loss: 12.964576721191406 test_loss:89.47024536132812\n",
      "2592/3000 train_loss: 16.28621482849121 test_loss:80.46654510498047\n",
      "2593/3000 train_loss: 12.40157413482666 test_loss:81.42007446289062\n",
      "2594/3000 train_loss: 12.365004539489746 test_loss:94.42765808105469\n",
      "2595/3000 train_loss: 13.290251731872559 test_loss:80.20652770996094\n",
      "2596/3000 train_loss: 13.703255653381348 test_loss:90.58843994140625\n",
      "2597/3000 train_loss: 14.59933853149414 test_loss:88.8584213256836\n",
      "2598/3000 train_loss: 20.07781410217285 test_loss:77.94720458984375\n",
      "2599/3000 train_loss: 13.800741195678711 test_loss:79.57347106933594\n",
      "2600/3000 train_loss: 13.2600679397583 test_loss:90.86549377441406\n",
      "2601/3000 train_loss: 13.806806564331055 test_loss:79.18661499023438\n",
      "2602/3000 train_loss: 13.18529987335205 test_loss:79.54141998291016\n",
      "2603/3000 train_loss: 13.956323623657227 test_loss:87.00717163085938\n",
      "2604/3000 train_loss: 12.266377449035645 test_loss:81.60227966308594\n",
      "2605/3000 train_loss: 13.434821128845215 test_loss:82.48717498779297\n",
      "2606/3000 train_loss: 13.463722229003906 test_loss:84.76533508300781\n",
      "2607/3000 train_loss: 13.278615951538086 test_loss:78.88291931152344\n",
      "2608/3000 train_loss: 11.961685180664062 test_loss:80.55123138427734\n",
      "2609/3000 train_loss: 10.340132713317871 test_loss:83.19182586669922\n",
      "2610/3000 train_loss: 12.509244918823242 test_loss:90.30364227294922\n",
      "2611/3000 train_loss: 14.050530433654785 test_loss:80.8033676147461\n",
      "2612/3000 train_loss: 13.889559745788574 test_loss:93.37035369873047\n",
      "2613/3000 train_loss: 13.958922386169434 test_loss:88.21812438964844\n",
      "2614/3000 train_loss: 13.879246711730957 test_loss:80.76590728759766\n",
      "2615/3000 train_loss: 10.77898120880127 test_loss:83.99561309814453\n",
      "2616/3000 train_loss: 10.636534690856934 test_loss:84.47569274902344\n",
      "2617/3000 train_loss: 11.504352569580078 test_loss:84.29013061523438\n",
      "2618/3000 train_loss: 14.554333686828613 test_loss:80.08472442626953\n",
      "2619/3000 train_loss: 14.353240966796875 test_loss:86.47213745117188\n",
      "2620/3000 train_loss: 14.882383346557617 test_loss:89.99827575683594\n",
      "2621/3000 train_loss: 13.568284034729004 test_loss:83.38468933105469\n",
      "2622/3000 train_loss: 16.86038589477539 test_loss:76.53363037109375\n",
      "2623/3000 train_loss: 11.47598648071289 test_loss:84.50309753417969\n",
      "2624/3000 train_loss: 10.968878746032715 test_loss:77.83291625976562\n",
      "2625/3000 train_loss: 13.994503021240234 test_loss:81.96629333496094\n",
      "2626/3000 train_loss: 16.399145126342773 test_loss:87.10768127441406\n",
      "2627/3000 train_loss: 13.362753868103027 test_loss:81.51604461669922\n",
      "2628/3000 train_loss: 14.177014350891113 test_loss:84.42294311523438\n",
      "2629/3000 train_loss: 14.188642501831055 test_loss:89.48177337646484\n",
      "2630/3000 train_loss: 13.145845413208008 test_loss:79.87303924560547\n",
      "2631/3000 train_loss: 13.083555221557617 test_loss:81.63346099853516\n",
      "2632/3000 train_loss: 14.845587730407715 test_loss:76.97272491455078\n",
      "2633/3000 train_loss: 15.960552215576172 test_loss:99.10335540771484\n",
      "2634/3000 train_loss: 14.616249084472656 test_loss:85.5043716430664\n",
      "2635/3000 train_loss: 15.467357635498047 test_loss:86.27584838867188\n",
      "2636/3000 train_loss: 13.340408325195312 test_loss:81.70501708984375\n",
      "2637/3000 train_loss: 13.873023986816406 test_loss:92.22798919677734\n",
      "2638/3000 train_loss: 12.78136920928955 test_loss:86.19847106933594\n",
      "2639/3000 train_loss: 12.466567039489746 test_loss:81.88203430175781\n",
      "2640/3000 train_loss: 11.430533409118652 test_loss:86.58935546875\n",
      "2641/3000 train_loss: 13.743730545043945 test_loss:83.3189926147461\n",
      "2642/3000 train_loss: 13.304998397827148 test_loss:81.15863037109375\n",
      "2643/3000 train_loss: 15.47875690460205 test_loss:84.49496459960938\n",
      "2644/3000 train_loss: 12.86314582824707 test_loss:87.93345642089844\n",
      "2645/3000 train_loss: 12.958413124084473 test_loss:81.62720489501953\n",
      "2646/3000 train_loss: 11.844610214233398 test_loss:84.70437622070312\n",
      "2647/3000 train_loss: 11.272981643676758 test_loss:84.468017578125\n",
      "2648/3000 train_loss: 12.131826400756836 test_loss:84.6541976928711\n",
      "2649/3000 train_loss: 11.787657737731934 test_loss:82.6775894165039\n",
      "2650/3000 train_loss: 11.225627899169922 test_loss:87.80474853515625\n",
      "2651/3000 train_loss: 13.376224517822266 test_loss:90.69801330566406\n",
      "2652/3000 train_loss: 12.260990142822266 test_loss:78.34696197509766\n",
      "2653/3000 train_loss: 14.160805702209473 test_loss:88.1199951171875\n",
      "2654/3000 train_loss: 13.245096206665039 test_loss:88.75149536132812\n",
      "2655/3000 train_loss: 10.988862991333008 test_loss:79.66529083251953\n",
      "2656/3000 train_loss: 14.291074752807617 test_loss:80.46965789794922\n",
      "2657/3000 train_loss: 14.103069305419922 test_loss:90.29057312011719\n",
      "2658/3000 train_loss: 15.600671768188477 test_loss:88.62150573730469\n",
      "2659/3000 train_loss: 14.55910873413086 test_loss:82.3704605102539\n",
      "2660/3000 train_loss: 13.347610473632812 test_loss:83.86390686035156\n",
      "2661/3000 train_loss: 13.688417434692383 test_loss:87.4097671508789\n",
      "2662/3000 train_loss: 11.311321258544922 test_loss:80.62584686279297\n",
      "2663/3000 train_loss: 13.677116394042969 test_loss:81.24860382080078\n",
      "2664/3000 train_loss: 13.516884803771973 test_loss:81.81565856933594\n",
      "2665/3000 train_loss: 11.800674438476562 test_loss:85.22913360595703\n",
      "2666/3000 train_loss: 12.920714378356934 test_loss:80.23648071289062\n",
      "2667/3000 train_loss: 12.297871589660645 test_loss:81.32726287841797\n",
      "2668/3000 train_loss: 12.420093536376953 test_loss:86.58119201660156\n",
      "2669/3000 train_loss: 15.865938186645508 test_loss:85.85488891601562\n",
      "2670/3000 train_loss: 12.258087158203125 test_loss:84.58949279785156\n",
      "2671/3000 train_loss: 15.541847229003906 test_loss:82.87903594970703\n",
      "2672/3000 train_loss: 15.553497314453125 test_loss:80.42979431152344\n",
      "2673/3000 train_loss: 12.485944747924805 test_loss:87.35270690917969\n",
      "2674/3000 train_loss: 13.564948081970215 test_loss:86.88223266601562\n",
      "2675/3000 train_loss: 11.787646293640137 test_loss:80.93612670898438\n",
      "2676/3000 train_loss: 12.20268440246582 test_loss:83.34774780273438\n",
      "2677/3000 train_loss: 14.036808967590332 test_loss:79.68524169921875\n",
      "2678/3000 train_loss: 12.963374137878418 test_loss:79.8638916015625\n",
      "2679/3000 train_loss: 13.981422424316406 test_loss:82.20852661132812\n",
      "2680/3000 train_loss: 13.555380821228027 test_loss:89.05741882324219\n",
      "2681/3000 train_loss: 16.270919799804688 test_loss:79.57218170166016\n",
      "2682/3000 train_loss: 13.44912052154541 test_loss:82.83211517333984\n",
      "2683/3000 train_loss: 12.7869873046875 test_loss:82.81044006347656\n",
      "2684/3000 train_loss: 11.345962524414062 test_loss:79.30825805664062\n",
      "2685/3000 train_loss: 12.52195930480957 test_loss:79.89070129394531\n",
      "2686/3000 train_loss: 14.938335418701172 test_loss:84.57014465332031\n",
      "2687/3000 train_loss: 16.101890563964844 test_loss:80.85779571533203\n",
      "2688/3000 train_loss: 14.309070587158203 test_loss:76.24658203125\n",
      "2689/3000 train_loss: 13.034133911132812 test_loss:75.56853485107422\n",
      "2690/3000 train_loss: 19.654029846191406 test_loss:82.82949829101562\n",
      "2691/3000 train_loss: 11.95769214630127 test_loss:78.65792846679688\n",
      "2692/3000 train_loss: 16.535873413085938 test_loss:75.38890075683594\n",
      "2693/3000 train_loss: 24.07614517211914 test_loss:75.87545013427734\n",
      "2694/3000 train_loss: 14.267849922180176 test_loss:82.26956176757812\n",
      "2695/3000 train_loss: 15.21905517578125 test_loss:84.69107818603516\n",
      "2696/3000 train_loss: 10.718698501586914 test_loss:80.60716247558594\n",
      "2697/3000 train_loss: 15.096187591552734 test_loss:85.8536376953125\n",
      "2698/3000 train_loss: 13.291810989379883 test_loss:84.60673522949219\n",
      "2699/3000 train_loss: 13.436653137207031 test_loss:79.58470153808594\n",
      "2700/3000 train_loss: 13.35535717010498 test_loss:82.34529113769531\n",
      "2701/3000 train_loss: 12.379199981689453 test_loss:90.98307800292969\n",
      "2702/3000 train_loss: 13.49023723602295 test_loss:80.2068862915039\n",
      "2703/3000 train_loss: 15.00639820098877 test_loss:79.63713073730469\n",
      "2704/3000 train_loss: 12.198395729064941 test_loss:83.62606811523438\n",
      "2705/3000 train_loss: 11.574333190917969 test_loss:83.16666412353516\n",
      "2706/3000 train_loss: 13.303879737854004 test_loss:92.1951904296875\n",
      "2707/3000 train_loss: 14.218966484069824 test_loss:85.45967864990234\n",
      "2708/3000 train_loss: 12.421382904052734 test_loss:77.1634521484375\n",
      "2709/3000 train_loss: 13.762456893920898 test_loss:85.89009094238281\n",
      "2710/3000 train_loss: 14.571720123291016 test_loss:82.64188385009766\n",
      "2711/3000 train_loss: 13.508247375488281 test_loss:83.138427734375\n",
      "2712/3000 train_loss: 13.021602630615234 test_loss:84.65979766845703\n",
      "2713/3000 train_loss: 10.46990966796875 test_loss:87.61479187011719\n",
      "2714/3000 train_loss: 12.306571006774902 test_loss:80.93106842041016\n",
      "2715/3000 train_loss: 14.801645278930664 test_loss:89.33236694335938\n",
      "2716/3000 train_loss: 13.74930191040039 test_loss:85.10906982421875\n",
      "2717/3000 train_loss: 13.380309104919434 test_loss:85.00151062011719\n",
      "2718/3000 train_loss: 14.238729476928711 test_loss:82.99717712402344\n",
      "2719/3000 train_loss: 16.615053176879883 test_loss:86.64090728759766\n",
      "2720/3000 train_loss: 12.019036293029785 test_loss:81.63018798828125\n",
      "2721/3000 train_loss: 11.24604606628418 test_loss:88.38223266601562\n",
      "2722/3000 train_loss: 10.804484367370605 test_loss:77.62322998046875\n",
      "2723/3000 train_loss: 25.175701141357422 test_loss:81.60800170898438\n",
      "2724/3000 train_loss: 20.0516357421875 test_loss:95.2992935180664\n",
      "2725/3000 train_loss: 16.653888702392578 test_loss:84.90982818603516\n",
      "2726/3000 train_loss: 12.730890274047852 test_loss:82.66558837890625\n",
      "2727/3000 train_loss: 13.969910621643066 test_loss:82.95539855957031\n",
      "2728/3000 train_loss: 13.442390441894531 test_loss:88.85077667236328\n",
      "2729/3000 train_loss: 15.553984642028809 test_loss:80.153564453125\n",
      "2730/3000 train_loss: 14.302541732788086 test_loss:82.75553894042969\n",
      "2731/3000 train_loss: 14.06305980682373 test_loss:83.9123764038086\n",
      "2732/3000 train_loss: 12.97950267791748 test_loss:83.09565734863281\n",
      "2733/3000 train_loss: 12.27370834350586 test_loss:85.75271606445312\n",
      "2734/3000 train_loss: 12.013026237487793 test_loss:89.92420959472656\n",
      "2735/3000 train_loss: 13.713109016418457 test_loss:84.56580352783203\n",
      "2736/3000 train_loss: 11.39811897277832 test_loss:82.31597900390625\n",
      "2737/3000 train_loss: 12.057427406311035 test_loss:91.08901977539062\n",
      "2738/3000 train_loss: 10.935979843139648 test_loss:81.76506805419922\n",
      "2739/3000 train_loss: 13.513517379760742 test_loss:78.3423843383789\n",
      "2740/3000 train_loss: 14.526020050048828 test_loss:88.96421813964844\n",
      "2741/3000 train_loss: 14.684432983398438 test_loss:79.69439697265625\n",
      "2742/3000 train_loss: 12.341377258300781 test_loss:88.46399688720703\n",
      "2743/3000 train_loss: 12.159577369689941 test_loss:84.92947387695312\n",
      "2744/3000 train_loss: 13.931992530822754 test_loss:86.02628326416016\n",
      "2745/3000 train_loss: 13.751461029052734 test_loss:86.82501983642578\n",
      "2746/3000 train_loss: 15.713163375854492 test_loss:85.52157592773438\n",
      "2747/3000 train_loss: 12.167728424072266 test_loss:80.46597290039062\n",
      "2748/3000 train_loss: 15.371610641479492 test_loss:84.30049133300781\n",
      "2749/3000 train_loss: 17.946430206298828 test_loss:80.70411682128906\n",
      "2750/3000 train_loss: 14.833600997924805 test_loss:81.70330810546875\n",
      "2751/3000 train_loss: 13.348246574401855 test_loss:83.93208312988281\n",
      "2752/3000 train_loss: 12.966715812683105 test_loss:87.3668212890625\n",
      "2753/3000 train_loss: 12.500163078308105 test_loss:84.42318725585938\n",
      "2754/3000 train_loss: 15.024030685424805 test_loss:88.25751495361328\n",
      "2755/3000 train_loss: 14.270082473754883 test_loss:78.55321502685547\n",
      "2756/3000 train_loss: 12.017845153808594 test_loss:86.230224609375\n",
      "2757/3000 train_loss: 11.48954963684082 test_loss:81.86520385742188\n",
      "2758/3000 train_loss: 11.615854263305664 test_loss:78.37443542480469\n",
      "2759/3000 train_loss: 11.682774543762207 test_loss:83.28543853759766\n",
      "2760/3000 train_loss: 10.79639720916748 test_loss:79.05232238769531\n",
      "2761/3000 train_loss: 10.894302368164062 test_loss:77.59103393554688\n",
      "2762/3000 train_loss: 13.343913078308105 test_loss:85.55635070800781\n",
      "2763/3000 train_loss: 12.995932579040527 test_loss:79.59994506835938\n",
      "2764/3000 train_loss: 10.812329292297363 test_loss:85.46653747558594\n",
      "2765/3000 train_loss: 15.661365509033203 test_loss:85.67247009277344\n",
      "2766/3000 train_loss: 12.023524284362793 test_loss:89.62100219726562\n",
      "2767/3000 train_loss: 14.664102554321289 test_loss:85.38699340820312\n",
      "2768/3000 train_loss: 13.473891258239746 test_loss:82.94764709472656\n",
      "2769/3000 train_loss: 12.62818431854248 test_loss:87.33808898925781\n",
      "2770/3000 train_loss: 11.196666717529297 test_loss:82.72625732421875\n",
      "2771/3000 train_loss: 13.857747077941895 test_loss:82.65834045410156\n",
      "2772/3000 train_loss: 14.490165710449219 test_loss:87.05270385742188\n",
      "2773/3000 train_loss: 14.495673179626465 test_loss:86.05067443847656\n",
      "2774/3000 train_loss: 12.718425750732422 test_loss:91.1709976196289\n",
      "2775/3000 train_loss: 16.21430015563965 test_loss:82.41432189941406\n",
      "2776/3000 train_loss: 13.309896469116211 test_loss:92.21070861816406\n",
      "2777/3000 train_loss: 12.349603652954102 test_loss:80.57723999023438\n",
      "2778/3000 train_loss: 13.718396186828613 test_loss:83.51797485351562\n",
      "2779/3000 train_loss: 12.950013160705566 test_loss:80.88543701171875\n",
      "2780/3000 train_loss: 13.071012496948242 test_loss:82.32275390625\n",
      "2781/3000 train_loss: 13.671382904052734 test_loss:87.24810028076172\n",
      "2782/3000 train_loss: 13.478228569030762 test_loss:86.62095642089844\n",
      "2783/3000 train_loss: 13.872879028320312 test_loss:83.6727294921875\n",
      "2784/3000 train_loss: 11.997961044311523 test_loss:93.2851791381836\n",
      "2785/3000 train_loss: 14.398624420166016 test_loss:82.37396240234375\n",
      "2786/3000 train_loss: 14.804841041564941 test_loss:76.87495422363281\n",
      "2787/3000 train_loss: 14.886151313781738 test_loss:86.33465576171875\n",
      "2788/3000 train_loss: 15.018220901489258 test_loss:83.80319213867188\n",
      "2789/3000 train_loss: 11.918429374694824 test_loss:84.55994415283203\n",
      "2790/3000 train_loss: 10.752240180969238 test_loss:80.15633392333984\n",
      "2791/3000 train_loss: 11.524591445922852 test_loss:78.833984375\n",
      "2792/3000 train_loss: 12.641053199768066 test_loss:82.11673736572266\n",
      "2793/3000 train_loss: 14.166326522827148 test_loss:80.9047622680664\n",
      "2794/3000 train_loss: 12.469736099243164 test_loss:80.78724670410156\n",
      "2795/3000 train_loss: 15.772414207458496 test_loss:77.37786865234375\n",
      "2796/3000 train_loss: 14.474630355834961 test_loss:89.34761810302734\n",
      "2797/3000 train_loss: 14.475379943847656 test_loss:84.32865142822266\n",
      "2798/3000 train_loss: 16.09368133544922 test_loss:85.49922180175781\n",
      "2799/3000 train_loss: 13.004225730895996 test_loss:89.28723907470703\n",
      "2800/3000 train_loss: 12.103979110717773 test_loss:82.35264587402344\n",
      "2801/3000 train_loss: 13.852119445800781 test_loss:84.22711944580078\n",
      "2802/3000 train_loss: 11.71748161315918 test_loss:73.76043701171875\n",
      "2803/3000 train_loss: 13.029486656188965 test_loss:78.21127319335938\n",
      "2804/3000 train_loss: 9.87109661102295 test_loss:74.91520690917969\n",
      "2805/3000 train_loss: 12.551119804382324 test_loss:77.78670501708984\n",
      "2806/3000 train_loss: 12.245919227600098 test_loss:80.48580169677734\n",
      "2807/3000 train_loss: 13.428489685058594 test_loss:89.26187133789062\n",
      "2808/3000 train_loss: 11.476158142089844 test_loss:76.62541961669922\n",
      "2809/3000 train_loss: 13.312278747558594 test_loss:75.05921936035156\n",
      "2810/3000 train_loss: 13.34432601928711 test_loss:83.47834777832031\n",
      "2811/3000 train_loss: 10.00094985961914 test_loss:81.87397766113281\n",
      "2812/3000 train_loss: 13.439051628112793 test_loss:76.31497955322266\n",
      "2813/3000 train_loss: 13.388291358947754 test_loss:82.97132873535156\n",
      "2814/3000 train_loss: 12.75716781616211 test_loss:85.83341979980469\n",
      "2815/3000 train_loss: 11.539876937866211 test_loss:78.98375701904297\n",
      "2816/3000 train_loss: 13.14944076538086 test_loss:91.64305114746094\n",
      "2817/3000 train_loss: 13.323760986328125 test_loss:76.58197021484375\n",
      "2818/3000 train_loss: 11.310535430908203 test_loss:77.37850952148438\n",
      "2819/3000 train_loss: 10.793600082397461 test_loss:80.38912963867188\n",
      "2820/3000 train_loss: 12.113632202148438 test_loss:81.2410659790039\n",
      "2821/3000 train_loss: 10.979039192199707 test_loss:78.5562744140625\n",
      "2822/3000 train_loss: 11.2905855178833 test_loss:79.24980163574219\n",
      "2823/3000 train_loss: 12.793652534484863 test_loss:78.90420532226562\n",
      "2824/3000 train_loss: 14.589783668518066 test_loss:81.43424987792969\n",
      "2825/3000 train_loss: 11.50662612915039 test_loss:76.24417114257812\n",
      "2826/3000 train_loss: 15.36043643951416 test_loss:77.39757537841797\n",
      "2827/3000 train_loss: 12.052505493164062 test_loss:78.95330810546875\n",
      "2828/3000 train_loss: 10.874183654785156 test_loss:79.63484191894531\n",
      "2829/3000 train_loss: 12.571303367614746 test_loss:81.18585205078125\n",
      "2830/3000 train_loss: 11.63929557800293 test_loss:77.24809265136719\n",
      "2831/3000 train_loss: 13.534936904907227 test_loss:74.46194458007812\n",
      "2832/3000 train_loss: 15.761482238769531 test_loss:75.71562194824219\n",
      "2833/3000 train_loss: 18.428430557250977 test_loss:93.55497741699219\n",
      "2834/3000 train_loss: 13.403241157531738 test_loss:95.45155334472656\n",
      "2835/3000 train_loss: 13.965490341186523 test_loss:95.42033386230469\n",
      "2836/3000 train_loss: 11.493917465209961 test_loss:86.46497344970703\n",
      "2837/3000 train_loss: 12.426027297973633 test_loss:84.17327117919922\n",
      "2838/3000 train_loss: 14.087837219238281 test_loss:89.51699829101562\n",
      "2839/3000 train_loss: 11.267910957336426 test_loss:87.09698486328125\n",
      "2840/3000 train_loss: 11.527652740478516 test_loss:84.90995788574219\n",
      "2841/3000 train_loss: 11.476312637329102 test_loss:88.39350891113281\n",
      "2842/3000 train_loss: 10.523588180541992 test_loss:88.53189849853516\n",
      "2843/3000 train_loss: 11.940664291381836 test_loss:84.55889892578125\n",
      "2844/3000 train_loss: 11.233023643493652 test_loss:85.33317565917969\n",
      "2845/3000 train_loss: 11.33989143371582 test_loss:81.35201263427734\n",
      "2846/3000 train_loss: 12.47992992401123 test_loss:91.39332580566406\n",
      "2847/3000 train_loss: 13.636667251586914 test_loss:85.8572998046875\n",
      "2848/3000 train_loss: 14.83029842376709 test_loss:82.18478393554688\n",
      "2849/3000 train_loss: 11.995183944702148 test_loss:77.90824127197266\n",
      "2850/3000 train_loss: 16.959117889404297 test_loss:78.35074615478516\n",
      "2851/3000 train_loss: 10.868234634399414 test_loss:90.6911849975586\n",
      "2852/3000 train_loss: 13.603376388549805 test_loss:86.05883026123047\n",
      "2853/3000 train_loss: 15.758759498596191 test_loss:88.83493041992188\n",
      "2854/3000 train_loss: 13.761643409729004 test_loss:86.28915405273438\n",
      "2855/3000 train_loss: 12.604297637939453 test_loss:85.08525085449219\n",
      "2856/3000 train_loss: 14.96292495727539 test_loss:87.52237701416016\n",
      "2857/3000 train_loss: 15.090861320495605 test_loss:88.518310546875\n",
      "2858/3000 train_loss: 12.981897354125977 test_loss:91.2776870727539\n",
      "2859/3000 train_loss: 12.001642227172852 test_loss:82.95945739746094\n",
      "2860/3000 train_loss: 13.198616027832031 test_loss:89.7426528930664\n",
      "2861/3000 train_loss: 11.900619506835938 test_loss:86.74285125732422\n",
      "2862/3000 train_loss: 11.434488296508789 test_loss:84.36961364746094\n",
      "2863/3000 train_loss: 11.853405952453613 test_loss:95.20348358154297\n",
      "2864/3000 train_loss: 11.837091445922852 test_loss:87.37200927734375\n",
      "2865/3000 train_loss: 11.434423446655273 test_loss:87.18618774414062\n",
      "2866/3000 train_loss: 11.880590438842773 test_loss:84.4359130859375\n",
      "2867/3000 train_loss: 13.077927589416504 test_loss:88.29151916503906\n",
      "2868/3000 train_loss: 12.407693862915039 test_loss:89.92387390136719\n",
      "2869/3000 train_loss: 12.569570541381836 test_loss:91.30032348632812\n",
      "2870/3000 train_loss: 12.900981903076172 test_loss:87.2843017578125\n",
      "2871/3000 train_loss: 12.80419921875 test_loss:83.4415283203125\n",
      "2872/3000 train_loss: 12.12357234954834 test_loss:91.57142639160156\n",
      "2873/3000 train_loss: 13.926517486572266 test_loss:92.3240737915039\n",
      "2874/3000 train_loss: 15.131096839904785 test_loss:82.80581665039062\n",
      "2875/3000 train_loss: 11.289010047912598 test_loss:82.88800048828125\n",
      "2876/3000 train_loss: 13.046619415283203 test_loss:83.82881164550781\n",
      "2877/3000 train_loss: 12.107901573181152 test_loss:81.53971099853516\n",
      "2878/3000 train_loss: 13.253549575805664 test_loss:87.39784240722656\n",
      "2879/3000 train_loss: 11.911044120788574 test_loss:80.98031616210938\n",
      "2880/3000 train_loss: 11.140738487243652 test_loss:82.78135681152344\n",
      "2881/3000 train_loss: 15.309224128723145 test_loss:80.96376037597656\n",
      "2882/3000 train_loss: 12.641890525817871 test_loss:92.70610046386719\n",
      "2883/3000 train_loss: 13.518905639648438 test_loss:81.96488189697266\n",
      "2884/3000 train_loss: 10.900872230529785 test_loss:79.5949478149414\n",
      "2885/3000 train_loss: 11.495211601257324 test_loss:91.50139617919922\n",
      "2886/3000 train_loss: 12.635159492492676 test_loss:81.09510803222656\n",
      "2887/3000 train_loss: 16.75979995727539 test_loss:83.10208892822266\n",
      "2888/3000 train_loss: 12.229682922363281 test_loss:84.39927673339844\n",
      "2889/3000 train_loss: 11.211546897888184 test_loss:88.80300903320312\n",
      "2890/3000 train_loss: 10.053426742553711 test_loss:84.6132583618164\n",
      "2891/3000 train_loss: 11.517937660217285 test_loss:85.61444091796875\n",
      "2892/3000 train_loss: 11.39079761505127 test_loss:83.39513397216797\n",
      "2893/3000 train_loss: 10.848292350769043 test_loss:85.74466705322266\n",
      "2894/3000 train_loss: 10.973467826843262 test_loss:87.0084228515625\n",
      "2895/3000 train_loss: 15.300138473510742 test_loss:94.53303527832031\n",
      "2896/3000 train_loss: 13.151745796203613 test_loss:85.46260833740234\n",
      "2897/3000 train_loss: 13.774580955505371 test_loss:83.20918273925781\n",
      "2898/3000 train_loss: 13.14560317993164 test_loss:86.48609924316406\n",
      "2899/3000 train_loss: 12.52083969116211 test_loss:86.69378662109375\n",
      "2900/3000 train_loss: 12.285313606262207 test_loss:92.51287841796875\n",
      "2901/3000 train_loss: 10.953315734863281 test_loss:82.11410522460938\n",
      "2902/3000 train_loss: 10.848257064819336 test_loss:96.41455078125\n",
      "2903/3000 train_loss: 12.413408279418945 test_loss:80.9046630859375\n",
      "2904/3000 train_loss: 12.5492525100708 test_loss:81.28877258300781\n",
      "2905/3000 train_loss: 14.067336082458496 test_loss:82.52469635009766\n",
      "2906/3000 train_loss: 13.21931266784668 test_loss:80.73703002929688\n",
      "2907/3000 train_loss: 14.754806518554688 test_loss:79.01820373535156\n",
      "2908/3000 train_loss: 14.569488525390625 test_loss:82.6434097290039\n",
      "2909/3000 train_loss: 13.591805458068848 test_loss:88.31610107421875\n",
      "2910/3000 train_loss: 11.998530387878418 test_loss:85.26405334472656\n",
      "2911/3000 train_loss: 15.089815139770508 test_loss:81.16510009765625\n",
      "2912/3000 train_loss: 11.331686019897461 test_loss:83.34578704833984\n",
      "2913/3000 train_loss: 10.657543182373047 test_loss:86.95295715332031\n",
      "2914/3000 train_loss: 11.935789108276367 test_loss:82.74456024169922\n",
      "2915/3000 train_loss: 11.651159286499023 test_loss:80.56350708007812\n",
      "2916/3000 train_loss: 11.153924942016602 test_loss:83.78187561035156\n",
      "2917/3000 train_loss: 12.071023941040039 test_loss:81.57424926757812\n",
      "2918/3000 train_loss: 11.608311653137207 test_loss:81.85845947265625\n",
      "2919/3000 train_loss: 12.09243106842041 test_loss:80.68667602539062\n",
      "2920/3000 train_loss: 11.503617286682129 test_loss:82.31924438476562\n",
      "2921/3000 train_loss: 15.611760139465332 test_loss:82.49971008300781\n",
      "2922/3000 train_loss: 15.978514671325684 test_loss:84.68878173828125\n",
      "2923/3000 train_loss: 14.909952163696289 test_loss:93.60360717773438\n",
      "2924/3000 train_loss: 13.702320098876953 test_loss:82.33570861816406\n",
      "2925/3000 train_loss: 14.781937599182129 test_loss:86.34905242919922\n",
      "2926/3000 train_loss: 10.875507354736328 test_loss:87.94868469238281\n",
      "2927/3000 train_loss: 10.849870681762695 test_loss:84.71837615966797\n",
      "2928/3000 train_loss: 13.512832641601562 test_loss:80.76335144042969\n",
      "2929/3000 train_loss: 14.742690086364746 test_loss:90.71371459960938\n",
      "2930/3000 train_loss: 14.084468841552734 test_loss:86.74019622802734\n",
      "2931/3000 train_loss: 12.568188667297363 test_loss:80.7179946899414\n",
      "2932/3000 train_loss: 13.321599960327148 test_loss:81.09280395507812\n",
      "2933/3000 train_loss: 11.87070369720459 test_loss:89.61941528320312\n",
      "2934/3000 train_loss: 13.246971130371094 test_loss:78.85885620117188\n",
      "2935/3000 train_loss: 12.796077728271484 test_loss:75.65028381347656\n",
      "2936/3000 train_loss: 14.89544677734375 test_loss:81.443359375\n",
      "2937/3000 train_loss: 12.930084228515625 test_loss:80.66503143310547\n",
      "2938/3000 train_loss: 13.74671459197998 test_loss:98.49274444580078\n",
      "2939/3000 train_loss: 16.46807098388672 test_loss:81.150146484375\n",
      "2940/3000 train_loss: 14.727267265319824 test_loss:81.04252624511719\n",
      "2941/3000 train_loss: 18.873857498168945 test_loss:88.99226379394531\n",
      "2942/3000 train_loss: 12.336453437805176 test_loss:84.00373077392578\n",
      "2943/3000 train_loss: 12.304251670837402 test_loss:85.94837188720703\n",
      "2944/3000 train_loss: 13.887787818908691 test_loss:83.14156341552734\n",
      "2945/3000 train_loss: 11.925975799560547 test_loss:79.50981903076172\n",
      "2946/3000 train_loss: 11.687628746032715 test_loss:85.25189208984375\n",
      "2947/3000 train_loss: 10.850863456726074 test_loss:81.48580169677734\n",
      "2948/3000 train_loss: 10.498982429504395 test_loss:79.87312316894531\n",
      "2949/3000 train_loss: 11.064796447753906 test_loss:77.37992095947266\n",
      "2950/3000 train_loss: 11.205131530761719 test_loss:87.14741516113281\n",
      "2951/3000 train_loss: 12.415301322937012 test_loss:80.86500549316406\n",
      "2952/3000 train_loss: 14.969682693481445 test_loss:73.20693969726562\n",
      "2953/3000 train_loss: 14.79125690460205 test_loss:75.57780456542969\n",
      "2954/3000 train_loss: 11.580641746520996 test_loss:81.23533630371094\n",
      "2955/3000 train_loss: 12.006916046142578 test_loss:80.3225326538086\n",
      "2956/3000 train_loss: 13.276121139526367 test_loss:86.39820861816406\n",
      "2957/3000 train_loss: 9.796219825744629 test_loss:85.00353240966797\n",
      "2958/3000 train_loss: 9.920543670654297 test_loss:78.95423889160156\n",
      "2959/3000 train_loss: 13.156061172485352 test_loss:76.73226928710938\n",
      "2960/3000 train_loss: 12.105630874633789 test_loss:89.00291442871094\n",
      "2961/3000 train_loss: 10.406116485595703 test_loss:80.57532501220703\n",
      "2962/3000 train_loss: 13.244351387023926 test_loss:81.6838150024414\n",
      "2963/3000 train_loss: 13.126849174499512 test_loss:82.50194549560547\n",
      "2964/3000 train_loss: 13.210612297058105 test_loss:79.0224838256836\n",
      "2965/3000 train_loss: 12.672253608703613 test_loss:77.91844940185547\n",
      "2966/3000 train_loss: 11.551011085510254 test_loss:81.59748077392578\n",
      "2967/3000 train_loss: 11.088048934936523 test_loss:77.5\n",
      "2968/3000 train_loss: 10.533308029174805 test_loss:85.18905639648438\n",
      "2969/3000 train_loss: 10.284950256347656 test_loss:82.50721740722656\n",
      "2970/3000 train_loss: 9.81957721710205 test_loss:81.4859390258789\n",
      "2971/3000 train_loss: 10.30125617980957 test_loss:81.97549438476562\n",
      "2972/3000 train_loss: 14.511204719543457 test_loss:82.27075958251953\n",
      "2973/3000 train_loss: 10.953349113464355 test_loss:86.67376708984375\n",
      "2974/3000 train_loss: 11.025467872619629 test_loss:81.76179504394531\n",
      "2975/3000 train_loss: 10.434109687805176 test_loss:78.0493392944336\n",
      "2976/3000 train_loss: 13.73845100402832 test_loss:83.5152816772461\n",
      "2977/3000 train_loss: 12.516741752624512 test_loss:87.5535888671875\n",
      "2978/3000 train_loss: 13.871020317077637 test_loss:75.82115173339844\n",
      "2979/3000 train_loss: 13.53281307220459 test_loss:82.49544525146484\n",
      "2980/3000 train_loss: 12.198858261108398 test_loss:89.6281967163086\n",
      "2981/3000 train_loss: 11.623397827148438 test_loss:85.22804260253906\n",
      "2982/3000 train_loss: 10.099120140075684 test_loss:82.68286895751953\n",
      "2983/3000 train_loss: 9.950966835021973 test_loss:76.75672912597656\n",
      "2984/3000 train_loss: 11.580267906188965 test_loss:86.37400817871094\n",
      "2985/3000 train_loss: 17.023094177246094 test_loss:78.15570068359375\n",
      "2986/3000 train_loss: 11.503618240356445 test_loss:77.6781234741211\n",
      "2987/3000 train_loss: 11.185444831848145 test_loss:83.38833618164062\n",
      "2988/3000 train_loss: 9.537537574768066 test_loss:81.63216400146484\n",
      "2989/3000 train_loss: 10.573190689086914 test_loss:81.31655883789062\n",
      "2990/3000 train_loss: 12.344786643981934 test_loss:87.42979431152344\n",
      "2991/3000 train_loss: 13.583958625793457 test_loss:80.1023178100586\n",
      "2992/3000 train_loss: 12.645967483520508 test_loss:79.59574890136719\n",
      "2993/3000 train_loss: 12.987566947937012 test_loss:78.93510437011719\n",
      "2994/3000 train_loss: 13.154812812805176 test_loss:87.54489135742188\n",
      "2995/3000 train_loss: 11.882417678833008 test_loss:77.28952026367188\n",
      "2996/3000 train_loss: 12.983869552612305 test_loss:85.832763671875\n",
      "2997/3000 train_loss: 9.21964168548584 test_loss:83.61986541748047\n",
      "2998/3000 train_loss: 11.573535919189453 test_loss:84.53718566894531\n",
      "2999/3000 train_loss: 11.446528434753418 test_loss:87.70098876953125\n",
      "3000/3000 train_loss: 12.507669448852539 test_loss:80.66775512695312\n"
     ]
    }
   ],
   "source": [
    "losses = train(model = unet, optimizer = optimizer, criterion=criterion, data_tr=train_data,\n",
    "               data_val = test_data, scheduler = scheduler,device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "6Ew7_F0-q7aL",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "7b5fd974-3750-4e8d-957c-04e2089c5fa1"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(80.6678)"
      ]
     },
     "metadata": {},
     "execution_count": 77
    }
   ],
   "source": [
    "avg_loss = 0\n",
    "# unet.eval()\n",
    "# unet.train()\n",
    "preds = []\n",
    "i = 0\n",
    "test_anomaly_losses = []\n",
    "test_normal_losses = []\n",
    "test_losses = []\n",
    "test_real = y_test.tolist()\n",
    "for batch in test_data:\n",
    "  with torch.no_grad():\n",
    "    # unet.train()\n",
    "    predictions = unet(batch.to(device)).cpu()\n",
    "    preds.append(predictions)\n",
    "  loss = criterion(predictions, batch.cpu())\n",
    "  for j in range(len(predictions)):\n",
    "    if int(y_test[i]) == 1:\n",
    "      test_anomaly_losses.append(float(criterion(predictions[j], batch[j])))\n",
    "    else:\n",
    "      test_normal_losses.append(float(criterion(predictions[j], batch[j])))\n",
    "    i += 1\n",
    "    test_losses.append(criterion(predictions[j], batch[j]))\n",
    "  # print(loss)\n",
    "  # print(loss)\n",
    "  avg_loss += loss / len(test_data)\n",
    "# avg_loss\n",
    "\n",
    "avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "VpDKorrRso9o",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "e15b032e-520d-42c1-b7eb-f84acf05bbef"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(159.21323392616128, 12.641271149090358)"
      ]
     },
     "metadata": {},
     "execution_count": 78
    }
   ],
   "source": [
    "sum(test_anomaly_losses)/len(test_anomaly_losses) , sum(test_normal_losses)/len(test_normal_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5LEvbZKYuh7J",
    "outputId": "198a9898-abb6-4b75-9017-ad6821ee7e2b"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.9772506738544473\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "vals = np.arange(40, 900, 0.5).tolist()\n",
    "for threshold in vals:\n",
    "  preds = []\n",
    "  for j in range(len(test_losses)):\n",
    "    if test_losses[j] > threshold:\n",
    "      preds.append(1)\n",
    "    else:\n",
    "      preds.append(0)\n",
    "  \n",
    "  results.append(roc_auc_score(test_real,preds))\n",
    "\n",
    "print(max(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QaSSqG8SbAw2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
