{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9SStKf4G0V5H"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim import Adam\n",
        "import torchaudio\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from skimage.util import img_as_ubyte\n",
        "import pandas as pd\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import librosa\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import io\n",
        "from sklearn.metrics import roc_auc_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XtxbKLZq5KX",
        "outputId": "8a37e895-6306-48d2-c10c-c18501261351"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYxHegIM0Z4i",
        "outputId": "1a35bf7e-76eb-4682-94b5-ed17dbb1e6cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "h9DATQwS0ivD"
      },
      "outputs": [],
      "source": [
        "class MimiiDataset(Dataset):\n",
        "    def __init__(self,audio_dir, n_fft = 1024, win_length = 1024,\n",
        "                 hop_length = 512,power = 2,n_mels = 128,pad_mode = 'reflect',\n",
        "                 sr = 16000,center = True,norm = None):\n",
        "      \n",
        "        super(MimiiDataset, self).__init__()\n",
        "        self.audio_dir = audio_dir\n",
        "        self.n_mels = n_mels\n",
        "        self.n_fft = n_fft\n",
        "        self.win_length = win_length\n",
        "        self.hop_length = hop_length\n",
        "        self.power = power\n",
        "        self.pad_mode = pad_mode\n",
        "        self.sr = sr\n",
        "        self.center = center\n",
        "        self.norm = norm\n",
        "\n",
        "    def get_files(self):\n",
        "       return self.train_files, self.test_files\n",
        "    \n",
        "    def get_data(self,device, id):\n",
        "        \n",
        "        self.train_files, self.train_labels = self._train_file_list(device, id)\n",
        "        self.test_files, self.test_labels = self._test_file_list(device, id)\n",
        "        \n",
        "        self.train_data = self.get_audios(self.train_files)\n",
        "        self.test_data = self.get_audios(self.test_files)\n",
        "        \n",
        "        return self.train_data, self.test_data, self.train_labels, self.test_labels\n",
        "    \n",
        "    def _train_file_list(self, device, id):\n",
        "        query = os.path.abspath(\n",
        "            f\"{self.audio_dir}/{device}/train/normal_id_0{id}*.wav\"\n",
        "        )\n",
        "        train_normal_files = sorted(glob.glob(query))\n",
        "        train_normal_labels = np.zeros(len(train_normal_files))\n",
        "        \n",
        "        query = os.path.abspath(\n",
        "                f\"{self.audio_dir}/{device}/train/anomaly_id_0{id}*.wav\"\n",
        "            )\n",
        "        train_anomaly_files = sorted(glob.glob(query))\n",
        "        train_anomaly_labels = np.ones(len(train_anomaly_files))\n",
        "        \n",
        "        train_file_list = np.concatenate((train_normal_files, train_anomaly_files), axis=0)\n",
        "        train_labels = np.concatenate((train_normal_labels, train_anomaly_labels), axis=0)\n",
        "        \n",
        "        return train_file_list, train_labels\n",
        "    \n",
        "    def _test_file_list(self, device, id):     \n",
        "        query = os.path.abspath(\n",
        "            f\"{self.audio_dir}/{device}/test/normal_id_0{id}*.wav\"\n",
        "            )\n",
        "        test_normal_files = sorted(glob.glob(query))\n",
        "        test_normal_labels = np.zeros(len(test_normal_files))\n",
        "        \n",
        "        query = os.path.abspath(\n",
        "            f\"{self.audio_dir}/{device}/test/anomaly_id_0{id}*.wav\"\n",
        "            )\n",
        "        test_anomaly_files = sorted(glob.glob(query))\n",
        "        test_anomaly_labels = np.ones(len(test_anomaly_files))\n",
        "        \n",
        "        test_file_list = np.concatenate((test_normal_files, \n",
        "                                          test_anomaly_files), axis=0)\n",
        "        test_labels = np.concatenate((test_normal_labels,\n",
        "                                      test_anomaly_labels), axis=0)\n",
        "          \n",
        "        return test_file_list, test_labels\n",
        "\n",
        "    def normalize(self,tensor):\n",
        "        tensor_minusmean = tensor - tensor.mean()\n",
        "        return tensor_minusmean/np.absolute(tensor_minusmean).max()\n",
        "\n",
        "    def make0min(self,tensornd):\n",
        "        tensor = tensornd.numpy()\n",
        "        res = np.where(tensor == 0, 1E-19 , tensor)\n",
        "        return torch.from_numpy(res)\n",
        "\n",
        "    def spectrogrameToImage(self,specgram):\n",
        "        # specgram = torchaudio.transforms.MelSpectrogram(n_fft=1024, win_length=1024, \n",
        "        #                                                 hop_length=512, power=2, \n",
        "        #                                                 normalized=True, n_mels=128)(waveform )\n",
        "        specgram= self.make0min(specgram)\n",
        "        specgram = specgram.log2()[0,:,:].numpy()\n",
        "        \n",
        "        tr2image = transforms.Compose([transforms.ToPILImage()])\n",
        "\n",
        "        specgram= self.normalize(specgram)\n",
        "        # specgram = img_as_ubyte(specgram)\n",
        "        specgramImage = tr2image(specgram)\n",
        "        return specgramImage\n",
        "\n",
        "    def get_logmelspectrogram(self, waveform):\n",
        "        melspec = librosa.feature.melspectrogram(\n",
        "          n_fft=self.n_fft, win_length=self.win_length, hop_length=self.hop_length,\n",
        "          power=self.power,n_mels=self.n_mels,pad_mode=self.pad_mode,sr=self.sr,\n",
        "          center=self.center,norm=self.norm,htk=True,\n",
        "          y = waveform.numpy()\n",
        "        )\n",
        "\n",
        "        logmelspec = librosa.power_to_db(melspec)\n",
        "\n",
        "        return logmelspec\n",
        "\n",
        "    def get_melspectrogram(self,waveform):\n",
        "        melspec = librosa.feature.melspectrogram(\n",
        "            n_fft=self.n_fft, win_length=self.win_length, hop_length=self.hop_length,\n",
        "            power=self.power,n_mels=self.n_mels,pad_mode=self.pad_mode,sr=self.sr,\n",
        "            center=self.center,norm=self.norm,htk=True,\n",
        "            y = waveform.numpy()\n",
        "        )\n",
        "\n",
        "        return melspec\n",
        "    \n",
        "    def get_mfcc(self,waveform):\n",
        "        mfcc = librosa.feature.mfcc(    \n",
        "            n_fft=self.n_fft, win_length=self.win_length, \n",
        "            hop_length=self.hop_length,pad_mode=self.pad_mode,sr=self.sr,\n",
        "            center=self.center,norm=self.norm,n_mfcc=40,\n",
        "            y = waveform.numpy()\n",
        "        )\n",
        "\n",
        "        return mfcc\n",
        "\n",
        "    def get_chroma_stft(self,waveform):\n",
        "        stft = librosa.feature.chroma_stft(\n",
        "            n_fft=self.n_fft, win_length=self.win_length, \n",
        "            hop_length=self.hop_length,pad_mode=self.pad_mode,sr=self.sr,\n",
        "            center=self.center,norm=self.norm,n_chroma=12,\n",
        "            y=waveform.numpy()\n",
        "        )\n",
        "\n",
        "        return stft\n",
        "\n",
        "    def get_spectral_contrast(self,waveform):\n",
        "        spec_contrast = librosa.feature.spectral_contrast(    \n",
        "            n_fft=self.n_fft, win_length=self.win_length,center=self.center,\n",
        "            hop_length=self.hop_length,pad_mode=self.pad_mode,sr=self.sr,\n",
        "            y = waveform.numpy()\n",
        "        )\n",
        "\n",
        "        return spec_contrast\n",
        "    \n",
        "    def get_tonnetz(self,waveform):\n",
        "        harmonic = librosa.effects.harmonic(waveform.numpy())\n",
        "        tonnetz = librosa.feature.tonnetz(y=harmonic,sr=self.sr)\n",
        "\n",
        "        return tonnetz\n",
        "\n",
        "    def get_audios(self, file_list):\n",
        "        data = []\n",
        "        for i in range(len(file_list)):\n",
        "          y, sr = torchaudio.load(file_list[i])  \n",
        "          data.append(y)\n",
        "\n",
        "        return data\n",
        "    def _derive_data(self, file_list):\n",
        "        train_data = []\n",
        "        test_data = []\n",
        "        train_mode = True\n",
        "        for file_list in [self.train_files, self.test_files]:\n",
        "          tr2tensor = transforms.Compose([transforms.PILToTensor()])\n",
        "          data = []\n",
        "          for j in range(len(file_list)):\n",
        "            y, sr = torchaudio.load(file_list[j])  \n",
        "            spec = self.get_melspectrogram(y)\n",
        "            spec = self.spectrogrameToImage(spec)\n",
        "            spec = spec.convert('RGB')\n",
        "            vectors = tr2tensor(spec)\n",
        "            if train_mode:     \n",
        "              train_data.append(vectors)\n",
        "            else:\n",
        "              test_data.append(vectors)\n",
        "            \n",
        "          train_mode = False\n",
        "                \n",
        "        return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "S96soeIc0o13"
      },
      "outputs": [],
      "source": [
        "dataset = MimiiDataset('/content/drive/MyDrive/mimii')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "Gn2zdn92doi1"
      },
      "outputs": [],
      "source": [
        "# _, _, y_train_fan0, y_test_fan0 = dataset.get_data('ToyCar', 1)\n",
        "_, _, y_train_fan2, y_test_fan2 = dataset.get_data('ToyConveyor', 2)\n",
        "# _, _, y_train_fan4, y_test_fan4 = dataset.get_data('ToyCar', 3)\n",
        "# _, _, y_train_fan6, y_test_fan6 = dataset.get_data('ToyCar', 4)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_mfccs(wave_list):\n",
        "  data = []\n",
        "  for wave in wave_list:\n",
        "    mfcc = np.mean(dataset.get_mfcc(wave)[0], axis = 1)\n",
        "    data.append(mfcc)\n",
        "  \n",
        "  return data\n",
        "\n",
        "def mean_stfts(wave_list):\n",
        "  data = []\n",
        "  for wave in wave_list:\n",
        "    stft = np.mean(dataset.get_chroma_stft(wave)[0], axis = 1)\n",
        "    data.append(stft)\n",
        "  \n",
        "  return data\n",
        "\n",
        "def mean_melspecs(wave_list):\n",
        "  data = []\n",
        "  for wave in wave_list:\n",
        "    melspec = np.mean(dataset.get_melspectrogram(wave)[0], axis = 1)\n",
        "    data.append(melspec)\n",
        "  \n",
        "  return data\n",
        "\n",
        "def mean_spec_contrasts(wave_list):\n",
        "  data = []\n",
        "  for wave in wave_list:\n",
        "    spec_contrast = np.mean(dataset.get_spectral_contrast(wave)[0], axis = 1)\n",
        "    data.append(spec_contrast)\n",
        "  \n",
        "  return data\n",
        "  \n",
        "def mean_tonnetzs(wave_list):\n",
        "  data = []\n",
        "  for wave in wave_list:\n",
        "    tonnetz = np.mean(dataset.get_tonnetz(wave)[0], axis = 1)\n",
        "    data.append(tonnetz)\n",
        "  \n",
        "  return data"
      ],
      "metadata": {
        "id": "quhoMJUJsqyU"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_train1, df_test1, y_train1, y_test1 = dataset.get_data('ToyConveyor', 1)\n",
        "# df_train2, df_test2, y_train2, y_test2 = dataset.get_data('ToyConveyor', 2)\n",
        "# df_train3, df_test3, y_train3, y_test3 = dataset.get_data('ToyConveyor', 3)\n",
        "df_train4, df_test4, y_train4, y_test4 = dataset.get_data('ToyConveyor', 4)"
      ],
      "metadata": {
        "id": "C70jD80rrQJ_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(df_test4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDPZ9Zua5M5C",
        "outputId": "2779c611-4b7a-49b4-8d4f-0ba8ee8ed730"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_melspecs, test_melspecs = mean_melspecs(df_train1), mean_melspecs(df_test1)\n",
        "train_mfccs, test_mfccs = mean_mfccs(df_train1), mean_mfccs(df_test1)\n",
        "train_stfts, test_stfts = mean_stfts(df_train1), mean_stfts(df_test1)\n",
        "train_spec_contrasts, test_spec_contrasts = mean_spec_contrasts(df_train1), mean_spec_contrasts(df_test1)\n",
        "train_tonnetzs, test_tonnetzs = mean_tonnetzs(df_train1), mean_tonnetzs(df_test1)\n",
        "\n",
        "train_mixed_f = []\n",
        "test_mixed_f = []\n",
        "\n",
        "for i in range(len(train_melspecs)):\n",
        "\n",
        "  train_mf = np.concatenate((train_melspecs[i],train_mfccs[i],train_stfts[i],\n",
        "                             train_spec_contrasts[i],train_tonnetzs[i])).tolist()\n",
        "\n",
        "  train_mixed_f.append(train_mf)\n",
        "\n",
        "for i in range(len(test_melspecs)):\n",
        "\n",
        "  test_mf = np.concatenate((test_melspecs[i],test_mfccs[i],test_stfts[i],\n",
        "                             test_spec_contrasts[i],test_tonnetzs[i])).tolist()\n",
        "\n",
        "  test_mixed_f.append(test_mf)\n",
        "\n",
        "train_mixed_f = torch.FloatTensor(train_mixed_f)\n",
        "test_mixed_f = torch.FloatTensor(test_mixed_f)"
      ],
      "metadata": {
        "id": "5NAMxzpLrYJm"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(train_mixed_f, '/content/drive/MyDrive/mixed_features/y_tr_toyconveyor1.pt')\n",
        "torch.save(test_mixed_f, '/content/drive/MyDrive/mixed_features/y_ts_toyconveyor1.pt')"
      ],
      "metadata": {
        "id": "sw7SAvRzrmqe"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.save(torch.FloatTensor(y_train_fan0.tolist()), '/content/drive/MyDrive/labels/y_tr_toycar1.pt')\n",
        "# torch.save(torch.FloatTensor(y_test_fan0.tolist()), '/content/drive/MyDrive/labels/y_ts_toycar1.pt')\n",
        "torch.save(torch.FloatTensor(y_train_fan2.tolist()), '/content/drive/MyDrive/labels/y_tr_toyconveyor2.pt')\n",
        "torch.save(torch.FloatTensor(y_test_fan2.tolist()), '/content/drive/MyDrive/labels/y_ts_toyconveyor2.pt')\n",
        "# torch.save(torch.FloatTensor(y_train_fan4.tolist()), '/content/drive/MyDrive/labels/y_tr_toycar3.pt')\n",
        "# torch.save(torch.FloatTensor(y_test_fan4.tolist()), '/content/drive/MyDrive/labels/y_ts_toycar3.pt')\n",
        "# torch.save(torch.FloatTensor(y_train_fan6.tolist()), '/content/drive/MyDrive/labels/y_tr_toycar4.pt')\n",
        "# torch.save(torch.FloatTensor(y_test_fan6.tolist()), '/content/drive/MyDrive/labels/y_ts_toycar4.pt')"
      ],
      "metadata": {
        "id": "BIY-zDvjrIcu"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train, y_test = torch.load('/content/drive/MyDrive/labels/y_tr_toycar3.pt'), torch.load('/content/drive/MyDrive/labels/y_ts_toycar3.pt')"
      ],
      "metadata": {
        "id": "siHf9ch8vFoI"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_mf, test_mf = torch.load('/content/drive/MyDrive/mixed_features/train_mf_toycar3.pt'), torch.load('/content/drive/MyDrive/mixed_features/test_mf_toycar3.pt')"
      ],
      "metadata": {
        "id": "WiRjmJe_yH6u"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "id": "jWMPVGu1qiEq"
      },
      "outputs": [],
      "source": [
        "train_data = DataLoader(train_mf, batch_size=64, shuffle = True)\n",
        "test_data = DataLoader(test_mf, batch_size = 64, shuffle= False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "id": "vNTBTRe6qnBq"
      },
      "outputs": [],
      "source": [
        "class UNet_FC(nn.Module):\n",
        "\n",
        "  def __init__(self, in_features):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.bn = nn.BatchNorm1d(128)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "    self.fc0 = nn.Linear(in_features=in_features,out_features=in_features)\n",
        "\n",
        "    # encoder\n",
        "    self.fc1 = nn.Linear(in_features=in_features, out_features=128)\n",
        "    self.fc2 = nn.Linear(in_features=128,out_features=128)\n",
        "    self.fc3 = nn.Linear(in_features=128, out_features=128)\n",
        "    self.fc4 = nn.Linear(in_features=128, out_features=128)\n",
        "    self.fc5 = nn.Linear(in_features=128, out_features=8)\n",
        "\n",
        "    # decoder\n",
        "    self.fc6 = nn.Linear(in_features=8, out_features=128)\n",
        "    self.fc7 = nn.Linear(in_features=128*2, out_features=128)\n",
        "    self.fc8 = nn.Linear(in_features=128*2, out_features=128)\n",
        "    self.fc9 = nn.Linear(in_features=128*2, out_features=128)\n",
        "\n",
        "    self.out = nn.Linear(in_features=128*2, out_features=in_features)\n",
        "\n",
        "  def encoder(self, x):\n",
        "    input = self.fc0(x)\n",
        "\n",
        "    x1 = self.relu(self.bn(self.fc1(input)))\n",
        "    x2 = self.relu(self.bn(self.fc2(x1)))\n",
        "    x3 = self.relu(self.bn(self.fc3(x2)))\n",
        "    x4 = self.relu(self.bn(self.fc4(x3)))\n",
        "    x5 = self.relu(self.fc5(x4))\n",
        "\n",
        "    return [x5, x4, x3, x2, x1]\n",
        "\n",
        "  def decoder(self, x):\n",
        "    x6 = self.relu(self.fc6(x[0]))\n",
        "    con1 = torch.cat((x6,x[1]), 1) \n",
        "    x7 = self.relu(self.bn(self.fc7(con1)))\n",
        "    con2 = torch.cat((x7,x[2]), 1)\n",
        "    x8 = self.relu(self.bn(self.fc8(con2)))\n",
        "    con3 = torch.cat((x8,x[3]), 1)\n",
        "    x9 = self.relu(self.bn(self.fc9(con3)))\n",
        "    con4 = torch.cat((x9,x[4]), 1)\n",
        "\n",
        "    x10 = self.out(con4)\n",
        "\n",
        "    return x10\n",
        "  \n",
        "  def forward(self, x):\n",
        "    # encoded = self.encoder(x)\n",
        "\n",
        "    # decoded = self.decoder(encoded)\n",
        "    input = self.fc0(x)\n",
        "\n",
        "    x1 = self.relu(self.bn(self.fc1(input)))\n",
        "    x2 = self.relu(self.bn(self.fc2(x1)))\n",
        "    x3 = self.relu(self.bn(self.fc3(x2)))\n",
        "    x4 = self.relu(self.bn(self.fc4(x3)))\n",
        "    x5 = self.relu(self.fc5(x4))\n",
        "\n",
        "    xy = [x5, x4, x3, x2, x1]\n",
        "\n",
        "    x6 = self.relu(self.fc6(xy[0]))\n",
        "    con1 = torch.cat((x6,xy[1]), 1) \n",
        "    x7 = self.relu(self.bn(self.fc7(con1)))\n",
        "    con2 = torch.cat((x7,xy[2]), 1)\n",
        "    x8 = self.relu(self.bn(self.fc8(con2)))\n",
        "    con3 = torch.cat((x8,xy[3]), 1)\n",
        "    x9 = self.relu(self.bn(self.fc9(con3)))\n",
        "    con4 = torch.cat((x9,xy[4]), 1)\n",
        "\n",
        "    x10 = self.out(con4)\n",
        "\n",
        "    # return decoded\n",
        "    return x10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "id": "ZfgcBtQ3qn5l"
      },
      "outputs": [],
      "source": [
        "def train(model, optimizer, criterion, data_tr, data_val, scheduler = None,\n",
        "          epochs = 3000, device = 'cpu'):\n",
        "    # X_val, Y_val = next(iter(data_val))\n",
        "    losses = []\n",
        "    prev_avg_loss = 100000\n",
        "    for epoch in range(epochs):\n",
        "        train_avg_loss = 0\n",
        "        test_avg_loss = 0\n",
        "        # model.train()  # train mode\n",
        "        for batch in data_tr:\n",
        "          # data to device\n",
        "          batch = batch.to(device)\n",
        "          # set parameter gradients to zero\n",
        "          optimizer.zero_grad()\n",
        "          # forward\n",
        "          # print(Y_batch.shape)\n",
        "          predictions = model(batch)\n",
        "          loss = criterion(predictions, batch)\n",
        "          loss.backward() # backward-pass\n",
        "          optimizer.step()  # update weights\n",
        "          # calculate loss to show the user\n",
        "          if scheduler:\n",
        "            scheduler.step(loss)\n",
        "          train_avg_loss += loss / len(data_tr)\n",
        "\n",
        "        # model.eval()\n",
        "        for batch in data_val:\n",
        "          with torch.no_grad():\n",
        "            preds = model(batch.to(device)).cpu()\n",
        "            loss = criterion(preds,batch)\n",
        "            test_avg_loss += loss / len(data_val)\n",
        "                    \n",
        "        losses.append(train_avg_loss.item())\n",
        "        if (epoch+1)%100 == 0:\n",
        "          print(\"{}/{} train_loss: {} test_loss:{}\".format(epoch+1, epochs, train_avg_loss, test_avg_loss))\n",
        "        # if test_avg_loss < 70:\n",
        "        #   break\n",
        "    return losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "id": "ptkVTF55quOL"
      },
      "outputs": [],
      "source": [
        "unet = UNet_FC(in_features=193).to(device)\n",
        "optimizer = Adam(params = unet.parameters(), lr = 10e-3)\n",
        "# optimizer = Adam(params = unet.parameters())\n",
        "criterion = nn.MSELoss()\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, \n",
        "                                                       min_lr=10e-4, mode = 'min',\n",
        "                                                       patience = 30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkfmYl9oXhcB",
        "outputId": "ac2f233f-c2b1-4c32-c920-09d33d695c1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100/3000 train_loss: 483.64129638671875 test_loss:536.5618896484375\n",
            "200/3000 train_loss: 230.18785095214844 test_loss:266.8545227050781\n",
            "300/3000 train_loss: 135.63563537597656 test_loss:187.9648895263672\n",
            "400/3000 train_loss: 126.90518188476562 test_loss:163.2621612548828\n",
            "500/3000 train_loss: 99.5207290649414 test_loss:142.3267059326172\n",
            "600/3000 train_loss: 83.69599914550781 test_loss:124.85167694091797\n",
            "700/3000 train_loss: 101.38629913330078 test_loss:116.65933227539062\n",
            "800/3000 train_loss: 58.31731414794922 test_loss:103.43871307373047\n",
            "900/3000 train_loss: 47.118343353271484 test_loss:91.62793731689453\n",
            "1000/3000 train_loss: 47.133888244628906 test_loss:93.16793823242188\n",
            "1100/3000 train_loss: 48.20487594604492 test_loss:87.07746887207031\n",
            "1200/3000 train_loss: 45.640174865722656 test_loss:86.55986785888672\n",
            "1300/3000 train_loss: 36.2952995300293 test_loss:79.6207275390625\n",
            "1400/3000 train_loss: 39.42591857910156 test_loss:78.20748138427734\n",
            "1500/3000 train_loss: 40.64147186279297 test_loss:79.15779113769531\n",
            "1600/3000 train_loss: 54.76765060424805 test_loss:77.5078353881836\n",
            "1700/3000 train_loss: 42.55304718017578 test_loss:72.85892486572266\n",
            "1800/3000 train_loss: 34.641632080078125 test_loss:73.41522979736328\n",
            "1900/3000 train_loss: 29.496536254882812 test_loss:70.21499633789062\n",
            "2000/3000 train_loss: 35.230079650878906 test_loss:71.64109802246094\n",
            "2100/3000 train_loss: 24.20708465576172 test_loss:67.11361694335938\n",
            "2200/3000 train_loss: 26.08100128173828 test_loss:65.4614486694336\n",
            "2300/3000 train_loss: 29.96990966796875 test_loss:66.82369995117188\n",
            "2400/3000 train_loss: 24.78963851928711 test_loss:63.40544891357422\n",
            "2500/3000 train_loss: 20.384889602661133 test_loss:60.100189208984375\n",
            "2600/3000 train_loss: 22.18819236755371 test_loss:59.173858642578125\n",
            "2700/3000 train_loss: 21.459136962890625 test_loss:60.12126159667969\n",
            "2800/3000 train_loss: 19.090660095214844 test_loss:60.537879943847656\n",
            "2900/3000 train_loss: 16.50478172302246 test_loss:58.14720153808594\n",
            "3000/3000 train_loss: 20.918079376220703 test_loss:57.855628967285156\n"
          ]
        }
      ],
      "source": [
        "losses= train(model = unet, optimizer = optimizer, criterion=criterion, data_tr=train_data,\n",
        "               data_val = test_data, scheduler = scheduler, device = device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Ew7_F0-q7aL",
        "outputId": "bc6f9e96-cf45-4419-ee24-017822ff2b09"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(57.8556)"
            ]
          },
          "metadata": {},
          "execution_count": 160
        }
      ],
      "source": [
        "avg_loss = 0\n",
        "# unet.eval()\n",
        "# unet.train()\n",
        "preds = []\n",
        "i = 0\n",
        "test_anomaly_losses = []\n",
        "test_normal_losses = []\n",
        "test_losses = []\n",
        "# test_real = y_test_fan0.tolist()\n",
        "# y_test_fan0 = y_test_fan0.tolist()\n",
        "for batch in test_data:\n",
        "  with torch.no_grad():\n",
        "    # unet.train()\n",
        "    predictions = unet(batch.to(device)).cpu()\n",
        "    preds.append(predictions)\n",
        "  loss = criterion(predictions, batch.cpu())\n",
        "  for j in range(len(predictions)):\n",
        "    if int(y_test[i]) == 1:\n",
        "      test_anomaly_losses.append(float(criterion(predictions[j], batch[j])))\n",
        "    else:\n",
        "      test_normal_losses.append(float(criterion(predictions[j], batch[j])))\n",
        "    i += 1\n",
        "    test_losses.append(criterion(predictions[j], batch[j]))\n",
        "  # print(loss)\n",
        "  # print(loss)\n",
        "  avg_loss += loss / len(test_data)\n",
        "# avg_loss\n",
        "\n",
        "avg_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VpDKorrRso9o",
        "outputId": "3a60b405-01e8-4615-8714-017a9eec7d2e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(91.31639330881947, 27.973398012433734)"
            ]
          },
          "metadata": {},
          "execution_count": 161
        }
      ],
      "source": [
        "sum(test_anomaly_losses)/len(test_anomaly_losses) , sum(test_normal_losses)/len(test_normal_losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "id": "cJE0-57Qts3E"
      },
      "outputs": [],
      "source": [
        "# torch.save(unet, \"unet_fan2_2\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(preds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDIzf9t_H3pt",
        "outputId": "10bdd9e0-8617-40c8-a55c-d3883bb6e907"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LEvbZKYuh7J",
        "outputId": "5d86ae0a-b5d8-45a2-9a76-95662979b0c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8912938005390836\n"
          ]
        }
      ],
      "source": [
        "results = []\n",
        "vals = np.arange(20, 60, 0.5).tolist()\n",
        "for threshold in vals:\n",
        "  preds = []\n",
        "  for j in range(len(test_losses)):\n",
        "    if test_losses[j] > threshold:\n",
        "      preds.append(1)\n",
        "    else:\n",
        "      preds.append(0)\n",
        "  \n",
        "  results.append(roc_auc_score(y_test,preds))\n",
        "\n",
        "print(max(results))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4H4vpFX35yK"
      },
      "outputs": [],
      "source": [
        "def get_logmelspectrogram(waveform):\n",
        "    melspec = librosa.feature.melspectrogram(y=waveform.numpy(), hop_length=250, n_mels = 304)\n",
        "\n",
        "    logmelspec = librosa.power_to_db(melspec)\n",
        "\n",
        "    return logmelspec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vo1-S_qcuUZR"
      },
      "outputs": [],
      "source": [
        "# train_logmelspecs, test_logmelspecs = mean_logmelspecs(df_train), mean_logmelspecs(df_test)\n",
        "train_data1 = []\n",
        "for wave in df_train:\n",
        "  train_data1.append(get_logmelspectrogram(wave)[0])\n",
        "\n",
        "test_data1 = []\n",
        "for wave in df_test:\n",
        "  test_data1.append(get_logmelspectrogram(wave)[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGd9oI5IEVMx",
        "outputId": "73a79d4e-9e50-4b29-e7ea-55ee60a08c22"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-30-68ec04120629>:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  train_data1 = torch.FloatTensor(train_data1)\n"
          ]
        }
      ],
      "source": [
        "train_data1 = torch.FloatTensor(train_data1)\n",
        "test_data1 = torch.FloatTensor(test_data1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMOi9331OVb4"
      },
      "outputs": [],
      "source": [
        "train_logs = DataLoader(train_data1.reshape(916*304,641),batch_size = 304)\n",
        "test_logs = DataLoader(test_data1.reshape(459*304,641),batch_size = 304)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9rD6tuI1rfe"
      },
      "outputs": [],
      "source": [
        "unet1 = UNet_FC(in_features=641).to(device)\n",
        "optimizer1 = Adam(params = unet1.parameters(), lr = 10e-3)\n",
        "# optimizer = Adam(params = unet.parameters())\n",
        "criterion1 = nn.MSELoss()\n",
        "scheduler1 = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer1, factor=0.5, \n",
        "                                                       min_lr=10e-4, mode = 'min',\n",
        "                                                       patience = 30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qr6k85ma3ftD",
        "outputId": "67174f45-79f9-41f1-8959-a0c75a2b17fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 train_loss: 21.703369140625 test_loss:17.446470260620117\n"
          ]
        }
      ],
      "source": [
        "losses = train(model = unet1, optimizer = optimizer1, criterion=criterion1, data_tr=train_logs,\n",
        "               data_val = test_logs, device = device, epochs = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrPbpKgSPx7v",
        "outputId": "a7bd10e8-e2bd-4579-8211-d6eaaa879711"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(17.4465)"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "avg_loss = 0\n",
        "# unet.eval()\n",
        "# unet.train()\n",
        "preds = []\n",
        "i = 0\n",
        "test_anomaly_losses = []\n",
        "test_normal_losses = []\n",
        "test_losses = []\n",
        "test_real = y_test.tolist()\n",
        "for batch in test_logs:\n",
        "  with torch.no_grad():\n",
        "    # unet.train()\n",
        "    predictions = unet1(batch.to(device)).cpu()\n",
        "    preds.append(predictions)\n",
        "    loss = criterion(predictions, batch.cpu())\n",
        "    test_losses.append(loss)\n",
        "    if int(y_test[i]) == 1:\n",
        "      test_anomaly_losses.append(loss)\n",
        "    else:\n",
        "      test_normal_losses.append(loss)\n",
        "    i += 1\n",
        "  # print(loss)\n",
        "  # print(loss)\n",
        "  avg_loss += loss / len(test_logs)\n",
        "# avg_loss\n",
        "\n",
        "avg_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Z5Z1XYFN_x2",
        "outputId": "9d767817-5525-443f-db72-2588edb4cfbc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor(18.0466), tensor(15.2920))"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sum(test_anomaly_losses)/len(test_anomaly_losses) , sum(test_normal_losses)/len(test_normal_losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "er74WfG7P_B1",
        "outputId": "884ef8f3-8bfe-4d71-bc01-534f5d5a0a03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.5875626740947075\n"
          ]
        }
      ],
      "source": [
        "results = []\n",
        "vals = np.arange(10, 21, 0.1).tolist()\n",
        "for threshold in vals:\n",
        "  preds = []\n",
        "  for j in range(len(test_losses)):\n",
        "    if test_losses[j] > threshold:\n",
        "      preds.append(1)\n",
        "    else:\n",
        "      preds.append(0)\n",
        "  \n",
        "  results.append(roc_auc_score(test_real,preds))\n",
        "\n",
        "print(max(results))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QaSSqG8SbAw2"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}