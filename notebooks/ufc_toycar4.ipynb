{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "9SStKf4G0V5H"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "import torchaudio\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from skimage.util import img_as_ubyte\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import io\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_XtxbKLZq5KX",
    "outputId": "0c6de58c-f736-440b-85ce-a6ad2ae77c29"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CYxHegIM0Z4i",
    "outputId": "8b35e1ba-3d36-4aca-c569-e94fbad47a14"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive/\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "h9DATQwS0ivD"
   },
   "outputs": [],
   "source": [
    "class MimiiDataset(Dataset):\n",
    "    def __init__(self,audio_dir, n_fft = 1024, win_length = 1024,\n",
    "                 hop_length = 512,power = 2,n_mels = 128,pad_mode = 'reflect',\n",
    "                 sr = 16000,center = True,norm = None):\n",
    "      \n",
    "        super(MimiiDataset, self).__init__()\n",
    "        self.audio_dir = audio_dir\n",
    "        self.n_mels = n_mels\n",
    "        self.n_fft = n_fft\n",
    "        self.win_length = win_length\n",
    "        self.hop_length = hop_length\n",
    "        self.power = power\n",
    "        self.pad_mode = pad_mode\n",
    "        self.sr = sr\n",
    "        self.center = center\n",
    "        self.norm = norm\n",
    "\n",
    "    def get_files(self):\n",
    "       return self.train_files, self.test_files\n",
    "    \n",
    "    def get_data(self,device, id):\n",
    "        \n",
    "        self.train_files, self.train_labels = self._train_file_list(device, id)\n",
    "        self.test_files, self.test_labels = self._test_file_list(device, id)\n",
    "        \n",
    "        self.train_data = self.get_audios(self.train_files)\n",
    "        self.test_data = self.get_audios(self.test_files)\n",
    "        \n",
    "        return self.train_data, self.test_data, self.train_labels, self.test_labels\n",
    "    \n",
    "    def _train_file_list(self, device, id):\n",
    "        query = os.path.abspath(\n",
    "            f\"{self.audio_dir}/{device}/train/normal_id_0{id}*.wav\"\n",
    "        )\n",
    "        train_normal_files = sorted(glob.glob(query))\n",
    "        train_normal_labels = np.zeros(len(train_normal_files))\n",
    "        \n",
    "        query = os.path.abspath(\n",
    "                f\"{self.audio_dir}/{device}/train/anomaly_id_0{id}*.wav\"\n",
    "            )\n",
    "        train_anomaly_files = sorted(glob.glob(query))\n",
    "        train_anomaly_labels = np.ones(len(train_anomaly_files))\n",
    "        \n",
    "        train_file_list = np.concatenate((train_normal_files, train_anomaly_files), axis=0)\n",
    "        train_labels = np.concatenate((train_normal_labels, train_anomaly_labels), axis=0)\n",
    "        \n",
    "        return train_file_list, train_labels\n",
    "    \n",
    "    def _test_file_list(self, device, id):     \n",
    "        query = os.path.abspath(\n",
    "            f\"{self.audio_dir}/{device}/test/normal_id_0{id}*.wav\"\n",
    "            )\n",
    "        test_normal_files = sorted(glob.glob(query))\n",
    "        test_normal_labels = np.zeros(len(test_normal_files))\n",
    "        \n",
    "        query = os.path.abspath(\n",
    "            f\"{self.audio_dir}/{device}/test/anomaly_id_0{id}*.wav\"\n",
    "            )\n",
    "        test_anomaly_files = sorted(glob.glob(query))\n",
    "        test_anomaly_labels = np.ones(len(test_anomaly_files))\n",
    "        \n",
    "        test_file_list = np.concatenate((test_normal_files, \n",
    "                                          test_anomaly_files), axis=0)\n",
    "        test_labels = np.concatenate((test_normal_labels,\n",
    "                                      test_anomaly_labels), axis=0)\n",
    "          \n",
    "        return test_file_list, test_labels\n",
    "\n",
    "    def normalize(self,tensor):\n",
    "        tensor_minusmean = tensor - tensor.mean()\n",
    "        return tensor_minusmean/np.absolute(tensor_minusmean).max()\n",
    "\n",
    "    def make0min(self,tensornd):\n",
    "        tensor = tensornd.numpy()\n",
    "        res = np.where(tensor == 0, 1E-19 , tensor)\n",
    "        return torch.from_numpy(res)\n",
    "\n",
    "    def spectrogrameToImage(self,specgram):\n",
    "        # specgram = torchaudio.transforms.MelSpectrogram(n_fft=1024, win_length=1024, \n",
    "        #                                                 hop_length=512, power=2, \n",
    "        #                                                 normalized=True, n_mels=128)(waveform )\n",
    "        specgram= self.make0min(specgram)\n",
    "        specgram = specgram.log2()[0,:,:].numpy()\n",
    "        \n",
    "        tr2image = transforms.Compose([transforms.ToPILImage()])\n",
    "\n",
    "        specgram= self.normalize(specgram)\n",
    "        # specgram = img_as_ubyte(specgram)\n",
    "        specgramImage = tr2image(specgram)\n",
    "        return specgramImage\n",
    "\n",
    "    def get_logmelspectrogram(self, waveform):\n",
    "        melspec = librosa.feature.melspectrogram(\n",
    "          n_fft=self.n_fft, win_length=self.win_length, hop_length=self.hop_length,\n",
    "          power=self.power,n_mels=self.n_mels,pad_mode=self.pad_mode,sr=self.sr,\n",
    "          center=self.center,norm=self.norm,htk=True,\n",
    "          y = waveform.numpy()\n",
    "        )\n",
    "\n",
    "        logmelspec = librosa.power_to_db(melspec)\n",
    "\n",
    "        return logmelspec\n",
    "\n",
    "    def get_melspectrogram(self,waveform):\n",
    "        melspec = librosa.feature.melspectrogram(\n",
    "            n_fft=self.n_fft, win_length=self.win_length, hop_length=self.hop_length,\n",
    "            power=self.power,n_mels=self.n_mels,pad_mode=self.pad_mode,sr=self.sr,\n",
    "            center=self.center,norm=self.norm,htk=True,\n",
    "            y = waveform.numpy()\n",
    "        )\n",
    "\n",
    "        return melspec\n",
    "    \n",
    "    def get_mfcc(self,waveform):\n",
    "        mfcc = librosa.feature.mfcc(    \n",
    "            n_fft=self.n_fft, win_length=self.win_length, \n",
    "            hop_length=self.hop_length,pad_mode=self.pad_mode,sr=self.sr,\n",
    "            center=self.center,norm=self.norm,n_mfcc=40,\n",
    "            y = waveform.numpy()\n",
    "        )\n",
    "\n",
    "        return mfcc\n",
    "\n",
    "    def get_chroma_stft(self,waveform):\n",
    "        stft = librosa.feature.chroma_stft(\n",
    "            n_fft=self.n_fft, win_length=self.win_length, \n",
    "            hop_length=self.hop_length,pad_mode=self.pad_mode,sr=self.sr,\n",
    "            center=self.center,norm=self.norm,n_chroma=12,\n",
    "            y=waveform.numpy()\n",
    "        )\n",
    "\n",
    "        return stft\n",
    "\n",
    "    def get_spectral_contrast(self,waveform):\n",
    "        spec_contrast = librosa.feature.spectral_contrast(    \n",
    "            n_fft=self.n_fft, win_length=self.win_length,center=self.center,\n",
    "            hop_length=self.hop_length,pad_mode=self.pad_mode,sr=self.sr,\n",
    "            y = waveform.numpy()\n",
    "        )\n",
    "\n",
    "        return spec_contrast\n",
    "    \n",
    "    def get_tonnetz(self,waveform):\n",
    "        harmonic = librosa.effects.harmonic(waveform.numpy())\n",
    "        tonnetz = librosa.feature.tonnetz(y=harmonic,sr=self.sr)\n",
    "\n",
    "        return tonnetz\n",
    "\n",
    "    def get_audios(self, file_list):\n",
    "        data = []\n",
    "        for i in range(len(file_list)):\n",
    "          y, sr = torchaudio.load(file_list[i])  \n",
    "          data.append(y)\n",
    "\n",
    "        return data\n",
    "    def _derive_data(self, file_list):\n",
    "        train_data = []\n",
    "        test_data = []\n",
    "        train_mode = True\n",
    "        for file_list in [self.train_files, self.test_files]:\n",
    "          tr2tensor = transforms.Compose([transforms.PILToTensor()])\n",
    "          data = []\n",
    "          for j in range(len(file_list)):\n",
    "            y, sr = torchaudio.load(file_list[j])  \n",
    "            spec = self.get_melspectrogram(y)\n",
    "            spec = self.spectrogrameToImage(spec)\n",
    "            spec = spec.convert('RGB')\n",
    "            vectors = tr2tensor(spec)\n",
    "            if train_mode:     \n",
    "              train_data.append(vectors)\n",
    "            else:\n",
    "              test_data.append(vectors)\n",
    "            \n",
    "          train_mode = False\n",
    "                \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "S96soeIc0o13"
   },
   "outputs": [],
   "source": [
    "dataset = MimiiDataset('/content/drive/MyDrive/mimii')"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "y_train, y_test = torch.load('/content/drive/MyDrive/mixed_features/train_mf_toycar4.pt'), torch.load('/content/drive/MyDrive/mixed_features/test_mf_toycar4.pt')"
   ],
   "metadata": {
    "id": "MY5guG8kXa-Z"
   },
   "execution_count": 92,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "SgjpeWy_RV1C"
   },
   "outputs": [],
   "source": [
    "train_mixed_f = torch.load('/content/drive/MyDrive/mixed_features/train_mf_toycar4.pt')\n",
    "test_mixed_f = torch.load('/content/drive/MyDrive/mixed_features/test_mf_toycar4.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "jWMPVGu1qiEq"
   },
   "outputs": [],
   "source": [
    "train_data = DataLoader(train_mixed_f, batch_size=64, shuffle = True)\n",
    "test_data = DataLoader(test_mixed_f, batch_size = 64, shuffle= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "vNTBTRe6qnBq"
   },
   "outputs": [],
   "source": [
    "class UNet_FC(nn.Module):\n",
    "\n",
    "  def __init__(self, in_features):\n",
    "    super().__init__()\n",
    "    \n",
    "    self.bn = nn.BatchNorm1d(128)\n",
    "    self.relu = nn.ReLU()\n",
    "\n",
    "    self.fc0 = nn.Linear(in_features=in_features,out_features=in_features)\n",
    "\n",
    "    # encoder\n",
    "    self.fc1 = nn.Linear(in_features=in_features, out_features=128)\n",
    "    self.fc2 = nn.Linear(in_features=128,out_features=128)\n",
    "    self.fc3 = nn.Linear(in_features=128, out_features=128)\n",
    "    self.fc4 = nn.Linear(in_features=128, out_features=128)\n",
    "    self.fc5 = nn.Linear(in_features=128, out_features=8)\n",
    "\n",
    "    # decoder\n",
    "    self.fc6 = nn.Linear(in_features=8, out_features=128)\n",
    "    self.fc7 = nn.Linear(in_features=128*2, out_features=128)\n",
    "    self.fc8 = nn.Linear(in_features=128*2, out_features=128)\n",
    "    self.fc9 = nn.Linear(in_features=128*2, out_features=128)\n",
    "\n",
    "    self.out = nn.Linear(in_features=128*2, out_features=in_features)\n",
    "\n",
    "  def forward(self, x):\n",
    "    # encoded = self.encoder(x)\n",
    "\n",
    "    # decoded = self.decoder(encoded)\n",
    "    input = self.fc0(x)\n",
    "\n",
    "    x1 = self.relu(self.bn(self.fc1(input)))\n",
    "    x2 = self.relu(self.bn(self.fc2(x1)))\n",
    "    x3 = self.relu(self.bn(self.fc3(x2)))\n",
    "    x4 = self.relu(self.bn(self.fc4(x3)))\n",
    "    x5 = self.relu(self.fc5(x4))\n",
    "\n",
    "    xy = [x5, x4, x3, x2, x1]\n",
    "\n",
    "    x6 = self.relu(self.fc6(xy[0]))\n",
    "    con1 = torch.cat((x6,xy[1]), 1) \n",
    "    x7 = self.relu(self.bn(self.fc7(con1)))\n",
    "    con2 = torch.cat((x7,xy[2]), 1)\n",
    "    x8 = self.relu(self.bn(self.fc8(con2)))\n",
    "    con3 = torch.cat((x8,xy[3]), 1)\n",
    "    x9 = self.relu(self.bn(self.fc9(con3)))\n",
    "    con4 = torch.cat((x9,xy[4]), 1)\n",
    "\n",
    "    x10 = self.out(con4)\n",
    "\n",
    "    return x10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "ZfgcBtQ3qn5l"
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, data_tr, data_val, scheduler = None,\n",
    "          epochs = 3000, device = 'cpu'):\n",
    "    # X_val, Y_val = next(iter(data_val))\n",
    "    losses = []\n",
    "    prev_avg_loss = 100000\n",
    "    for epoch in range(epochs):\n",
    "        train_avg_loss = 0\n",
    "        test_avg_loss = 0\n",
    "        # model.train()  # train mode\n",
    "        for batch in data_tr:\n",
    "          # data to device\n",
    "          batch = batch.to(device)\n",
    "          # set parameter gradients to zero\n",
    "          optimizer.zero_grad()\n",
    "          # forward\n",
    "          # print(Y_batch.shape)\n",
    "          predictions = model(batch)\n",
    "          loss = criterion(predictions, batch)\n",
    "          loss.backward() # backward-pass\n",
    "          optimizer.step()  # update weights\n",
    "          # calculate loss to show the user\n",
    "          if scheduler:\n",
    "            scheduler.step(loss)\n",
    "          train_avg_loss += loss / len(data_tr)\n",
    "\n",
    "        # model.eval()\n",
    "        for batch in data_val:\n",
    "          with torch.no_grad():\n",
    "            preds = model(batch.to(device)).cpu()\n",
    "            loss = criterion(preds,batch)\n",
    "            test_avg_loss += loss / len(data_val)\n",
    "                    \n",
    "        losses.append(train_avg_loss.item())\n",
    "        # if (epoch+1)%50 == 0:\n",
    "        print(\"{}/{} train_loss: {} test_loss:{}\".format(epoch+1, epochs, train_avg_loss, test_avg_loss))\n",
    "        # if test_avg_loss < 70:\n",
    "        #   break\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "ptkVTF55quOL"
   },
   "outputs": [],
   "source": [
    "unet = UNet_FC(in_features=193).to(device)\n",
    "optimizer = Adam(params = unet.parameters(), lr = 10e-3)\n",
    "# optimizer = Adam(params = unet.parameters())\n",
    "criterion = nn.MSELoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, \n",
    "                                                       min_lr=10e-4, mode = 'min',\n",
    "                                                       patience = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QkfmYl9oXhcB",
    "outputId": "18f94769-f6a9-41d8-9263-e8ce001a523c"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1/3000 train_loss: 281748.375 test_loss:281650.28125\n",
      "2/3000 train_loss: 277924.71875 test_loss:276594.09375\n",
      "3/3000 train_loss: 272073.0625 test_loss:269760.09375\n",
      "4/3000 train_loss: 263901.8125 test_loss:261015.46875\n",
      "5/3000 train_loss: 253748.890625 test_loss:249026.0625\n",
      "6/3000 train_loss: 240720.4375 test_loss:234476.515625\n",
      "7/3000 train_loss: 224639.5625 test_loss:217567.296875\n",
      "8/3000 train_loss: 207166.46875 test_loss:199665.09375\n",
      "9/3000 train_loss: 189679.15625 test_loss:181244.703125\n",
      "10/3000 train_loss: 169678.5625 test_loss:160756.9375\n",
      "11/3000 train_loss: 149844.203125 test_loss:141141.65625\n",
      "12/3000 train_loss: 130388.6875 test_loss:122560.265625\n",
      "13/3000 train_loss: 112827.21875 test_loss:105742.015625\n",
      "14/3000 train_loss: 95832.0859375 test_loss:89879.796875\n",
      "15/3000 train_loss: 79951.8671875 test_loss:74162.296875\n",
      "16/3000 train_loss: 65625.21875 test_loss:60237.5234375\n",
      "17/3000 train_loss: 52438.125 test_loss:48061.6015625\n",
      "18/3000 train_loss: 41162.125 test_loss:37730.3671875\n",
      "19/3000 train_loss: 31675.576171875 test_loss:29368.6953125\n",
      "20/3000 train_loss: 23722.46484375 test_loss:22098.990234375\n",
      "21/3000 train_loss: 17821.6015625 test_loss:16539.3515625\n",
      "22/3000 train_loss: 12912.609375 test_loss:12264.6240234375\n",
      "23/3000 train_loss: 9270.154296875 test_loss:9298.3564453125\n",
      "24/3000 train_loss: 6740.5380859375 test_loss:6953.0341796875\n",
      "25/3000 train_loss: 4644.6103515625 test_loss:5188.994140625\n",
      "26/3000 train_loss: 3298.987548828125 test_loss:4068.203125\n",
      "27/3000 train_loss: 2120.44091796875 test_loss:3131.716796875\n",
      "28/3000 train_loss: 1843.85302734375 test_loss:2796.081298828125\n",
      "29/3000 train_loss: 1542.8743896484375 test_loss:2095.16015625\n",
      "30/3000 train_loss: 1066.16064453125 test_loss:1829.49853515625\n",
      "31/3000 train_loss: 1085.7108154296875 test_loss:1573.3572998046875\n",
      "32/3000 train_loss: 523.8602905273438 test_loss:1411.3466796875\n",
      "33/3000 train_loss: 586.1159057617188 test_loss:1316.519775390625\n",
      "34/3000 train_loss: 504.96966552734375 test_loss:1235.797119140625\n",
      "35/3000 train_loss: 490.1191101074219 test_loss:1165.218505859375\n",
      "36/3000 train_loss: 547.1207275390625 test_loss:1138.74560546875\n",
      "37/3000 train_loss: 320.4834289550781 test_loss:1093.4857177734375\n",
      "38/3000 train_loss: 529.5392456054688 test_loss:1090.99658203125\n",
      "39/3000 train_loss: 346.9191589355469 test_loss:1098.8609619140625\n",
      "40/3000 train_loss: 461.8978576660156 test_loss:1052.493896484375\n",
      "41/3000 train_loss: 631.6148681640625 test_loss:1047.2154541015625\n",
      "42/3000 train_loss: 407.8263854980469 test_loss:1035.8802490234375\n",
      "43/3000 train_loss: 378.9704895019531 test_loss:1034.507080078125\n",
      "44/3000 train_loss: 386.92669677734375 test_loss:1025.4205322265625\n",
      "45/3000 train_loss: 481.09014892578125 test_loss:1017.2098388671875\n",
      "46/3000 train_loss: 306.3247985839844 test_loss:1015.4708862304688\n",
      "47/3000 train_loss: 285.0995788574219 test_loss:1009.2811279296875\n",
      "48/3000 train_loss: 407.6042785644531 test_loss:1009.5795288085938\n",
      "49/3000 train_loss: 353.1499938964844 test_loss:1003.84619140625\n",
      "50/3000 train_loss: 266.85284423828125 test_loss:999.6253051757812\n",
      "51/3000 train_loss: 597.34130859375 test_loss:998.4277954101562\n",
      "52/3000 train_loss: 341.2222900390625 test_loss:992.3922729492188\n",
      "53/3000 train_loss: 437.3592224121094 test_loss:989.3580932617188\n",
      "54/3000 train_loss: 460.5657043457031 test_loss:984.4158325195312\n",
      "55/3000 train_loss: 387.0231018066406 test_loss:978.7789306640625\n",
      "56/3000 train_loss: 428.7014465332031 test_loss:973.9389038085938\n",
      "57/3000 train_loss: 311.1271667480469 test_loss:968.7003173828125\n",
      "58/3000 train_loss: 255.80691528320312 test_loss:965.8370361328125\n",
      "59/3000 train_loss: 337.8341369628906 test_loss:963.1975708007812\n",
      "60/3000 train_loss: 341.60382080078125 test_loss:964.6486206054688\n",
      "61/3000 train_loss: 323.48651123046875 test_loss:960.2385864257812\n",
      "62/3000 train_loss: 361.3095703125 test_loss:956.973876953125\n",
      "63/3000 train_loss: 655.622314453125 test_loss:952.5455322265625\n",
      "64/3000 train_loss: 531.8517456054688 test_loss:950.9442138671875\n",
      "65/3000 train_loss: 379.4862060546875 test_loss:943.008056640625\n",
      "66/3000 train_loss: 363.5536804199219 test_loss:940.7762451171875\n",
      "67/3000 train_loss: 439.16607666015625 test_loss:935.5643920898438\n",
      "68/3000 train_loss: 246.13894653320312 test_loss:929.9557495117188\n",
      "69/3000 train_loss: 398.72393798828125 test_loss:932.0748291015625\n",
      "70/3000 train_loss: 297.2343444824219 test_loss:921.9638061523438\n",
      "71/3000 train_loss: 321.63592529296875 test_loss:921.0048828125\n",
      "72/3000 train_loss: 264.34722900390625 test_loss:917.3634033203125\n",
      "73/3000 train_loss: 464.34490966796875 test_loss:919.2264404296875\n",
      "74/3000 train_loss: 246.74322509765625 test_loss:910.4690551757812\n",
      "75/3000 train_loss: 265.5171813964844 test_loss:908.2032470703125\n",
      "76/3000 train_loss: 332.9572448730469 test_loss:904.5845947265625\n",
      "77/3000 train_loss: 219.21226501464844 test_loss:903.1387939453125\n",
      "78/3000 train_loss: 260.02630615234375 test_loss:900.02294921875\n",
      "79/3000 train_loss: 368.39306640625 test_loss:898.0755615234375\n",
      "80/3000 train_loss: 491.1109313964844 test_loss:889.8760986328125\n",
      "81/3000 train_loss: 239.7661590576172 test_loss:885.1447143554688\n",
      "82/3000 train_loss: 316.36102294921875 test_loss:880.5704345703125\n",
      "83/3000 train_loss: 472.4242858886719 test_loss:876.1626586914062\n",
      "84/3000 train_loss: 228.57212829589844 test_loss:872.698486328125\n",
      "85/3000 train_loss: 284.7967224121094 test_loss:871.6521606445312\n",
      "86/3000 train_loss: 453.887939453125 test_loss:869.7236938476562\n",
      "87/3000 train_loss: 236.63134765625 test_loss:864.616455078125\n",
      "88/3000 train_loss: 277.6745910644531 test_loss:863.73291015625\n",
      "89/3000 train_loss: 295.7040710449219 test_loss:859.2628173828125\n",
      "90/3000 train_loss: 282.8694152832031 test_loss:855.027587890625\n",
      "91/3000 train_loss: 261.1488037109375 test_loss:855.849853515625\n",
      "92/3000 train_loss: 260.46551513671875 test_loss:851.0399169921875\n",
      "93/3000 train_loss: 308.1554260253906 test_loss:851.5471801757812\n",
      "94/3000 train_loss: 254.8686065673828 test_loss:844.3914794921875\n",
      "95/3000 train_loss: 292.4617919921875 test_loss:848.41455078125\n",
      "96/3000 train_loss: 336.64874267578125 test_loss:837.2373046875\n",
      "97/3000 train_loss: 244.5958251953125 test_loss:835.2236328125\n",
      "98/3000 train_loss: 253.19065856933594 test_loss:834.9852294921875\n",
      "99/3000 train_loss: 371.03533935546875 test_loss:838.168701171875\n",
      "100/3000 train_loss: 224.04196166992188 test_loss:830.3456420898438\n",
      "101/3000 train_loss: 388.939697265625 test_loss:822.0034790039062\n",
      "102/3000 train_loss: 218.37432861328125 test_loss:821.9878540039062\n",
      "103/3000 train_loss: 316.3987731933594 test_loss:832.7166137695312\n",
      "104/3000 train_loss: 241.63424682617188 test_loss:823.42236328125\n",
      "105/3000 train_loss: 305.0482177734375 test_loss:815.3854370117188\n",
      "106/3000 train_loss: 232.4081268310547 test_loss:811.7198486328125\n",
      "107/3000 train_loss: 318.2706298828125 test_loss:812.3402709960938\n",
      "108/3000 train_loss: 279.2082824707031 test_loss:813.8561401367188\n",
      "109/3000 train_loss: 313.162353515625 test_loss:814.333984375\n",
      "110/3000 train_loss: 219.58108520507812 test_loss:807.23828125\n",
      "111/3000 train_loss: 258.7591552734375 test_loss:799.33837890625\n",
      "112/3000 train_loss: 235.8612823486328 test_loss:797.7473754882812\n",
      "113/3000 train_loss: 201.5275115966797 test_loss:799.7661743164062\n",
      "114/3000 train_loss: 242.59872436523438 test_loss:792.3997192382812\n",
      "115/3000 train_loss: 277.64068603515625 test_loss:790.2654418945312\n",
      "116/3000 train_loss: 281.50604248046875 test_loss:788.2796020507812\n",
      "117/3000 train_loss: 341.2340393066406 test_loss:784.7245483398438\n",
      "118/3000 train_loss: 287.336669921875 test_loss:781.2183837890625\n",
      "119/3000 train_loss: 431.1323547363281 test_loss:781.6726684570312\n",
      "120/3000 train_loss: 283.53857421875 test_loss:774.3207397460938\n",
      "121/3000 train_loss: 352.28143310546875 test_loss:766.3168334960938\n",
      "122/3000 train_loss: 277.6761474609375 test_loss:767.8585205078125\n",
      "123/3000 train_loss: 270.3307800292969 test_loss:749.9481811523438\n",
      "124/3000 train_loss: 236.77853393554688 test_loss:711.5209350585938\n",
      "125/3000 train_loss: 195.47799682617188 test_loss:707.6929321289062\n",
      "126/3000 train_loss: 210.98788452148438 test_loss:706.7389526367188\n",
      "127/3000 train_loss: 240.2343292236328 test_loss:708.8328247070312\n",
      "128/3000 train_loss: 285.5105895996094 test_loss:701.7694091796875\n",
      "129/3000 train_loss: 226.43353271484375 test_loss:703.434326171875\n",
      "130/3000 train_loss: 194.88180541992188 test_loss:699.5314331054688\n",
      "131/3000 train_loss: 281.9133605957031 test_loss:702.1773681640625\n",
      "132/3000 train_loss: 247.7160186767578 test_loss:696.5223999023438\n",
      "133/3000 train_loss: 313.15643310546875 test_loss:698.3092041015625\n",
      "134/3000 train_loss: 239.20138549804688 test_loss:695.3524780273438\n",
      "135/3000 train_loss: 350.58056640625 test_loss:691.4158935546875\n",
      "136/3000 train_loss: 319.8776550292969 test_loss:683.3220825195312\n",
      "137/3000 train_loss: 214.44296264648438 test_loss:686.0423583984375\n",
      "138/3000 train_loss: 344.9210510253906 test_loss:680.2181396484375\n",
      "139/3000 train_loss: 316.02801513671875 test_loss:675.997314453125\n",
      "140/3000 train_loss: 194.4384765625 test_loss:667.2321166992188\n",
      "141/3000 train_loss: 320.9335021972656 test_loss:682.2386474609375\n",
      "142/3000 train_loss: 236.9088134765625 test_loss:670.0271606445312\n",
      "143/3000 train_loss: 206.36802673339844 test_loss:632.2809448242188\n",
      "144/3000 train_loss: 283.29376220703125 test_loss:629.666015625\n",
      "145/3000 train_loss: 245.84007263183594 test_loss:623.6166381835938\n",
      "146/3000 train_loss: 261.1556091308594 test_loss:630.8677368164062\n",
      "147/3000 train_loss: 212.73414611816406 test_loss:616.3756713867188\n",
      "148/3000 train_loss: 192.541748046875 test_loss:615.1234741210938\n",
      "149/3000 train_loss: 207.41799926757812 test_loss:615.3745727539062\n",
      "150/3000 train_loss: 200.19497680664062 test_loss:619.8060302734375\n",
      "151/3000 train_loss: 210.7661590576172 test_loss:615.8030395507812\n",
      "152/3000 train_loss: 253.82518005371094 test_loss:613.3935546875\n",
      "153/3000 train_loss: 206.26126098632812 test_loss:616.2992553710938\n",
      "154/3000 train_loss: 221.1030731201172 test_loss:608.7366943359375\n",
      "155/3000 train_loss: 244.43894958496094 test_loss:603.6707153320312\n",
      "156/3000 train_loss: 205.172607421875 test_loss:614.772216796875\n",
      "157/3000 train_loss: 237.3438262939453 test_loss:606.0233764648438\n",
      "158/3000 train_loss: 222.11514282226562 test_loss:604.6448974609375\n",
      "159/3000 train_loss: 204.08567810058594 test_loss:606.0685424804688\n",
      "160/3000 train_loss: 171.69142150878906 test_loss:597.9653930664062\n",
      "161/3000 train_loss: 174.15440368652344 test_loss:597.9841918945312\n",
      "162/3000 train_loss: 218.40423583984375 test_loss:601.92626953125\n",
      "163/3000 train_loss: 677.6507568359375 test_loss:584.2471313476562\n",
      "164/3000 train_loss: 200.36944580078125 test_loss:564.0230712890625\n",
      "165/3000 train_loss: 246.90176391601562 test_loss:566.3317260742188\n",
      "166/3000 train_loss: 207.83285522460938 test_loss:539.3272705078125\n",
      "167/3000 train_loss: 205.54945373535156 test_loss:530.8965454101562\n",
      "168/3000 train_loss: 195.0932159423828 test_loss:536.8565673828125\n",
      "169/3000 train_loss: 223.9246826171875 test_loss:526.5189819335938\n",
      "170/3000 train_loss: 181.74009704589844 test_loss:526.7054443359375\n",
      "171/3000 train_loss: 255.77647399902344 test_loss:527.1666870117188\n",
      "172/3000 train_loss: 163.80685424804688 test_loss:521.6611938476562\n",
      "173/3000 train_loss: 184.68325805664062 test_loss:524.0625610351562\n",
      "174/3000 train_loss: 183.60594177246094 test_loss:521.0260620117188\n",
      "175/3000 train_loss: 159.26905822753906 test_loss:522.3587036132812\n",
      "176/3000 train_loss: 178.96151733398438 test_loss:534.848388671875\n",
      "177/3000 train_loss: 173.8756103515625 test_loss:519.5455322265625\n",
      "178/3000 train_loss: 179.9541778564453 test_loss:521.3271484375\n",
      "179/3000 train_loss: 306.3874206542969 test_loss:528.9642333984375\n",
      "180/3000 train_loss: 148.80731201171875 test_loss:468.43292236328125\n",
      "181/3000 train_loss: 171.46295166015625 test_loss:473.3553771972656\n",
      "182/3000 train_loss: 189.13778686523438 test_loss:470.0891418457031\n",
      "183/3000 train_loss: 170.74954223632812 test_loss:452.5407409667969\n",
      "184/3000 train_loss: 177.08474731445312 test_loss:454.473876953125\n",
      "185/3000 train_loss: 205.18765258789062 test_loss:439.2085266113281\n",
      "186/3000 train_loss: 154.4105682373047 test_loss:431.17755126953125\n",
      "187/3000 train_loss: 158.03683471679688 test_loss:447.418701171875\n",
      "188/3000 train_loss: 220.41014099121094 test_loss:430.8437194824219\n",
      "189/3000 train_loss: 170.2960968017578 test_loss:424.9739990234375\n",
      "190/3000 train_loss: 158.34483337402344 test_loss:456.66717529296875\n",
      "191/3000 train_loss: 175.5648193359375 test_loss:420.2125549316406\n",
      "192/3000 train_loss: 171.27102661132812 test_loss:421.46649169921875\n",
      "193/3000 train_loss: 184.2438507080078 test_loss:431.5129089355469\n",
      "194/3000 train_loss: 172.08876037597656 test_loss:424.8180236816406\n",
      "195/3000 train_loss: 151.8433837890625 test_loss:424.9169006347656\n",
      "196/3000 train_loss: 153.5514373779297 test_loss:427.86285400390625\n",
      "197/3000 train_loss: 181.71578979492188 test_loss:429.4658508300781\n",
      "198/3000 train_loss: 198.89503479003906 test_loss:416.7436218261719\n",
      "199/3000 train_loss: 147.5486602783203 test_loss:434.2502746582031\n",
      "200/3000 train_loss: 184.13230895996094 test_loss:408.9888610839844\n",
      "201/3000 train_loss: 171.35435485839844 test_loss:434.8698425292969\n",
      "202/3000 train_loss: 154.46633911132812 test_loss:414.7213134765625\n",
      "203/3000 train_loss: 148.7210235595703 test_loss:429.4273681640625\n",
      "204/3000 train_loss: 160.94410705566406 test_loss:433.42572021484375\n",
      "205/3000 train_loss: 150.9667205810547 test_loss:443.06866455078125\n",
      "206/3000 train_loss: 153.35247802734375 test_loss:417.86962890625\n",
      "207/3000 train_loss: 142.35609436035156 test_loss:424.6495666503906\n",
      "208/3000 train_loss: 140.26559448242188 test_loss:413.2991943359375\n",
      "209/3000 train_loss: 150.71823120117188 test_loss:432.32373046875\n",
      "210/3000 train_loss: 198.17910766601562 test_loss:397.9091796875\n",
      "211/3000 train_loss: 191.80551147460938 test_loss:414.6439208984375\n",
      "212/3000 train_loss: 145.24037170410156 test_loss:410.590576171875\n",
      "213/3000 train_loss: 143.30467224121094 test_loss:398.0610656738281\n",
      "214/3000 train_loss: 139.96893310546875 test_loss:395.6435241699219\n",
      "215/3000 train_loss: 143.96133422851562 test_loss:396.5885009765625\n",
      "216/3000 train_loss: 142.4786376953125 test_loss:395.1044921875\n",
      "217/3000 train_loss: 179.270751953125 test_loss:412.64508056640625\n",
      "218/3000 train_loss: 141.7456817626953 test_loss:437.99920654296875\n",
      "219/3000 train_loss: 151.35906982421875 test_loss:453.8332214355469\n",
      "220/3000 train_loss: 167.02804565429688 test_loss:408.20721435546875\n",
      "221/3000 train_loss: 169.1776885986328 test_loss:384.024658203125\n",
      "222/3000 train_loss: 147.39822387695312 test_loss:376.8117980957031\n",
      "223/3000 train_loss: 159.47024536132812 test_loss:382.9254150390625\n",
      "224/3000 train_loss: 203.63294982910156 test_loss:386.43792724609375\n",
      "225/3000 train_loss: 185.96385192871094 test_loss:390.8641662597656\n",
      "226/3000 train_loss: 138.27340698242188 test_loss:396.69964599609375\n",
      "227/3000 train_loss: 178.03102111816406 test_loss:384.78900146484375\n",
      "228/3000 train_loss: 164.751708984375 test_loss:385.9619445800781\n",
      "229/3000 train_loss: 146.01678466796875 test_loss:380.626708984375\n",
      "230/3000 train_loss: 143.32119750976562 test_loss:377.38568115234375\n",
      "231/3000 train_loss: 173.45887756347656 test_loss:374.79608154296875\n",
      "232/3000 train_loss: 143.16806030273438 test_loss:377.20977783203125\n",
      "233/3000 train_loss: 147.34878540039062 test_loss:379.6579284667969\n",
      "234/3000 train_loss: 155.3429718017578 test_loss:380.591552734375\n",
      "235/3000 train_loss: 141.8458709716797 test_loss:379.6990051269531\n",
      "236/3000 train_loss: 133.06121826171875 test_loss:373.89996337890625\n",
      "237/3000 train_loss: 136.5946807861328 test_loss:405.3935546875\n",
      "238/3000 train_loss: 158.51974487304688 test_loss:383.5363464355469\n",
      "239/3000 train_loss: 147.00042724609375 test_loss:366.2698974609375\n",
      "240/3000 train_loss: 157.11782836914062 test_loss:360.8128967285156\n",
      "241/3000 train_loss: 154.81593322753906 test_loss:358.3700866699219\n",
      "242/3000 train_loss: 136.59471130371094 test_loss:358.8267517089844\n",
      "243/3000 train_loss: 160.5312957763672 test_loss:370.0986328125\n",
      "244/3000 train_loss: 154.01864624023438 test_loss:368.6297302246094\n",
      "245/3000 train_loss: 153.90530395507812 test_loss:372.9555969238281\n",
      "246/3000 train_loss: 158.15179443359375 test_loss:379.5202331542969\n",
      "247/3000 train_loss: 153.83705139160156 test_loss:372.21990966796875\n",
      "248/3000 train_loss: 136.9720916748047 test_loss:365.261474609375\n",
      "249/3000 train_loss: 161.06948852539062 test_loss:351.0372619628906\n",
      "250/3000 train_loss: 149.7969207763672 test_loss:352.59942626953125\n",
      "251/3000 train_loss: 140.61489868164062 test_loss:352.16619873046875\n",
      "252/3000 train_loss: 123.47502899169922 test_loss:350.59344482421875\n",
      "253/3000 train_loss: 140.87875366210938 test_loss:349.8604431152344\n",
      "254/3000 train_loss: 131.02145385742188 test_loss:348.94232177734375\n",
      "255/3000 train_loss: 130.9286346435547 test_loss:352.671630859375\n",
      "256/3000 train_loss: 156.7997283935547 test_loss:352.3357238769531\n",
      "257/3000 train_loss: 123.24662017822266 test_loss:357.0390625\n",
      "258/3000 train_loss: 126.0777816772461 test_loss:351.8218078613281\n",
      "259/3000 train_loss: 155.7587890625 test_loss:346.97393798828125\n",
      "260/3000 train_loss: 134.3651123046875 test_loss:342.5450439453125\n",
      "261/3000 train_loss: 128.04319763183594 test_loss:348.5510559082031\n",
      "262/3000 train_loss: 136.54803466796875 test_loss:351.080810546875\n",
      "263/3000 train_loss: 128.03614807128906 test_loss:342.942138671875\n",
      "264/3000 train_loss: 126.50166320800781 test_loss:350.892578125\n",
      "265/3000 train_loss: 130.32913208007812 test_loss:354.6009826660156\n",
      "266/3000 train_loss: 129.03515625 test_loss:343.7012939453125\n",
      "267/3000 train_loss: 159.77020263671875 test_loss:345.20037841796875\n",
      "268/3000 train_loss: 149.11241149902344 test_loss:365.1754455566406\n",
      "269/3000 train_loss: 137.73284912109375 test_loss:358.6681213378906\n",
      "270/3000 train_loss: 129.1099853515625 test_loss:361.7255554199219\n",
      "271/3000 train_loss: 130.67811584472656 test_loss:350.3831787109375\n",
      "272/3000 train_loss: 123.38002014160156 test_loss:349.7073974609375\n",
      "273/3000 train_loss: 135.79718017578125 test_loss:349.95623779296875\n",
      "274/3000 train_loss: 136.85279846191406 test_loss:342.2743225097656\n",
      "275/3000 train_loss: 132.7941131591797 test_loss:324.7004089355469\n",
      "276/3000 train_loss: 151.50802612304688 test_loss:327.572998046875\n",
      "277/3000 train_loss: 128.86439514160156 test_loss:336.20172119140625\n",
      "278/3000 train_loss: 128.3112335205078 test_loss:335.35113525390625\n",
      "279/3000 train_loss: 135.52471923828125 test_loss:342.134033203125\n",
      "280/3000 train_loss: 133.3551483154297 test_loss:326.8546447753906\n",
      "281/3000 train_loss: 121.8699722290039 test_loss:328.2898254394531\n",
      "282/3000 train_loss: 180.43861389160156 test_loss:357.20159912109375\n",
      "283/3000 train_loss: 149.55908203125 test_loss:354.5705261230469\n",
      "284/3000 train_loss: 135.98403930664062 test_loss:366.3138427734375\n",
      "285/3000 train_loss: 128.5269317626953 test_loss:350.5430603027344\n",
      "286/3000 train_loss: 110.47943878173828 test_loss:363.4424133300781\n",
      "287/3000 train_loss: 123.19332885742188 test_loss:367.9334411621094\n",
      "288/3000 train_loss: 163.52769470214844 test_loss:351.0307312011719\n",
      "289/3000 train_loss: 128.5391845703125 test_loss:342.3333435058594\n",
      "290/3000 train_loss: 113.92984771728516 test_loss:323.17462158203125\n",
      "291/3000 train_loss: 117.34506225585938 test_loss:330.8624267578125\n",
      "292/3000 train_loss: 121.46818542480469 test_loss:334.1582336425781\n",
      "293/3000 train_loss: 124.05487060546875 test_loss:313.58001708984375\n",
      "294/3000 train_loss: 137.6188507080078 test_loss:321.38348388671875\n",
      "295/3000 train_loss: 133.91134643554688 test_loss:329.9748840332031\n",
      "296/3000 train_loss: 113.97364044189453 test_loss:325.072998046875\n",
      "297/3000 train_loss: 120.13035583496094 test_loss:331.5176696777344\n",
      "298/3000 train_loss: 134.17466735839844 test_loss:337.8089904785156\n",
      "299/3000 train_loss: 122.98975372314453 test_loss:320.80096435546875\n",
      "300/3000 train_loss: 113.34700775146484 test_loss:311.4461364746094\n",
      "301/3000 train_loss: 129.39016723632812 test_loss:316.84552001953125\n",
      "302/3000 train_loss: 119.82469177246094 test_loss:320.6018371582031\n",
      "303/3000 train_loss: 116.42784881591797 test_loss:318.9280700683594\n",
      "304/3000 train_loss: 124.12523651123047 test_loss:312.8320617675781\n",
      "305/3000 train_loss: 115.95463562011719 test_loss:315.4957580566406\n",
      "306/3000 train_loss: 121.43467712402344 test_loss:312.6053771972656\n",
      "307/3000 train_loss: 113.73494720458984 test_loss:309.5889587402344\n",
      "308/3000 train_loss: 120.14779663085938 test_loss:317.6341552734375\n",
      "309/3000 train_loss: 151.05625915527344 test_loss:319.2758483886719\n",
      "310/3000 train_loss: 120.35400390625 test_loss:311.21722412109375\n",
      "311/3000 train_loss: 112.52494049072266 test_loss:320.759033203125\n",
      "312/3000 train_loss: 122.63404083251953 test_loss:326.4684143066406\n",
      "313/3000 train_loss: 118.70834350585938 test_loss:330.57476806640625\n",
      "314/3000 train_loss: 107.77824401855469 test_loss:317.3285217285156\n",
      "315/3000 train_loss: 116.2166748046875 test_loss:320.5632629394531\n",
      "316/3000 train_loss: 120.80599212646484 test_loss:310.888916015625\n",
      "317/3000 train_loss: 122.31510925292969 test_loss:302.56854248046875\n",
      "318/3000 train_loss: 112.22713470458984 test_loss:322.4569396972656\n",
      "319/3000 train_loss: 121.21501159667969 test_loss:326.02239990234375\n",
      "320/3000 train_loss: 122.11607360839844 test_loss:314.8310852050781\n",
      "321/3000 train_loss: 113.08236694335938 test_loss:320.7525329589844\n",
      "322/3000 train_loss: 104.53362274169922 test_loss:323.9140319824219\n",
      "323/3000 train_loss: 111.33384704589844 test_loss:317.3817443847656\n",
      "324/3000 train_loss: 116.46003723144531 test_loss:305.5899353027344\n",
      "325/3000 train_loss: 114.2572021484375 test_loss:304.24847412109375\n",
      "326/3000 train_loss: 105.94807434082031 test_loss:307.82513427734375\n",
      "327/3000 train_loss: 113.37178039550781 test_loss:316.27044677734375\n",
      "328/3000 train_loss: 116.85159301757812 test_loss:298.24432373046875\n",
      "329/3000 train_loss: 113.45831298828125 test_loss:312.263916015625\n",
      "330/3000 train_loss: 104.90372467041016 test_loss:304.03619384765625\n",
      "331/3000 train_loss: 113.05892181396484 test_loss:308.8070373535156\n",
      "332/3000 train_loss: 121.2371826171875 test_loss:304.7004089355469\n",
      "333/3000 train_loss: 135.17092895507812 test_loss:306.8053283691406\n",
      "334/3000 train_loss: 117.44969177246094 test_loss:315.3810119628906\n",
      "335/3000 train_loss: 109.13287353515625 test_loss:306.75372314453125\n",
      "336/3000 train_loss: 108.46487426757812 test_loss:300.54296875\n",
      "337/3000 train_loss: 106.16925811767578 test_loss:302.7128601074219\n",
      "338/3000 train_loss: 117.13221740722656 test_loss:315.87005615234375\n",
      "339/3000 train_loss: 101.05168914794922 test_loss:311.41900634765625\n",
      "340/3000 train_loss: 100.92572784423828 test_loss:314.2723388671875\n",
      "341/3000 train_loss: 100.14440155029297 test_loss:313.3295593261719\n",
      "342/3000 train_loss: 96.02474975585938 test_loss:303.5001220703125\n",
      "343/3000 train_loss: 94.96351623535156 test_loss:308.5814208984375\n",
      "344/3000 train_loss: 125.29814147949219 test_loss:304.6943664550781\n",
      "345/3000 train_loss: 104.499755859375 test_loss:314.5599060058594\n",
      "346/3000 train_loss: 113.84938049316406 test_loss:314.52386474609375\n",
      "347/3000 train_loss: 102.08496856689453 test_loss:305.6275634765625\n",
      "348/3000 train_loss: 112.59754180908203 test_loss:313.83355712890625\n",
      "349/3000 train_loss: 103.97607421875 test_loss:300.15753173828125\n",
      "350/3000 train_loss: 102.22651672363281 test_loss:300.83856201171875\n",
      "351/3000 train_loss: 94.05208587646484 test_loss:303.7408752441406\n",
      "352/3000 train_loss: 101.31535339355469 test_loss:304.4659118652344\n",
      "353/3000 train_loss: 95.0412368774414 test_loss:301.0882263183594\n",
      "354/3000 train_loss: 123.65290069580078 test_loss:303.71563720703125\n",
      "355/3000 train_loss: 96.04439544677734 test_loss:298.31903076171875\n",
      "356/3000 train_loss: 105.91983032226562 test_loss:302.37469482421875\n",
      "357/3000 train_loss: 138.76727294921875 test_loss:288.81298828125\n",
      "358/3000 train_loss: 134.64901733398438 test_loss:294.3039855957031\n",
      "359/3000 train_loss: 107.68473052978516 test_loss:289.7154235839844\n",
      "360/3000 train_loss: 106.84107971191406 test_loss:295.52764892578125\n",
      "361/3000 train_loss: 94.50513458251953 test_loss:301.44525146484375\n",
      "362/3000 train_loss: 99.03697967529297 test_loss:296.8462829589844\n",
      "363/3000 train_loss: 94.27181243896484 test_loss:294.132568359375\n",
      "364/3000 train_loss: 104.03211212158203 test_loss:297.9082336425781\n",
      "365/3000 train_loss: 114.60684204101562 test_loss:309.3448791503906\n",
      "366/3000 train_loss: 111.3609848022461 test_loss:299.3780517578125\n",
      "367/3000 train_loss: 99.36932373046875 test_loss:285.8439636230469\n",
      "368/3000 train_loss: 98.20614624023438 test_loss:291.29986572265625\n",
      "369/3000 train_loss: 101.93787384033203 test_loss:290.78271484375\n",
      "370/3000 train_loss: 102.20365142822266 test_loss:291.90020751953125\n",
      "371/3000 train_loss: 93.94091033935547 test_loss:298.444580078125\n",
      "372/3000 train_loss: 96.4506607055664 test_loss:298.43145751953125\n",
      "373/3000 train_loss: 108.9208984375 test_loss:291.5072326660156\n",
      "374/3000 train_loss: 100.83338165283203 test_loss:291.6728515625\n",
      "375/3000 train_loss: 103.3823471069336 test_loss:291.5168762207031\n",
      "376/3000 train_loss: 94.71660614013672 test_loss:292.8713684082031\n",
      "377/3000 train_loss: 90.862548828125 test_loss:292.47357177734375\n",
      "378/3000 train_loss: 93.58009338378906 test_loss:298.3577575683594\n",
      "379/3000 train_loss: 100.46266174316406 test_loss:290.0135803222656\n",
      "380/3000 train_loss: 100.46250915527344 test_loss:294.5785217285156\n",
      "381/3000 train_loss: 107.42713928222656 test_loss:289.2076110839844\n",
      "382/3000 train_loss: 92.4581069946289 test_loss:295.2212829589844\n",
      "383/3000 train_loss: 98.5799789428711 test_loss:289.0090026855469\n",
      "384/3000 train_loss: 104.38153076171875 test_loss:288.1956787109375\n",
      "385/3000 train_loss: 115.36173248291016 test_loss:285.673828125\n",
      "386/3000 train_loss: 93.85472869873047 test_loss:284.8490295410156\n",
      "387/3000 train_loss: 94.0239028930664 test_loss:298.841064453125\n",
      "388/3000 train_loss: 98.65034484863281 test_loss:298.0579528808594\n",
      "389/3000 train_loss: 90.54109954833984 test_loss:290.5549621582031\n",
      "390/3000 train_loss: 108.39945983886719 test_loss:296.9402770996094\n",
      "391/3000 train_loss: 94.23542785644531 test_loss:292.3797607421875\n",
      "392/3000 train_loss: 96.77944946289062 test_loss:290.64813232421875\n",
      "393/3000 train_loss: 96.26957702636719 test_loss:285.140625\n",
      "394/3000 train_loss: 105.28813171386719 test_loss:285.234130859375\n",
      "395/3000 train_loss: 101.4343032836914 test_loss:284.80010986328125\n",
      "396/3000 train_loss: 98.04278564453125 test_loss:289.60400390625\n",
      "397/3000 train_loss: 101.91410064697266 test_loss:285.6077880859375\n",
      "398/3000 train_loss: 92.68875122070312 test_loss:285.5292053222656\n",
      "399/3000 train_loss: 87.06560516357422 test_loss:288.7047424316406\n",
      "400/3000 train_loss: 111.71825408935547 test_loss:293.5296936035156\n",
      "401/3000 train_loss: 96.9254379272461 test_loss:289.08416748046875\n",
      "402/3000 train_loss: 84.41383361816406 test_loss:282.6062927246094\n",
      "403/3000 train_loss: 89.2787094116211 test_loss:303.7756042480469\n",
      "404/3000 train_loss: 90.01568603515625 test_loss:282.94927978515625\n",
      "405/3000 train_loss: 102.05011749267578 test_loss:283.3390808105469\n",
      "406/3000 train_loss: 98.68231964111328 test_loss:284.8725280761719\n",
      "407/3000 train_loss: 96.87153625488281 test_loss:287.9260559082031\n",
      "408/3000 train_loss: 93.11883544921875 test_loss:290.86053466796875\n",
      "409/3000 train_loss: 92.7593994140625 test_loss:283.5323486328125\n",
      "410/3000 train_loss: 95.05941772460938 test_loss:284.6717529296875\n",
      "411/3000 train_loss: 106.11677551269531 test_loss:291.2440490722656\n",
      "412/3000 train_loss: 89.50914764404297 test_loss:284.1325378417969\n",
      "413/3000 train_loss: 92.66925811767578 test_loss:277.74658203125\n",
      "414/3000 train_loss: 86.82472229003906 test_loss:281.4627685546875\n",
      "415/3000 train_loss: 88.98127746582031 test_loss:281.4131164550781\n",
      "416/3000 train_loss: 96.57145690917969 test_loss:278.30108642578125\n",
      "417/3000 train_loss: 84.19883728027344 test_loss:277.0537109375\n",
      "418/3000 train_loss: 89.7086181640625 test_loss:281.4451904296875\n",
      "419/3000 train_loss: 96.72775268554688 test_loss:281.5962829589844\n",
      "420/3000 train_loss: 82.1513671875 test_loss:278.43133544921875\n",
      "421/3000 train_loss: 89.89334106445312 test_loss:280.8642578125\n",
      "422/3000 train_loss: 86.01371002197266 test_loss:278.6506042480469\n",
      "423/3000 train_loss: 91.48075866699219 test_loss:275.53985595703125\n",
      "424/3000 train_loss: 93.35397338867188 test_loss:274.19171142578125\n",
      "425/3000 train_loss: 82.26268768310547 test_loss:272.0370178222656\n",
      "426/3000 train_loss: 89.12014770507812 test_loss:275.0394287109375\n",
      "427/3000 train_loss: 88.90859985351562 test_loss:273.8305969238281\n",
      "428/3000 train_loss: 87.82013702392578 test_loss:280.6549072265625\n",
      "429/3000 train_loss: 87.45230865478516 test_loss:275.0686340332031\n",
      "430/3000 train_loss: 84.04557037353516 test_loss:284.32318115234375\n",
      "431/3000 train_loss: 87.4706039428711 test_loss:280.1408386230469\n",
      "432/3000 train_loss: 83.38431549072266 test_loss:278.4337158203125\n",
      "433/3000 train_loss: 86.31489562988281 test_loss:282.01611328125\n",
      "434/3000 train_loss: 81.47904968261719 test_loss:282.18896484375\n",
      "435/3000 train_loss: 94.80219268798828 test_loss:273.51495361328125\n",
      "436/3000 train_loss: 98.10820007324219 test_loss:295.6869201660156\n",
      "437/3000 train_loss: 96.48868560791016 test_loss:273.7336730957031\n",
      "438/3000 train_loss: 89.87645721435547 test_loss:280.6221618652344\n",
      "439/3000 train_loss: 77.2293472290039 test_loss:273.86370849609375\n",
      "440/3000 train_loss: 89.30485534667969 test_loss:279.1128234863281\n",
      "441/3000 train_loss: 81.82584381103516 test_loss:278.84088134765625\n",
      "442/3000 train_loss: 80.25990295410156 test_loss:271.85736083984375\n",
      "443/3000 train_loss: 90.78981018066406 test_loss:271.62554931640625\n",
      "444/3000 train_loss: 85.48853302001953 test_loss:278.453369140625\n",
      "445/3000 train_loss: 82.29637908935547 test_loss:275.1994323730469\n",
      "446/3000 train_loss: 94.20829010009766 test_loss:273.94940185546875\n",
      "447/3000 train_loss: 75.5118637084961 test_loss:269.18890380859375\n",
      "448/3000 train_loss: 84.95174407958984 test_loss:268.59130859375\n",
      "449/3000 train_loss: 95.31062316894531 test_loss:273.5875244140625\n",
      "450/3000 train_loss: 83.59017181396484 test_loss:267.4963684082031\n",
      "451/3000 train_loss: 78.58049011230469 test_loss:263.53802490234375\n",
      "452/3000 train_loss: 90.55597686767578 test_loss:269.62066650390625\n",
      "453/3000 train_loss: 82.0197525024414 test_loss:275.8185729980469\n",
      "454/3000 train_loss: 83.36125183105469 test_loss:273.1756286621094\n",
      "455/3000 train_loss: 87.62159729003906 test_loss:263.9342041015625\n",
      "456/3000 train_loss: 77.77615356445312 test_loss:267.70538330078125\n",
      "457/3000 train_loss: 78.33477783203125 test_loss:275.70013427734375\n",
      "458/3000 train_loss: 96.71359252929688 test_loss:264.2057189941406\n",
      "459/3000 train_loss: 84.61483001708984 test_loss:283.58343505859375\n",
      "460/3000 train_loss: 94.69522094726562 test_loss:283.7498474121094\n",
      "461/3000 train_loss: 92.23111724853516 test_loss:263.01287841796875\n",
      "462/3000 train_loss: 90.2966079711914 test_loss:290.70452880859375\n",
      "463/3000 train_loss: 82.64999389648438 test_loss:271.36962890625\n",
      "464/3000 train_loss: 77.72444152832031 test_loss:277.0488586425781\n",
      "465/3000 train_loss: 86.11697387695312 test_loss:272.7149658203125\n",
      "466/3000 train_loss: 89.1751480102539 test_loss:278.43408203125\n",
      "467/3000 train_loss: 77.19458770751953 test_loss:276.5374450683594\n",
      "468/3000 train_loss: 80.45417785644531 test_loss:266.6564636230469\n",
      "469/3000 train_loss: 70.66255187988281 test_loss:271.18927001953125\n",
      "470/3000 train_loss: 86.38362884521484 test_loss:268.83331298828125\n",
      "471/3000 train_loss: 74.25105285644531 test_loss:275.482666015625\n",
      "472/3000 train_loss: 79.53121948242188 test_loss:263.64569091796875\n",
      "473/3000 train_loss: 96.06195068359375 test_loss:264.5047607421875\n",
      "474/3000 train_loss: 88.53071594238281 test_loss:261.1316833496094\n",
      "475/3000 train_loss: 90.82755279541016 test_loss:262.87200927734375\n",
      "476/3000 train_loss: 79.09268951416016 test_loss:272.6602783203125\n",
      "477/3000 train_loss: 73.548095703125 test_loss:257.80419921875\n",
      "478/3000 train_loss: 76.92106628417969 test_loss:264.8608093261719\n",
      "479/3000 train_loss: 75.03600311279297 test_loss:267.6661071777344\n",
      "480/3000 train_loss: 92.61771392822266 test_loss:256.7755126953125\n",
      "481/3000 train_loss: 73.53069305419922 test_loss:261.1004943847656\n",
      "482/3000 train_loss: 83.6380844116211 test_loss:257.15594482421875\n",
      "483/3000 train_loss: 78.07107543945312 test_loss:264.4747314453125\n",
      "484/3000 train_loss: 77.65829467773438 test_loss:257.6907958984375\n",
      "485/3000 train_loss: 70.48076629638672 test_loss:260.9960021972656\n",
      "486/3000 train_loss: 68.79991149902344 test_loss:260.9284973144531\n",
      "487/3000 train_loss: 72.93387603759766 test_loss:266.7607421875\n",
      "488/3000 train_loss: 92.56380462646484 test_loss:268.4465637207031\n",
      "489/3000 train_loss: 73.9875717163086 test_loss:264.17010498046875\n",
      "490/3000 train_loss: 75.59141540527344 test_loss:274.3233337402344\n",
      "491/3000 train_loss: 70.61941528320312 test_loss:263.3162841796875\n",
      "492/3000 train_loss: 65.84732055664062 test_loss:263.8957824707031\n",
      "493/3000 train_loss: 68.48645782470703 test_loss:261.31756591796875\n",
      "494/3000 train_loss: 81.25153350830078 test_loss:253.15431213378906\n",
      "495/3000 train_loss: 80.66071319580078 test_loss:259.3033447265625\n",
      "496/3000 train_loss: 110.91709899902344 test_loss:284.5857238769531\n",
      "497/3000 train_loss: 78.16850280761719 test_loss:268.88494873046875\n",
      "498/3000 train_loss: 75.65435791015625 test_loss:263.250244140625\n",
      "499/3000 train_loss: 73.39915466308594 test_loss:262.83514404296875\n",
      "500/3000 train_loss: 83.92486572265625 test_loss:260.224609375\n",
      "501/3000 train_loss: 69.45947265625 test_loss:254.59616088867188\n",
      "502/3000 train_loss: 71.38172149658203 test_loss:262.37664794921875\n",
      "503/3000 train_loss: 72.92163848876953 test_loss:256.45391845703125\n",
      "504/3000 train_loss: 65.57433319091797 test_loss:260.55108642578125\n",
      "505/3000 train_loss: 66.96049499511719 test_loss:259.961181640625\n",
      "506/3000 train_loss: 77.14080047607422 test_loss:251.98846435546875\n",
      "507/3000 train_loss: 69.11390686035156 test_loss:258.548583984375\n",
      "508/3000 train_loss: 78.72322082519531 test_loss:255.47537231445312\n",
      "509/3000 train_loss: 69.37481689453125 test_loss:256.7244567871094\n",
      "510/3000 train_loss: 75.87480163574219 test_loss:265.96282958984375\n",
      "511/3000 train_loss: 270.79736328125 test_loss:300.2628173828125\n",
      "512/3000 train_loss: 193.77284240722656 test_loss:309.7794494628906\n",
      "513/3000 train_loss: 94.26542663574219 test_loss:290.46826171875\n",
      "514/3000 train_loss: 82.0709228515625 test_loss:283.5404357910156\n",
      "515/3000 train_loss: 92.78640747070312 test_loss:274.6714782714844\n",
      "516/3000 train_loss: 71.30001831054688 test_loss:266.6531982421875\n",
      "517/3000 train_loss: 89.0448989868164 test_loss:272.2369689941406\n",
      "518/3000 train_loss: 78.59707641601562 test_loss:261.89990234375\n",
      "519/3000 train_loss: 83.37462615966797 test_loss:259.1439514160156\n",
      "520/3000 train_loss: 74.27083587646484 test_loss:250.99026489257812\n",
      "521/3000 train_loss: 64.53748321533203 test_loss:261.04461669921875\n",
      "522/3000 train_loss: 75.98577117919922 test_loss:256.0841064453125\n",
      "523/3000 train_loss: 68.15706634521484 test_loss:255.30824279785156\n",
      "524/3000 train_loss: 71.6314697265625 test_loss:256.33544921875\n",
      "525/3000 train_loss: 74.99453735351562 test_loss:257.99017333984375\n",
      "526/3000 train_loss: 64.38887023925781 test_loss:257.84814453125\n",
      "527/3000 train_loss: 73.81522369384766 test_loss:262.1473693847656\n",
      "528/3000 train_loss: 69.59512329101562 test_loss:262.6745300292969\n",
      "529/3000 train_loss: 70.05915069580078 test_loss:255.74313354492188\n",
      "530/3000 train_loss: 69.9720687866211 test_loss:251.4139404296875\n",
      "531/3000 train_loss: 69.13777160644531 test_loss:253.9558563232422\n",
      "532/3000 train_loss: 77.15330505371094 test_loss:269.5115661621094\n",
      "533/3000 train_loss: 73.63065338134766 test_loss:260.4502258300781\n",
      "534/3000 train_loss: 84.44383239746094 test_loss:250.43927001953125\n",
      "535/3000 train_loss: 68.24532318115234 test_loss:254.94505310058594\n",
      "536/3000 train_loss: 67.311279296875 test_loss:256.19232177734375\n",
      "537/3000 train_loss: 57.60266876220703 test_loss:257.2684631347656\n",
      "538/3000 train_loss: 73.20989990234375 test_loss:252.15823364257812\n",
      "539/3000 train_loss: 61.331748962402344 test_loss:258.54571533203125\n",
      "540/3000 train_loss: 65.75174713134766 test_loss:259.2652587890625\n",
      "541/3000 train_loss: 67.2485122680664 test_loss:256.9780578613281\n",
      "542/3000 train_loss: 66.70648956298828 test_loss:257.7062683105469\n",
      "543/3000 train_loss: 60.653038024902344 test_loss:254.66921997070312\n",
      "544/3000 train_loss: 71.55342102050781 test_loss:264.2840270996094\n",
      "545/3000 train_loss: 71.112548828125 test_loss:257.1207275390625\n",
      "546/3000 train_loss: 66.8269271850586 test_loss:254.16238403320312\n",
      "547/3000 train_loss: 58.98058319091797 test_loss:253.2845916748047\n",
      "548/3000 train_loss: 63.80622100830078 test_loss:254.190673828125\n",
      "549/3000 train_loss: 59.48244094848633 test_loss:257.9116516113281\n",
      "550/3000 train_loss: 60.729637145996094 test_loss:253.2056884765625\n",
      "551/3000 train_loss: 63.81031799316406 test_loss:259.51812744140625\n",
      "552/3000 train_loss: 64.25906372070312 test_loss:257.06195068359375\n",
      "553/3000 train_loss: 66.27438354492188 test_loss:265.9474182128906\n",
      "554/3000 train_loss: 72.95471954345703 test_loss:255.22792053222656\n",
      "555/3000 train_loss: 73.26280975341797 test_loss:259.4577331542969\n",
      "556/3000 train_loss: 61.55977249145508 test_loss:254.30145263671875\n",
      "557/3000 train_loss: 87.01261901855469 test_loss:249.8056640625\n",
      "558/3000 train_loss: 64.00492858886719 test_loss:249.93283081054688\n",
      "559/3000 train_loss: 73.83771514892578 test_loss:254.84371948242188\n",
      "560/3000 train_loss: 67.11138153076172 test_loss:259.71209716796875\n",
      "561/3000 train_loss: 61.16170883178711 test_loss:249.83116149902344\n",
      "562/3000 train_loss: 58.91425323486328 test_loss:253.44351196289062\n",
      "563/3000 train_loss: 62.84031295776367 test_loss:254.21238708496094\n",
      "564/3000 train_loss: 62.045860290527344 test_loss:253.394775390625\n",
      "565/3000 train_loss: 67.45173645019531 test_loss:252.85494995117188\n",
      "566/3000 train_loss: 60.775447845458984 test_loss:246.44956970214844\n",
      "567/3000 train_loss: 75.34578704833984 test_loss:245.46157836914062\n",
      "568/3000 train_loss: 63.824806213378906 test_loss:244.5897674560547\n",
      "569/3000 train_loss: 58.00959014892578 test_loss:254.78012084960938\n",
      "570/3000 train_loss: 63.28598403930664 test_loss:251.156005859375\n",
      "571/3000 train_loss: 62.9534797668457 test_loss:244.62429809570312\n",
      "572/3000 train_loss: 60.81468963623047 test_loss:247.28012084960938\n",
      "573/3000 train_loss: 59.808719635009766 test_loss:253.77883911132812\n",
      "574/3000 train_loss: 56.02418518066406 test_loss:252.5506591796875\n",
      "575/3000 train_loss: 64.19062042236328 test_loss:257.9849853515625\n",
      "576/3000 train_loss: 59.89479064941406 test_loss:248.64315795898438\n",
      "577/3000 train_loss: 58.07386779785156 test_loss:247.90435791015625\n",
      "578/3000 train_loss: 57.93589782714844 test_loss:251.47088623046875\n",
      "579/3000 train_loss: 63.00250244140625 test_loss:245.42620849609375\n",
      "580/3000 train_loss: 54.69776153564453 test_loss:255.08999633789062\n",
      "581/3000 train_loss: 60.44001388549805 test_loss:255.03237915039062\n",
      "582/3000 train_loss: 62.78968811035156 test_loss:254.7008056640625\n",
      "583/3000 train_loss: 63.7474250793457 test_loss:247.6444091796875\n",
      "584/3000 train_loss: 63.21904754638672 test_loss:258.3206481933594\n",
      "585/3000 train_loss: 74.05279541015625 test_loss:253.20391845703125\n",
      "586/3000 train_loss: 58.147796630859375 test_loss:243.81002807617188\n",
      "587/3000 train_loss: 67.5501480102539 test_loss:260.5689697265625\n",
      "588/3000 train_loss: 63.016963958740234 test_loss:246.2790985107422\n",
      "589/3000 train_loss: 64.62543487548828 test_loss:260.6757507324219\n",
      "590/3000 train_loss: 70.66748809814453 test_loss:249.07330322265625\n",
      "591/3000 train_loss: 80.53706359863281 test_loss:249.52603149414062\n",
      "592/3000 train_loss: 53.37605285644531 test_loss:249.11917114257812\n",
      "593/3000 train_loss: 69.82099914550781 test_loss:260.7409973144531\n",
      "594/3000 train_loss: 60.78626251220703 test_loss:250.10791015625\n",
      "595/3000 train_loss: 59.02824783325195 test_loss:261.5057373046875\n",
      "596/3000 train_loss: 58.80262756347656 test_loss:251.5382080078125\n",
      "597/3000 train_loss: 57.92556381225586 test_loss:251.28025817871094\n",
      "598/3000 train_loss: 65.6248779296875 test_loss:252.54705810546875\n",
      "599/3000 train_loss: 61.81474685668945 test_loss:255.39041137695312\n",
      "600/3000 train_loss: 61.60155487060547 test_loss:252.75782775878906\n",
      "601/3000 train_loss: 60.50953674316406 test_loss:246.49774169921875\n",
      "602/3000 train_loss: 54.220542907714844 test_loss:245.90655517578125\n",
      "603/3000 train_loss: 65.42989349365234 test_loss:245.8975830078125\n",
      "604/3000 train_loss: 85.08399200439453 test_loss:247.89566040039062\n",
      "605/3000 train_loss: 74.35670471191406 test_loss:259.28631591796875\n",
      "606/3000 train_loss: 68.62711334228516 test_loss:241.54849243164062\n",
      "607/3000 train_loss: 64.73880767822266 test_loss:252.22817993164062\n",
      "608/3000 train_loss: 69.9859390258789 test_loss:248.3514404296875\n",
      "609/3000 train_loss: 69.41465759277344 test_loss:257.8793029785156\n",
      "610/3000 train_loss: 72.02950286865234 test_loss:249.4144287109375\n",
      "611/3000 train_loss: 65.82267761230469 test_loss:253.32443237304688\n",
      "612/3000 train_loss: 55.90199279785156 test_loss:252.66246032714844\n",
      "613/3000 train_loss: 60.56333923339844 test_loss:245.3341064453125\n",
      "614/3000 train_loss: 56.99857711791992 test_loss:249.41470336914062\n",
      "615/3000 train_loss: 56.562835693359375 test_loss:253.36904907226562\n",
      "616/3000 train_loss: 71.40562438964844 test_loss:249.18402099609375\n",
      "617/3000 train_loss: 66.26103973388672 test_loss:246.13861083984375\n",
      "618/3000 train_loss: 61.68932342529297 test_loss:250.17294311523438\n",
      "619/3000 train_loss: 55.340030670166016 test_loss:246.47422790527344\n",
      "620/3000 train_loss: 56.998077392578125 test_loss:251.382568359375\n",
      "621/3000 train_loss: 55.73780822753906 test_loss:249.5322265625\n",
      "622/3000 train_loss: 65.89686584472656 test_loss:244.1967315673828\n",
      "623/3000 train_loss: 65.48383331298828 test_loss:243.1410675048828\n",
      "624/3000 train_loss: 56.976749420166016 test_loss:252.90997314453125\n",
      "625/3000 train_loss: 57.775421142578125 test_loss:246.55419921875\n",
      "626/3000 train_loss: 58.981040954589844 test_loss:241.5576171875\n",
      "627/3000 train_loss: 54.55611038208008 test_loss:244.24465942382812\n",
      "628/3000 train_loss: 57.022308349609375 test_loss:249.02207946777344\n",
      "629/3000 train_loss: 56.594791412353516 test_loss:254.70228576660156\n",
      "630/3000 train_loss: 63.9034538269043 test_loss:244.2271728515625\n",
      "631/3000 train_loss: 65.16853332519531 test_loss:240.08389282226562\n",
      "632/3000 train_loss: 58.48003387451172 test_loss:249.5689697265625\n",
      "633/3000 train_loss: 87.98323822021484 test_loss:241.5202178955078\n",
      "634/3000 train_loss: 54.60125732421875 test_loss:250.83810424804688\n",
      "635/3000 train_loss: 55.05623245239258 test_loss:252.4844970703125\n",
      "636/3000 train_loss: 58.88188552856445 test_loss:247.75613403320312\n",
      "637/3000 train_loss: 54.45677185058594 test_loss:252.78260803222656\n",
      "638/3000 train_loss: 62.625396728515625 test_loss:251.50637817382812\n",
      "639/3000 train_loss: 61.95722198486328 test_loss:252.39710998535156\n",
      "640/3000 train_loss: 55.271419525146484 test_loss:245.79324340820312\n",
      "641/3000 train_loss: 53.81669998168945 test_loss:251.62313842773438\n",
      "642/3000 train_loss: 57.64832305908203 test_loss:246.5016632080078\n",
      "643/3000 train_loss: 63.966278076171875 test_loss:253.22463989257812\n",
      "644/3000 train_loss: 70.93071746826172 test_loss:245.87596130371094\n",
      "645/3000 train_loss: 63.5918083190918 test_loss:244.66912841796875\n",
      "646/3000 train_loss: 57.74919128417969 test_loss:256.3098449707031\n",
      "647/3000 train_loss: 58.141151428222656 test_loss:239.8936004638672\n",
      "648/3000 train_loss: 57.01976013183594 test_loss:245.60594177246094\n",
      "649/3000 train_loss: 60.566551208496094 test_loss:246.27476501464844\n",
      "650/3000 train_loss: 61.47145462036133 test_loss:245.8849639892578\n",
      "651/3000 train_loss: 58.34354782104492 test_loss:251.31796264648438\n",
      "652/3000 train_loss: 52.10738754272461 test_loss:247.458740234375\n",
      "653/3000 train_loss: 56.03846740722656 test_loss:245.31735229492188\n",
      "654/3000 train_loss: 58.70621109008789 test_loss:247.6105194091797\n",
      "655/3000 train_loss: 51.05222702026367 test_loss:248.0006103515625\n",
      "656/3000 train_loss: 57.316749572753906 test_loss:244.4815673828125\n",
      "657/3000 train_loss: 64.92420196533203 test_loss:243.33990478515625\n",
      "658/3000 train_loss: 59.09359359741211 test_loss:236.21380615234375\n",
      "659/3000 train_loss: 56.232933044433594 test_loss:246.9319610595703\n",
      "660/3000 train_loss: 71.63818359375 test_loss:253.186767578125\n",
      "661/3000 train_loss: 57.65132141113281 test_loss:243.00143432617188\n",
      "662/3000 train_loss: 61.31136703491211 test_loss:244.3422393798828\n",
      "663/3000 train_loss: 56.7378044128418 test_loss:239.65591430664062\n",
      "664/3000 train_loss: 71.06509399414062 test_loss:243.2677459716797\n",
      "665/3000 train_loss: 53.34830856323242 test_loss:242.70974731445312\n",
      "666/3000 train_loss: 57.274261474609375 test_loss:248.35438537597656\n",
      "667/3000 train_loss: 49.58158874511719 test_loss:248.95960998535156\n",
      "668/3000 train_loss: 70.49231719970703 test_loss:239.9161376953125\n",
      "669/3000 train_loss: 58.61417007446289 test_loss:249.302001953125\n",
      "670/3000 train_loss: 54.14167404174805 test_loss:247.48643493652344\n",
      "671/3000 train_loss: 68.54507446289062 test_loss:236.8818359375\n",
      "672/3000 train_loss: 58.334999084472656 test_loss:259.0105285644531\n",
      "673/3000 train_loss: 67.63753509521484 test_loss:238.46018981933594\n",
      "674/3000 train_loss: 53.089515686035156 test_loss:252.86463928222656\n",
      "675/3000 train_loss: 62.19439697265625 test_loss:242.04286193847656\n",
      "676/3000 train_loss: 56.034942626953125 test_loss:244.86546325683594\n",
      "677/3000 train_loss: 52.5324592590332 test_loss:243.8729248046875\n",
      "678/3000 train_loss: 57.3007926940918 test_loss:254.26893615722656\n",
      "679/3000 train_loss: 60.46065139770508 test_loss:243.84646606445312\n",
      "680/3000 train_loss: 58.126495361328125 test_loss:241.55104064941406\n",
      "681/3000 train_loss: 54.89694595336914 test_loss:246.32386779785156\n",
      "682/3000 train_loss: 56.59184265136719 test_loss:239.90701293945312\n",
      "683/3000 train_loss: 52.94573211669922 test_loss:238.8586883544922\n",
      "684/3000 train_loss: 55.95675277709961 test_loss:244.6851348876953\n",
      "685/3000 train_loss: 55.14884567260742 test_loss:244.82974243164062\n",
      "686/3000 train_loss: 61.91942596435547 test_loss:253.5775604248047\n",
      "687/3000 train_loss: 53.259124755859375 test_loss:247.23104858398438\n",
      "688/3000 train_loss: 51.77097702026367 test_loss:245.43768310546875\n",
      "689/3000 train_loss: 55.27534484863281 test_loss:243.3923797607422\n",
      "690/3000 train_loss: 65.51686096191406 test_loss:244.23463439941406\n",
      "691/3000 train_loss: 58.44464111328125 test_loss:255.69137573242188\n",
      "692/3000 train_loss: 50.91363525390625 test_loss:239.7124481201172\n",
      "693/3000 train_loss: 61.02805709838867 test_loss:247.9868927001953\n",
      "694/3000 train_loss: 52.54099655151367 test_loss:246.77328491210938\n",
      "695/3000 train_loss: 50.42392349243164 test_loss:241.48329162597656\n",
      "696/3000 train_loss: 57.346832275390625 test_loss:241.14842224121094\n",
      "697/3000 train_loss: 50.11283493041992 test_loss:246.81614685058594\n",
      "698/3000 train_loss: 58.35297393798828 test_loss:240.1869354248047\n",
      "699/3000 train_loss: 51.5730094909668 test_loss:250.2943115234375\n",
      "700/3000 train_loss: 53.76421356201172 test_loss:240.632568359375\n",
      "701/3000 train_loss: 68.3345718383789 test_loss:252.92315673828125\n",
      "702/3000 train_loss: 56.278743743896484 test_loss:243.2130584716797\n",
      "703/3000 train_loss: 59.26063537597656 test_loss:247.28854370117188\n",
      "704/3000 train_loss: 55.3182373046875 test_loss:249.89364624023438\n",
      "705/3000 train_loss: 58.34437942504883 test_loss:244.60809326171875\n",
      "706/3000 train_loss: 57.00354766845703 test_loss:242.7620849609375\n",
      "707/3000 train_loss: 54.634437561035156 test_loss:244.54486083984375\n",
      "708/3000 train_loss: 49.59560775756836 test_loss:247.54466247558594\n",
      "709/3000 train_loss: 56.20782470703125 test_loss:243.04592895507812\n",
      "710/3000 train_loss: 57.52012252807617 test_loss:241.53366088867188\n",
      "711/3000 train_loss: 61.7033576965332 test_loss:243.3546905517578\n",
      "712/3000 train_loss: 56.57054901123047 test_loss:243.98243713378906\n",
      "713/3000 train_loss: 51.66842269897461 test_loss:241.33445739746094\n",
      "714/3000 train_loss: 56.017799377441406 test_loss:242.26760864257812\n",
      "715/3000 train_loss: 49.02824020385742 test_loss:241.9169158935547\n",
      "716/3000 train_loss: 52.791194915771484 test_loss:241.48330688476562\n",
      "717/3000 train_loss: 52.86396789550781 test_loss:237.85848999023438\n",
      "718/3000 train_loss: 56.29674530029297 test_loss:243.75823974609375\n",
      "719/3000 train_loss: 54.87679672241211 test_loss:240.1259002685547\n",
      "720/3000 train_loss: 49.200965881347656 test_loss:247.14932250976562\n",
      "721/3000 train_loss: 59.11827850341797 test_loss:232.84353637695312\n",
      "722/3000 train_loss: 50.86302185058594 test_loss:245.1271514892578\n",
      "723/3000 train_loss: 63.48728942871094 test_loss:242.22756958007812\n",
      "724/3000 train_loss: 57.342926025390625 test_loss:236.84194946289062\n",
      "725/3000 train_loss: 49.34239959716797 test_loss:248.4816131591797\n",
      "726/3000 train_loss: 60.53062438964844 test_loss:241.21591186523438\n",
      "727/3000 train_loss: 54.75425720214844 test_loss:242.38925170898438\n",
      "728/3000 train_loss: 55.34996795654297 test_loss:246.94210815429688\n",
      "729/3000 train_loss: 56.74203872680664 test_loss:240.7103271484375\n",
      "730/3000 train_loss: 53.941200256347656 test_loss:241.79983520507812\n",
      "731/3000 train_loss: 50.36470413208008 test_loss:239.60025024414062\n",
      "732/3000 train_loss: 61.672760009765625 test_loss:237.97467041015625\n",
      "733/3000 train_loss: 52.504638671875 test_loss:247.90542602539062\n",
      "734/3000 train_loss: 51.061588287353516 test_loss:244.25003051757812\n",
      "735/3000 train_loss: 52.109867095947266 test_loss:243.09771728515625\n",
      "736/3000 train_loss: 49.66708755493164 test_loss:246.11888122558594\n",
      "737/3000 train_loss: 58.078826904296875 test_loss:238.95675659179688\n",
      "738/3000 train_loss: 59.3348503112793 test_loss:245.38836669921875\n",
      "739/3000 train_loss: 56.09175109863281 test_loss:247.21739196777344\n",
      "740/3000 train_loss: 54.299991607666016 test_loss:244.3887939453125\n",
      "741/3000 train_loss: 52.11232376098633 test_loss:243.34130859375\n",
      "742/3000 train_loss: 52.29212951660156 test_loss:239.8031005859375\n",
      "743/3000 train_loss: 49.73984909057617 test_loss:251.8335723876953\n",
      "744/3000 train_loss: 51.362552642822266 test_loss:239.78292846679688\n",
      "745/3000 train_loss: 48.08076095581055 test_loss:243.8560791015625\n",
      "746/3000 train_loss: 51.551082611083984 test_loss:243.60598754882812\n",
      "747/3000 train_loss: 53.32547378540039 test_loss:241.58480834960938\n",
      "748/3000 train_loss: 51.903194427490234 test_loss:246.79348754882812\n",
      "749/3000 train_loss: 50.64596939086914 test_loss:244.359375\n",
      "750/3000 train_loss: 55.19679260253906 test_loss:239.93096923828125\n",
      "751/3000 train_loss: 60.532630920410156 test_loss:236.58502197265625\n",
      "752/3000 train_loss: 52.244441986083984 test_loss:240.70745849609375\n",
      "753/3000 train_loss: 59.06036376953125 test_loss:236.08810424804688\n",
      "754/3000 train_loss: 53.94484329223633 test_loss:235.68499755859375\n",
      "755/3000 train_loss: 50.948974609375 test_loss:235.44802856445312\n",
      "756/3000 train_loss: 52.031227111816406 test_loss:239.47947692871094\n",
      "757/3000 train_loss: 52.5018310546875 test_loss:228.85073852539062\n",
      "758/3000 train_loss: 49.489131927490234 test_loss:234.83253479003906\n",
      "759/3000 train_loss: 51.27790832519531 test_loss:228.34518432617188\n",
      "760/3000 train_loss: 49.37919235229492 test_loss:235.8868865966797\n",
      "761/3000 train_loss: 52.76402282714844 test_loss:230.64529418945312\n",
      "762/3000 train_loss: 53.948463439941406 test_loss:229.23829650878906\n",
      "763/3000 train_loss: 51.22594451904297 test_loss:226.03323364257812\n",
      "764/3000 train_loss: 54.525901794433594 test_loss:232.9394073486328\n",
      "765/3000 train_loss: 49.50468826293945 test_loss:235.21405029296875\n",
      "766/3000 train_loss: 52.533416748046875 test_loss:227.81265258789062\n",
      "767/3000 train_loss: 55.17738723754883 test_loss:226.7223663330078\n",
      "768/3000 train_loss: 48.71299743652344 test_loss:234.91622924804688\n",
      "769/3000 train_loss: 57.300697326660156 test_loss:227.39654541015625\n",
      "770/3000 train_loss: 53.17275619506836 test_loss:232.50387573242188\n",
      "771/3000 train_loss: 61.57757568359375 test_loss:239.720703125\n",
      "772/3000 train_loss: 60.270912170410156 test_loss:234.69805908203125\n",
      "773/3000 train_loss: 49.391387939453125 test_loss:242.21685791015625\n",
      "774/3000 train_loss: 52.69910430908203 test_loss:236.9578857421875\n",
      "775/3000 train_loss: 48.34370040893555 test_loss:241.76116943359375\n",
      "776/3000 train_loss: 51.344635009765625 test_loss:229.14419555664062\n",
      "777/3000 train_loss: 58.188228607177734 test_loss:238.2372589111328\n",
      "778/3000 train_loss: 47.92610549926758 test_loss:240.3608856201172\n",
      "779/3000 train_loss: 47.553810119628906 test_loss:241.99114990234375\n",
      "780/3000 train_loss: 50.10188293457031 test_loss:241.96255493164062\n",
      "781/3000 train_loss: 50.93702697753906 test_loss:232.0335693359375\n",
      "782/3000 train_loss: 51.710838317871094 test_loss:241.81549072265625\n",
      "783/3000 train_loss: 51.10198974609375 test_loss:240.83984375\n",
      "784/3000 train_loss: 43.53525161743164 test_loss:244.14013671875\n",
      "785/3000 train_loss: 54.49066162109375 test_loss:239.77505493164062\n",
      "786/3000 train_loss: 55.924560546875 test_loss:245.25567626953125\n",
      "787/3000 train_loss: 60.56123733520508 test_loss:236.20045471191406\n",
      "788/3000 train_loss: 48.38727951049805 test_loss:242.53546142578125\n",
      "789/3000 train_loss: 56.14738082885742 test_loss:234.58966064453125\n",
      "790/3000 train_loss: 60.849632263183594 test_loss:243.24380493164062\n",
      "791/3000 train_loss: 49.56248092651367 test_loss:236.82086181640625\n",
      "792/3000 train_loss: 46.42044448852539 test_loss:233.98690795898438\n",
      "793/3000 train_loss: 48.15082550048828 test_loss:237.1736297607422\n",
      "794/3000 train_loss: 48.738525390625 test_loss:233.29052734375\n",
      "795/3000 train_loss: 48.54816818237305 test_loss:235.80630493164062\n",
      "796/3000 train_loss: 52.280033111572266 test_loss:244.0607147216797\n",
      "797/3000 train_loss: 55.69430923461914 test_loss:237.5168914794922\n",
      "798/3000 train_loss: 54.337921142578125 test_loss:237.0808868408203\n",
      "799/3000 train_loss: 46.05489730834961 test_loss:238.9457550048828\n",
      "800/3000 train_loss: 47.41886520385742 test_loss:233.94581604003906\n",
      "801/3000 train_loss: 48.928340911865234 test_loss:234.30783081054688\n",
      "802/3000 train_loss: 50.857933044433594 test_loss:245.65162658691406\n",
      "803/3000 train_loss: 54.047607421875 test_loss:231.64996337890625\n",
      "804/3000 train_loss: 50.744842529296875 test_loss:238.1714630126953\n",
      "805/3000 train_loss: 53.36351013183594 test_loss:232.0250244140625\n",
      "806/3000 train_loss: 50.06610107421875 test_loss:235.49668884277344\n",
      "807/3000 train_loss: 58.088157653808594 test_loss:228.12750244140625\n",
      "808/3000 train_loss: 52.56399154663086 test_loss:228.4259796142578\n",
      "809/3000 train_loss: 53.856781005859375 test_loss:235.58314514160156\n",
      "810/3000 train_loss: 50.09651184082031 test_loss:225.0789337158203\n",
      "811/3000 train_loss: 56.480281829833984 test_loss:230.0029296875\n",
      "812/3000 train_loss: 66.30223846435547 test_loss:233.56434631347656\n",
      "813/3000 train_loss: 50.825103759765625 test_loss:231.33807373046875\n",
      "814/3000 train_loss: 46.21305465698242 test_loss:231.16188049316406\n",
      "815/3000 train_loss: 49.08168029785156 test_loss:234.10177612304688\n",
      "816/3000 train_loss: 55.803504943847656 test_loss:233.9451446533203\n",
      "817/3000 train_loss: 45.68238830566406 test_loss:233.41224670410156\n",
      "818/3000 train_loss: 56.40970993041992 test_loss:229.60293579101562\n",
      "819/3000 train_loss: 52.94150924682617 test_loss:228.79202270507812\n",
      "820/3000 train_loss: 54.9886474609375 test_loss:227.61398315429688\n",
      "821/3000 train_loss: 44.66636276245117 test_loss:236.79452514648438\n",
      "822/3000 train_loss: 49.91923522949219 test_loss:226.3308868408203\n",
      "823/3000 train_loss: 51.455528259277344 test_loss:241.61618041992188\n",
      "824/3000 train_loss: 43.87434387207031 test_loss:227.126953125\n",
      "825/3000 train_loss: 56.149288177490234 test_loss:228.90151977539062\n",
      "826/3000 train_loss: 48.50114059448242 test_loss:238.99215698242188\n",
      "827/3000 train_loss: 71.93829345703125 test_loss:225.55899047851562\n",
      "828/3000 train_loss: 48.32621765136719 test_loss:226.11978149414062\n",
      "829/3000 train_loss: 45.52497100830078 test_loss:230.97669982910156\n",
      "830/3000 train_loss: 43.925193786621094 test_loss:225.06227111816406\n",
      "831/3000 train_loss: 49.59688186645508 test_loss:225.70468139648438\n",
      "832/3000 train_loss: 55.59711456298828 test_loss:239.29104614257812\n",
      "833/3000 train_loss: 58.768211364746094 test_loss:224.5401611328125\n",
      "834/3000 train_loss: 45.098899841308594 test_loss:238.88409423828125\n",
      "835/3000 train_loss: 47.96345901489258 test_loss:226.18563842773438\n",
      "836/3000 train_loss: 47.01504898071289 test_loss:223.73318481445312\n",
      "837/3000 train_loss: 45.725162506103516 test_loss:227.51007080078125\n",
      "838/3000 train_loss: 50.670841217041016 test_loss:226.03143310546875\n",
      "839/3000 train_loss: 46.56483840942383 test_loss:238.63037109375\n",
      "840/3000 train_loss: 47.13236999511719 test_loss:228.29705810546875\n",
      "841/3000 train_loss: 43.41236114501953 test_loss:231.79664611816406\n",
      "842/3000 train_loss: 44.23408508300781 test_loss:227.92144775390625\n",
      "843/3000 train_loss: 48.43212127685547 test_loss:228.1257781982422\n",
      "844/3000 train_loss: 44.0919303894043 test_loss:229.18606567382812\n",
      "845/3000 train_loss: 48.511199951171875 test_loss:225.002685546875\n",
      "846/3000 train_loss: 45.02484893798828 test_loss:228.27618408203125\n",
      "847/3000 train_loss: 45.897308349609375 test_loss:224.1288604736328\n",
      "848/3000 train_loss: 57.21646499633789 test_loss:231.16073608398438\n",
      "849/3000 train_loss: 61.45326614379883 test_loss:229.53448486328125\n",
      "850/3000 train_loss: 183.28497314453125 test_loss:325.86712646484375\n",
      "851/3000 train_loss: 106.53244018554688 test_loss:252.61932373046875\n",
      "852/3000 train_loss: 70.31388854980469 test_loss:236.77658081054688\n",
      "853/3000 train_loss: 50.26642990112305 test_loss:232.38327026367188\n",
      "854/3000 train_loss: 49.77836227416992 test_loss:230.17794799804688\n",
      "855/3000 train_loss: 47.33026123046875 test_loss:238.54202270507812\n",
      "856/3000 train_loss: 51.6705436706543 test_loss:235.02828979492188\n",
      "857/3000 train_loss: 55.53445053100586 test_loss:244.644775390625\n",
      "858/3000 train_loss: 55.69136428833008 test_loss:242.99249267578125\n",
      "859/3000 train_loss: 50.247230529785156 test_loss:246.99595642089844\n",
      "860/3000 train_loss: 47.979434967041016 test_loss:242.83572387695312\n",
      "861/3000 train_loss: 50.45354461669922 test_loss:237.12246704101562\n",
      "862/3000 train_loss: 47.068634033203125 test_loss:239.6106414794922\n",
      "863/3000 train_loss: 49.57729721069336 test_loss:242.2665557861328\n",
      "864/3000 train_loss: 48.81293869018555 test_loss:244.650146484375\n",
      "865/3000 train_loss: 44.56603240966797 test_loss:241.70294189453125\n",
      "866/3000 train_loss: 52.39385986328125 test_loss:228.72549438476562\n",
      "867/3000 train_loss: 50.387020111083984 test_loss:230.2585906982422\n",
      "868/3000 train_loss: 47.38470458984375 test_loss:233.08270263671875\n",
      "869/3000 train_loss: 42.17638397216797 test_loss:233.44094848632812\n",
      "870/3000 train_loss: 48.56169128417969 test_loss:231.32125854492188\n",
      "871/3000 train_loss: 47.535579681396484 test_loss:242.21395874023438\n",
      "872/3000 train_loss: 50.490543365478516 test_loss:222.37771606445312\n",
      "873/3000 train_loss: 53.10121154785156 test_loss:242.03079223632812\n",
      "874/3000 train_loss: 45.572059631347656 test_loss:226.07705688476562\n",
      "875/3000 train_loss: 50.67103958129883 test_loss:225.73597717285156\n",
      "876/3000 train_loss: 47.4667854309082 test_loss:226.4777069091797\n",
      "877/3000 train_loss: 51.790443420410156 test_loss:235.6275634765625\n",
      "878/3000 train_loss: 53.408531188964844 test_loss:223.86453247070312\n",
      "879/3000 train_loss: 44.23527908325195 test_loss:225.30752563476562\n",
      "880/3000 train_loss: 52.08145523071289 test_loss:225.44851684570312\n",
      "881/3000 train_loss: 47.30638885498047 test_loss:220.35488891601562\n",
      "882/3000 train_loss: 50.402427673339844 test_loss:226.13160705566406\n",
      "883/3000 train_loss: 49.76012420654297 test_loss:226.1194305419922\n",
      "884/3000 train_loss: 52.64282989501953 test_loss:222.11447143554688\n",
      "885/3000 train_loss: 41.800899505615234 test_loss:227.98663330078125\n",
      "886/3000 train_loss: 52.06834030151367 test_loss:228.0261993408203\n",
      "887/3000 train_loss: 53.0627326965332 test_loss:227.62203979492188\n",
      "888/3000 train_loss: 45.25257110595703 test_loss:228.3924560546875\n",
      "889/3000 train_loss: 44.220035552978516 test_loss:224.95367431640625\n",
      "890/3000 train_loss: 48.61933135986328 test_loss:228.31362915039062\n",
      "891/3000 train_loss: 41.090301513671875 test_loss:228.9854736328125\n",
      "892/3000 train_loss: 41.88166427612305 test_loss:228.82809448242188\n",
      "893/3000 train_loss: 43.40617752075195 test_loss:233.7498016357422\n",
      "894/3000 train_loss: 41.8420524597168 test_loss:238.71505737304688\n",
      "895/3000 train_loss: 53.57994079589844 test_loss:229.29812622070312\n",
      "896/3000 train_loss: 42.712928771972656 test_loss:244.0685272216797\n",
      "897/3000 train_loss: 53.9054069519043 test_loss:243.117919921875\n",
      "898/3000 train_loss: 51.660804748535156 test_loss:227.83135986328125\n",
      "899/3000 train_loss: 46.257789611816406 test_loss:230.394775390625\n",
      "900/3000 train_loss: 42.6756706237793 test_loss:226.21495056152344\n",
      "901/3000 train_loss: 48.18747329711914 test_loss:224.56227111816406\n",
      "902/3000 train_loss: 55.065460205078125 test_loss:222.04037475585938\n",
      "903/3000 train_loss: 41.68013381958008 test_loss:225.65139770507812\n",
      "904/3000 train_loss: 48.889591217041016 test_loss:219.65447998046875\n",
      "905/3000 train_loss: 44.30888366699219 test_loss:226.68618774414062\n",
      "906/3000 train_loss: 44.484161376953125 test_loss:222.8283233642578\n",
      "907/3000 train_loss: 41.583187103271484 test_loss:232.75833129882812\n",
      "908/3000 train_loss: 40.060951232910156 test_loss:228.54598999023438\n",
      "909/3000 train_loss: 39.60007858276367 test_loss:227.8217010498047\n",
      "910/3000 train_loss: 46.695030212402344 test_loss:229.14141845703125\n",
      "911/3000 train_loss: 49.3043098449707 test_loss:223.52847290039062\n",
      "912/3000 train_loss: 41.42787551879883 test_loss:228.70367431640625\n",
      "913/3000 train_loss: 42.990684509277344 test_loss:228.96026611328125\n",
      "914/3000 train_loss: 44.81604766845703 test_loss:226.96258544921875\n",
      "915/3000 train_loss: 46.28408432006836 test_loss:224.0618438720703\n",
      "916/3000 train_loss: 40.6911506652832 test_loss:221.35459899902344\n",
      "917/3000 train_loss: 42.03739929199219 test_loss:229.85806274414062\n",
      "918/3000 train_loss: 40.787933349609375 test_loss:227.2646026611328\n",
      "919/3000 train_loss: 59.65651321411133 test_loss:229.803955078125\n",
      "920/3000 train_loss: 48.66645050048828 test_loss:223.33062744140625\n",
      "921/3000 train_loss: 47.72733688354492 test_loss:234.54811096191406\n",
      "922/3000 train_loss: 44.22624969482422 test_loss:229.27171325683594\n",
      "923/3000 train_loss: 46.11124801635742 test_loss:234.66566467285156\n",
      "924/3000 train_loss: 37.28404235839844 test_loss:233.5608367919922\n",
      "925/3000 train_loss: 44.49283981323242 test_loss:227.54751586914062\n",
      "926/3000 train_loss: 41.6044807434082 test_loss:243.55825805664062\n",
      "927/3000 train_loss: 40.736366271972656 test_loss:231.9688262939453\n",
      "928/3000 train_loss: 43.26882553100586 test_loss:236.38955688476562\n",
      "929/3000 train_loss: 36.8140983581543 test_loss:231.44622802734375\n",
      "930/3000 train_loss: 39.22572326660156 test_loss:232.63059997558594\n",
      "931/3000 train_loss: 46.62529754638672 test_loss:224.31137084960938\n",
      "932/3000 train_loss: 41.28150939941406 test_loss:224.56344604492188\n",
      "933/3000 train_loss: 39.41991424560547 test_loss:229.19320678710938\n",
      "934/3000 train_loss: 45.12995147705078 test_loss:234.83645629882812\n",
      "935/3000 train_loss: 54.90690612792969 test_loss:231.61953735351562\n",
      "936/3000 train_loss: 43.88405990600586 test_loss:224.0096435546875\n",
      "937/3000 train_loss: 46.92096710205078 test_loss:223.7346954345703\n",
      "938/3000 train_loss: 38.88565444946289 test_loss:223.98619079589844\n",
      "939/3000 train_loss: 44.57762145996094 test_loss:235.50404357910156\n",
      "940/3000 train_loss: 38.85860061645508 test_loss:221.02268981933594\n",
      "941/3000 train_loss: 38.05095672607422 test_loss:229.5531005859375\n",
      "942/3000 train_loss: 39.07101821899414 test_loss:230.291748046875\n",
      "943/3000 train_loss: 61.106201171875 test_loss:220.10910034179688\n",
      "944/3000 train_loss: 44.72303771972656 test_loss:227.52520751953125\n",
      "945/3000 train_loss: 40.628143310546875 test_loss:225.42953491210938\n",
      "946/3000 train_loss: 41.91858673095703 test_loss:222.0889892578125\n",
      "947/3000 train_loss: 41.818702697753906 test_loss:225.8389892578125\n",
      "948/3000 train_loss: 42.67429733276367 test_loss:230.47549438476562\n",
      "949/3000 train_loss: 43.46895980834961 test_loss:227.15045166015625\n",
      "950/3000 train_loss: 39.57508087158203 test_loss:235.79397583007812\n",
      "951/3000 train_loss: 44.137367248535156 test_loss:224.2197265625\n",
      "952/3000 train_loss: 40.101829528808594 test_loss:232.651611328125\n",
      "953/3000 train_loss: 41.62445831298828 test_loss:231.07156372070312\n",
      "954/3000 train_loss: 46.12355041503906 test_loss:238.38351440429688\n",
      "955/3000 train_loss: 43.43373489379883 test_loss:233.96160888671875\n",
      "956/3000 train_loss: 46.34673309326172 test_loss:227.80206298828125\n",
      "957/3000 train_loss: 44.86833190917969 test_loss:226.4698944091797\n",
      "958/3000 train_loss: 41.35285949707031 test_loss:226.63671875\n",
      "959/3000 train_loss: 37.67066192626953 test_loss:230.41163635253906\n",
      "960/3000 train_loss: 42.146976470947266 test_loss:233.04205322265625\n",
      "961/3000 train_loss: 39.69404983520508 test_loss:227.05540466308594\n",
      "962/3000 train_loss: 48.13874435424805 test_loss:227.27838134765625\n",
      "963/3000 train_loss: 41.86164093017578 test_loss:231.42820739746094\n",
      "964/3000 train_loss: 42.899688720703125 test_loss:226.93881225585938\n",
      "965/3000 train_loss: 42.95382308959961 test_loss:226.9492950439453\n",
      "966/3000 train_loss: 41.63612365722656 test_loss:228.31741333007812\n",
      "967/3000 train_loss: 38.75409698486328 test_loss:229.96112060546875\n",
      "968/3000 train_loss: 37.60906982421875 test_loss:225.00140380859375\n",
      "969/3000 train_loss: 38.854515075683594 test_loss:227.66934204101562\n",
      "970/3000 train_loss: 40.98472595214844 test_loss:232.3174591064453\n",
      "971/3000 train_loss: 39.253936767578125 test_loss:225.3988037109375\n",
      "972/3000 train_loss: 39.857486724853516 test_loss:225.06439208984375\n",
      "973/3000 train_loss: 41.00377655029297 test_loss:233.02157592773438\n",
      "974/3000 train_loss: 40.27463150024414 test_loss:228.54827880859375\n",
      "975/3000 train_loss: 39.341026306152344 test_loss:238.367431640625\n",
      "976/3000 train_loss: 39.636470794677734 test_loss:231.5686798095703\n",
      "977/3000 train_loss: 36.68984603881836 test_loss:225.32447814941406\n",
      "978/3000 train_loss: 41.896026611328125 test_loss:230.12637329101562\n",
      "979/3000 train_loss: 39.384422302246094 test_loss:230.1608123779297\n",
      "980/3000 train_loss: 44.769325256347656 test_loss:223.38377380371094\n",
      "981/3000 train_loss: 39.4018669128418 test_loss:230.06202697753906\n",
      "982/3000 train_loss: 40.124969482421875 test_loss:225.04864501953125\n",
      "983/3000 train_loss: 39.65252685546875 test_loss:243.23135375976562\n",
      "984/3000 train_loss: 41.19361114501953 test_loss:231.207763671875\n",
      "985/3000 train_loss: 49.89053726196289 test_loss:235.2442169189453\n",
      "986/3000 train_loss: 38.93716049194336 test_loss:235.82594299316406\n",
      "987/3000 train_loss: 44.943241119384766 test_loss:228.230712890625\n",
      "988/3000 train_loss: 44.294212341308594 test_loss:230.1675262451172\n",
      "989/3000 train_loss: 40.33330535888672 test_loss:225.2709197998047\n",
      "990/3000 train_loss: 40.233211517333984 test_loss:224.34335327148438\n",
      "991/3000 train_loss: 42.92668914794922 test_loss:228.853759765625\n",
      "992/3000 train_loss: 46.45997619628906 test_loss:225.1593017578125\n",
      "993/3000 train_loss: 34.002769470214844 test_loss:234.31878662109375\n",
      "994/3000 train_loss: 50.32041549682617 test_loss:224.7984619140625\n",
      "995/3000 train_loss: 39.61104965209961 test_loss:229.0501708984375\n",
      "996/3000 train_loss: 41.872802734375 test_loss:226.82643127441406\n",
      "997/3000 train_loss: 37.25541687011719 test_loss:232.9174346923828\n",
      "998/3000 train_loss: 41.5737419128418 test_loss:228.27317810058594\n",
      "999/3000 train_loss: 59.506622314453125 test_loss:228.62832641601562\n",
      "1000/3000 train_loss: 40.823822021484375 test_loss:230.68106079101562\n",
      "1001/3000 train_loss: 40.197750091552734 test_loss:225.21478271484375\n",
      "1002/3000 train_loss: 40.58431625366211 test_loss:232.23602294921875\n",
      "1003/3000 train_loss: 48.40972900390625 test_loss:225.47296142578125\n",
      "1004/3000 train_loss: 39.95149612426758 test_loss:225.94447326660156\n",
      "1005/3000 train_loss: 52.349735260009766 test_loss:227.5032196044922\n",
      "1006/3000 train_loss: 47.326480865478516 test_loss:221.15533447265625\n",
      "1007/3000 train_loss: 40.310546875 test_loss:236.86300659179688\n",
      "1008/3000 train_loss: 45.302391052246094 test_loss:227.69976806640625\n",
      "1009/3000 train_loss: 46.1305046081543 test_loss:230.77317810058594\n",
      "1010/3000 train_loss: 41.48908615112305 test_loss:226.06329345703125\n",
      "1011/3000 train_loss: 42.459407806396484 test_loss:224.54527282714844\n",
      "1012/3000 train_loss: 31.879987716674805 test_loss:225.802001953125\n",
      "1013/3000 train_loss: 41.56132888793945 test_loss:223.828125\n",
      "1014/3000 train_loss: 37.50497055053711 test_loss:227.54541015625\n",
      "1015/3000 train_loss: 37.54994201660156 test_loss:223.60464477539062\n",
      "1016/3000 train_loss: 42.14020538330078 test_loss:229.94085693359375\n",
      "1017/3000 train_loss: 39.79243469238281 test_loss:231.57504272460938\n",
      "1018/3000 train_loss: 38.85445785522461 test_loss:231.04539489746094\n",
      "1019/3000 train_loss: 36.19404983520508 test_loss:229.98654174804688\n",
      "1020/3000 train_loss: 43.480560302734375 test_loss:215.29782104492188\n",
      "1021/3000 train_loss: 42.177490234375 test_loss:225.5040283203125\n",
      "1022/3000 train_loss: 38.68543243408203 test_loss:216.8704376220703\n",
      "1023/3000 train_loss: 35.83125686645508 test_loss:229.58438110351562\n",
      "1024/3000 train_loss: 40.98196029663086 test_loss:218.07022094726562\n",
      "1025/3000 train_loss: 42.559024810791016 test_loss:224.08865356445312\n",
      "1026/3000 train_loss: 46.029014587402344 test_loss:213.80186462402344\n",
      "1027/3000 train_loss: 44.46519470214844 test_loss:218.32684326171875\n",
      "1028/3000 train_loss: 38.20522689819336 test_loss:215.20452880859375\n",
      "1029/3000 train_loss: 42.493385314941406 test_loss:212.80885314941406\n",
      "1030/3000 train_loss: 40.37805938720703 test_loss:223.0634307861328\n",
      "1031/3000 train_loss: 45.66368865966797 test_loss:221.71363830566406\n",
      "1032/3000 train_loss: 46.87232971191406 test_loss:219.712890625\n",
      "1033/3000 train_loss: 44.56279754638672 test_loss:218.49217224121094\n",
      "1034/3000 train_loss: 38.842716217041016 test_loss:219.46615600585938\n",
      "1035/3000 train_loss: 36.713321685791016 test_loss:218.54931640625\n",
      "1036/3000 train_loss: 42.82901382446289 test_loss:224.07366943359375\n",
      "1037/3000 train_loss: 33.954559326171875 test_loss:216.71945190429688\n",
      "1038/3000 train_loss: 37.631805419921875 test_loss:220.08522033691406\n",
      "1039/3000 train_loss: 40.46940612792969 test_loss:218.03851318359375\n",
      "1040/3000 train_loss: 60.73601150512695 test_loss:226.6027374267578\n",
      "1041/3000 train_loss: 37.9615364074707 test_loss:216.89170837402344\n",
      "1042/3000 train_loss: 37.771324157714844 test_loss:217.653076171875\n",
      "1043/3000 train_loss: 42.82211685180664 test_loss:215.77227783203125\n",
      "1044/3000 train_loss: 39.33689880371094 test_loss:217.90611267089844\n",
      "1045/3000 train_loss: 37.33229446411133 test_loss:221.52976989746094\n",
      "1046/3000 train_loss: 40.19955062866211 test_loss:219.52517700195312\n",
      "1047/3000 train_loss: 37.758365631103516 test_loss:217.35902404785156\n",
      "1048/3000 train_loss: 39.19243621826172 test_loss:216.48641967773438\n",
      "1049/3000 train_loss: 39.660804748535156 test_loss:212.69403076171875\n",
      "1050/3000 train_loss: 42.10221481323242 test_loss:214.52169799804688\n",
      "1051/3000 train_loss: 38.9074821472168 test_loss:217.08888244628906\n",
      "1052/3000 train_loss: 37.439666748046875 test_loss:228.40701293945312\n",
      "1053/3000 train_loss: 40.90694046020508 test_loss:223.68017578125\n",
      "1054/3000 train_loss: 53.26658248901367 test_loss:231.15792846679688\n",
      "1055/3000 train_loss: 35.587806701660156 test_loss:224.6495361328125\n",
      "1056/3000 train_loss: 38.59170150756836 test_loss:219.32669067382812\n",
      "1057/3000 train_loss: 38.55498504638672 test_loss:228.71656799316406\n",
      "1058/3000 train_loss: 34.1347541809082 test_loss:220.60443115234375\n",
      "1059/3000 train_loss: 38.52703094482422 test_loss:219.5673370361328\n",
      "1060/3000 train_loss: 34.537628173828125 test_loss:217.02560424804688\n",
      "1061/3000 train_loss: 37.72187805175781 test_loss:219.7697296142578\n",
      "1062/3000 train_loss: 32.20443344116211 test_loss:219.86924743652344\n",
      "1063/3000 train_loss: 36.25410842895508 test_loss:223.86102294921875\n",
      "1064/3000 train_loss: 35.54547119140625 test_loss:222.79476928710938\n",
      "1065/3000 train_loss: 33.5441780090332 test_loss:226.09796142578125\n",
      "1066/3000 train_loss: 37.99934387207031 test_loss:228.85934448242188\n",
      "1067/3000 train_loss: 45.3873291015625 test_loss:230.12371826171875\n",
      "1068/3000 train_loss: 41.1200065612793 test_loss:222.45968627929688\n",
      "1069/3000 train_loss: 42.450775146484375 test_loss:221.48141479492188\n",
      "1070/3000 train_loss: 35.560523986816406 test_loss:223.56898498535156\n",
      "1071/3000 train_loss: 40.98381423950195 test_loss:224.40455627441406\n",
      "1072/3000 train_loss: 35.3817253112793 test_loss:223.75811767578125\n",
      "1073/3000 train_loss: 40.44231414794922 test_loss:220.76454162597656\n",
      "1074/3000 train_loss: 46.00938415527344 test_loss:224.17849731445312\n",
      "1075/3000 train_loss: 39.61833190917969 test_loss:220.53387451171875\n",
      "1076/3000 train_loss: 34.52444839477539 test_loss:224.72616577148438\n",
      "1077/3000 train_loss: 36.0644416809082 test_loss:215.39105224609375\n",
      "1078/3000 train_loss: 33.99605941772461 test_loss:232.7969970703125\n",
      "1079/3000 train_loss: 37.668968200683594 test_loss:219.1492919921875\n",
      "1080/3000 train_loss: 37.272438049316406 test_loss:227.31007385253906\n",
      "1081/3000 train_loss: 33.68663024902344 test_loss:222.12857055664062\n",
      "1082/3000 train_loss: 36.48859786987305 test_loss:222.77313232421875\n",
      "1083/3000 train_loss: 38.498348236083984 test_loss:230.73873901367188\n",
      "1084/3000 train_loss: 42.807315826416016 test_loss:225.0194091796875\n",
      "1085/3000 train_loss: 39.426300048828125 test_loss:225.98873901367188\n",
      "1086/3000 train_loss: 41.979068756103516 test_loss:228.17486572265625\n",
      "1087/3000 train_loss: 36.7524299621582 test_loss:220.68865966796875\n",
      "1088/3000 train_loss: 46.40272521972656 test_loss:225.895263671875\n",
      "1089/3000 train_loss: 37.38687515258789 test_loss:224.91387939453125\n",
      "1090/3000 train_loss: 35.52655029296875 test_loss:227.68814086914062\n",
      "1091/3000 train_loss: 33.99106979370117 test_loss:230.8089599609375\n",
      "1092/3000 train_loss: 36.279319763183594 test_loss:220.1275177001953\n",
      "1093/3000 train_loss: 33.13182830810547 test_loss:223.55841064453125\n",
      "1094/3000 train_loss: 36.76145553588867 test_loss:216.27877807617188\n",
      "1095/3000 train_loss: 34.02450942993164 test_loss:230.30296325683594\n",
      "1096/3000 train_loss: 33.84574508666992 test_loss:218.76162719726562\n",
      "1097/3000 train_loss: 33.837406158447266 test_loss:227.5354461669922\n",
      "1098/3000 train_loss: 37.02156066894531 test_loss:218.89019775390625\n",
      "1099/3000 train_loss: 43.814903259277344 test_loss:229.59239196777344\n",
      "1100/3000 train_loss: 37.954307556152344 test_loss:219.48681640625\n",
      "1101/3000 train_loss: 37.86077880859375 test_loss:228.9320068359375\n",
      "1102/3000 train_loss: 40.96388244628906 test_loss:218.90658569335938\n",
      "1103/3000 train_loss: 43.83221435546875 test_loss:219.97503662109375\n",
      "1104/3000 train_loss: 39.767860412597656 test_loss:229.6570281982422\n",
      "1105/3000 train_loss: 38.32178497314453 test_loss:222.41986083984375\n",
      "1106/3000 train_loss: 37.293418884277344 test_loss:216.93350219726562\n",
      "1107/3000 train_loss: 45.26151657104492 test_loss:212.83297729492188\n",
      "1108/3000 train_loss: 41.00630187988281 test_loss:218.9711456298828\n",
      "1109/3000 train_loss: 34.3370475769043 test_loss:216.95571899414062\n",
      "1110/3000 train_loss: 37.02906036376953 test_loss:218.359130859375\n",
      "1111/3000 train_loss: 32.95391082763672 test_loss:222.94363403320312\n",
      "1112/3000 train_loss: 31.521133422851562 test_loss:221.3285675048828\n",
      "1113/3000 train_loss: 33.43788528442383 test_loss:228.44509887695312\n",
      "1114/3000 train_loss: 32.814048767089844 test_loss:215.374267578125\n",
      "1115/3000 train_loss: 45.81398391723633 test_loss:215.98806762695312\n",
      "1116/3000 train_loss: 36.66431427001953 test_loss:218.8865509033203\n",
      "1117/3000 train_loss: 40.19779968261719 test_loss:224.82928466796875\n",
      "1118/3000 train_loss: 34.18708038330078 test_loss:220.91297912597656\n",
      "1119/3000 train_loss: 32.72275924682617 test_loss:220.91168212890625\n",
      "1120/3000 train_loss: 34.99513626098633 test_loss:222.2718505859375\n",
      "1121/3000 train_loss: 36.65357208251953 test_loss:224.48410034179688\n",
      "1122/3000 train_loss: 34.6125373840332 test_loss:216.30413818359375\n",
      "1123/3000 train_loss: 37.183982849121094 test_loss:214.708251953125\n",
      "1124/3000 train_loss: 35.826900482177734 test_loss:223.81826782226562\n",
      "1125/3000 train_loss: 39.6697883605957 test_loss:230.17633056640625\n",
      "1126/3000 train_loss: 41.03278350830078 test_loss:223.2459259033203\n",
      "1127/3000 train_loss: 32.83826446533203 test_loss:214.068115234375\n",
      "1128/3000 train_loss: 36.27904510498047 test_loss:219.0176544189453\n",
      "1129/3000 train_loss: 34.039974212646484 test_loss:224.9100341796875\n",
      "1130/3000 train_loss: 37.29486083984375 test_loss:226.1237030029297\n",
      "1131/3000 train_loss: 39.368751525878906 test_loss:228.44741821289062\n",
      "1132/3000 train_loss: 33.273948669433594 test_loss:215.22695922851562\n",
      "1133/3000 train_loss: 34.65217971801758 test_loss:225.4059295654297\n",
      "1134/3000 train_loss: 37.15018081665039 test_loss:217.88095092773438\n",
      "1135/3000 train_loss: 34.549861907958984 test_loss:231.48805236816406\n",
      "1136/3000 train_loss: 31.47862434387207 test_loss:216.0913543701172\n",
      "1137/3000 train_loss: 34.80524444580078 test_loss:219.62974548339844\n",
      "1138/3000 train_loss: 31.993003845214844 test_loss:222.61341857910156\n",
      "1139/3000 train_loss: 35.36494064331055 test_loss:220.9703369140625\n",
      "1140/3000 train_loss: 35.9417839050293 test_loss:213.16769409179688\n",
      "1141/3000 train_loss: 38.30171585083008 test_loss:219.8650360107422\n",
      "1142/3000 train_loss: 34.3436164855957 test_loss:211.89373779296875\n",
      "1143/3000 train_loss: 34.44303512573242 test_loss:217.18331909179688\n",
      "1144/3000 train_loss: 34.00764465332031 test_loss:213.52218627929688\n",
      "1145/3000 train_loss: 35.107810974121094 test_loss:218.99374389648438\n",
      "1146/3000 train_loss: 30.411834716796875 test_loss:209.48965454101562\n",
      "1147/3000 train_loss: 36.1248664855957 test_loss:222.9655303955078\n",
      "1148/3000 train_loss: 35.021602630615234 test_loss:214.22598266601562\n",
      "1149/3000 train_loss: 34.28424835205078 test_loss:221.80494689941406\n",
      "1150/3000 train_loss: 49.36163330078125 test_loss:221.39332580566406\n",
      "1151/3000 train_loss: 39.90898132324219 test_loss:219.36280822753906\n",
      "1152/3000 train_loss: 37.02956771850586 test_loss:207.1899871826172\n",
      "1153/3000 train_loss: 31.790658950805664 test_loss:225.88241577148438\n",
      "1154/3000 train_loss: 32.69974136352539 test_loss:216.5406494140625\n",
      "1155/3000 train_loss: 36.990211486816406 test_loss:232.8712921142578\n",
      "1156/3000 train_loss: 38.28806686401367 test_loss:218.85899353027344\n",
      "1157/3000 train_loss: 31.252872467041016 test_loss:223.53268432617188\n",
      "1158/3000 train_loss: 37.59230422973633 test_loss:225.61123657226562\n",
      "1159/3000 train_loss: 33.352813720703125 test_loss:220.1479034423828\n",
      "1160/3000 train_loss: 35.367584228515625 test_loss:224.08294677734375\n",
      "1161/3000 train_loss: 29.342056274414062 test_loss:222.20980834960938\n",
      "1162/3000 train_loss: 38.53535842895508 test_loss:216.25930786132812\n",
      "1163/3000 train_loss: 36.96582794189453 test_loss:222.8343048095703\n",
      "1164/3000 train_loss: 31.842613220214844 test_loss:229.30587768554688\n",
      "1165/3000 train_loss: 41.16457748413086 test_loss:222.77244567871094\n",
      "1166/3000 train_loss: 39.46976089477539 test_loss:218.86395263671875\n",
      "1167/3000 train_loss: 33.071353912353516 test_loss:228.46636962890625\n",
      "1168/3000 train_loss: 33.09333419799805 test_loss:223.11183166503906\n",
      "1169/3000 train_loss: 37.25908279418945 test_loss:228.93408203125\n",
      "1170/3000 train_loss: 31.064788818359375 test_loss:226.09732055664062\n",
      "1171/3000 train_loss: 34.91945266723633 test_loss:219.17324829101562\n",
      "1172/3000 train_loss: 38.416263580322266 test_loss:217.85597229003906\n",
      "1173/3000 train_loss: 31.678573608398438 test_loss:222.30911254882812\n",
      "1174/3000 train_loss: 36.58477783203125 test_loss:217.0782928466797\n",
      "1175/3000 train_loss: 41.356849670410156 test_loss:215.8157958984375\n",
      "1176/3000 train_loss: 40.86518478393555 test_loss:224.62989807128906\n",
      "1177/3000 train_loss: 40.6128044128418 test_loss:231.18612670898438\n",
      "1178/3000 train_loss: 38.67594909667969 test_loss:229.00454711914062\n",
      "1179/3000 train_loss: 33.86772918701172 test_loss:221.59707641601562\n",
      "1180/3000 train_loss: 35.76734924316406 test_loss:225.47348022460938\n",
      "1181/3000 train_loss: 37.16363525390625 test_loss:218.7039337158203\n",
      "1182/3000 train_loss: 31.696964263916016 test_loss:228.15655517578125\n",
      "1183/3000 train_loss: 37.6314697265625 test_loss:220.32652282714844\n",
      "1184/3000 train_loss: 35.183372497558594 test_loss:224.3538360595703\n",
      "1185/3000 train_loss: 32.605770111083984 test_loss:218.26971435546875\n",
      "1186/3000 train_loss: 35.17899703979492 test_loss:228.42147827148438\n",
      "1187/3000 train_loss: 37.85066223144531 test_loss:214.4358673095703\n",
      "1188/3000 train_loss: 39.634010314941406 test_loss:212.4387664794922\n",
      "1189/3000 train_loss: 42.280887603759766 test_loss:221.78756713867188\n",
      "1190/3000 train_loss: 34.728755950927734 test_loss:222.7637176513672\n",
      "1191/3000 train_loss: 35.66630172729492 test_loss:227.5828399658203\n",
      "1192/3000 train_loss: 33.460758209228516 test_loss:217.4614715576172\n",
      "1193/3000 train_loss: 33.01396560668945 test_loss:215.99566650390625\n",
      "1194/3000 train_loss: 37.0823860168457 test_loss:212.4355926513672\n",
      "1195/3000 train_loss: 37.68928909301758 test_loss:216.8820343017578\n",
      "1196/3000 train_loss: 31.32488441467285 test_loss:218.9562530517578\n",
      "1197/3000 train_loss: 36.12433624267578 test_loss:221.6095428466797\n",
      "1198/3000 train_loss: 32.45689010620117 test_loss:219.87265014648438\n",
      "1199/3000 train_loss: 35.99338912963867 test_loss:219.88873291015625\n",
      "1200/3000 train_loss: 39.71794891357422 test_loss:213.76223754882812\n",
      "1201/3000 train_loss: 33.518550872802734 test_loss:222.1403350830078\n",
      "1202/3000 train_loss: 34.07998275756836 test_loss:217.74452209472656\n",
      "1203/3000 train_loss: 31.867290496826172 test_loss:221.54452514648438\n",
      "1204/3000 train_loss: 31.701841354370117 test_loss:221.6475830078125\n",
      "1205/3000 train_loss: 30.718669891357422 test_loss:217.42039489746094\n",
      "1206/3000 train_loss: 34.154571533203125 test_loss:220.25439453125\n",
      "1207/3000 train_loss: 34.484779357910156 test_loss:217.80517578125\n",
      "1208/3000 train_loss: 32.67878723144531 test_loss:222.85899353027344\n",
      "1209/3000 train_loss: 31.701257705688477 test_loss:219.84228515625\n",
      "1210/3000 train_loss: 36.33228302001953 test_loss:216.92916870117188\n",
      "1211/3000 train_loss: 32.15690231323242 test_loss:219.67071533203125\n",
      "1212/3000 train_loss: 55.42371368408203 test_loss:211.38442993164062\n",
      "1213/3000 train_loss: 32.299530029296875 test_loss:220.7438507080078\n",
      "1214/3000 train_loss: 33.81454086303711 test_loss:210.34396362304688\n",
      "1215/3000 train_loss: 38.805076599121094 test_loss:228.75131225585938\n",
      "1216/3000 train_loss: 32.95261001586914 test_loss:216.0458984375\n",
      "1217/3000 train_loss: 38.60049057006836 test_loss:214.43373107910156\n",
      "1218/3000 train_loss: 32.91747283935547 test_loss:218.32362365722656\n",
      "1219/3000 train_loss: 37.060855865478516 test_loss:220.5247802734375\n",
      "1220/3000 train_loss: 36.68408966064453 test_loss:214.29742431640625\n",
      "1221/3000 train_loss: 31.911880493164062 test_loss:217.19325256347656\n",
      "1222/3000 train_loss: 30.584211349487305 test_loss:223.92938232421875\n",
      "1223/3000 train_loss: 41.69712448120117 test_loss:217.86863708496094\n",
      "1224/3000 train_loss: 34.649070739746094 test_loss:231.3114013671875\n",
      "1225/3000 train_loss: 34.90607452392578 test_loss:213.39852905273438\n",
      "1226/3000 train_loss: 33.26876449584961 test_loss:218.25267028808594\n",
      "1227/3000 train_loss: 28.79874610900879 test_loss:217.56903076171875\n",
      "1228/3000 train_loss: 32.01960372924805 test_loss:216.44691467285156\n",
      "1229/3000 train_loss: 40.15471649169922 test_loss:222.79464721679688\n",
      "1230/3000 train_loss: 33.447696685791016 test_loss:223.09996032714844\n",
      "1231/3000 train_loss: 36.202980041503906 test_loss:219.73922729492188\n",
      "1232/3000 train_loss: 34.43886184692383 test_loss:225.18658447265625\n",
      "1233/3000 train_loss: 36.476261138916016 test_loss:218.12594604492188\n",
      "1234/3000 train_loss: 33.239402770996094 test_loss:225.3677978515625\n",
      "1235/3000 train_loss: 33.01160430908203 test_loss:206.753662109375\n",
      "1236/3000 train_loss: 30.83931541442871 test_loss:221.39962768554688\n",
      "1237/3000 train_loss: 33.51076126098633 test_loss:215.0760498046875\n",
      "1238/3000 train_loss: 33.969661712646484 test_loss:218.7885284423828\n",
      "1239/3000 train_loss: 35.662994384765625 test_loss:224.1087188720703\n",
      "1240/3000 train_loss: 33.74028015136719 test_loss:219.46566772460938\n",
      "1241/3000 train_loss: 30.598690032958984 test_loss:220.0720672607422\n",
      "1242/3000 train_loss: 34.08832550048828 test_loss:223.192138671875\n",
      "1243/3000 train_loss: 31.21662139892578 test_loss:219.93368530273438\n",
      "1244/3000 train_loss: 38.740055084228516 test_loss:211.49984741210938\n",
      "1245/3000 train_loss: 34.56950759887695 test_loss:218.31260681152344\n",
      "1246/3000 train_loss: 31.813674926757812 test_loss:219.88172912597656\n",
      "1247/3000 train_loss: 31.401811599731445 test_loss:218.87457275390625\n",
      "1248/3000 train_loss: 31.448745727539062 test_loss:214.4585418701172\n",
      "1249/3000 train_loss: 30.860475540161133 test_loss:225.42205810546875\n",
      "1250/3000 train_loss: 32.5811767578125 test_loss:225.63958740234375\n",
      "1251/3000 train_loss: 43.583858489990234 test_loss:221.62619018554688\n",
      "1252/3000 train_loss: 37.140594482421875 test_loss:213.90289306640625\n",
      "1253/3000 train_loss: 38.51668167114258 test_loss:219.17095947265625\n",
      "1254/3000 train_loss: 45.49749755859375 test_loss:216.2857208251953\n",
      "1255/3000 train_loss: 38.716163635253906 test_loss:217.61512756347656\n",
      "1256/3000 train_loss: 31.26233673095703 test_loss:214.71044921875\n",
      "1257/3000 train_loss: 45.688072204589844 test_loss:208.7400665283203\n",
      "1258/3000 train_loss: 32.280303955078125 test_loss:218.5841827392578\n",
      "1259/3000 train_loss: 31.359132766723633 test_loss:218.2140350341797\n",
      "1260/3000 train_loss: 33.455841064453125 test_loss:218.60275268554688\n",
      "1261/3000 train_loss: 31.19895362854004 test_loss:222.61329650878906\n",
      "1262/3000 train_loss: 35.70723342895508 test_loss:218.99642944335938\n",
      "1263/3000 train_loss: 34.34708786010742 test_loss:217.10525512695312\n",
      "1264/3000 train_loss: 30.185123443603516 test_loss:221.80111694335938\n",
      "1265/3000 train_loss: 32.60905456542969 test_loss:215.35540771484375\n",
      "1266/3000 train_loss: 30.99872398376465 test_loss:217.76300048828125\n",
      "1267/3000 train_loss: 32.268455505371094 test_loss:219.285400390625\n",
      "1268/3000 train_loss: 32.54465103149414 test_loss:220.86297607421875\n",
      "1269/3000 train_loss: 39.681304931640625 test_loss:223.80471801757812\n",
      "1270/3000 train_loss: 31.738391876220703 test_loss:215.95977783203125\n",
      "1271/3000 train_loss: 31.965307235717773 test_loss:219.39796447753906\n",
      "1272/3000 train_loss: 38.83320617675781 test_loss:213.91140747070312\n",
      "1273/3000 train_loss: 33.68897247314453 test_loss:214.5889892578125\n",
      "1274/3000 train_loss: 27.274250030517578 test_loss:212.87991333007812\n",
      "1275/3000 train_loss: 31.437355041503906 test_loss:218.97357177734375\n",
      "1276/3000 train_loss: 33.67867660522461 test_loss:215.45098876953125\n",
      "1277/3000 train_loss: 30.168506622314453 test_loss:213.77508544921875\n",
      "1278/3000 train_loss: 35.82695770263672 test_loss:228.21823120117188\n",
      "1279/3000 train_loss: 38.0045280456543 test_loss:208.64547729492188\n",
      "1280/3000 train_loss: 30.132747650146484 test_loss:220.65121459960938\n",
      "1281/3000 train_loss: 36.9221305847168 test_loss:215.7208251953125\n",
      "1282/3000 train_loss: 32.75959014892578 test_loss:220.36349487304688\n",
      "1283/3000 train_loss: 30.95721435546875 test_loss:217.00778198242188\n",
      "1284/3000 train_loss: 34.6899299621582 test_loss:217.3951416015625\n",
      "1285/3000 train_loss: 33.27620315551758 test_loss:212.59573364257812\n",
      "1286/3000 train_loss: 33.69658279418945 test_loss:209.5675811767578\n",
      "1287/3000 train_loss: 32.42501449584961 test_loss:215.89242553710938\n",
      "1288/3000 train_loss: 32.00261306762695 test_loss:212.65939331054688\n",
      "1289/3000 train_loss: 33.16387939453125 test_loss:210.7868194580078\n",
      "1290/3000 train_loss: 31.53734588623047 test_loss:218.119140625\n",
      "1291/3000 train_loss: 32.41067123413086 test_loss:213.3388214111328\n",
      "1292/3000 train_loss: 32.22563171386719 test_loss:220.85830688476562\n",
      "1293/3000 train_loss: 31.833005905151367 test_loss:217.5211181640625\n",
      "1294/3000 train_loss: 27.057100296020508 test_loss:220.71389770507812\n",
      "1295/3000 train_loss: 30.84417152404785 test_loss:215.53977966308594\n",
      "1296/3000 train_loss: 31.591257095336914 test_loss:211.43844604492188\n",
      "1297/3000 train_loss: 36.802276611328125 test_loss:223.14385986328125\n",
      "1298/3000 train_loss: 30.886293411254883 test_loss:213.5184783935547\n",
      "1299/3000 train_loss: 29.021852493286133 test_loss:213.2021942138672\n",
      "1300/3000 train_loss: 27.67351722717285 test_loss:216.24957275390625\n",
      "1301/3000 train_loss: 25.571643829345703 test_loss:216.20944213867188\n",
      "1302/3000 train_loss: 32.63914108276367 test_loss:216.8624267578125\n",
      "1303/3000 train_loss: 35.39649200439453 test_loss:220.47891235351562\n",
      "1304/3000 train_loss: 32.58761978149414 test_loss:222.60385131835938\n",
      "1305/3000 train_loss: 29.621326446533203 test_loss:211.7624969482422\n",
      "1306/3000 train_loss: 33.349063873291016 test_loss:213.1368408203125\n",
      "1307/3000 train_loss: 50.089900970458984 test_loss:221.88442993164062\n",
      "1308/3000 train_loss: 34.455787658691406 test_loss:215.42498779296875\n",
      "1309/3000 train_loss: 41.215919494628906 test_loss:211.2486572265625\n",
      "1310/3000 train_loss: 37.72784423828125 test_loss:224.54598999023438\n",
      "1311/3000 train_loss: 37.68210983276367 test_loss:202.773681640625\n",
      "1312/3000 train_loss: 33.24370193481445 test_loss:220.67083740234375\n",
      "1313/3000 train_loss: 33.94093704223633 test_loss:211.9233856201172\n",
      "1314/3000 train_loss: 32.97263717651367 test_loss:214.57130432128906\n",
      "1315/3000 train_loss: 27.516075134277344 test_loss:213.61122131347656\n",
      "1316/3000 train_loss: 27.79621124267578 test_loss:211.97744750976562\n",
      "1317/3000 train_loss: 33.702388763427734 test_loss:206.41094970703125\n",
      "1318/3000 train_loss: 28.639432907104492 test_loss:221.35401916503906\n",
      "1319/3000 train_loss: 32.0452766418457 test_loss:220.58926391601562\n",
      "1320/3000 train_loss: 34.86140441894531 test_loss:208.60476684570312\n",
      "1321/3000 train_loss: 31.369037628173828 test_loss:219.57545471191406\n",
      "1322/3000 train_loss: 35.030364990234375 test_loss:207.9143829345703\n",
      "1323/3000 train_loss: 24.674945831298828 test_loss:215.28091430664062\n",
      "1324/3000 train_loss: 28.929243087768555 test_loss:211.50576782226562\n",
      "1325/3000 train_loss: 35.54323959350586 test_loss:209.5129852294922\n",
      "1326/3000 train_loss: 30.565608978271484 test_loss:220.31427001953125\n",
      "1327/3000 train_loss: 34.300601959228516 test_loss:214.6368408203125\n",
      "1328/3000 train_loss: 36.85038757324219 test_loss:215.4259796142578\n",
      "1329/3000 train_loss: 34.485103607177734 test_loss:211.2407684326172\n",
      "1330/3000 train_loss: 40.476627349853516 test_loss:222.3319549560547\n",
      "1331/3000 train_loss: 39.68560791015625 test_loss:210.5169677734375\n",
      "1332/3000 train_loss: 30.437559127807617 test_loss:207.52557373046875\n",
      "1333/3000 train_loss: 33.857948303222656 test_loss:212.41162109375\n",
      "1334/3000 train_loss: 32.0616340637207 test_loss:210.09640502929688\n",
      "1335/3000 train_loss: 27.518774032592773 test_loss:214.21189880371094\n",
      "1336/3000 train_loss: 28.32420539855957 test_loss:210.9755859375\n",
      "1337/3000 train_loss: 35.98078536987305 test_loss:226.69158935546875\n",
      "1338/3000 train_loss: 34.964900970458984 test_loss:215.13568115234375\n",
      "1339/3000 train_loss: 30.706310272216797 test_loss:210.69955444335938\n",
      "1340/3000 train_loss: 27.84181785583496 test_loss:217.13894653320312\n",
      "1341/3000 train_loss: 30.193235397338867 test_loss:208.85643005371094\n",
      "1342/3000 train_loss: 30.3704833984375 test_loss:210.68479919433594\n",
      "1343/3000 train_loss: 32.85789108276367 test_loss:207.02377319335938\n",
      "1344/3000 train_loss: 32.34943389892578 test_loss:212.132568359375\n",
      "1345/3000 train_loss: 37.3740234375 test_loss:220.21109008789062\n",
      "1346/3000 train_loss: 29.4505558013916 test_loss:216.1878662109375\n",
      "1347/3000 train_loss: 27.557315826416016 test_loss:218.80067443847656\n",
      "1348/3000 train_loss: 31.337602615356445 test_loss:207.6915283203125\n",
      "1349/3000 train_loss: 30.439388275146484 test_loss:216.38809204101562\n",
      "1350/3000 train_loss: 27.731664657592773 test_loss:209.54754638671875\n",
      "1351/3000 train_loss: 32.15789794921875 test_loss:213.0106658935547\n",
      "1352/3000 train_loss: 36.143585205078125 test_loss:206.99508666992188\n",
      "1353/3000 train_loss: 42.92399597167969 test_loss:215.53488159179688\n",
      "1354/3000 train_loss: 32.802635192871094 test_loss:207.23233032226562\n",
      "1355/3000 train_loss: 32.2432975769043 test_loss:221.52850341796875\n",
      "1356/3000 train_loss: 32.25152587890625 test_loss:219.1038818359375\n",
      "1357/3000 train_loss: 33.970672607421875 test_loss:208.45721435546875\n",
      "1358/3000 train_loss: 31.488296508789062 test_loss:207.86724853515625\n",
      "1359/3000 train_loss: 29.539066314697266 test_loss:218.12353515625\n",
      "1360/3000 train_loss: 33.06511306762695 test_loss:212.96096801757812\n",
      "1361/3000 train_loss: 34.530029296875 test_loss:215.93145751953125\n",
      "1362/3000 train_loss: 33.55162811279297 test_loss:214.1816864013672\n",
      "1363/3000 train_loss: 26.864234924316406 test_loss:217.88429260253906\n",
      "1364/3000 train_loss: 29.827085494995117 test_loss:218.99728393554688\n",
      "1365/3000 train_loss: 34.222015380859375 test_loss:213.8080291748047\n",
      "1366/3000 train_loss: 36.419921875 test_loss:211.657470703125\n",
      "1367/3000 train_loss: 32.70219802856445 test_loss:221.42279052734375\n",
      "1368/3000 train_loss: 28.0864315032959 test_loss:209.03955078125\n",
      "1369/3000 train_loss: 33.71110534667969 test_loss:219.25164794921875\n",
      "1370/3000 train_loss: 31.662267684936523 test_loss:214.7165069580078\n",
      "1371/3000 train_loss: 30.903011322021484 test_loss:206.7398681640625\n",
      "1372/3000 train_loss: 44.70481872558594 test_loss:217.45870971679688\n",
      "1373/3000 train_loss: 35.27568817138672 test_loss:224.98922729492188\n",
      "1374/3000 train_loss: 37.86946487426758 test_loss:212.60708618164062\n",
      "1375/3000 train_loss: 32.860931396484375 test_loss:214.08863830566406\n",
      "1376/3000 train_loss: 27.76375961303711 test_loss:225.34686279296875\n",
      "1377/3000 train_loss: 32.16893768310547 test_loss:210.90188598632812\n",
      "1378/3000 train_loss: 29.915573120117188 test_loss:220.60711669921875\n",
      "1379/3000 train_loss: 30.478914260864258 test_loss:217.72198486328125\n",
      "1380/3000 train_loss: 27.071014404296875 test_loss:221.43392944335938\n",
      "1381/3000 train_loss: 29.893959045410156 test_loss:218.36550903320312\n",
      "1382/3000 train_loss: 29.750194549560547 test_loss:212.99203491210938\n",
      "1383/3000 train_loss: 26.069711685180664 test_loss:215.77792358398438\n",
      "1384/3000 train_loss: 31.32175064086914 test_loss:208.0317840576172\n",
      "1385/3000 train_loss: 27.658754348754883 test_loss:209.70098876953125\n",
      "1386/3000 train_loss: 30.179601669311523 test_loss:219.98736572265625\n",
      "1387/3000 train_loss: 40.809478759765625 test_loss:205.774658203125\n",
      "1388/3000 train_loss: 32.79168701171875 test_loss:219.24539184570312\n",
      "1389/3000 train_loss: 27.63431167602539 test_loss:217.79391479492188\n",
      "1390/3000 train_loss: 27.464820861816406 test_loss:211.5849151611328\n",
      "1391/3000 train_loss: 31.03151512145996 test_loss:220.3782958984375\n",
      "1392/3000 train_loss: 26.81566619873047 test_loss:214.16604614257812\n",
      "1393/3000 train_loss: 26.951936721801758 test_loss:216.09735107421875\n",
      "1394/3000 train_loss: 29.539602279663086 test_loss:230.23118591308594\n",
      "1395/3000 train_loss: 28.698938369750977 test_loss:213.260498046875\n",
      "1396/3000 train_loss: 31.955787658691406 test_loss:214.53970336914062\n",
      "1397/3000 train_loss: 29.384084701538086 test_loss:216.84832763671875\n",
      "1398/3000 train_loss: 32.59075164794922 test_loss:211.57119750976562\n",
      "1399/3000 train_loss: 29.656885147094727 test_loss:223.0653839111328\n",
      "1400/3000 train_loss: 29.866931915283203 test_loss:211.7720947265625\n",
      "1401/3000 train_loss: 30.227880477905273 test_loss:213.2975311279297\n",
      "1402/3000 train_loss: 33.68816375732422 test_loss:218.89288330078125\n",
      "1403/3000 train_loss: 30.473787307739258 test_loss:212.58734130859375\n",
      "1404/3000 train_loss: 31.74224281311035 test_loss:215.63467407226562\n",
      "1405/3000 train_loss: 27.721271514892578 test_loss:212.08973693847656\n",
      "1406/3000 train_loss: 28.68012809753418 test_loss:219.06101989746094\n",
      "1407/3000 train_loss: 27.144515991210938 test_loss:207.53952026367188\n",
      "1408/3000 train_loss: 26.07179832458496 test_loss:215.42605590820312\n",
      "1409/3000 train_loss: 26.433151245117188 test_loss:215.12701416015625\n",
      "1410/3000 train_loss: 30.17915916442871 test_loss:211.93455505371094\n",
      "1411/3000 train_loss: 29.258466720581055 test_loss:210.1300506591797\n",
      "1412/3000 train_loss: 29.627717971801758 test_loss:218.72418212890625\n",
      "1413/3000 train_loss: 35.816680908203125 test_loss:214.6693115234375\n",
      "1414/3000 train_loss: 36.765445709228516 test_loss:214.97988891601562\n",
      "1415/3000 train_loss: 33.067081451416016 test_loss:203.67242431640625\n",
      "1416/3000 train_loss: 29.863967895507812 test_loss:225.78500366210938\n",
      "1417/3000 train_loss: 33.71760177612305 test_loss:216.94671630859375\n",
      "1418/3000 train_loss: 29.90730094909668 test_loss:215.47381591796875\n",
      "1419/3000 train_loss: 30.480009078979492 test_loss:214.73910522460938\n",
      "1420/3000 train_loss: 29.83124542236328 test_loss:209.34197998046875\n",
      "1421/3000 train_loss: 26.769100189208984 test_loss:209.5177764892578\n",
      "1422/3000 train_loss: 26.46592140197754 test_loss:212.80740356445312\n",
      "1423/3000 train_loss: 27.679616928100586 test_loss:208.93612670898438\n",
      "1424/3000 train_loss: 26.321428298950195 test_loss:210.15005493164062\n",
      "1425/3000 train_loss: 25.657743453979492 test_loss:216.57589721679688\n",
      "1426/3000 train_loss: 26.915321350097656 test_loss:212.6688690185547\n",
      "1427/3000 train_loss: 28.2362060546875 test_loss:209.6584014892578\n",
      "1428/3000 train_loss: 27.22795295715332 test_loss:219.76425170898438\n",
      "1429/3000 train_loss: 27.7574405670166 test_loss:215.3277587890625\n",
      "1430/3000 train_loss: 27.051971435546875 test_loss:220.9098663330078\n",
      "1431/3000 train_loss: 30.23349380493164 test_loss:211.73931884765625\n",
      "1432/3000 train_loss: 30.04506492614746 test_loss:211.34490966796875\n",
      "1433/3000 train_loss: 28.61424446105957 test_loss:217.99110412597656\n",
      "1434/3000 train_loss: 25.283981323242188 test_loss:218.7848358154297\n",
      "1435/3000 train_loss: 25.971017837524414 test_loss:212.99349975585938\n",
      "1436/3000 train_loss: 32.364349365234375 test_loss:218.74937438964844\n",
      "1437/3000 train_loss: 38.81642532348633 test_loss:202.6492919921875\n",
      "1438/3000 train_loss: 29.467391967773438 test_loss:231.40333557128906\n",
      "1439/3000 train_loss: 29.255062103271484 test_loss:215.29710388183594\n",
      "1440/3000 train_loss: 28.865148544311523 test_loss:219.51388549804688\n",
      "1441/3000 train_loss: 30.75737190246582 test_loss:214.0005340576172\n",
      "1442/3000 train_loss: 37.70333480834961 test_loss:217.54379272460938\n",
      "1443/3000 train_loss: 34.64165115356445 test_loss:221.91542053222656\n",
      "1444/3000 train_loss: 36.16933822631836 test_loss:210.86819458007812\n",
      "1445/3000 train_loss: 30.161113739013672 test_loss:215.2132568359375\n",
      "1446/3000 train_loss: 30.266023635864258 test_loss:208.9499053955078\n",
      "1447/3000 train_loss: 25.718801498413086 test_loss:211.14047241210938\n",
      "1448/3000 train_loss: 27.729068756103516 test_loss:214.9099884033203\n",
      "1449/3000 train_loss: 27.86690330505371 test_loss:213.5513153076172\n",
      "1450/3000 train_loss: 27.891782760620117 test_loss:213.02297973632812\n",
      "1451/3000 train_loss: 28.22501564025879 test_loss:216.57827758789062\n",
      "1452/3000 train_loss: 28.24431037902832 test_loss:224.62091064453125\n",
      "1453/3000 train_loss: 28.300765991210938 test_loss:214.95321655273438\n",
      "1454/3000 train_loss: 32.23735046386719 test_loss:213.94163513183594\n",
      "1455/3000 train_loss: 28.402259826660156 test_loss:212.173095703125\n",
      "1456/3000 train_loss: 27.419628143310547 test_loss:216.15679931640625\n",
      "1457/3000 train_loss: 27.53919219970703 test_loss:214.47940063476562\n",
      "1458/3000 train_loss: 24.841699600219727 test_loss:213.41970825195312\n",
      "1459/3000 train_loss: 25.103092193603516 test_loss:218.6204833984375\n",
      "1460/3000 train_loss: 27.805768966674805 test_loss:221.5702667236328\n",
      "1461/3000 train_loss: 30.15500831604004 test_loss:214.96629333496094\n",
      "1462/3000 train_loss: 29.769922256469727 test_loss:213.48348999023438\n",
      "1463/3000 train_loss: 27.67125701904297 test_loss:217.63604736328125\n",
      "1464/3000 train_loss: 30.87040901184082 test_loss:217.91415405273438\n",
      "1465/3000 train_loss: 28.378198623657227 test_loss:219.56005859375\n",
      "1466/3000 train_loss: 29.54927635192871 test_loss:218.79031372070312\n",
      "1467/3000 train_loss: 28.73922348022461 test_loss:216.557861328125\n",
      "1468/3000 train_loss: 26.822376251220703 test_loss:212.05308532714844\n",
      "1469/3000 train_loss: 27.562192916870117 test_loss:216.51754760742188\n",
      "1470/3000 train_loss: 34.38394546508789 test_loss:210.12847900390625\n",
      "1471/3000 train_loss: 28.690473556518555 test_loss:207.06927490234375\n",
      "1472/3000 train_loss: 25.837635040283203 test_loss:215.183837890625\n",
      "1473/3000 train_loss: 29.505605697631836 test_loss:213.82696533203125\n",
      "1474/3000 train_loss: 26.44569206237793 test_loss:211.02725219726562\n",
      "1475/3000 train_loss: 41.620277404785156 test_loss:219.67848205566406\n",
      "1476/3000 train_loss: 25.73686408996582 test_loss:215.01028442382812\n",
      "1477/3000 train_loss: 34.62055969238281 test_loss:211.15806579589844\n",
      "1478/3000 train_loss: 33.15370559692383 test_loss:222.93972778320312\n",
      "1479/3000 train_loss: 25.966524124145508 test_loss:215.03370666503906\n",
      "1480/3000 train_loss: 32.00443649291992 test_loss:212.77456665039062\n",
      "1481/3000 train_loss: 32.37568283081055 test_loss:217.0011444091797\n",
      "1482/3000 train_loss: 31.495412826538086 test_loss:213.057861328125\n",
      "1483/3000 train_loss: 28.876516342163086 test_loss:213.6000518798828\n",
      "1484/3000 train_loss: 27.628021240234375 test_loss:220.03787231445312\n",
      "1485/3000 train_loss: 25.301034927368164 test_loss:219.16671752929688\n",
      "1486/3000 train_loss: 32.31781005859375 test_loss:209.56475830078125\n",
      "1487/3000 train_loss: 26.585664749145508 test_loss:214.2278289794922\n",
      "1488/3000 train_loss: 28.240678787231445 test_loss:216.20001220703125\n",
      "1489/3000 train_loss: 30.893205642700195 test_loss:213.23153686523438\n",
      "1490/3000 train_loss: 27.808780670166016 test_loss:214.88931274414062\n",
      "1491/3000 train_loss: 27.610532760620117 test_loss:214.81385803222656\n",
      "1492/3000 train_loss: 28.564495086669922 test_loss:214.6155242919922\n",
      "1493/3000 train_loss: 26.18889045715332 test_loss:212.3206024169922\n",
      "1494/3000 train_loss: 31.812294006347656 test_loss:215.4062042236328\n",
      "1495/3000 train_loss: 26.75849151611328 test_loss:209.0361785888672\n",
      "1496/3000 train_loss: 26.42755126953125 test_loss:213.49075317382812\n",
      "1497/3000 train_loss: 27.676273345947266 test_loss:206.10255432128906\n",
      "1498/3000 train_loss: 27.261150360107422 test_loss:207.65457153320312\n",
      "1499/3000 train_loss: 29.746435165405273 test_loss:212.30177307128906\n",
      "1500/3000 train_loss: 25.870893478393555 test_loss:211.3544158935547\n",
      "1501/3000 train_loss: 26.724523544311523 test_loss:214.65744018554688\n",
      "1502/3000 train_loss: 26.29563331604004 test_loss:210.32833862304688\n",
      "1503/3000 train_loss: 26.29703712463379 test_loss:213.02734375\n",
      "1504/3000 train_loss: 27.980567932128906 test_loss:216.37684631347656\n",
      "1505/3000 train_loss: 24.130495071411133 test_loss:206.3465576171875\n",
      "1506/3000 train_loss: 27.80282211303711 test_loss:211.11676025390625\n",
      "1507/3000 train_loss: 27.051692962646484 test_loss:215.67559814453125\n",
      "1508/3000 train_loss: 28.365236282348633 test_loss:219.10614013671875\n",
      "1509/3000 train_loss: 25.711713790893555 test_loss:207.23345947265625\n",
      "1510/3000 train_loss: 24.606199264526367 test_loss:212.42568969726562\n",
      "1511/3000 train_loss: 27.927734375 test_loss:206.4401397705078\n",
      "1512/3000 train_loss: 30.552387237548828 test_loss:208.4783935546875\n",
      "1513/3000 train_loss: 30.892406463623047 test_loss:220.015625\n",
      "1514/3000 train_loss: 26.064210891723633 test_loss:206.01919555664062\n",
      "1515/3000 train_loss: 30.922983169555664 test_loss:209.74937438964844\n",
      "1516/3000 train_loss: 24.750343322753906 test_loss:206.10081481933594\n",
      "1517/3000 train_loss: 27.950273513793945 test_loss:217.38113403320312\n",
      "1518/3000 train_loss: 31.29511260986328 test_loss:197.37954711914062\n",
      "1519/3000 train_loss: 30.229127883911133 test_loss:213.05499267578125\n",
      "1520/3000 train_loss: 30.14987564086914 test_loss:205.91903686523438\n",
      "1521/3000 train_loss: 24.7775821685791 test_loss:205.66571044921875\n",
      "1522/3000 train_loss: 23.112140655517578 test_loss:204.2264404296875\n",
      "1523/3000 train_loss: 28.02731704711914 test_loss:205.50350952148438\n",
      "1524/3000 train_loss: 24.218036651611328 test_loss:209.37387084960938\n",
      "1525/3000 train_loss: 28.77326011657715 test_loss:208.02981567382812\n",
      "1526/3000 train_loss: 25.802629470825195 test_loss:206.51551818847656\n",
      "1527/3000 train_loss: 26.17917823791504 test_loss:211.30587768554688\n",
      "1528/3000 train_loss: 24.432825088500977 test_loss:199.89492797851562\n",
      "1529/3000 train_loss: 31.684335708618164 test_loss:213.9047088623047\n",
      "1530/3000 train_loss: 35.42426300048828 test_loss:206.52081298828125\n",
      "1531/3000 train_loss: 26.472694396972656 test_loss:205.0310516357422\n",
      "1532/3000 train_loss: 24.560956954956055 test_loss:215.12213134765625\n",
      "1533/3000 train_loss: 23.922344207763672 test_loss:207.890625\n",
      "1534/3000 train_loss: 24.40611457824707 test_loss:204.03167724609375\n",
      "1535/3000 train_loss: 31.34837532043457 test_loss:223.06134033203125\n",
      "1536/3000 train_loss: 24.181119918823242 test_loss:207.15292358398438\n",
      "1537/3000 train_loss: 26.847421646118164 test_loss:214.42633056640625\n",
      "1538/3000 train_loss: 23.38999366760254 test_loss:206.97303771972656\n",
      "1539/3000 train_loss: 26.711950302124023 test_loss:206.94528198242188\n",
      "1540/3000 train_loss: 25.03346061706543 test_loss:212.46656799316406\n",
      "1541/3000 train_loss: 26.315357208251953 test_loss:209.09457397460938\n",
      "1542/3000 train_loss: 25.075634002685547 test_loss:207.95596313476562\n",
      "1543/3000 train_loss: 27.834863662719727 test_loss:219.31246948242188\n",
      "1544/3000 train_loss: 31.401636123657227 test_loss:203.12684631347656\n",
      "1545/3000 train_loss: 26.66305923461914 test_loss:223.3974609375\n",
      "1546/3000 train_loss: 27.887041091918945 test_loss:216.42864990234375\n",
      "1547/3000 train_loss: 24.726436614990234 test_loss:206.80247497558594\n",
      "1548/3000 train_loss: 29.891653060913086 test_loss:217.30303955078125\n",
      "1549/3000 train_loss: 28.33783721923828 test_loss:206.6361083984375\n",
      "1550/3000 train_loss: 22.883195877075195 test_loss:212.96047973632812\n",
      "1551/3000 train_loss: 31.127038955688477 test_loss:206.00128173828125\n",
      "1552/3000 train_loss: 27.634199142456055 test_loss:213.6836700439453\n",
      "1553/3000 train_loss: 24.718107223510742 test_loss:207.21685791015625\n",
      "1554/3000 train_loss: 28.413639068603516 test_loss:224.0775146484375\n",
      "1555/3000 train_loss: 33.70172119140625 test_loss:211.21389770507812\n",
      "1556/3000 train_loss: 28.274755477905273 test_loss:215.49484252929688\n",
      "1557/3000 train_loss: 31.109664916992188 test_loss:218.02896118164062\n",
      "1558/3000 train_loss: 27.931392669677734 test_loss:204.22662353515625\n",
      "1559/3000 train_loss: 24.84659767150879 test_loss:212.6945343017578\n",
      "1560/3000 train_loss: 27.960906982421875 test_loss:208.8397674560547\n",
      "1561/3000 train_loss: 23.24095916748047 test_loss:211.36795043945312\n",
      "1562/3000 train_loss: 27.015106201171875 test_loss:203.32058715820312\n",
      "1563/3000 train_loss: 26.356813430786133 test_loss:207.89266967773438\n",
      "1564/3000 train_loss: 25.72749900817871 test_loss:201.62261962890625\n",
      "1565/3000 train_loss: 24.05417823791504 test_loss:206.68418884277344\n",
      "1566/3000 train_loss: 26.418010711669922 test_loss:209.2847900390625\n",
      "1567/3000 train_loss: 24.40679931640625 test_loss:209.13323974609375\n",
      "1568/3000 train_loss: 24.649761199951172 test_loss:206.79193115234375\n",
      "1569/3000 train_loss: 32.368988037109375 test_loss:212.2694091796875\n",
      "1570/3000 train_loss: 27.253520965576172 test_loss:206.0518798828125\n",
      "1571/3000 train_loss: 26.272096633911133 test_loss:211.29391479492188\n",
      "1572/3000 train_loss: 28.182493209838867 test_loss:213.96449279785156\n",
      "1573/3000 train_loss: 28.515338897705078 test_loss:207.37771606445312\n",
      "1574/3000 train_loss: 24.747337341308594 test_loss:210.6016387939453\n",
      "1575/3000 train_loss: 27.649293899536133 test_loss:205.0462188720703\n",
      "1576/3000 train_loss: 25.434770584106445 test_loss:208.96890258789062\n",
      "1577/3000 train_loss: 28.001358032226562 test_loss:206.90162658691406\n",
      "1578/3000 train_loss: 25.789533615112305 test_loss:210.82049560546875\n",
      "1579/3000 train_loss: 28.0052547454834 test_loss:202.9051055908203\n",
      "1580/3000 train_loss: 28.39013671875 test_loss:203.41571044921875\n",
      "1581/3000 train_loss: 20.659038543701172 test_loss:212.61813354492188\n",
      "1582/3000 train_loss: 30.348888397216797 test_loss:207.71353149414062\n",
      "1583/3000 train_loss: 28.341960906982422 test_loss:207.43301391601562\n",
      "1584/3000 train_loss: 25.98200798034668 test_loss:204.286865234375\n",
      "1585/3000 train_loss: 32.83280563354492 test_loss:210.10501098632812\n",
      "1586/3000 train_loss: 25.07428741455078 test_loss:200.27182006835938\n",
      "1587/3000 train_loss: 24.560897827148438 test_loss:218.52085876464844\n",
      "1588/3000 train_loss: 25.811874389648438 test_loss:200.85281372070312\n",
      "1589/3000 train_loss: 24.23711585998535 test_loss:215.21249389648438\n",
      "1590/3000 train_loss: 26.48338508605957 test_loss:218.4434814453125\n",
      "1591/3000 train_loss: 23.844694137573242 test_loss:210.8792724609375\n",
      "1592/3000 train_loss: 24.55938148498535 test_loss:207.06210327148438\n",
      "1593/3000 train_loss: 23.251848220825195 test_loss:210.18829345703125\n",
      "1594/3000 train_loss: 24.011857986450195 test_loss:198.91229248046875\n",
      "1595/3000 train_loss: 27.82906723022461 test_loss:216.43014526367188\n",
      "1596/3000 train_loss: 25.383182525634766 test_loss:206.768310546875\n",
      "1597/3000 train_loss: 29.594043731689453 test_loss:206.0846405029297\n",
      "1598/3000 train_loss: 25.957199096679688 test_loss:211.33526611328125\n",
      "1599/3000 train_loss: 22.151756286621094 test_loss:203.3448486328125\n",
      "1600/3000 train_loss: 26.6511287689209 test_loss:207.54098510742188\n",
      "1601/3000 train_loss: 23.23223876953125 test_loss:208.8214569091797\n",
      "1602/3000 train_loss: 26.455183029174805 test_loss:212.12588500976562\n",
      "1603/3000 train_loss: 24.393402099609375 test_loss:206.29208374023438\n",
      "1604/3000 train_loss: 26.331552505493164 test_loss:210.19049072265625\n",
      "1605/3000 train_loss: 26.24291229248047 test_loss:199.05264282226562\n",
      "1606/3000 train_loss: 24.200960159301758 test_loss:205.46273803710938\n",
      "1607/3000 train_loss: 23.861865997314453 test_loss:205.33206176757812\n",
      "1608/3000 train_loss: 22.02887725830078 test_loss:214.97113037109375\n",
      "1609/3000 train_loss: 25.68598747253418 test_loss:207.03240966796875\n",
      "1610/3000 train_loss: 29.92240333557129 test_loss:213.38623046875\n",
      "1611/3000 train_loss: 23.903911590576172 test_loss:214.71343994140625\n",
      "1612/3000 train_loss: 39.04254913330078 test_loss:206.229736328125\n",
      "1613/3000 train_loss: 26.619121551513672 test_loss:215.29074096679688\n",
      "1614/3000 train_loss: 26.621633529663086 test_loss:212.60946655273438\n",
      "1615/3000 train_loss: 27.055923461914062 test_loss:212.6117401123047\n",
      "1616/3000 train_loss: 25.758817672729492 test_loss:207.044677734375\n",
      "1617/3000 train_loss: 25.541852951049805 test_loss:211.83251953125\n",
      "1618/3000 train_loss: 23.500865936279297 test_loss:207.1404266357422\n",
      "1619/3000 train_loss: 30.92854118347168 test_loss:207.18182373046875\n",
      "1620/3000 train_loss: 25.641727447509766 test_loss:201.56793212890625\n",
      "1621/3000 train_loss: 21.885284423828125 test_loss:207.7328643798828\n",
      "1622/3000 train_loss: 27.30351448059082 test_loss:202.84771728515625\n",
      "1623/3000 train_loss: 50.54553985595703 test_loss:207.89999389648438\n",
      "1624/3000 train_loss: 27.467418670654297 test_loss:201.98583984375\n",
      "1625/3000 train_loss: 30.963260650634766 test_loss:206.52645874023438\n",
      "1626/3000 train_loss: 29.51569366455078 test_loss:199.04220581054688\n",
      "1627/3000 train_loss: 26.94634437561035 test_loss:207.95950317382812\n",
      "1628/3000 train_loss: 23.783580780029297 test_loss:200.02943420410156\n",
      "1629/3000 train_loss: 30.466997146606445 test_loss:207.3868865966797\n",
      "1630/3000 train_loss: 25.97211265563965 test_loss:209.97422790527344\n",
      "1631/3000 train_loss: 26.46564483642578 test_loss:202.65872192382812\n",
      "1632/3000 train_loss: 28.89374542236328 test_loss:211.71652221679688\n",
      "1633/3000 train_loss: 25.66188621520996 test_loss:207.56089782714844\n",
      "1634/3000 train_loss: 27.50151824951172 test_loss:219.29197692871094\n",
      "1635/3000 train_loss: 28.04012680053711 test_loss:207.18157958984375\n",
      "1636/3000 train_loss: 26.344717025756836 test_loss:210.36280822753906\n",
      "1637/3000 train_loss: 30.683361053466797 test_loss:204.27816772460938\n",
      "1638/3000 train_loss: 26.04558753967285 test_loss:215.27349853515625\n",
      "1639/3000 train_loss: 24.908187866210938 test_loss:218.42666625976562\n",
      "1640/3000 train_loss: 23.737510681152344 test_loss:210.67189025878906\n",
      "1641/3000 train_loss: 26.546424865722656 test_loss:214.60009765625\n",
      "1642/3000 train_loss: 25.840953826904297 test_loss:212.2950439453125\n",
      "1643/3000 train_loss: 25.003843307495117 test_loss:214.54705810546875\n",
      "1644/3000 train_loss: 29.446935653686523 test_loss:208.310302734375\n",
      "1645/3000 train_loss: 27.425884246826172 test_loss:206.194580078125\n",
      "1646/3000 train_loss: 28.303218841552734 test_loss:211.5243682861328\n",
      "1647/3000 train_loss: 23.56380844116211 test_loss:204.83546447753906\n",
      "1648/3000 train_loss: 32.88838195800781 test_loss:208.45477294921875\n",
      "1649/3000 train_loss: 26.87107276916504 test_loss:206.02342224121094\n",
      "1650/3000 train_loss: 30.918947219848633 test_loss:203.16346740722656\n",
      "1651/3000 train_loss: 24.76166343688965 test_loss:206.67056274414062\n",
      "1652/3000 train_loss: 22.131277084350586 test_loss:204.3817138671875\n",
      "1653/3000 train_loss: 22.882831573486328 test_loss:203.96495056152344\n",
      "1654/3000 train_loss: 28.218467712402344 test_loss:204.3740692138672\n",
      "1655/3000 train_loss: 30.816328048706055 test_loss:204.83050537109375\n",
      "1656/3000 train_loss: 25.267641067504883 test_loss:209.063720703125\n",
      "1657/3000 train_loss: 24.444000244140625 test_loss:208.134033203125\n",
      "1658/3000 train_loss: 25.188844680786133 test_loss:204.3533172607422\n",
      "1659/3000 train_loss: 24.330772399902344 test_loss:199.39395141601562\n",
      "1660/3000 train_loss: 20.520641326904297 test_loss:211.69354248046875\n",
      "1661/3000 train_loss: 26.158872604370117 test_loss:201.18441772460938\n",
      "1662/3000 train_loss: 27.3995361328125 test_loss:204.3223419189453\n",
      "1663/3000 train_loss: 24.656362533569336 test_loss:207.8554229736328\n",
      "1664/3000 train_loss: 29.614627838134766 test_loss:221.66781616210938\n",
      "1665/3000 train_loss: 31.286710739135742 test_loss:216.7910614013672\n",
      "1666/3000 train_loss: 27.355323791503906 test_loss:206.79403686523438\n",
      "1667/3000 train_loss: 26.69529914855957 test_loss:215.3614501953125\n",
      "1668/3000 train_loss: 32.41856002807617 test_loss:208.5354766845703\n",
      "1669/3000 train_loss: 24.63983917236328 test_loss:207.4603271484375\n",
      "1670/3000 train_loss: 25.490360260009766 test_loss:207.70620727539062\n",
      "1671/3000 train_loss: 22.921852111816406 test_loss:210.04754638671875\n",
      "1672/3000 train_loss: 27.127487182617188 test_loss:217.41200256347656\n",
      "1673/3000 train_loss: 23.52716064453125 test_loss:206.20802307128906\n",
      "1674/3000 train_loss: 24.589332580566406 test_loss:212.48240661621094\n",
      "1675/3000 train_loss: 22.671300888061523 test_loss:206.91644287109375\n",
      "1676/3000 train_loss: 32.52297592163086 test_loss:202.72735595703125\n",
      "1677/3000 train_loss: 23.221952438354492 test_loss:211.77011108398438\n",
      "1678/3000 train_loss: 25.285491943359375 test_loss:206.767333984375\n",
      "1679/3000 train_loss: 23.689332962036133 test_loss:207.22857666015625\n",
      "1680/3000 train_loss: 24.26051902770996 test_loss:211.38772583007812\n",
      "1681/3000 train_loss: 30.02305793762207 test_loss:203.07199096679688\n",
      "1682/3000 train_loss: 28.11601448059082 test_loss:206.98974609375\n",
      "1683/3000 train_loss: 27.093706130981445 test_loss:206.54953002929688\n",
      "1684/3000 train_loss: 28.385282516479492 test_loss:205.13294982910156\n",
      "1685/3000 train_loss: 28.870494842529297 test_loss:212.78138732910156\n",
      "1686/3000 train_loss: 25.69491958618164 test_loss:208.848388671875\n",
      "1687/3000 train_loss: 22.1263427734375 test_loss:210.12786865234375\n",
      "1688/3000 train_loss: 26.754043579101562 test_loss:207.77862548828125\n",
      "1689/3000 train_loss: 25.545625686645508 test_loss:210.011962890625\n",
      "1690/3000 train_loss: 27.494720458984375 test_loss:206.49932861328125\n",
      "1691/3000 train_loss: 24.795734405517578 test_loss:203.805908203125\n",
      "1692/3000 train_loss: 27.34754180908203 test_loss:208.15264892578125\n",
      "1693/3000 train_loss: 27.57992172241211 test_loss:211.74160766601562\n",
      "1694/3000 train_loss: 27.107666015625 test_loss:209.12879943847656\n",
      "1695/3000 train_loss: 35.196502685546875 test_loss:226.11346435546875\n",
      "1696/3000 train_loss: 28.728759765625 test_loss:212.86865234375\n",
      "1697/3000 train_loss: 22.63345718383789 test_loss:212.4071044921875\n",
      "1698/3000 train_loss: 32.25572967529297 test_loss:210.954833984375\n",
      "1699/3000 train_loss: 24.171850204467773 test_loss:215.95773315429688\n",
      "1700/3000 train_loss: 28.398820877075195 test_loss:209.5849609375\n",
      "1701/3000 train_loss: 25.44571304321289 test_loss:210.5598602294922\n",
      "1702/3000 train_loss: 25.720674514770508 test_loss:212.91082763671875\n",
      "1703/3000 train_loss: 21.258670806884766 test_loss:207.64996337890625\n",
      "1704/3000 train_loss: 23.205446243286133 test_loss:211.67962646484375\n",
      "1705/3000 train_loss: 26.11602783203125 test_loss:212.9519500732422\n",
      "1706/3000 train_loss: 24.730947494506836 test_loss:212.91851806640625\n",
      "1707/3000 train_loss: 23.062829971313477 test_loss:207.9534149169922\n",
      "1708/3000 train_loss: 26.062301635742188 test_loss:202.52670288085938\n",
      "1709/3000 train_loss: 26.0908203125 test_loss:213.47450256347656\n",
      "1710/3000 train_loss: 21.683549880981445 test_loss:217.62440490722656\n",
      "1711/3000 train_loss: 24.70193862915039 test_loss:207.53396606445312\n",
      "1712/3000 train_loss: 26.589380264282227 test_loss:211.9455108642578\n",
      "1713/3000 train_loss: 29.213764190673828 test_loss:220.15289306640625\n",
      "1714/3000 train_loss: 24.79325294494629 test_loss:210.9501953125\n",
      "1715/3000 train_loss: 25.192514419555664 test_loss:205.21694946289062\n",
      "1716/3000 train_loss: 24.593242645263672 test_loss:208.82174682617188\n",
      "1717/3000 train_loss: 24.626819610595703 test_loss:208.75656127929688\n",
      "1718/3000 train_loss: 28.349328994750977 test_loss:215.8982696533203\n",
      "1719/3000 train_loss: 26.765932083129883 test_loss:201.1561737060547\n",
      "1720/3000 train_loss: 21.71635627746582 test_loss:214.3223114013672\n",
      "1721/3000 train_loss: 24.116384506225586 test_loss:210.46295166015625\n",
      "1722/3000 train_loss: 23.53144073486328 test_loss:213.06080627441406\n",
      "1723/3000 train_loss: 21.984514236450195 test_loss:207.84359741210938\n",
      "1724/3000 train_loss: 31.23421859741211 test_loss:210.01986694335938\n",
      "1725/3000 train_loss: 27.015270233154297 test_loss:220.82855224609375\n",
      "1726/3000 train_loss: 27.30281639099121 test_loss:204.09188842773438\n",
      "1727/3000 train_loss: 25.982128143310547 test_loss:221.0756072998047\n",
      "1728/3000 train_loss: 23.374919891357422 test_loss:209.113037109375\n",
      "1729/3000 train_loss: 26.666915893554688 test_loss:207.6815185546875\n",
      "1730/3000 train_loss: 26.20897102355957 test_loss:209.61654663085938\n",
      "1731/3000 train_loss: 23.470115661621094 test_loss:205.0472412109375\n",
      "1732/3000 train_loss: 23.64775848388672 test_loss:219.20437622070312\n",
      "1733/3000 train_loss: 25.242124557495117 test_loss:204.0274200439453\n",
      "1734/3000 train_loss: 25.72480010986328 test_loss:217.18702697753906\n",
      "1735/3000 train_loss: 27.345134735107422 test_loss:211.80508422851562\n",
      "1736/3000 train_loss: 26.281898498535156 test_loss:201.46133422851562\n",
      "1737/3000 train_loss: 28.436630249023438 test_loss:206.1110382080078\n",
      "1738/3000 train_loss: 24.43893814086914 test_loss:201.46804809570312\n",
      "1739/3000 train_loss: 31.974124908447266 test_loss:211.46295166015625\n",
      "1740/3000 train_loss: 26.862110137939453 test_loss:210.03231811523438\n",
      "1741/3000 train_loss: 25.07920265197754 test_loss:205.3914337158203\n",
      "1742/3000 train_loss: 28.66523551940918 test_loss:203.69989013671875\n",
      "1743/3000 train_loss: 24.81201171875 test_loss:202.00164794921875\n",
      "1744/3000 train_loss: 26.768096923828125 test_loss:209.14300537109375\n",
      "1745/3000 train_loss: 29.09255027770996 test_loss:208.99932861328125\n",
      "1746/3000 train_loss: 24.76211166381836 test_loss:212.49072265625\n",
      "1747/3000 train_loss: 23.93604850769043 test_loss:207.64352416992188\n",
      "1748/3000 train_loss: 24.963294982910156 test_loss:203.447021484375\n",
      "1749/3000 train_loss: 22.72275161743164 test_loss:209.88543701171875\n",
      "1750/3000 train_loss: 24.158733367919922 test_loss:211.93174743652344\n",
      "1751/3000 train_loss: 26.40691566467285 test_loss:203.82196044921875\n",
      "1752/3000 train_loss: 24.157072067260742 test_loss:202.20535278320312\n",
      "1753/3000 train_loss: 24.274250030517578 test_loss:223.9946746826172\n",
      "1754/3000 train_loss: 29.259536743164062 test_loss:201.94924926757812\n",
      "1755/3000 train_loss: 25.225391387939453 test_loss:214.58139038085938\n",
      "1756/3000 train_loss: 24.6456298828125 test_loss:209.4402313232422\n",
      "1757/3000 train_loss: 21.444339752197266 test_loss:208.43060302734375\n",
      "1758/3000 train_loss: 25.438472747802734 test_loss:213.72592163085938\n",
      "1759/3000 train_loss: 26.423429489135742 test_loss:211.02188110351562\n",
      "1760/3000 train_loss: 26.876672744750977 test_loss:209.95449829101562\n",
      "1761/3000 train_loss: 27.725196838378906 test_loss:212.07211303710938\n",
      "1762/3000 train_loss: 27.880447387695312 test_loss:209.9898681640625\n",
      "1763/3000 train_loss: 22.483949661254883 test_loss:208.20257568359375\n",
      "1764/3000 train_loss: 21.831846237182617 test_loss:214.1123504638672\n",
      "1765/3000 train_loss: 23.696800231933594 test_loss:212.1715087890625\n",
      "1766/3000 train_loss: 25.485748291015625 test_loss:206.56155395507812\n",
      "1767/3000 train_loss: 22.46149253845215 test_loss:213.10128784179688\n",
      "1768/3000 train_loss: 23.940441131591797 test_loss:212.33201599121094\n",
      "1769/3000 train_loss: 23.250415802001953 test_loss:207.86007690429688\n",
      "1770/3000 train_loss: 23.066062927246094 test_loss:210.273193359375\n",
      "1771/3000 train_loss: 23.935819625854492 test_loss:209.27413940429688\n",
      "1772/3000 train_loss: 22.058698654174805 test_loss:210.01223754882812\n",
      "1773/3000 train_loss: 22.40057373046875 test_loss:211.49505615234375\n",
      "1774/3000 train_loss: 25.132015228271484 test_loss:207.5830078125\n",
      "1775/3000 train_loss: 25.672534942626953 test_loss:205.88504028320312\n",
      "1776/3000 train_loss: 25.161907196044922 test_loss:205.23390197753906\n",
      "1777/3000 train_loss: 27.43307876586914 test_loss:212.93821716308594\n",
      "1778/3000 train_loss: 21.790653228759766 test_loss:207.3960418701172\n",
      "1779/3000 train_loss: 24.002342224121094 test_loss:207.20388793945312\n",
      "1780/3000 train_loss: 22.3923397064209 test_loss:212.86373901367188\n",
      "1781/3000 train_loss: 21.041826248168945 test_loss:206.61251831054688\n",
      "1782/3000 train_loss: 22.278947830200195 test_loss:209.0633544921875\n",
      "1783/3000 train_loss: 30.261829376220703 test_loss:200.07217407226562\n",
      "1784/3000 train_loss: 26.52988624572754 test_loss:211.08529663085938\n",
      "1785/3000 train_loss: 21.619970321655273 test_loss:207.4024658203125\n",
      "1786/3000 train_loss: 21.454729080200195 test_loss:202.01904296875\n",
      "1787/3000 train_loss: 25.100645065307617 test_loss:215.9127197265625\n",
      "1788/3000 train_loss: 23.432531356811523 test_loss:205.87548828125\n",
      "1789/3000 train_loss: 26.445068359375 test_loss:209.46185302734375\n",
      "1790/3000 train_loss: 21.219621658325195 test_loss:206.28887939453125\n",
      "1791/3000 train_loss: 23.764270782470703 test_loss:212.43089294433594\n",
      "1792/3000 train_loss: 23.862220764160156 test_loss:207.8453369140625\n",
      "1793/3000 train_loss: 20.001937866210938 test_loss:205.7855224609375\n",
      "1794/3000 train_loss: 22.9249267578125 test_loss:203.21844482421875\n",
      "1795/3000 train_loss: 26.802631378173828 test_loss:211.0336151123047\n",
      "1796/3000 train_loss: 23.39423179626465 test_loss:204.71173095703125\n",
      "1797/3000 train_loss: 24.585742950439453 test_loss:199.62515258789062\n",
      "1798/3000 train_loss: 24.510311126708984 test_loss:210.08026123046875\n",
      "1799/3000 train_loss: 26.613109588623047 test_loss:210.99839782714844\n",
      "1800/3000 train_loss: 23.21884536743164 test_loss:203.3163604736328\n",
      "1801/3000 train_loss: 24.411230087280273 test_loss:205.09494018554688\n",
      "1802/3000 train_loss: 23.919240951538086 test_loss:209.662353515625\n",
      "1803/3000 train_loss: 23.59567642211914 test_loss:199.66796875\n",
      "1804/3000 train_loss: 22.479381561279297 test_loss:209.10772705078125\n",
      "1805/3000 train_loss: 25.726058959960938 test_loss:205.77297973632812\n",
      "1806/3000 train_loss: 23.600357055664062 test_loss:205.281982421875\n",
      "1807/3000 train_loss: 20.837732315063477 test_loss:209.4049530029297\n",
      "1808/3000 train_loss: 21.97948455810547 test_loss:207.4407501220703\n",
      "1809/3000 train_loss: 22.956073760986328 test_loss:209.32443237304688\n",
      "1810/3000 train_loss: 22.749677658081055 test_loss:211.3839569091797\n",
      "1811/3000 train_loss: 23.62822723388672 test_loss:210.13914489746094\n",
      "1812/3000 train_loss: 25.105573654174805 test_loss:201.5749969482422\n",
      "1813/3000 train_loss: 25.891178131103516 test_loss:213.4730224609375\n",
      "1814/3000 train_loss: 21.303808212280273 test_loss:208.8708038330078\n",
      "1815/3000 train_loss: 22.127222061157227 test_loss:203.15280151367188\n",
      "1816/3000 train_loss: 19.512428283691406 test_loss:209.50433349609375\n",
      "1817/3000 train_loss: 24.792076110839844 test_loss:194.46017456054688\n",
      "1818/3000 train_loss: 22.01845932006836 test_loss:209.0260009765625\n",
      "1819/3000 train_loss: 22.629472732543945 test_loss:201.0955810546875\n",
      "1820/3000 train_loss: 25.486732482910156 test_loss:201.8258514404297\n",
      "1821/3000 train_loss: 21.767148971557617 test_loss:210.46249389648438\n",
      "1822/3000 train_loss: 21.090484619140625 test_loss:206.40231323242188\n",
      "1823/3000 train_loss: 21.982370376586914 test_loss:209.73680114746094\n",
      "1824/3000 train_loss: 23.7967529296875 test_loss:212.800048828125\n",
      "1825/3000 train_loss: 23.19654083251953 test_loss:203.51808166503906\n",
      "1826/3000 train_loss: 20.871034622192383 test_loss:205.1304931640625\n",
      "1827/3000 train_loss: 22.70587921142578 test_loss:204.2887725830078\n",
      "1828/3000 train_loss: 21.599952697753906 test_loss:203.11280822753906\n",
      "1829/3000 train_loss: 25.387985229492188 test_loss:210.6547393798828\n",
      "1830/3000 train_loss: 24.134849548339844 test_loss:195.3134765625\n",
      "1831/3000 train_loss: 20.74501609802246 test_loss:208.82754516601562\n",
      "1832/3000 train_loss: 23.291095733642578 test_loss:205.64627075195312\n",
      "1833/3000 train_loss: 25.671993255615234 test_loss:199.83543395996094\n",
      "1834/3000 train_loss: 22.687824249267578 test_loss:198.99156188964844\n",
      "1835/3000 train_loss: 27.729902267456055 test_loss:205.47171020507812\n",
      "1836/3000 train_loss: 21.878616333007812 test_loss:199.2332000732422\n",
      "1837/3000 train_loss: 24.30755615234375 test_loss:198.35012817382812\n",
      "1838/3000 train_loss: 25.17171287536621 test_loss:200.28724670410156\n",
      "1839/3000 train_loss: 24.011192321777344 test_loss:200.24398803710938\n",
      "1840/3000 train_loss: 23.357563018798828 test_loss:204.99533081054688\n",
      "1841/3000 train_loss: 26.6450138092041 test_loss:210.8058624267578\n",
      "1842/3000 train_loss: 21.567235946655273 test_loss:206.21592712402344\n",
      "1843/3000 train_loss: 22.820472717285156 test_loss:203.64920043945312\n",
      "1844/3000 train_loss: 21.64124870300293 test_loss:206.5333251953125\n",
      "1845/3000 train_loss: 23.264381408691406 test_loss:199.2237091064453\n",
      "1846/3000 train_loss: 20.51437759399414 test_loss:207.11892700195312\n",
      "1847/3000 train_loss: 29.950035095214844 test_loss:205.171630859375\n",
      "1848/3000 train_loss: 22.509105682373047 test_loss:208.51820373535156\n",
      "1849/3000 train_loss: 22.91714096069336 test_loss:207.42572021484375\n",
      "1850/3000 train_loss: 19.878568649291992 test_loss:207.61557006835938\n",
      "1851/3000 train_loss: 26.165218353271484 test_loss:206.54608154296875\n",
      "1852/3000 train_loss: 22.684417724609375 test_loss:203.42283630371094\n",
      "1853/3000 train_loss: 22.933576583862305 test_loss:202.61944580078125\n",
      "1854/3000 train_loss: 22.68309211730957 test_loss:206.99502563476562\n",
      "1855/3000 train_loss: 22.028972625732422 test_loss:202.95413208007812\n",
      "1856/3000 train_loss: 22.18320655822754 test_loss:211.8272705078125\n",
      "1857/3000 train_loss: 22.41039276123047 test_loss:213.48153686523438\n",
      "1858/3000 train_loss: 22.24785614013672 test_loss:204.8581085205078\n",
      "1859/3000 train_loss: 23.684946060180664 test_loss:209.78866577148438\n",
      "1860/3000 train_loss: 25.27707290649414 test_loss:211.98941040039062\n",
      "1861/3000 train_loss: 23.65190315246582 test_loss:200.4566650390625\n",
      "1862/3000 train_loss: 21.662015914916992 test_loss:204.57421875\n",
      "1863/3000 train_loss: 21.44367027282715 test_loss:205.65487670898438\n",
      "1864/3000 train_loss: 20.41621971130371 test_loss:203.34095764160156\n",
      "1865/3000 train_loss: 25.033954620361328 test_loss:196.89418029785156\n",
      "1866/3000 train_loss: 24.208110809326172 test_loss:205.47732543945312\n",
      "1867/3000 train_loss: 21.42478370666504 test_loss:205.64834594726562\n",
      "1868/3000 train_loss: 23.218303680419922 test_loss:207.54527282714844\n",
      "1869/3000 train_loss: 21.49046516418457 test_loss:208.42962646484375\n",
      "1870/3000 train_loss: 22.998638153076172 test_loss:208.86102294921875\n",
      "1871/3000 train_loss: 24.012237548828125 test_loss:203.91543579101562\n",
      "1872/3000 train_loss: 24.689361572265625 test_loss:202.7846221923828\n",
      "1873/3000 train_loss: 23.48102378845215 test_loss:212.21145629882812\n",
      "1874/3000 train_loss: 24.392127990722656 test_loss:205.33041381835938\n",
      "1875/3000 train_loss: 21.18720817565918 test_loss:208.67047119140625\n",
      "1876/3000 train_loss: 18.88658905029297 test_loss:203.2666473388672\n",
      "1877/3000 train_loss: 26.722612380981445 test_loss:201.40719604492188\n",
      "1878/3000 train_loss: 24.494579315185547 test_loss:213.7562255859375\n",
      "1879/3000 train_loss: 22.75461769104004 test_loss:204.19931030273438\n",
      "1880/3000 train_loss: 23.696109771728516 test_loss:213.4294891357422\n",
      "1881/3000 train_loss: 19.889007568359375 test_loss:203.9604949951172\n",
      "1882/3000 train_loss: 29.503305435180664 test_loss:222.10243225097656\n",
      "1883/3000 train_loss: 26.60821533203125 test_loss:211.51693725585938\n",
      "1884/3000 train_loss: 26.16907501220703 test_loss:213.28976440429688\n",
      "1885/3000 train_loss: 22.545337677001953 test_loss:215.51507568359375\n",
      "1886/3000 train_loss: 21.85698699951172 test_loss:204.62405395507812\n",
      "1887/3000 train_loss: 26.42302894592285 test_loss:209.09014892578125\n",
      "1888/3000 train_loss: 23.376266479492188 test_loss:208.15228271484375\n",
      "1889/3000 train_loss: 20.30045509338379 test_loss:215.3681640625\n",
      "1890/3000 train_loss: 20.85178565979004 test_loss:209.727783203125\n",
      "1891/3000 train_loss: 19.381155014038086 test_loss:207.0181884765625\n",
      "1892/3000 train_loss: 24.390705108642578 test_loss:203.8023223876953\n",
      "1893/3000 train_loss: 23.03478240966797 test_loss:211.0502471923828\n",
      "1894/3000 train_loss: 22.603565216064453 test_loss:213.6160125732422\n",
      "1895/3000 train_loss: 19.97300148010254 test_loss:210.06137084960938\n",
      "1896/3000 train_loss: 21.56816291809082 test_loss:200.12457275390625\n",
      "1897/3000 train_loss: 23.665794372558594 test_loss:205.13864135742188\n",
      "1898/3000 train_loss: 25.281015396118164 test_loss:208.50885009765625\n",
      "1899/3000 train_loss: 23.05336570739746 test_loss:210.12039184570312\n",
      "1900/3000 train_loss: 30.579578399658203 test_loss:214.05540466308594\n",
      "1901/3000 train_loss: 25.903865814208984 test_loss:199.63064575195312\n",
      "1902/3000 train_loss: 21.90574836730957 test_loss:201.39817810058594\n",
      "1903/3000 train_loss: 22.26920509338379 test_loss:207.3687744140625\n",
      "1904/3000 train_loss: 21.324445724487305 test_loss:205.91737365722656\n",
      "1905/3000 train_loss: 18.868000030517578 test_loss:205.10540771484375\n",
      "1906/3000 train_loss: 22.647478103637695 test_loss:201.15087890625\n",
      "1907/3000 train_loss: 21.947668075561523 test_loss:206.09078979492188\n",
      "1908/3000 train_loss: 29.580820083618164 test_loss:196.99896240234375\n",
      "1909/3000 train_loss: 23.572402954101562 test_loss:211.28756713867188\n",
      "1910/3000 train_loss: 20.30264663696289 test_loss:203.7965087890625\n",
      "1911/3000 train_loss: 25.598247528076172 test_loss:200.43716430664062\n",
      "1912/3000 train_loss: 23.46834373474121 test_loss:208.07275390625\n",
      "1913/3000 train_loss: 25.061208724975586 test_loss:206.13406372070312\n",
      "1914/3000 train_loss: 22.568111419677734 test_loss:203.51858520507812\n",
      "1915/3000 train_loss: 21.856342315673828 test_loss:212.50485229492188\n",
      "1916/3000 train_loss: 21.680118560791016 test_loss:207.6246795654297\n",
      "1917/3000 train_loss: 20.955760955810547 test_loss:208.38455200195312\n",
      "1918/3000 train_loss: 19.306312561035156 test_loss:201.54962158203125\n",
      "1919/3000 train_loss: 30.943492889404297 test_loss:214.88388061523438\n",
      "1920/3000 train_loss: 29.719980239868164 test_loss:206.66763305664062\n",
      "1921/3000 train_loss: 24.54344367980957 test_loss:209.7755126953125\n",
      "1922/3000 train_loss: 24.703933715820312 test_loss:202.62603759765625\n",
      "1923/3000 train_loss: 23.964778900146484 test_loss:211.6486053466797\n",
      "1924/3000 train_loss: 25.533004760742188 test_loss:206.0068817138672\n",
      "1925/3000 train_loss: 22.68075180053711 test_loss:212.07852172851562\n",
      "1926/3000 train_loss: 20.503244400024414 test_loss:207.4801483154297\n",
      "1927/3000 train_loss: 22.122634887695312 test_loss:197.08001708984375\n",
      "1928/3000 train_loss: 19.976072311401367 test_loss:211.1184844970703\n",
      "1929/3000 train_loss: 23.374937057495117 test_loss:206.97787475585938\n",
      "1930/3000 train_loss: 17.859960556030273 test_loss:205.000732421875\n",
      "1931/3000 train_loss: 24.304153442382812 test_loss:196.50375366210938\n",
      "1932/3000 train_loss: 19.946338653564453 test_loss:206.33328247070312\n",
      "1933/3000 train_loss: 19.813261032104492 test_loss:208.6940460205078\n",
      "1934/3000 train_loss: 23.821063995361328 test_loss:202.40231323242188\n",
      "1935/3000 train_loss: 21.11551284790039 test_loss:201.53390502929688\n",
      "1936/3000 train_loss: 20.786527633666992 test_loss:214.75343322753906\n",
      "1937/3000 train_loss: 18.284543991088867 test_loss:207.91917419433594\n",
      "1938/3000 train_loss: 23.869251251220703 test_loss:206.2325897216797\n",
      "1939/3000 train_loss: 25.589502334594727 test_loss:207.18820190429688\n",
      "1940/3000 train_loss: 23.213653564453125 test_loss:220.83447265625\n",
      "1941/3000 train_loss: 20.194591522216797 test_loss:208.0562286376953\n",
      "1942/3000 train_loss: 25.03076171875 test_loss:200.06039428710938\n",
      "1943/3000 train_loss: 19.043689727783203 test_loss:207.9189453125\n",
      "1944/3000 train_loss: 24.87401580810547 test_loss:214.04461669921875\n",
      "1945/3000 train_loss: 26.307558059692383 test_loss:202.83233642578125\n",
      "1946/3000 train_loss: 24.131439208984375 test_loss:219.7696533203125\n",
      "1947/3000 train_loss: 20.903404235839844 test_loss:203.57821655273438\n",
      "1948/3000 train_loss: 24.44216537475586 test_loss:204.36849975585938\n",
      "1949/3000 train_loss: 23.472381591796875 test_loss:208.35562133789062\n",
      "1950/3000 train_loss: 21.474056243896484 test_loss:202.76742553710938\n",
      "1951/3000 train_loss: 19.207042694091797 test_loss:203.37982177734375\n",
      "1952/3000 train_loss: 23.914609909057617 test_loss:198.93345642089844\n",
      "1953/3000 train_loss: 21.745948791503906 test_loss:204.18359375\n",
      "1954/3000 train_loss: 22.011789321899414 test_loss:202.72708129882812\n",
      "1955/3000 train_loss: 24.7900390625 test_loss:201.84243774414062\n",
      "1956/3000 train_loss: 21.169708251953125 test_loss:203.03118896484375\n",
      "1957/3000 train_loss: 20.745697021484375 test_loss:199.72093200683594\n",
      "1958/3000 train_loss: 22.155956268310547 test_loss:197.31277465820312\n",
      "1959/3000 train_loss: 24.158493041992188 test_loss:198.05126953125\n",
      "1960/3000 train_loss: 24.58551025390625 test_loss:203.07350158691406\n",
      "1961/3000 train_loss: 19.81067657470703 test_loss:204.0899658203125\n",
      "1962/3000 train_loss: 21.688858032226562 test_loss:202.17904663085938\n",
      "1963/3000 train_loss: 20.021696090698242 test_loss:203.68536376953125\n",
      "1964/3000 train_loss: 16.94330406188965 test_loss:202.2153778076172\n",
      "1965/3000 train_loss: 20.384164810180664 test_loss:193.12832641601562\n",
      "1966/3000 train_loss: 21.620546340942383 test_loss:206.20916748046875\n",
      "1967/3000 train_loss: 19.412677764892578 test_loss:194.61526489257812\n",
      "1968/3000 train_loss: 28.233139038085938 test_loss:194.57534790039062\n",
      "1969/3000 train_loss: 22.065101623535156 test_loss:211.4849395751953\n",
      "1970/3000 train_loss: 18.82817840576172 test_loss:206.54031372070312\n",
      "1971/3000 train_loss: 19.50368309020996 test_loss:210.9844970703125\n",
      "1972/3000 train_loss: 20.201019287109375 test_loss:203.30641174316406\n",
      "1973/3000 train_loss: 22.456811904907227 test_loss:196.75503540039062\n",
      "1974/3000 train_loss: 20.352705001831055 test_loss:205.93179321289062\n",
      "1975/3000 train_loss: 19.5347843170166 test_loss:196.14820861816406\n",
      "1976/3000 train_loss: 22.925369262695312 test_loss:196.00347900390625\n",
      "1977/3000 train_loss: 22.93618392944336 test_loss:195.82318115234375\n",
      "1978/3000 train_loss: 19.321735382080078 test_loss:197.9471435546875\n",
      "1979/3000 train_loss: 19.42349624633789 test_loss:194.30062866210938\n",
      "1980/3000 train_loss: 24.302751541137695 test_loss:193.22293090820312\n",
      "1981/3000 train_loss: 25.38886260986328 test_loss:217.0009765625\n",
      "1982/3000 train_loss: 25.76519775390625 test_loss:192.5291290283203\n",
      "1983/3000 train_loss: 19.692245483398438 test_loss:205.94375610351562\n",
      "1984/3000 train_loss: 22.933048248291016 test_loss:191.03260803222656\n",
      "1985/3000 train_loss: 20.547279357910156 test_loss:197.9529266357422\n",
      "1986/3000 train_loss: 18.94254493713379 test_loss:203.71820068359375\n",
      "1987/3000 train_loss: 25.43218231201172 test_loss:195.460205078125\n",
      "1988/3000 train_loss: 24.669387817382812 test_loss:201.02264404296875\n",
      "1989/3000 train_loss: 20.45026206970215 test_loss:201.10830688476562\n",
      "1990/3000 train_loss: 20.222902297973633 test_loss:196.91380310058594\n",
      "1991/3000 train_loss: 23.098020553588867 test_loss:199.888671875\n",
      "1992/3000 train_loss: 26.933547973632812 test_loss:203.36624145507812\n",
      "1993/3000 train_loss: 19.41172981262207 test_loss:201.497314453125\n",
      "1994/3000 train_loss: 21.85757827758789 test_loss:191.45962524414062\n",
      "1995/3000 train_loss: 20.695154190063477 test_loss:205.9796142578125\n",
      "1996/3000 train_loss: 19.05672836303711 test_loss:202.47181701660156\n",
      "1997/3000 train_loss: 18.158246994018555 test_loss:198.485107421875\n",
      "1998/3000 train_loss: 20.4190616607666 test_loss:203.16741943359375\n",
      "1999/3000 train_loss: 21.289836883544922 test_loss:196.82449340820312\n",
      "2000/3000 train_loss: 22.00581169128418 test_loss:196.7858123779297\n",
      "2001/3000 train_loss: 20.682310104370117 test_loss:197.71710205078125\n",
      "2002/3000 train_loss: 19.935466766357422 test_loss:194.97897338867188\n",
      "2003/3000 train_loss: 21.15007972717285 test_loss:192.245849609375\n",
      "2004/3000 train_loss: 19.158721923828125 test_loss:203.23226928710938\n",
      "2005/3000 train_loss: 20.787195205688477 test_loss:192.92471313476562\n",
      "2006/3000 train_loss: 19.770130157470703 test_loss:199.784423828125\n",
      "2007/3000 train_loss: 22.772306442260742 test_loss:193.74871826171875\n",
      "2008/3000 train_loss: 24.551658630371094 test_loss:204.706787109375\n",
      "2009/3000 train_loss: 30.51898193359375 test_loss:195.65338134765625\n",
      "2010/3000 train_loss: 21.234899520874023 test_loss:211.55953979492188\n",
      "2011/3000 train_loss: 19.593852996826172 test_loss:199.822265625\n",
      "2012/3000 train_loss: 21.947538375854492 test_loss:212.4185791015625\n",
      "2013/3000 train_loss: 21.230857849121094 test_loss:209.87246704101562\n",
      "2014/3000 train_loss: 18.777599334716797 test_loss:212.87356567382812\n",
      "2015/3000 train_loss: 21.027074813842773 test_loss:201.47018432617188\n",
      "2016/3000 train_loss: 17.54686737060547 test_loss:207.77182006835938\n",
      "2017/3000 train_loss: 21.076345443725586 test_loss:206.6926727294922\n",
      "2018/3000 train_loss: 22.795652389526367 test_loss:207.54837036132812\n",
      "2019/3000 train_loss: 21.436676025390625 test_loss:214.98983764648438\n",
      "2020/3000 train_loss: 19.498703002929688 test_loss:206.11529541015625\n",
      "2021/3000 train_loss: 19.59926986694336 test_loss:198.94161987304688\n",
      "2022/3000 train_loss: 20.03464126586914 test_loss:200.75357055664062\n",
      "2023/3000 train_loss: 19.60910987854004 test_loss:203.7244415283203\n",
      "2024/3000 train_loss: 22.673667907714844 test_loss:205.50958251953125\n",
      "2025/3000 train_loss: 21.40104866027832 test_loss:199.7325439453125\n",
      "2026/3000 train_loss: 22.762968063354492 test_loss:206.76275634765625\n",
      "2027/3000 train_loss: 22.410005569458008 test_loss:211.21405029296875\n",
      "2028/3000 train_loss: 20.557605743408203 test_loss:208.22964477539062\n",
      "2029/3000 train_loss: 22.970226287841797 test_loss:205.6912384033203\n",
      "2030/3000 train_loss: 22.68889045715332 test_loss:199.26112365722656\n",
      "2031/3000 train_loss: 18.534757614135742 test_loss:196.8457489013672\n",
      "2032/3000 train_loss: 18.916015625 test_loss:200.90371704101562\n",
      "2033/3000 train_loss: 21.244306564331055 test_loss:205.71676635742188\n",
      "2034/3000 train_loss: 22.92578125 test_loss:205.8042449951172\n",
      "2035/3000 train_loss: 21.32134246826172 test_loss:206.5308837890625\n",
      "2036/3000 train_loss: 25.654396057128906 test_loss:206.90188598632812\n",
      "2037/3000 train_loss: 21.474124908447266 test_loss:205.84222412109375\n",
      "2038/3000 train_loss: 19.78460693359375 test_loss:197.5279083251953\n",
      "2039/3000 train_loss: 18.169967651367188 test_loss:203.2440948486328\n",
      "2040/3000 train_loss: 19.89304542541504 test_loss:203.97486877441406\n",
      "2041/3000 train_loss: 24.436138153076172 test_loss:203.02981567382812\n",
      "2042/3000 train_loss: 21.75829315185547 test_loss:221.34588623046875\n",
      "2043/3000 train_loss: 22.62118911743164 test_loss:202.18182373046875\n",
      "2044/3000 train_loss: 21.57658576965332 test_loss:208.7512969970703\n",
      "2045/3000 train_loss: 23.52142333984375 test_loss:213.5575408935547\n",
      "2046/3000 train_loss: 17.520034790039062 test_loss:199.28228759765625\n",
      "2047/3000 train_loss: 18.943614959716797 test_loss:206.1021728515625\n",
      "2048/3000 train_loss: 19.20326042175293 test_loss:203.19772338867188\n",
      "2049/3000 train_loss: 18.558732986450195 test_loss:201.42874145507812\n",
      "2050/3000 train_loss: 19.180492401123047 test_loss:203.91490173339844\n",
      "2051/3000 train_loss: 21.380849838256836 test_loss:208.74154663085938\n",
      "2052/3000 train_loss: 19.225194931030273 test_loss:197.62570190429688\n",
      "2053/3000 train_loss: 18.014175415039062 test_loss:205.46023559570312\n",
      "2054/3000 train_loss: 16.694944381713867 test_loss:195.20419311523438\n",
      "2055/3000 train_loss: 20.002302169799805 test_loss:207.01181030273438\n",
      "2056/3000 train_loss: 20.12220573425293 test_loss:204.1248779296875\n",
      "2057/3000 train_loss: 26.390336990356445 test_loss:195.12298583984375\n",
      "2058/3000 train_loss: 23.77059555053711 test_loss:207.3026123046875\n",
      "2059/3000 train_loss: 21.3906307220459 test_loss:201.548583984375\n",
      "2060/3000 train_loss: 22.036605834960938 test_loss:190.48655700683594\n",
      "2061/3000 train_loss: 20.53919792175293 test_loss:203.2069091796875\n",
      "2062/3000 train_loss: 20.014144897460938 test_loss:196.19305419921875\n",
      "2063/3000 train_loss: 20.758159637451172 test_loss:196.90289306640625\n",
      "2064/3000 train_loss: 18.793956756591797 test_loss:201.52297973632812\n",
      "2065/3000 train_loss: 16.430879592895508 test_loss:201.72813415527344\n",
      "2066/3000 train_loss: 17.807199478149414 test_loss:200.34088134765625\n",
      "2067/3000 train_loss: 19.60411262512207 test_loss:201.9954071044922\n",
      "2068/3000 train_loss: 20.10320472717285 test_loss:206.85067749023438\n",
      "2069/3000 train_loss: 22.771547317504883 test_loss:208.0235137939453\n",
      "2070/3000 train_loss: 18.946453094482422 test_loss:207.9124755859375\n",
      "2071/3000 train_loss: 27.89303970336914 test_loss:210.43191528320312\n",
      "2072/3000 train_loss: 19.065710067749023 test_loss:206.97467041015625\n",
      "2073/3000 train_loss: 23.545581817626953 test_loss:197.00875854492188\n",
      "2074/3000 train_loss: 20.930959701538086 test_loss:199.75076293945312\n",
      "2075/3000 train_loss: 22.199052810668945 test_loss:216.7537841796875\n",
      "2076/3000 train_loss: 22.16379165649414 test_loss:199.3812255859375\n",
      "2077/3000 train_loss: 20.73797035217285 test_loss:198.616943359375\n",
      "2078/3000 train_loss: 19.775217056274414 test_loss:203.03912353515625\n",
      "2079/3000 train_loss: 18.529964447021484 test_loss:199.74636840820312\n",
      "2080/3000 train_loss: 19.973243713378906 test_loss:214.47982788085938\n",
      "2081/3000 train_loss: 18.700542449951172 test_loss:200.11375427246094\n",
      "2082/3000 train_loss: 19.22745132446289 test_loss:204.68959045410156\n",
      "2083/3000 train_loss: 21.529788970947266 test_loss:202.86398315429688\n",
      "2084/3000 train_loss: 19.77577018737793 test_loss:208.38272094726562\n",
      "2085/3000 train_loss: 21.441415786743164 test_loss:198.73080444335938\n",
      "2086/3000 train_loss: 19.545473098754883 test_loss:209.6038360595703\n",
      "2087/3000 train_loss: 21.203279495239258 test_loss:200.6163787841797\n",
      "2088/3000 train_loss: 18.510719299316406 test_loss:199.6061248779297\n",
      "2089/3000 train_loss: 20.261146545410156 test_loss:212.13697814941406\n",
      "2090/3000 train_loss: 21.409849166870117 test_loss:202.19361877441406\n",
      "2091/3000 train_loss: 18.733152389526367 test_loss:212.53004455566406\n",
      "2092/3000 train_loss: 21.875263214111328 test_loss:204.65011596679688\n",
      "2093/3000 train_loss: 19.681148529052734 test_loss:203.19236755371094\n",
      "2094/3000 train_loss: 17.755334854125977 test_loss:202.7790069580078\n",
      "2095/3000 train_loss: 18.219736099243164 test_loss:208.69918823242188\n",
      "2096/3000 train_loss: 21.790834426879883 test_loss:191.62130737304688\n",
      "2097/3000 train_loss: 22.393892288208008 test_loss:204.3706512451172\n",
      "2098/3000 train_loss: 20.34901237487793 test_loss:199.51992797851562\n",
      "2099/3000 train_loss: 19.248252868652344 test_loss:210.44363403320312\n",
      "2100/3000 train_loss: 20.18045425415039 test_loss:197.82562255859375\n",
      "2101/3000 train_loss: 20.18735122680664 test_loss:202.98178100585938\n",
      "2102/3000 train_loss: 17.877151489257812 test_loss:193.834716796875\n",
      "2103/3000 train_loss: 24.56732749938965 test_loss:199.51255798339844\n",
      "2104/3000 train_loss: 21.752641677856445 test_loss:193.55343627929688\n",
      "2105/3000 train_loss: 22.00809097290039 test_loss:200.16854858398438\n",
      "2106/3000 train_loss: 22.835098266601562 test_loss:204.29495239257812\n",
      "2107/3000 train_loss: 22.165292739868164 test_loss:203.27127075195312\n",
      "2108/3000 train_loss: 19.49424171447754 test_loss:205.081787109375\n",
      "2109/3000 train_loss: 18.72818946838379 test_loss:207.52383422851562\n",
      "2110/3000 train_loss: 22.31637954711914 test_loss:202.74034118652344\n",
      "2111/3000 train_loss: 19.622339248657227 test_loss:202.7091064453125\n",
      "2112/3000 train_loss: 21.80716323852539 test_loss:206.59140014648438\n",
      "2113/3000 train_loss: 19.549619674682617 test_loss:209.60601806640625\n",
      "2114/3000 train_loss: 20.063310623168945 test_loss:200.9219970703125\n",
      "2115/3000 train_loss: 19.79344940185547 test_loss:207.18211364746094\n",
      "2116/3000 train_loss: 22.249855041503906 test_loss:203.9017333984375\n",
      "2117/3000 train_loss: 18.885175704956055 test_loss:203.30528259277344\n",
      "2118/3000 train_loss: 19.607343673706055 test_loss:211.50283813476562\n",
      "2119/3000 train_loss: 16.96213722229004 test_loss:200.99557495117188\n",
      "2120/3000 train_loss: 21.28467559814453 test_loss:204.53033447265625\n",
      "2121/3000 train_loss: 19.37035369873047 test_loss:200.928955078125\n",
      "2122/3000 train_loss: 23.854665756225586 test_loss:210.36050415039062\n",
      "2123/3000 train_loss: 19.70731544494629 test_loss:203.8893585205078\n",
      "2124/3000 train_loss: 27.531444549560547 test_loss:213.05029296875\n",
      "2125/3000 train_loss: 19.01422119140625 test_loss:200.52142333984375\n",
      "2126/3000 train_loss: 18.72779083251953 test_loss:205.3492431640625\n",
      "2127/3000 train_loss: 22.269351959228516 test_loss:211.34664916992188\n",
      "2128/3000 train_loss: 20.195446014404297 test_loss:196.26376342773438\n",
      "2129/3000 train_loss: 18.149518966674805 test_loss:198.6883544921875\n",
      "2130/3000 train_loss: 18.53399085998535 test_loss:201.57850646972656\n",
      "2131/3000 train_loss: 22.631650924682617 test_loss:192.36569213867188\n",
      "2132/3000 train_loss: 22.622039794921875 test_loss:205.51214599609375\n",
      "2133/3000 train_loss: 19.382408142089844 test_loss:197.80181884765625\n",
      "2134/3000 train_loss: 16.12000846862793 test_loss:200.42538452148438\n",
      "2135/3000 train_loss: 17.771270751953125 test_loss:195.52517700195312\n",
      "2136/3000 train_loss: 18.61888313293457 test_loss:206.04788208007812\n",
      "2137/3000 train_loss: 19.56483268737793 test_loss:204.66741943359375\n",
      "2138/3000 train_loss: 34.01799011230469 test_loss:210.75857543945312\n",
      "2139/3000 train_loss: 24.309131622314453 test_loss:195.5917510986328\n",
      "2140/3000 train_loss: 17.222869873046875 test_loss:203.59286499023438\n",
      "2141/3000 train_loss: 20.9279727935791 test_loss:208.34432983398438\n",
      "2142/3000 train_loss: 22.012981414794922 test_loss:203.26046752929688\n",
      "2143/3000 train_loss: 18.36815643310547 test_loss:208.21231079101562\n",
      "2144/3000 train_loss: 19.379711151123047 test_loss:196.7879180908203\n",
      "2145/3000 train_loss: 19.138071060180664 test_loss:201.2855682373047\n",
      "2146/3000 train_loss: 18.222129821777344 test_loss:201.64501953125\n",
      "2147/3000 train_loss: 19.061132431030273 test_loss:199.18020629882812\n",
      "2148/3000 train_loss: 22.007295608520508 test_loss:197.358154296875\n",
      "2149/3000 train_loss: 28.457290649414062 test_loss:198.6376953125\n",
      "2150/3000 train_loss: 19.380290985107422 test_loss:204.9885711669922\n",
      "2151/3000 train_loss: 23.006546020507812 test_loss:209.08901977539062\n",
      "2152/3000 train_loss: 19.835081100463867 test_loss:207.87330627441406\n",
      "2153/3000 train_loss: 18.743820190429688 test_loss:208.70281982421875\n",
      "2154/3000 train_loss: 21.94135856628418 test_loss:201.46388244628906\n",
      "2155/3000 train_loss: 21.296337127685547 test_loss:198.21128845214844\n",
      "2156/3000 train_loss: 18.881328582763672 test_loss:205.68292236328125\n",
      "2157/3000 train_loss: 21.1307430267334 test_loss:205.0941925048828\n",
      "2158/3000 train_loss: 25.783668518066406 test_loss:200.42886352539062\n",
      "2159/3000 train_loss: 19.172508239746094 test_loss:201.11248779296875\n",
      "2160/3000 train_loss: 19.15030860900879 test_loss:195.33204650878906\n",
      "2161/3000 train_loss: 20.43967628479004 test_loss:196.99761962890625\n",
      "2162/3000 train_loss: 17.64313316345215 test_loss:194.33148193359375\n",
      "2163/3000 train_loss: 18.933490753173828 test_loss:199.23681640625\n",
      "2164/3000 train_loss: 22.247966766357422 test_loss:196.14846801757812\n",
      "2165/3000 train_loss: 21.377492904663086 test_loss:198.31271362304688\n",
      "2166/3000 train_loss: 20.06915855407715 test_loss:205.97787475585938\n",
      "2167/3000 train_loss: 18.36292266845703 test_loss:211.87088012695312\n",
      "2168/3000 train_loss: 20.97062110900879 test_loss:197.77102661132812\n",
      "2169/3000 train_loss: 20.669639587402344 test_loss:213.26617431640625\n",
      "2170/3000 train_loss: 17.37257194519043 test_loss:202.59970092773438\n",
      "2171/3000 train_loss: 17.359710693359375 test_loss:204.04122924804688\n",
      "2172/3000 train_loss: 15.588093757629395 test_loss:200.63922119140625\n",
      "2173/3000 train_loss: 17.951505661010742 test_loss:203.2010498046875\n",
      "2174/3000 train_loss: 19.993539810180664 test_loss:206.2628936767578\n",
      "2175/3000 train_loss: 19.914396286010742 test_loss:199.5877685546875\n",
      "2176/3000 train_loss: 19.907934188842773 test_loss:195.4356689453125\n",
      "2177/3000 train_loss: 19.201913833618164 test_loss:208.34927368164062\n",
      "2178/3000 train_loss: 15.157301902770996 test_loss:209.59414672851562\n",
      "2179/3000 train_loss: 20.083223342895508 test_loss:200.121826171875\n",
      "2180/3000 train_loss: 18.016895294189453 test_loss:195.96243286132812\n",
      "2181/3000 train_loss: 20.12653923034668 test_loss:207.50721740722656\n",
      "2182/3000 train_loss: 17.820438385009766 test_loss:195.61146545410156\n",
      "2183/3000 train_loss: 22.559741973876953 test_loss:202.39352416992188\n",
      "2184/3000 train_loss: 16.430801391601562 test_loss:206.25286865234375\n",
      "2185/3000 train_loss: 16.709909439086914 test_loss:194.42984008789062\n",
      "2186/3000 train_loss: 18.800861358642578 test_loss:217.32476806640625\n",
      "2187/3000 train_loss: 17.49737548828125 test_loss:207.044677734375\n",
      "2188/3000 train_loss: 19.40033531188965 test_loss:202.15542602539062\n",
      "2189/3000 train_loss: 15.205368995666504 test_loss:200.16079711914062\n",
      "2190/3000 train_loss: 21.322078704833984 test_loss:203.0669708251953\n",
      "2191/3000 train_loss: 19.652114868164062 test_loss:197.4893798828125\n",
      "2192/3000 train_loss: 16.893409729003906 test_loss:208.58985900878906\n",
      "2193/3000 train_loss: 19.2457332611084 test_loss:193.36329650878906\n",
      "2194/3000 train_loss: 21.417768478393555 test_loss:204.5493621826172\n",
      "2195/3000 train_loss: 19.398303985595703 test_loss:205.2212677001953\n",
      "2196/3000 train_loss: 16.708837509155273 test_loss:207.7194061279297\n",
      "2197/3000 train_loss: 18.41463851928711 test_loss:201.05462646484375\n",
      "2198/3000 train_loss: 20.44394874572754 test_loss:212.35702514648438\n",
      "2199/3000 train_loss: 17.5980281829834 test_loss:199.91673278808594\n",
      "2200/3000 train_loss: 19.294475555419922 test_loss:206.46893310546875\n",
      "2201/3000 train_loss: 21.811872482299805 test_loss:204.249755859375\n",
      "2202/3000 train_loss: 15.425928115844727 test_loss:199.49221801757812\n",
      "2203/3000 train_loss: 21.058137893676758 test_loss:205.05023193359375\n",
      "2204/3000 train_loss: 17.379093170166016 test_loss:199.5429229736328\n",
      "2205/3000 train_loss: 16.771451950073242 test_loss:209.3241729736328\n",
      "2206/3000 train_loss: 18.99432373046875 test_loss:200.6593017578125\n",
      "2207/3000 train_loss: 17.47237777709961 test_loss:197.16189575195312\n",
      "2208/3000 train_loss: 18.293102264404297 test_loss:209.54833984375\n",
      "2209/3000 train_loss: 17.433382034301758 test_loss:200.81988525390625\n",
      "2210/3000 train_loss: 22.57492446899414 test_loss:207.68789672851562\n",
      "2211/3000 train_loss: 19.539316177368164 test_loss:196.0439910888672\n",
      "2212/3000 train_loss: 18.571670532226562 test_loss:202.6824188232422\n",
      "2213/3000 train_loss: 19.47037124633789 test_loss:204.82748413085938\n",
      "2214/3000 train_loss: 20.26723289489746 test_loss:198.455322265625\n",
      "2215/3000 train_loss: 16.997774124145508 test_loss:200.23338317871094\n",
      "2216/3000 train_loss: 19.344053268432617 test_loss:196.2626495361328\n",
      "2217/3000 train_loss: 19.3420352935791 test_loss:215.23654174804688\n",
      "2218/3000 train_loss: 23.305143356323242 test_loss:208.87205505371094\n",
      "2219/3000 train_loss: 20.278072357177734 test_loss:202.57720947265625\n",
      "2220/3000 train_loss: 17.59820556640625 test_loss:201.47030639648438\n",
      "2221/3000 train_loss: 17.991392135620117 test_loss:199.38461303710938\n",
      "2222/3000 train_loss: 16.661724090576172 test_loss:198.26950073242188\n",
      "2223/3000 train_loss: 18.05921173095703 test_loss:204.59303283691406\n",
      "2224/3000 train_loss: 19.279796600341797 test_loss:201.08102416992188\n",
      "2225/3000 train_loss: 21.887723922729492 test_loss:206.54095458984375\n",
      "2226/3000 train_loss: 19.695568084716797 test_loss:199.29354858398438\n",
      "2227/3000 train_loss: 17.094390869140625 test_loss:205.25669860839844\n",
      "2228/3000 train_loss: 17.01641082763672 test_loss:206.82882690429688\n",
      "2229/3000 train_loss: 15.641786575317383 test_loss:205.2916717529297\n",
      "2230/3000 train_loss: 17.445011138916016 test_loss:203.95791625976562\n",
      "2231/3000 train_loss: 18.528921127319336 test_loss:197.45651245117188\n",
      "2232/3000 train_loss: 17.046939849853516 test_loss:208.28524780273438\n",
      "2233/3000 train_loss: 16.02556037902832 test_loss:199.54046630859375\n",
      "2234/3000 train_loss: 19.379627227783203 test_loss:198.44500732421875\n",
      "2235/3000 train_loss: 20.86810302734375 test_loss:202.7039794921875\n",
      "2236/3000 train_loss: 18.710224151611328 test_loss:196.9388427734375\n",
      "2237/3000 train_loss: 19.084260940551758 test_loss:207.46792602539062\n",
      "2238/3000 train_loss: 14.895145416259766 test_loss:204.61007690429688\n",
      "2239/3000 train_loss: 16.61851692199707 test_loss:199.38381958007812\n",
      "2240/3000 train_loss: 24.777938842773438 test_loss:202.09298706054688\n",
      "2241/3000 train_loss: 19.613698959350586 test_loss:204.40216064453125\n",
      "2242/3000 train_loss: 17.578311920166016 test_loss:192.01730346679688\n",
      "2243/3000 train_loss: 20.539627075195312 test_loss:190.52171325683594\n",
      "2244/3000 train_loss: 16.142377853393555 test_loss:197.78262329101562\n",
      "2245/3000 train_loss: 17.600704193115234 test_loss:193.94839477539062\n",
      "2246/3000 train_loss: 15.835445404052734 test_loss:205.4937744140625\n",
      "2247/3000 train_loss: 15.633262634277344 test_loss:203.1114501953125\n",
      "2248/3000 train_loss: 21.39139175415039 test_loss:209.31869506835938\n",
      "2249/3000 train_loss: 19.38589096069336 test_loss:203.80923461914062\n",
      "2250/3000 train_loss: 16.707725524902344 test_loss:195.44717407226562\n",
      "2251/3000 train_loss: 16.26371955871582 test_loss:208.86849975585938\n",
      "2252/3000 train_loss: 17.877151489257812 test_loss:192.39541625976562\n",
      "2253/3000 train_loss: 23.617431640625 test_loss:210.12156677246094\n",
      "2254/3000 train_loss: 23.309188842773438 test_loss:206.4881134033203\n",
      "2255/3000 train_loss: 20.638025283813477 test_loss:202.94033813476562\n",
      "2256/3000 train_loss: 20.555660247802734 test_loss:204.36904907226562\n",
      "2257/3000 train_loss: 18.219329833984375 test_loss:197.41253662109375\n",
      "2258/3000 train_loss: 18.990875244140625 test_loss:200.2001495361328\n",
      "2259/3000 train_loss: 19.636707305908203 test_loss:208.5318603515625\n",
      "2260/3000 train_loss: 17.739721298217773 test_loss:207.82232666015625\n",
      "2261/3000 train_loss: 15.26907730102539 test_loss:199.95574951171875\n",
      "2262/3000 train_loss: 14.534727096557617 test_loss:203.06759643554688\n",
      "2263/3000 train_loss: 17.05893325805664 test_loss:203.2805938720703\n",
      "2264/3000 train_loss: 17.055185317993164 test_loss:204.29779052734375\n",
      "2265/3000 train_loss: 17.635610580444336 test_loss:207.483154296875\n",
      "2266/3000 train_loss: 16.632774353027344 test_loss:210.9576416015625\n",
      "2267/3000 train_loss: 19.585311889648438 test_loss:202.85507202148438\n",
      "2268/3000 train_loss: 15.541377067565918 test_loss:205.1868896484375\n",
      "2269/3000 train_loss: 22.85401725769043 test_loss:203.7914581298828\n",
      "2270/3000 train_loss: 17.574230194091797 test_loss:203.6533660888672\n",
      "2271/3000 train_loss: 17.735750198364258 test_loss:196.72109985351562\n",
      "2272/3000 train_loss: 18.81035614013672 test_loss:203.30563354492188\n",
      "2273/3000 train_loss: 20.57750129699707 test_loss:204.07415771484375\n",
      "2274/3000 train_loss: 14.05286693572998 test_loss:203.8399200439453\n",
      "2275/3000 train_loss: 21.585134506225586 test_loss:200.14694213867188\n",
      "2276/3000 train_loss: 16.647260665893555 test_loss:199.2300262451172\n",
      "2277/3000 train_loss: 17.916946411132812 test_loss:201.8074951171875\n",
      "2278/3000 train_loss: 20.742595672607422 test_loss:190.16319274902344\n",
      "2279/3000 train_loss: 18.296207427978516 test_loss:217.5400848388672\n",
      "2280/3000 train_loss: 22.099336624145508 test_loss:202.1236572265625\n",
      "2281/3000 train_loss: 18.214162826538086 test_loss:196.089111328125\n",
      "2282/3000 train_loss: 19.31416893005371 test_loss:213.8279266357422\n",
      "2283/3000 train_loss: 15.1875581741333 test_loss:195.3092041015625\n",
      "2284/3000 train_loss: 19.127506256103516 test_loss:202.43527221679688\n",
      "2285/3000 train_loss: 15.63456916809082 test_loss:205.2476348876953\n",
      "2286/3000 train_loss: 19.02522087097168 test_loss:205.79473876953125\n",
      "2287/3000 train_loss: 16.092174530029297 test_loss:202.4031982421875\n",
      "2288/3000 train_loss: 19.623382568359375 test_loss:194.56369018554688\n",
      "2289/3000 train_loss: 21.63130760192871 test_loss:206.73257446289062\n",
      "2290/3000 train_loss: 17.268213272094727 test_loss:207.86199951171875\n",
      "2291/3000 train_loss: 19.2719669342041 test_loss:195.92303466796875\n",
      "2292/3000 train_loss: 18.76552391052246 test_loss:207.30517578125\n",
      "2293/3000 train_loss: 16.565053939819336 test_loss:200.52462768554688\n",
      "2294/3000 train_loss: 17.16313362121582 test_loss:208.09849548339844\n",
      "2295/3000 train_loss: 22.03994369506836 test_loss:202.86251831054688\n",
      "2296/3000 train_loss: 18.50543212890625 test_loss:206.32017517089844\n",
      "2297/3000 train_loss: 18.86057472229004 test_loss:210.14620971679688\n",
      "2298/3000 train_loss: 20.569931030273438 test_loss:198.59873962402344\n",
      "2299/3000 train_loss: 20.318857192993164 test_loss:205.40655517578125\n",
      "2300/3000 train_loss: 17.510778427124023 test_loss:208.3726806640625\n",
      "2301/3000 train_loss: 15.528892517089844 test_loss:204.96417236328125\n",
      "2302/3000 train_loss: 21.162273406982422 test_loss:207.40127563476562\n",
      "2303/3000 train_loss: 16.39932632446289 test_loss:200.30740356445312\n",
      "2304/3000 train_loss: 15.568306922912598 test_loss:207.64932250976562\n",
      "2305/3000 train_loss: 16.707752227783203 test_loss:203.58529663085938\n",
      "2306/3000 train_loss: 16.969728469848633 test_loss:202.26129150390625\n",
      "2307/3000 train_loss: 16.353132247924805 test_loss:199.59597778320312\n",
      "2308/3000 train_loss: 15.88219165802002 test_loss:195.78268432617188\n",
      "2309/3000 train_loss: 16.203079223632812 test_loss:206.38600158691406\n",
      "2310/3000 train_loss: 14.864727973937988 test_loss:195.24526977539062\n",
      "2311/3000 train_loss: 13.825037956237793 test_loss:199.84625244140625\n",
      "2312/3000 train_loss: 16.743881225585938 test_loss:214.2407989501953\n",
      "2313/3000 train_loss: 20.83661651611328 test_loss:192.5775146484375\n",
      "2314/3000 train_loss: 17.429325103759766 test_loss:208.73516845703125\n",
      "2315/3000 train_loss: 18.694293975830078 test_loss:200.63821411132812\n",
      "2316/3000 train_loss: 14.845575332641602 test_loss:202.71246337890625\n",
      "2317/3000 train_loss: 15.097352981567383 test_loss:197.00814819335938\n",
      "2318/3000 train_loss: 18.349578857421875 test_loss:204.79295349121094\n",
      "2319/3000 train_loss: 16.4099063873291 test_loss:195.61221313476562\n",
      "2320/3000 train_loss: 18.13927459716797 test_loss:199.07981872558594\n",
      "2321/3000 train_loss: 15.721170425415039 test_loss:198.37142944335938\n",
      "2322/3000 train_loss: 18.481966018676758 test_loss:202.89788818359375\n",
      "2323/3000 train_loss: 15.22401237487793 test_loss:198.99765014648438\n",
      "2324/3000 train_loss: 19.96013832092285 test_loss:198.87051391601562\n",
      "2325/3000 train_loss: 19.263954162597656 test_loss:214.50225830078125\n",
      "2326/3000 train_loss: 22.25440216064453 test_loss:212.54881286621094\n",
      "2327/3000 train_loss: 19.915706634521484 test_loss:192.56431579589844\n",
      "2328/3000 train_loss: 21.91221809387207 test_loss:204.58970642089844\n",
      "2329/3000 train_loss: 16.14597511291504 test_loss:199.5442657470703\n",
      "2330/3000 train_loss: 18.9842586517334 test_loss:205.95394897460938\n",
      "2331/3000 train_loss: 15.9020414352417 test_loss:196.36135864257812\n",
      "2332/3000 train_loss: 17.870868682861328 test_loss:203.37269592285156\n",
      "2333/3000 train_loss: 16.65077781677246 test_loss:206.9906005859375\n",
      "2334/3000 train_loss: 18.151073455810547 test_loss:206.21653747558594\n",
      "2335/3000 train_loss: 17.05340576171875 test_loss:202.41127014160156\n",
      "2336/3000 train_loss: 18.563392639160156 test_loss:199.84597778320312\n",
      "2337/3000 train_loss: 17.680721282958984 test_loss:205.31668090820312\n",
      "2338/3000 train_loss: 18.208751678466797 test_loss:209.2686309814453\n",
      "2339/3000 train_loss: 17.542078018188477 test_loss:204.56961059570312\n",
      "2340/3000 train_loss: 19.962343215942383 test_loss:199.26210021972656\n",
      "2341/3000 train_loss: 17.25367546081543 test_loss:217.1124267578125\n",
      "2342/3000 train_loss: 17.229488372802734 test_loss:200.4483642578125\n",
      "2343/3000 train_loss: 17.505069732666016 test_loss:211.49978637695312\n",
      "2344/3000 train_loss: 21.381399154663086 test_loss:213.39044189453125\n",
      "2345/3000 train_loss: 17.111940383911133 test_loss:200.08340454101562\n",
      "2346/3000 train_loss: 20.437639236450195 test_loss:206.2526092529297\n",
      "2347/3000 train_loss: 17.429292678833008 test_loss:204.66116333007812\n",
      "2348/3000 train_loss: 23.883255004882812 test_loss:205.00482177734375\n",
      "2349/3000 train_loss: 16.00868034362793 test_loss:202.27200317382812\n",
      "2350/3000 train_loss: 23.213821411132812 test_loss:207.69064331054688\n",
      "2351/3000 train_loss: 23.598716735839844 test_loss:213.48341369628906\n",
      "2352/3000 train_loss: 18.342981338500977 test_loss:212.88555908203125\n",
      "2353/3000 train_loss: 21.106311798095703 test_loss:212.197998046875\n",
      "2354/3000 train_loss: 19.382991790771484 test_loss:201.29449462890625\n",
      "2355/3000 train_loss: 17.781496047973633 test_loss:199.1045379638672\n",
      "2356/3000 train_loss: 18.852087020874023 test_loss:193.33859252929688\n",
      "2357/3000 train_loss: 17.626239776611328 test_loss:198.14801025390625\n",
      "2358/3000 train_loss: 17.38055992126465 test_loss:204.42999267578125\n",
      "2359/3000 train_loss: 17.443561553955078 test_loss:198.79611206054688\n",
      "2360/3000 train_loss: 14.972284317016602 test_loss:201.32247924804688\n",
      "2361/3000 train_loss: 15.338250160217285 test_loss:197.59361267089844\n",
      "2362/3000 train_loss: 17.302011489868164 test_loss:200.175537109375\n",
      "2363/3000 train_loss: 16.392175674438477 test_loss:201.79946899414062\n",
      "2364/3000 train_loss: 21.084421157836914 test_loss:203.29989624023438\n",
      "2365/3000 train_loss: 20.088619232177734 test_loss:191.51123046875\n",
      "2366/3000 train_loss: 24.35556983947754 test_loss:217.64576721191406\n",
      "2367/3000 train_loss: 22.895679473876953 test_loss:201.0395050048828\n",
      "2368/3000 train_loss: 16.38896942138672 test_loss:197.34072875976562\n",
      "2369/3000 train_loss: 15.63338565826416 test_loss:201.82546997070312\n",
      "2370/3000 train_loss: 14.914606094360352 test_loss:195.52542114257812\n",
      "2371/3000 train_loss: 15.570356369018555 test_loss:202.61358642578125\n",
      "2372/3000 train_loss: 19.4515380859375 test_loss:196.29794311523438\n",
      "2373/3000 train_loss: 16.954099655151367 test_loss:200.93539428710938\n",
      "2374/3000 train_loss: 14.169184684753418 test_loss:203.47093200683594\n",
      "2375/3000 train_loss: 15.70936393737793 test_loss:196.44873046875\n",
      "2376/3000 train_loss: 17.47918128967285 test_loss:197.8187255859375\n",
      "2377/3000 train_loss: 17.731483459472656 test_loss:200.68936157226562\n",
      "2378/3000 train_loss: 17.947635650634766 test_loss:196.88482666015625\n",
      "2379/3000 train_loss: 19.166181564331055 test_loss:198.49386596679688\n",
      "2380/3000 train_loss: 16.397266387939453 test_loss:202.71249389648438\n",
      "2381/3000 train_loss: 18.59557342529297 test_loss:192.21087646484375\n",
      "2382/3000 train_loss: 15.938401222229004 test_loss:206.83163452148438\n",
      "2383/3000 train_loss: 20.85388946533203 test_loss:196.3796844482422\n",
      "2384/3000 train_loss: 19.86892318725586 test_loss:204.28329467773438\n",
      "2385/3000 train_loss: 18.27121925354004 test_loss:198.2274169921875\n",
      "2386/3000 train_loss: 16.58331871032715 test_loss:200.2501220703125\n",
      "2387/3000 train_loss: 20.635648727416992 test_loss:196.2134552001953\n",
      "2388/3000 train_loss: 22.334651947021484 test_loss:205.18405151367188\n",
      "2389/3000 train_loss: 16.962249755859375 test_loss:202.53892517089844\n",
      "2390/3000 train_loss: 16.65104866027832 test_loss:196.25509643554688\n",
      "2391/3000 train_loss: 18.25607681274414 test_loss:190.56988525390625\n",
      "2392/3000 train_loss: 15.757078170776367 test_loss:203.1632080078125\n",
      "2393/3000 train_loss: 19.73093605041504 test_loss:201.06581115722656\n",
      "2394/3000 train_loss: 14.967415809631348 test_loss:200.15130615234375\n",
      "2395/3000 train_loss: 18.773601531982422 test_loss:193.93020629882812\n",
      "2396/3000 train_loss: 17.109525680541992 test_loss:194.33465576171875\n",
      "2397/3000 train_loss: 19.653854370117188 test_loss:206.9136199951172\n",
      "2398/3000 train_loss: 17.52659034729004 test_loss:202.8809814453125\n",
      "2399/3000 train_loss: 18.548824310302734 test_loss:204.2789306640625\n",
      "2400/3000 train_loss: 16.106529235839844 test_loss:201.7007598876953\n",
      "2401/3000 train_loss: 16.525724411010742 test_loss:208.793212890625\n",
      "2402/3000 train_loss: 17.75055694580078 test_loss:203.2344970703125\n",
      "2403/3000 train_loss: 15.37939739227295 test_loss:207.01162719726562\n",
      "2404/3000 train_loss: 16.76328468322754 test_loss:191.863037109375\n",
      "2405/3000 train_loss: 16.822952270507812 test_loss:203.1666259765625\n",
      "2406/3000 train_loss: 21.030977249145508 test_loss:200.33242797851562\n",
      "2407/3000 train_loss: 16.204423904418945 test_loss:202.67599487304688\n",
      "2408/3000 train_loss: 14.096648216247559 test_loss:197.80899047851562\n",
      "2409/3000 train_loss: 15.968063354492188 test_loss:201.786376953125\n",
      "2410/3000 train_loss: 19.282474517822266 test_loss:194.49671936035156\n",
      "2411/3000 train_loss: 17.939712524414062 test_loss:186.0108642578125\n",
      "2412/3000 train_loss: 17.95072364807129 test_loss:202.31898498535156\n",
      "2413/3000 train_loss: 19.1092529296875 test_loss:201.01742553710938\n",
      "2414/3000 train_loss: 17.247838973999023 test_loss:196.75033569335938\n",
      "2415/3000 train_loss: 21.59733772277832 test_loss:193.80960083007812\n",
      "2416/3000 train_loss: 20.674732208251953 test_loss:199.23745727539062\n",
      "2417/3000 train_loss: 17.734333038330078 test_loss:199.2279510498047\n",
      "2418/3000 train_loss: 18.29621124267578 test_loss:193.75013732910156\n",
      "2419/3000 train_loss: 15.112863540649414 test_loss:195.25137329101562\n",
      "2420/3000 train_loss: 14.063854217529297 test_loss:206.43673706054688\n",
      "2421/3000 train_loss: 17.131607055664062 test_loss:196.50892639160156\n",
      "2422/3000 train_loss: 16.826852798461914 test_loss:198.88790893554688\n",
      "2423/3000 train_loss: 16.10854721069336 test_loss:211.05648803710938\n",
      "2424/3000 train_loss: 23.940492630004883 test_loss:197.30393981933594\n",
      "2425/3000 train_loss: 17.340150833129883 test_loss:202.6247100830078\n",
      "2426/3000 train_loss: 15.852295875549316 test_loss:196.740234375\n",
      "2427/3000 train_loss: 17.688434600830078 test_loss:199.88790893554688\n",
      "2428/3000 train_loss: 18.78831672668457 test_loss:202.7440185546875\n",
      "2429/3000 train_loss: 15.982213020324707 test_loss:197.87322998046875\n",
      "2430/3000 train_loss: 15.755570411682129 test_loss:202.50299072265625\n",
      "2431/3000 train_loss: 18.227516174316406 test_loss:195.37765502929688\n",
      "2432/3000 train_loss: 16.308265686035156 test_loss:208.61300659179688\n",
      "2433/3000 train_loss: 14.689297676086426 test_loss:195.5506591796875\n",
      "2434/3000 train_loss: 14.551616668701172 test_loss:200.9040985107422\n",
      "2435/3000 train_loss: 16.369667053222656 test_loss:193.09420776367188\n",
      "2436/3000 train_loss: 18.848234176635742 test_loss:199.8131103515625\n",
      "2437/3000 train_loss: 19.048418045043945 test_loss:198.16326904296875\n",
      "2438/3000 train_loss: 16.14614486694336 test_loss:202.2381591796875\n",
      "2439/3000 train_loss: 14.649346351623535 test_loss:199.12831115722656\n",
      "2440/3000 train_loss: 14.224628448486328 test_loss:202.59619140625\n",
      "2441/3000 train_loss: 17.171947479248047 test_loss:193.36268615722656\n",
      "2442/3000 train_loss: 15.591303825378418 test_loss:201.41111755371094\n",
      "2443/3000 train_loss: 19.88191795349121 test_loss:195.84518432617188\n",
      "2444/3000 train_loss: 19.902429580688477 test_loss:189.8118896484375\n",
      "2445/3000 train_loss: 16.40237808227539 test_loss:200.1547088623047\n",
      "2446/3000 train_loss: 14.548702239990234 test_loss:192.55780029296875\n",
      "2447/3000 train_loss: 13.498869895935059 test_loss:199.20504760742188\n",
      "2448/3000 train_loss: 15.406333923339844 test_loss:203.50405883789062\n",
      "2449/3000 train_loss: 19.261566162109375 test_loss:203.567138671875\n",
      "2450/3000 train_loss: 15.764105796813965 test_loss:197.10302734375\n",
      "2451/3000 train_loss: 22.310571670532227 test_loss:195.7540740966797\n",
      "2452/3000 train_loss: 19.279541015625 test_loss:200.991455078125\n",
      "2453/3000 train_loss: 17.902301788330078 test_loss:187.76686096191406\n",
      "2454/3000 train_loss: 19.351417541503906 test_loss:199.31527709960938\n",
      "2455/3000 train_loss: 17.329483032226562 test_loss:202.57598876953125\n",
      "2456/3000 train_loss: 17.761316299438477 test_loss:200.3694305419922\n",
      "2457/3000 train_loss: 18.03661346435547 test_loss:206.1280517578125\n",
      "2458/3000 train_loss: 20.286865234375 test_loss:196.39007568359375\n",
      "2459/3000 train_loss: 19.158466339111328 test_loss:194.675048828125\n",
      "2460/3000 train_loss: 14.164648056030273 test_loss:199.19122314453125\n",
      "2461/3000 train_loss: 21.03376007080078 test_loss:193.99459838867188\n",
      "2462/3000 train_loss: 16.767831802368164 test_loss:197.40054321289062\n",
      "2463/3000 train_loss: 16.211349487304688 test_loss:199.03924560546875\n",
      "2464/3000 train_loss: 17.58478546142578 test_loss:197.24649047851562\n",
      "2465/3000 train_loss: 21.42770767211914 test_loss:197.57505798339844\n",
      "2466/3000 train_loss: 16.531169891357422 test_loss:200.69741821289062\n",
      "2467/3000 train_loss: 16.83850860595703 test_loss:202.0383758544922\n",
      "2468/3000 train_loss: 17.375381469726562 test_loss:201.30880737304688\n",
      "2469/3000 train_loss: 14.79791259765625 test_loss:198.50283813476562\n",
      "2470/3000 train_loss: 16.206357955932617 test_loss:199.9185791015625\n",
      "2471/3000 train_loss: 14.030287742614746 test_loss:197.17337036132812\n",
      "2472/3000 train_loss: 18.064180374145508 test_loss:201.06723022460938\n",
      "2473/3000 train_loss: 19.59079360961914 test_loss:196.630126953125\n",
      "2474/3000 train_loss: 15.027092933654785 test_loss:194.01593017578125\n",
      "2475/3000 train_loss: 18.057476043701172 test_loss:187.26605224609375\n",
      "2476/3000 train_loss: 16.455270767211914 test_loss:195.0567626953125\n",
      "2477/3000 train_loss: 18.13655662536621 test_loss:203.18450927734375\n",
      "2478/3000 train_loss: 15.785357475280762 test_loss:199.73101806640625\n",
      "2479/3000 train_loss: 16.01581382751465 test_loss:203.25442504882812\n",
      "2480/3000 train_loss: 16.042644500732422 test_loss:202.06845092773438\n",
      "2481/3000 train_loss: 16.767581939697266 test_loss:205.3143768310547\n",
      "2482/3000 train_loss: 14.552011489868164 test_loss:208.06219482421875\n",
      "2483/3000 train_loss: 17.272363662719727 test_loss:200.6053466796875\n",
      "2484/3000 train_loss: 17.67538833618164 test_loss:203.366455078125\n",
      "2485/3000 train_loss: 14.702519416809082 test_loss:205.21405029296875\n",
      "2486/3000 train_loss: 16.374526977539062 test_loss:201.61566162109375\n",
      "2487/3000 train_loss: 16.694198608398438 test_loss:205.5623016357422\n",
      "2488/3000 train_loss: 16.823089599609375 test_loss:199.11944580078125\n",
      "2489/3000 train_loss: 14.140003204345703 test_loss:199.94776916503906\n",
      "2490/3000 train_loss: 14.457529067993164 test_loss:195.68026733398438\n",
      "2491/3000 train_loss: 17.846403121948242 test_loss:196.04132080078125\n",
      "2492/3000 train_loss: 15.636098861694336 test_loss:202.0289306640625\n",
      "2493/3000 train_loss: 14.165922164916992 test_loss:201.726806640625\n",
      "2494/3000 train_loss: 15.103918075561523 test_loss:199.72872924804688\n",
      "2495/3000 train_loss: 15.468427658081055 test_loss:204.79812622070312\n",
      "2496/3000 train_loss: 16.77224349975586 test_loss:192.69102478027344\n",
      "2497/3000 train_loss: 17.407154083251953 test_loss:206.3349609375\n",
      "2498/3000 train_loss: 15.70821475982666 test_loss:197.97332763671875\n",
      "2499/3000 train_loss: 19.314273834228516 test_loss:202.11111450195312\n",
      "2500/3000 train_loss: 15.186421394348145 test_loss:207.3519287109375\n",
      "2501/3000 train_loss: 17.660703659057617 test_loss:200.97854614257812\n",
      "2502/3000 train_loss: 17.033042907714844 test_loss:199.25108337402344\n",
      "2503/3000 train_loss: 18.412315368652344 test_loss:209.27957153320312\n",
      "2504/3000 train_loss: 18.932579040527344 test_loss:193.64923095703125\n",
      "2505/3000 train_loss: 18.037492752075195 test_loss:199.7421112060547\n",
      "2506/3000 train_loss: 17.74250030517578 test_loss:201.72979736328125\n",
      "2507/3000 train_loss: 14.812423706054688 test_loss:189.6515350341797\n",
      "2508/3000 train_loss: 15.527792930603027 test_loss:208.187744140625\n",
      "2509/3000 train_loss: 21.066120147705078 test_loss:202.11151123046875\n",
      "2510/3000 train_loss: 19.740678787231445 test_loss:203.16383361816406\n",
      "2511/3000 train_loss: 17.369325637817383 test_loss:199.91075134277344\n",
      "2512/3000 train_loss: 17.958589553833008 test_loss:201.7388458251953\n",
      "2513/3000 train_loss: 15.559943199157715 test_loss:212.0665283203125\n",
      "2514/3000 train_loss: 16.07199478149414 test_loss:195.552001953125\n",
      "2515/3000 train_loss: 20.34305191040039 test_loss:210.78469848632812\n",
      "2516/3000 train_loss: 17.036001205444336 test_loss:206.60693359375\n",
      "2517/3000 train_loss: 14.545581817626953 test_loss:197.11514282226562\n",
      "2518/3000 train_loss: 14.217262268066406 test_loss:194.5723876953125\n",
      "2519/3000 train_loss: 15.485162734985352 test_loss:202.86300659179688\n",
      "2520/3000 train_loss: 16.775136947631836 test_loss:206.46527099609375\n",
      "2521/3000 train_loss: 17.52449607849121 test_loss:200.46644592285156\n",
      "2522/3000 train_loss: 18.52384376525879 test_loss:202.15985107421875\n",
      "2523/3000 train_loss: 15.39323902130127 test_loss:198.0997314453125\n",
      "2524/3000 train_loss: 14.907785415649414 test_loss:196.88070678710938\n",
      "2525/3000 train_loss: 13.943263053894043 test_loss:201.88143920898438\n",
      "2526/3000 train_loss: 17.028657913208008 test_loss:199.20498657226562\n",
      "2527/3000 train_loss: 13.64571475982666 test_loss:199.3917999267578\n",
      "2528/3000 train_loss: 16.967811584472656 test_loss:199.99655151367188\n",
      "2529/3000 train_loss: 20.02935791015625 test_loss:194.57717895507812\n",
      "2530/3000 train_loss: 17.223739624023438 test_loss:193.6451873779297\n",
      "2531/3000 train_loss: 16.34261131286621 test_loss:202.2836456298828\n",
      "2532/3000 train_loss: 18.107463836669922 test_loss:202.84744262695312\n",
      "2533/3000 train_loss: 13.44174575805664 test_loss:201.0803680419922\n",
      "2534/3000 train_loss: 14.977811813354492 test_loss:198.34275817871094\n",
      "2535/3000 train_loss: 13.539313316345215 test_loss:197.20736694335938\n",
      "2536/3000 train_loss: 15.157456398010254 test_loss:193.8692169189453\n",
      "2537/3000 train_loss: 16.411344528198242 test_loss:194.8829345703125\n",
      "2538/3000 train_loss: 16.046337127685547 test_loss:200.14492797851562\n",
      "2539/3000 train_loss: 18.38613510131836 test_loss:194.49339294433594\n",
      "2540/3000 train_loss: 12.668310165405273 test_loss:197.12948608398438\n",
      "2541/3000 train_loss: 15.128922462463379 test_loss:198.48806762695312\n",
      "2542/3000 train_loss: 16.8386173248291 test_loss:202.00173950195312\n",
      "2543/3000 train_loss: 13.351879119873047 test_loss:194.65322875976562\n",
      "2544/3000 train_loss: 14.11453628540039 test_loss:203.3152313232422\n",
      "2545/3000 train_loss: 14.188051223754883 test_loss:203.66250610351562\n",
      "2546/3000 train_loss: 17.011011123657227 test_loss:207.328857421875\n",
      "2547/3000 train_loss: 19.923715591430664 test_loss:205.99948120117188\n",
      "2548/3000 train_loss: 20.61972427368164 test_loss:202.08660888671875\n",
      "2549/3000 train_loss: 16.947032928466797 test_loss:201.07220458984375\n",
      "2550/3000 train_loss: 14.881003379821777 test_loss:197.99951171875\n",
      "2551/3000 train_loss: 15.167162895202637 test_loss:208.11866760253906\n",
      "2552/3000 train_loss: 15.001387596130371 test_loss:203.01040649414062\n",
      "2553/3000 train_loss: 18.177772521972656 test_loss:202.7394256591797\n",
      "2554/3000 train_loss: 14.616488456726074 test_loss:205.52468872070312\n",
      "2555/3000 train_loss: 16.535175323486328 test_loss:198.37911987304688\n",
      "2556/3000 train_loss: 12.93040943145752 test_loss:199.34695434570312\n",
      "2557/3000 train_loss: 13.456414222717285 test_loss:206.43344116210938\n",
      "2558/3000 train_loss: 14.642121315002441 test_loss:201.5703125\n",
      "2559/3000 train_loss: 14.884552955627441 test_loss:206.5394744873047\n",
      "2560/3000 train_loss: 22.804014205932617 test_loss:193.65386962890625\n",
      "2561/3000 train_loss: 15.629853248596191 test_loss:203.59942626953125\n",
      "2562/3000 train_loss: 15.138285636901855 test_loss:202.71786499023438\n",
      "2563/3000 train_loss: 17.61460304260254 test_loss:195.7760467529297\n",
      "2564/3000 train_loss: 16.23600959777832 test_loss:205.04067993164062\n",
      "2565/3000 train_loss: 13.453777313232422 test_loss:198.3846893310547\n",
      "2566/3000 train_loss: 13.453635215759277 test_loss:204.1095428466797\n",
      "2567/3000 train_loss: 13.245243072509766 test_loss:203.887451171875\n",
      "2568/3000 train_loss: 15.211752891540527 test_loss:197.50662231445312\n",
      "2569/3000 train_loss: 15.222394943237305 test_loss:193.1700897216797\n",
      "2570/3000 train_loss: 15.05469799041748 test_loss:208.641845703125\n",
      "2571/3000 train_loss: 13.172368049621582 test_loss:199.49684143066406\n",
      "2572/3000 train_loss: 17.17946434020996 test_loss:199.25335693359375\n",
      "2573/3000 train_loss: 13.572539329528809 test_loss:206.9075164794922\n",
      "2574/3000 train_loss: 16.009504318237305 test_loss:194.23309326171875\n",
      "2575/3000 train_loss: 14.317495346069336 test_loss:205.31259155273438\n",
      "2576/3000 train_loss: 15.861653327941895 test_loss:194.8959503173828\n",
      "2577/3000 train_loss: 14.95134449005127 test_loss:209.36441040039062\n",
      "2578/3000 train_loss: 12.894560813903809 test_loss:198.39047241210938\n",
      "2579/3000 train_loss: 15.020904541015625 test_loss:196.7095947265625\n",
      "2580/3000 train_loss: 16.228551864624023 test_loss:200.22833251953125\n",
      "2581/3000 train_loss: 14.503995895385742 test_loss:193.37179565429688\n",
      "2582/3000 train_loss: 12.48464584350586 test_loss:201.50717163085938\n",
      "2583/3000 train_loss: 17.804128646850586 test_loss:201.24732971191406\n",
      "2584/3000 train_loss: 19.86928939819336 test_loss:199.13748168945312\n",
      "2585/3000 train_loss: 16.92666244506836 test_loss:202.11480712890625\n",
      "2586/3000 train_loss: 16.1169490814209 test_loss:199.16400146484375\n",
      "2587/3000 train_loss: 16.980514526367188 test_loss:197.82461547851562\n",
      "2588/3000 train_loss: 15.004002571105957 test_loss:199.75225830078125\n",
      "2589/3000 train_loss: 17.9213924407959 test_loss:209.02169799804688\n",
      "2590/3000 train_loss: 16.169279098510742 test_loss:199.64974975585938\n",
      "2591/3000 train_loss: 16.06407928466797 test_loss:200.9283447265625\n",
      "2592/3000 train_loss: 14.544951438903809 test_loss:198.8341064453125\n",
      "2593/3000 train_loss: 12.397496223449707 test_loss:197.42758178710938\n",
      "2594/3000 train_loss: 13.337871551513672 test_loss:207.77499389648438\n",
      "2595/3000 train_loss: 16.558286666870117 test_loss:190.86367797851562\n",
      "2596/3000 train_loss: 21.03401756286621 test_loss:209.5668487548828\n",
      "2597/3000 train_loss: 15.290806770324707 test_loss:202.1781005859375\n",
      "2598/3000 train_loss: 18.452360153198242 test_loss:189.44833374023438\n",
      "2599/3000 train_loss: 17.27665901184082 test_loss:196.51820373535156\n",
      "2600/3000 train_loss: 12.794111251831055 test_loss:196.94081115722656\n",
      "2601/3000 train_loss: 11.597381591796875 test_loss:195.82998657226562\n",
      "2602/3000 train_loss: 13.307586669921875 test_loss:207.08331298828125\n",
      "2603/3000 train_loss: 18.372657775878906 test_loss:195.42486572265625\n",
      "2604/3000 train_loss: 14.36910343170166 test_loss:203.48782348632812\n",
      "2605/3000 train_loss: 14.264399528503418 test_loss:193.84384155273438\n",
      "2606/3000 train_loss: 17.58748435974121 test_loss:209.7050323486328\n",
      "2607/3000 train_loss: 15.398567199707031 test_loss:193.4755859375\n",
      "2608/3000 train_loss: 14.45376205444336 test_loss:198.91026306152344\n",
      "2609/3000 train_loss: 15.720088958740234 test_loss:200.5068817138672\n",
      "2610/3000 train_loss: 16.05930519104004 test_loss:194.3344268798828\n",
      "2611/3000 train_loss: 13.672536849975586 test_loss:205.88409423828125\n",
      "2612/3000 train_loss: 15.990653038024902 test_loss:190.70619201660156\n",
      "2613/3000 train_loss: 17.425512313842773 test_loss:200.68174743652344\n",
      "2614/3000 train_loss: 13.90749454498291 test_loss:199.38922119140625\n",
      "2615/3000 train_loss: 14.313825607299805 test_loss:198.20077514648438\n",
      "2616/3000 train_loss: 12.162496566772461 test_loss:196.09283447265625\n",
      "2617/3000 train_loss: 13.458319664001465 test_loss:202.9508056640625\n",
      "2618/3000 train_loss: 17.60268211364746 test_loss:196.51434326171875\n",
      "2619/3000 train_loss: 14.147018432617188 test_loss:200.4996337890625\n",
      "2620/3000 train_loss: 13.35842227935791 test_loss:202.73446655273438\n",
      "2621/3000 train_loss: 14.428146362304688 test_loss:200.15530395507812\n",
      "2622/3000 train_loss: 13.89476203918457 test_loss:198.76907348632812\n",
      "2623/3000 train_loss: 17.02184295654297 test_loss:206.18405151367188\n",
      "2624/3000 train_loss: 15.9035005569458 test_loss:197.78530883789062\n",
      "2625/3000 train_loss: 17.197240829467773 test_loss:212.91061401367188\n",
      "2626/3000 train_loss: 17.112224578857422 test_loss:203.1573486328125\n",
      "2627/3000 train_loss: 17.505762100219727 test_loss:195.69027709960938\n",
      "2628/3000 train_loss: 16.308195114135742 test_loss:207.062744140625\n",
      "2629/3000 train_loss: 15.924478530883789 test_loss:199.36611938476562\n",
      "2630/3000 train_loss: 15.091581344604492 test_loss:204.90457153320312\n",
      "2631/3000 train_loss: 14.190350532531738 test_loss:206.09625244140625\n",
      "2632/3000 train_loss: 14.959385871887207 test_loss:196.57418823242188\n",
      "2633/3000 train_loss: 14.416831016540527 test_loss:203.8892059326172\n",
      "2634/3000 train_loss: 19.55927276611328 test_loss:198.66461181640625\n",
      "2635/3000 train_loss: 18.937395095825195 test_loss:194.31515502929688\n",
      "2636/3000 train_loss: 16.0819091796875 test_loss:203.92526245117188\n",
      "2637/3000 train_loss: 18.027769088745117 test_loss:201.1326904296875\n",
      "2638/3000 train_loss: 16.82996940612793 test_loss:191.34092712402344\n",
      "2639/3000 train_loss: 14.979418754577637 test_loss:203.85565185546875\n",
      "2640/3000 train_loss: 12.659637451171875 test_loss:198.9702606201172\n",
      "2641/3000 train_loss: 13.226255416870117 test_loss:202.10147094726562\n",
      "2642/3000 train_loss: 14.033453941345215 test_loss:193.85421752929688\n",
      "2643/3000 train_loss: 13.143558502197266 test_loss:201.1545867919922\n",
      "2644/3000 train_loss: 15.86350154876709 test_loss:191.05067443847656\n",
      "2645/3000 train_loss: 15.219808578491211 test_loss:205.6913604736328\n",
      "2646/3000 train_loss: 14.25021743774414 test_loss:200.04168701171875\n",
      "2647/3000 train_loss: 14.157392501831055 test_loss:202.03819274902344\n",
      "2648/3000 train_loss: 12.801529884338379 test_loss:197.6252899169922\n",
      "2649/3000 train_loss: 13.924098014831543 test_loss:199.45858764648438\n",
      "2650/3000 train_loss: 11.980314254760742 test_loss:197.70541381835938\n",
      "2651/3000 train_loss: 12.998263359069824 test_loss:199.56082153320312\n",
      "2652/3000 train_loss: 15.563432693481445 test_loss:197.84381103515625\n",
      "2653/3000 train_loss: 14.478633880615234 test_loss:203.99496459960938\n",
      "2654/3000 train_loss: 14.241418838500977 test_loss:200.44020080566406\n",
      "2655/3000 train_loss: 14.028193473815918 test_loss:201.5462646484375\n",
      "2656/3000 train_loss: 12.34589958190918 test_loss:207.05450439453125\n",
      "2657/3000 train_loss: 15.370861053466797 test_loss:199.6616973876953\n",
      "2658/3000 train_loss: 15.404561996459961 test_loss:195.55422973632812\n",
      "2659/3000 train_loss: 13.929682731628418 test_loss:203.13844299316406\n",
      "2660/3000 train_loss: 16.015872955322266 test_loss:203.11135864257812\n",
      "2661/3000 train_loss: 15.862064361572266 test_loss:200.91546630859375\n",
      "2662/3000 train_loss: 14.388423919677734 test_loss:201.55859375\n",
      "2663/3000 train_loss: 13.112309455871582 test_loss:202.90988159179688\n",
      "2664/3000 train_loss: 22.63511848449707 test_loss:200.1240234375\n",
      "2665/3000 train_loss: 15.801389694213867 test_loss:207.4979705810547\n",
      "2666/3000 train_loss: 16.969444274902344 test_loss:202.99432373046875\n",
      "2667/3000 train_loss: 15.689353942871094 test_loss:202.9798583984375\n",
      "2668/3000 train_loss: 12.838512420654297 test_loss:205.34744262695312\n",
      "2669/3000 train_loss: 14.194489479064941 test_loss:194.7506561279297\n",
      "2670/3000 train_loss: 13.640515327453613 test_loss:197.611083984375\n",
      "2671/3000 train_loss: 13.211455345153809 test_loss:207.3289031982422\n",
      "2672/3000 train_loss: 14.75362777709961 test_loss:211.71853637695312\n",
      "2673/3000 train_loss: 14.15993881225586 test_loss:196.12921142578125\n",
      "2674/3000 train_loss: 14.207021713256836 test_loss:205.17605590820312\n",
      "2675/3000 train_loss: 14.437836647033691 test_loss:196.5414581298828\n",
      "2676/3000 train_loss: 15.876923561096191 test_loss:211.48678588867188\n",
      "2677/3000 train_loss: 13.109430313110352 test_loss:199.36923217773438\n",
      "2678/3000 train_loss: 15.863199234008789 test_loss:204.21090698242188\n",
      "2679/3000 train_loss: 16.834617614746094 test_loss:191.3589324951172\n",
      "2680/3000 train_loss: 15.094306945800781 test_loss:202.60020446777344\n",
      "2681/3000 train_loss: 16.327621459960938 test_loss:199.1425018310547\n",
      "2682/3000 train_loss: 15.025547981262207 test_loss:209.40821838378906\n",
      "2683/3000 train_loss: 13.848588943481445 test_loss:195.26617431640625\n",
      "2684/3000 train_loss: 15.705699920654297 test_loss:214.7557373046875\n",
      "2685/3000 train_loss: 14.241377830505371 test_loss:200.27272033691406\n",
      "2686/3000 train_loss: 15.05280590057373 test_loss:199.8485107421875\n",
      "2687/3000 train_loss: 14.469013214111328 test_loss:195.30226135253906\n",
      "2688/3000 train_loss: 14.67253589630127 test_loss:205.56808471679688\n",
      "2689/3000 train_loss: 14.005366325378418 test_loss:200.6510772705078\n",
      "2690/3000 train_loss: 12.803177833557129 test_loss:203.10232543945312\n",
      "2691/3000 train_loss: 13.891886711120605 test_loss:197.46372985839844\n",
      "2692/3000 train_loss: 13.377551078796387 test_loss:193.08499145507812\n",
      "2693/3000 train_loss: 15.211252212524414 test_loss:208.1865234375\n",
      "2694/3000 train_loss: 13.839917182922363 test_loss:198.48236083984375\n",
      "2695/3000 train_loss: 12.08730697631836 test_loss:203.7154541015625\n",
      "2696/3000 train_loss: 15.382229804992676 test_loss:205.1684112548828\n",
      "2697/3000 train_loss: 16.45888328552246 test_loss:191.20022583007812\n",
      "2698/3000 train_loss: 14.750073432922363 test_loss:205.97576904296875\n",
      "2699/3000 train_loss: 15.061582565307617 test_loss:195.65313720703125\n",
      "2700/3000 train_loss: 14.690767288208008 test_loss:198.72129821777344\n",
      "2701/3000 train_loss: 15.693425178527832 test_loss:197.75216674804688\n",
      "2702/3000 train_loss: 14.625868797302246 test_loss:198.55850219726562\n",
      "2703/3000 train_loss: 14.37925910949707 test_loss:200.34255981445312\n",
      "2704/3000 train_loss: 13.49622917175293 test_loss:206.38717651367188\n",
      "2705/3000 train_loss: 13.380233764648438 test_loss:201.27301025390625\n",
      "2706/3000 train_loss: 13.188230514526367 test_loss:200.0377197265625\n",
      "2707/3000 train_loss: 15.439230918884277 test_loss:203.83946228027344\n",
      "2708/3000 train_loss: 15.320796966552734 test_loss:191.55819702148438\n",
      "2709/3000 train_loss: 16.265159606933594 test_loss:204.02728271484375\n",
      "2710/3000 train_loss: 13.665511131286621 test_loss:198.79092407226562\n",
      "2711/3000 train_loss: 15.073784828186035 test_loss:199.842529296875\n",
      "2712/3000 train_loss: 17.688447952270508 test_loss:202.27639770507812\n",
      "2713/3000 train_loss: 14.290909767150879 test_loss:210.92137145996094\n",
      "2714/3000 train_loss: 14.254950523376465 test_loss:198.71414184570312\n",
      "2715/3000 train_loss: 14.61886215209961 test_loss:206.64779663085938\n",
      "2716/3000 train_loss: 14.913737297058105 test_loss:194.8104248046875\n",
      "2717/3000 train_loss: 19.70235252380371 test_loss:211.86666870117188\n",
      "2718/3000 train_loss: 17.790191650390625 test_loss:204.96475219726562\n",
      "2719/3000 train_loss: 15.312987327575684 test_loss:201.796630859375\n",
      "2720/3000 train_loss: 16.572246551513672 test_loss:197.9592742919922\n",
      "2721/3000 train_loss: 16.71289825439453 test_loss:198.35418701171875\n",
      "2722/3000 train_loss: 13.705034255981445 test_loss:193.73048400878906\n",
      "2723/3000 train_loss: 13.618003845214844 test_loss:192.09759521484375\n",
      "2724/3000 train_loss: 14.620388984680176 test_loss:194.44503784179688\n",
      "2725/3000 train_loss: 13.441585540771484 test_loss:201.25802612304688\n",
      "2726/3000 train_loss: 16.626041412353516 test_loss:192.3693084716797\n",
      "2727/3000 train_loss: 13.021629333496094 test_loss:194.63619995117188\n",
      "2728/3000 train_loss: 13.978182792663574 test_loss:196.26242065429688\n",
      "2729/3000 train_loss: 12.66125774383545 test_loss:200.80233764648438\n",
      "2730/3000 train_loss: 13.629352569580078 test_loss:192.6878204345703\n",
      "2731/3000 train_loss: 13.361953735351562 test_loss:195.77334594726562\n",
      "2732/3000 train_loss: 17.044374465942383 test_loss:203.03704833984375\n",
      "2733/3000 train_loss: 12.656184196472168 test_loss:201.50970458984375\n",
      "2734/3000 train_loss: 14.111045837402344 test_loss:195.52325439453125\n",
      "2735/3000 train_loss: 14.439271926879883 test_loss:207.0845489501953\n",
      "2736/3000 train_loss: 12.910233497619629 test_loss:196.61968994140625\n",
      "2737/3000 train_loss: 15.0162353515625 test_loss:204.37142944335938\n",
      "2738/3000 train_loss: 19.191425323486328 test_loss:203.78797912597656\n",
      "2739/3000 train_loss: 16.712665557861328 test_loss:193.8209991455078\n",
      "2740/3000 train_loss: 14.517335891723633 test_loss:196.85775756835938\n",
      "2741/3000 train_loss: 12.621736526489258 test_loss:203.10568237304688\n",
      "2742/3000 train_loss: 14.541914939880371 test_loss:197.4359893798828\n",
      "2743/3000 train_loss: 14.190712928771973 test_loss:209.884033203125\n",
      "2744/3000 train_loss: 15.29377269744873 test_loss:203.07723999023438\n",
      "2745/3000 train_loss: 16.842201232910156 test_loss:195.70236206054688\n",
      "2746/3000 train_loss: 12.68118667602539 test_loss:200.19400024414062\n",
      "2747/3000 train_loss: 14.737751960754395 test_loss:206.59078979492188\n",
      "2748/3000 train_loss: 15.92335319519043 test_loss:198.14288330078125\n",
      "2749/3000 train_loss: 13.394761085510254 test_loss:205.05120849609375\n",
      "2750/3000 train_loss: 16.86141586303711 test_loss:205.33493041992188\n",
      "2751/3000 train_loss: 15.843015670776367 test_loss:192.62606811523438\n",
      "2752/3000 train_loss: 15.11004638671875 test_loss:203.69131469726562\n",
      "2753/3000 train_loss: 16.721092224121094 test_loss:196.57449340820312\n",
      "2754/3000 train_loss: 16.54167366027832 test_loss:198.3253173828125\n",
      "2755/3000 train_loss: 13.420795440673828 test_loss:202.0296173095703\n",
      "2756/3000 train_loss: 12.562299728393555 test_loss:197.58840942382812\n",
      "2757/3000 train_loss: 15.905220985412598 test_loss:202.01077270507812\n",
      "2758/3000 train_loss: 13.642918586730957 test_loss:198.75086975097656\n",
      "2759/3000 train_loss: 13.907696723937988 test_loss:206.65414428710938\n",
      "2760/3000 train_loss: 13.02112102508545 test_loss:202.2994384765625\n",
      "2761/3000 train_loss: 17.295120239257812 test_loss:205.5708465576172\n",
      "2762/3000 train_loss: 16.8201961517334 test_loss:215.15921020507812\n",
      "2763/3000 train_loss: 13.806809425354004 test_loss:191.87283325195312\n",
      "2764/3000 train_loss: 14.1598482131958 test_loss:200.600830078125\n",
      "2765/3000 train_loss: 13.687950134277344 test_loss:207.727294921875\n",
      "2766/3000 train_loss: 13.006643295288086 test_loss:196.41696166992188\n",
      "2767/3000 train_loss: 12.730049133300781 test_loss:199.50448608398438\n",
      "2768/3000 train_loss: 13.32059097290039 test_loss:196.12733459472656\n",
      "2769/3000 train_loss: 13.26921272277832 test_loss:205.1574249267578\n",
      "2770/3000 train_loss: 34.12992858886719 test_loss:189.58497619628906\n",
      "2771/3000 train_loss: 19.55109405517578 test_loss:215.67604064941406\n",
      "2772/3000 train_loss: 17.927528381347656 test_loss:191.18899536132812\n",
      "2773/3000 train_loss: 18.709096908569336 test_loss:197.39358520507812\n",
      "2774/3000 train_loss: 18.94116973876953 test_loss:211.28265380859375\n",
      "2775/3000 train_loss: 15.368191719055176 test_loss:198.64710998535156\n",
      "2776/3000 train_loss: 16.215662002563477 test_loss:213.64666748046875\n",
      "2777/3000 train_loss: 17.723838806152344 test_loss:208.49295043945312\n",
      "2778/3000 train_loss: 14.225419998168945 test_loss:192.20928955078125\n",
      "2779/3000 train_loss: 13.7660493850708 test_loss:202.1798553466797\n",
      "2780/3000 train_loss: 15.748640060424805 test_loss:195.2107391357422\n",
      "2781/3000 train_loss: 15.09384822845459 test_loss:201.18807983398438\n",
      "2782/3000 train_loss: 15.876777648925781 test_loss:206.64849853515625\n",
      "2783/3000 train_loss: 14.371903419494629 test_loss:204.3089599609375\n",
      "2784/3000 train_loss: 13.728448867797852 test_loss:202.80035400390625\n",
      "2785/3000 train_loss: 13.369670867919922 test_loss:199.4374237060547\n",
      "2786/3000 train_loss: 16.505409240722656 test_loss:198.68470764160156\n",
      "2787/3000 train_loss: 13.634212493896484 test_loss:199.71969604492188\n",
      "2788/3000 train_loss: 15.166400909423828 test_loss:209.8219451904297\n",
      "2789/3000 train_loss: 13.345144271850586 test_loss:195.84722900390625\n",
      "2790/3000 train_loss: 12.784639358520508 test_loss:205.15606689453125\n",
      "2791/3000 train_loss: 13.918757438659668 test_loss:209.63124084472656\n",
      "2792/3000 train_loss: 14.857250213623047 test_loss:196.72842407226562\n",
      "2793/3000 train_loss: 15.202760696411133 test_loss:194.29566955566406\n",
      "2794/3000 train_loss: 14.238713264465332 test_loss:201.9014434814453\n",
      "2795/3000 train_loss: 12.395035743713379 test_loss:201.40866088867188\n",
      "2796/3000 train_loss: 14.022233009338379 test_loss:196.73904418945312\n",
      "2797/3000 train_loss: 13.676567077636719 test_loss:197.1392822265625\n",
      "2798/3000 train_loss: 12.322853088378906 test_loss:196.71774291992188\n",
      "2799/3000 train_loss: 12.549336433410645 test_loss:196.14434814453125\n",
      "2800/3000 train_loss: 15.701639175415039 test_loss:198.56089782714844\n",
      "2801/3000 train_loss: 14.337727546691895 test_loss:199.4462127685547\n",
      "2802/3000 train_loss: 13.357002258300781 test_loss:196.46923828125\n",
      "2803/3000 train_loss: 13.808894157409668 test_loss:196.50482177734375\n",
      "2804/3000 train_loss: 13.727289199829102 test_loss:200.86111450195312\n",
      "2805/3000 train_loss: 14.697382926940918 test_loss:199.84400939941406\n",
      "2806/3000 train_loss: 12.582989692687988 test_loss:198.33926391601562\n",
      "2807/3000 train_loss: 15.641433715820312 test_loss:195.20700073242188\n",
      "2808/3000 train_loss: 13.427021026611328 test_loss:195.86578369140625\n",
      "2809/3000 train_loss: 19.936439514160156 test_loss:193.3749237060547\n",
      "2810/3000 train_loss: 16.13931655883789 test_loss:201.62704467773438\n",
      "2811/3000 train_loss: 14.21379280090332 test_loss:194.0545654296875\n",
      "2812/3000 train_loss: 12.759476661682129 test_loss:202.76693725585938\n",
      "2813/3000 train_loss: 11.534886360168457 test_loss:200.90472412109375\n",
      "2814/3000 train_loss: 16.60102081298828 test_loss:201.91323852539062\n",
      "2815/3000 train_loss: 17.47699546813965 test_loss:198.235107421875\n",
      "2816/3000 train_loss: 12.113373756408691 test_loss:192.3301239013672\n",
      "2817/3000 train_loss: 11.84099006652832 test_loss:194.1727294921875\n",
      "2818/3000 train_loss: 14.193016052246094 test_loss:195.41220092773438\n",
      "2819/3000 train_loss: 22.71520233154297 test_loss:193.26602172851562\n",
      "2820/3000 train_loss: 13.918891906738281 test_loss:203.43414306640625\n",
      "2821/3000 train_loss: 14.301193237304688 test_loss:200.5547332763672\n",
      "2822/3000 train_loss: 14.36843490600586 test_loss:193.2870330810547\n",
      "2823/3000 train_loss: 16.844684600830078 test_loss:199.343994140625\n",
      "2824/3000 train_loss: 13.187183380126953 test_loss:209.85589599609375\n",
      "2825/3000 train_loss: 15.626396179199219 test_loss:193.14547729492188\n",
      "2826/3000 train_loss: 16.764915466308594 test_loss:197.06036376953125\n",
      "2827/3000 train_loss: 15.287793159484863 test_loss:200.5665283203125\n",
      "2828/3000 train_loss: 12.21214771270752 test_loss:189.61911010742188\n",
      "2829/3000 train_loss: 15.407768249511719 test_loss:205.59017944335938\n",
      "2830/3000 train_loss: 15.207229614257812 test_loss:197.57972717285156\n",
      "2831/3000 train_loss: 14.976533889770508 test_loss:205.03286743164062\n",
      "2832/3000 train_loss: 17.913606643676758 test_loss:198.8271484375\n",
      "2833/3000 train_loss: 12.95289421081543 test_loss:198.84329223632812\n",
      "2834/3000 train_loss: 15.091959953308105 test_loss:193.71246337890625\n",
      "2835/3000 train_loss: 13.162302017211914 test_loss:204.94247436523438\n",
      "2836/3000 train_loss: 15.196346282958984 test_loss:196.992919921875\n",
      "2837/3000 train_loss: 15.07914924621582 test_loss:204.72525024414062\n",
      "2838/3000 train_loss: 13.993237495422363 test_loss:203.74449157714844\n",
      "2839/3000 train_loss: 12.173336029052734 test_loss:200.00582885742188\n",
      "2840/3000 train_loss: 11.30865478515625 test_loss:203.16073608398438\n",
      "2841/3000 train_loss: 14.275732040405273 test_loss:197.77780151367188\n",
      "2842/3000 train_loss: 14.965510368347168 test_loss:196.05685424804688\n",
      "2843/3000 train_loss: 11.292665481567383 test_loss:193.36517333984375\n",
      "2844/3000 train_loss: 16.723499298095703 test_loss:202.8467559814453\n",
      "2845/3000 train_loss: 12.532271385192871 test_loss:204.31549072265625\n",
      "2846/3000 train_loss: 14.931733131408691 test_loss:207.46920776367188\n",
      "2847/3000 train_loss: 12.177472114562988 test_loss:201.32150268554688\n",
      "2848/3000 train_loss: 14.976358413696289 test_loss:218.28195190429688\n",
      "2849/3000 train_loss: 16.61091423034668 test_loss:199.91098022460938\n",
      "2850/3000 train_loss: 15.589444160461426 test_loss:196.458984375\n",
      "2851/3000 train_loss: 13.594733238220215 test_loss:205.32017517089844\n",
      "2852/3000 train_loss: 15.68285846710205 test_loss:193.688232421875\n",
      "2853/3000 train_loss: 14.394857406616211 test_loss:205.60409545898438\n",
      "2854/3000 train_loss: 19.35543441772461 test_loss:205.25726318359375\n",
      "2855/3000 train_loss: 19.975723266601562 test_loss:201.08016967773438\n",
      "2856/3000 train_loss: 15.775659561157227 test_loss:195.41259765625\n",
      "2857/3000 train_loss: 18.47166633605957 test_loss:200.4330291748047\n",
      "2858/3000 train_loss: 12.336991310119629 test_loss:204.04051208496094\n",
      "2859/3000 train_loss: 14.621415138244629 test_loss:205.9888916015625\n",
      "2860/3000 train_loss: 14.418888092041016 test_loss:199.4327850341797\n",
      "2861/3000 train_loss: 16.134492874145508 test_loss:200.25677490234375\n",
      "2862/3000 train_loss: 14.192072868347168 test_loss:196.18902587890625\n",
      "2863/3000 train_loss: 12.673962593078613 test_loss:201.15811157226562\n",
      "2864/3000 train_loss: 16.609209060668945 test_loss:192.54466247558594\n",
      "2865/3000 train_loss: 12.964320182800293 test_loss:192.4676971435547\n",
      "2866/3000 train_loss: 12.597833633422852 test_loss:203.3939666748047\n",
      "2867/3000 train_loss: 13.967205047607422 test_loss:192.27914428710938\n",
      "2868/3000 train_loss: 15.098878860473633 test_loss:196.94482421875\n",
      "2869/3000 train_loss: 12.573199272155762 test_loss:200.8438262939453\n",
      "2870/3000 train_loss: 12.289447784423828 test_loss:193.8918914794922\n",
      "2871/3000 train_loss: 13.65900993347168 test_loss:188.57928466796875\n",
      "2872/3000 train_loss: 20.591224670410156 test_loss:206.07550048828125\n",
      "2873/3000 train_loss: 15.4642972946167 test_loss:204.0571746826172\n",
      "2874/3000 train_loss: 12.444490432739258 test_loss:199.70669555664062\n",
      "2875/3000 train_loss: 14.56535816192627 test_loss:201.94424438476562\n",
      "2876/3000 train_loss: 13.514487266540527 test_loss:202.76806640625\n",
      "2877/3000 train_loss: 14.02573299407959 test_loss:201.20120239257812\n",
      "2878/3000 train_loss: 13.262791633605957 test_loss:197.1956787109375\n",
      "2879/3000 train_loss: 12.527843475341797 test_loss:190.31858825683594\n",
      "2880/3000 train_loss: 15.924622535705566 test_loss:196.70709228515625\n",
      "2881/3000 train_loss: 15.027097702026367 test_loss:190.6029510498047\n",
      "2882/3000 train_loss: 12.696089744567871 test_loss:194.7754364013672\n",
      "2883/3000 train_loss: 14.363731384277344 test_loss:190.49472045898438\n",
      "2884/3000 train_loss: 13.389896392822266 test_loss:201.32839965820312\n",
      "2885/3000 train_loss: 13.503643989562988 test_loss:201.07162475585938\n",
      "2886/3000 train_loss: 13.056336402893066 test_loss:191.21913146972656\n",
      "2887/3000 train_loss: 13.918850898742676 test_loss:196.64572143554688\n",
      "2888/3000 train_loss: 11.956412315368652 test_loss:198.35116577148438\n",
      "2889/3000 train_loss: 12.70377254486084 test_loss:203.51358032226562\n",
      "2890/3000 train_loss: 13.757763862609863 test_loss:191.95123291015625\n",
      "2891/3000 train_loss: 13.503998756408691 test_loss:204.6466064453125\n",
      "2892/3000 train_loss: 13.531383514404297 test_loss:199.332275390625\n",
      "2893/3000 train_loss: 13.968889236450195 test_loss:195.2021484375\n",
      "2894/3000 train_loss: 10.874090194702148 test_loss:195.3182373046875\n",
      "2895/3000 train_loss: 12.958597183227539 test_loss:201.51138305664062\n",
      "2896/3000 train_loss: 27.160465240478516 test_loss:191.27584838867188\n",
      "2897/3000 train_loss: 15.306154251098633 test_loss:199.62611389160156\n",
      "2898/3000 train_loss: 13.736763000488281 test_loss:201.6805419921875\n",
      "2899/3000 train_loss: 15.0097074508667 test_loss:199.7541046142578\n",
      "2900/3000 train_loss: 15.412914276123047 test_loss:197.19114685058594\n",
      "2901/3000 train_loss: 15.09201717376709 test_loss:206.78073120117188\n",
      "2902/3000 train_loss: 15.740994453430176 test_loss:187.03199768066406\n",
      "2903/3000 train_loss: 17.199024200439453 test_loss:192.40170288085938\n",
      "2904/3000 train_loss: 15.037723541259766 test_loss:197.90673828125\n",
      "2905/3000 train_loss: 13.020011901855469 test_loss:193.23858642578125\n",
      "2906/3000 train_loss: 12.219085693359375 test_loss:193.72283935546875\n",
      "2907/3000 train_loss: 15.030795097351074 test_loss:204.87860107421875\n",
      "2908/3000 train_loss: 13.840706825256348 test_loss:184.90658569335938\n",
      "2909/3000 train_loss: 13.548018455505371 test_loss:197.9114990234375\n",
      "2910/3000 train_loss: 14.720684051513672 test_loss:189.2403564453125\n",
      "2911/3000 train_loss: 14.14566421508789 test_loss:204.02261352539062\n",
      "2912/3000 train_loss: 14.902791976928711 test_loss:189.02554321289062\n",
      "2913/3000 train_loss: 12.375163078308105 test_loss:202.53878784179688\n",
      "2914/3000 train_loss: 13.745138168334961 test_loss:195.05133056640625\n",
      "2915/3000 train_loss: 14.235308647155762 test_loss:191.4704132080078\n",
      "2916/3000 train_loss: 14.626535415649414 test_loss:192.88241577148438\n",
      "2917/3000 train_loss: 12.402652740478516 test_loss:199.8460693359375\n",
      "2918/3000 train_loss: 11.738849639892578 test_loss:193.61965942382812\n",
      "2919/3000 train_loss: 16.15570831298828 test_loss:190.34698486328125\n",
      "2920/3000 train_loss: 12.54125690460205 test_loss:195.73904418945312\n",
      "2921/3000 train_loss: 13.775552749633789 test_loss:199.90155029296875\n",
      "2922/3000 train_loss: 14.640899658203125 test_loss:191.70809936523438\n",
      "2923/3000 train_loss: 16.468217849731445 test_loss:193.8028564453125\n",
      "2924/3000 train_loss: 12.42393684387207 test_loss:197.80599975585938\n",
      "2925/3000 train_loss: 14.040630340576172 test_loss:199.05023193359375\n",
      "2926/3000 train_loss: 12.729557037353516 test_loss:201.40797424316406\n",
      "2927/3000 train_loss: 14.331268310546875 test_loss:192.68807983398438\n",
      "2928/3000 train_loss: 14.051559448242188 test_loss:184.09701538085938\n",
      "2929/3000 train_loss: 11.872187614440918 test_loss:198.05819702148438\n",
      "2930/3000 train_loss: 14.697315216064453 test_loss:195.02728271484375\n",
      "2931/3000 train_loss: 15.165483474731445 test_loss:189.37701416015625\n",
      "2932/3000 train_loss: 14.774645805358887 test_loss:187.142578125\n",
      "2933/3000 train_loss: 15.132685661315918 test_loss:191.88748168945312\n",
      "2934/3000 train_loss: 13.951567649841309 test_loss:198.28036499023438\n",
      "2935/3000 train_loss: 11.53274917602539 test_loss:193.14866638183594\n",
      "2936/3000 train_loss: 12.772950172424316 test_loss:192.4956512451172\n",
      "2937/3000 train_loss: 12.480565071105957 test_loss:194.54461669921875\n",
      "2938/3000 train_loss: 11.50407886505127 test_loss:192.33717346191406\n",
      "2939/3000 train_loss: 10.72835636138916 test_loss:203.31031799316406\n",
      "2940/3000 train_loss: 15.978153228759766 test_loss:190.44224548339844\n",
      "2941/3000 train_loss: 13.051834106445312 test_loss:191.95138549804688\n",
      "2942/3000 train_loss: 14.151250839233398 test_loss:195.08360290527344\n",
      "2943/3000 train_loss: 12.75836181640625 test_loss:198.79183959960938\n",
      "2944/3000 train_loss: 14.845399856567383 test_loss:193.86312866210938\n",
      "2945/3000 train_loss: 13.167686462402344 test_loss:200.0867462158203\n",
      "2946/3000 train_loss: 13.213775634765625 test_loss:189.84213256835938\n",
      "2947/3000 train_loss: 14.130148887634277 test_loss:195.73521423339844\n",
      "2948/3000 train_loss: 12.392584800720215 test_loss:197.61256408691406\n",
      "2949/3000 train_loss: 13.238790512084961 test_loss:192.48580932617188\n",
      "2950/3000 train_loss: 13.810884475708008 test_loss:200.5284881591797\n",
      "2951/3000 train_loss: 13.600913047790527 test_loss:190.68313598632812\n",
      "2952/3000 train_loss: 12.831592559814453 test_loss:200.47225952148438\n",
      "2953/3000 train_loss: 11.962308883666992 test_loss:198.34207153320312\n",
      "2954/3000 train_loss: 11.229159355163574 test_loss:192.9220733642578\n",
      "2955/3000 train_loss: 12.672982215881348 test_loss:194.2469940185547\n",
      "2956/3000 train_loss: 15.769916534423828 test_loss:191.29173278808594\n",
      "2957/3000 train_loss: 14.312389373779297 test_loss:195.937255859375\n",
      "2958/3000 train_loss: 10.346639633178711 test_loss:194.4397430419922\n",
      "2959/3000 train_loss: 11.577848434448242 test_loss:197.29287719726562\n",
      "2960/3000 train_loss: 12.114640235900879 test_loss:198.62625122070312\n",
      "2961/3000 train_loss: 13.987893104553223 test_loss:188.28550720214844\n",
      "2962/3000 train_loss: 14.779197692871094 test_loss:201.10989379882812\n",
      "2963/3000 train_loss: 12.308700561523438 test_loss:197.447265625\n",
      "2964/3000 train_loss: 11.900677680969238 test_loss:195.54132080078125\n",
      "2965/3000 train_loss: 12.724411010742188 test_loss:193.4646759033203\n",
      "2966/3000 train_loss: 12.563592910766602 test_loss:196.97958374023438\n",
      "2967/3000 train_loss: 15.988070487976074 test_loss:183.71466064453125\n",
      "2968/3000 train_loss: 14.198451042175293 test_loss:196.48257446289062\n",
      "2969/3000 train_loss: 11.817330360412598 test_loss:193.81134033203125\n",
      "2970/3000 train_loss: 12.569232940673828 test_loss:196.24655151367188\n",
      "2971/3000 train_loss: 14.300917625427246 test_loss:208.9791259765625\n",
      "2972/3000 train_loss: 13.924262046813965 test_loss:193.97511291503906\n",
      "2973/3000 train_loss: 12.164196014404297 test_loss:195.37057495117188\n",
      "2974/3000 train_loss: 11.829989433288574 test_loss:193.02667236328125\n",
      "2975/3000 train_loss: 11.386924743652344 test_loss:195.41891479492188\n",
      "2976/3000 train_loss: 13.024322509765625 test_loss:201.01870727539062\n",
      "2977/3000 train_loss: 14.966445922851562 test_loss:188.6182098388672\n",
      "2978/3000 train_loss: 14.279417037963867 test_loss:206.76385498046875\n",
      "2979/3000 train_loss: 16.847660064697266 test_loss:193.377685546875\n",
      "2980/3000 train_loss: 12.197696685791016 test_loss:200.3418426513672\n",
      "2981/3000 train_loss: 14.095977783203125 test_loss:204.40554809570312\n",
      "2982/3000 train_loss: 13.67507553100586 test_loss:201.34921264648438\n",
      "2983/3000 train_loss: 13.35506534576416 test_loss:196.82745361328125\n",
      "2984/3000 train_loss: 16.218658447265625 test_loss:190.4748992919922\n",
      "2985/3000 train_loss: 11.14474105834961 test_loss:193.77540588378906\n",
      "2986/3000 train_loss: 14.39018440246582 test_loss:196.72207641601562\n",
      "2987/3000 train_loss: 13.635627746582031 test_loss:200.13656616210938\n",
      "2988/3000 train_loss: 12.226824760437012 test_loss:197.4762420654297\n",
      "2989/3000 train_loss: 14.887918472290039 test_loss:199.71746826171875\n",
      "2990/3000 train_loss: 14.512682914733887 test_loss:205.9026641845703\n",
      "2991/3000 train_loss: 11.852575302124023 test_loss:189.40560913085938\n",
      "2992/3000 train_loss: 11.594952583312988 test_loss:195.51258850097656\n",
      "2993/3000 train_loss: 11.16959285736084 test_loss:194.79505920410156\n",
      "2994/3000 train_loss: 13.074539184570312 test_loss:192.34774780273438\n",
      "2995/3000 train_loss: 14.135404586791992 test_loss:194.3181610107422\n",
      "2996/3000 train_loss: 11.534165382385254 test_loss:192.705810546875\n",
      "2997/3000 train_loss: 14.326517105102539 test_loss:192.84483337402344\n",
      "2998/3000 train_loss: 13.31045913696289 test_loss:203.36541748046875\n",
      "2999/3000 train_loss: 12.078407287597656 test_loss:187.47952270507812\n",
      "3000/3000 train_loss: 13.424030303955078 test_loss:189.0563507080078\n"
     ]
    }
   ],
   "source": [
    "losses = train(model = unet, optimizer = optimizer, criterion=criterion, data_tr=train_data,\n",
    "               data_val = test_data, scheduler = scheduler,device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "6Ew7_F0-q7aL",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "97ce7e7a-20fe-4bb9-c357-e6a862572ddc"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(189.0564)"
      ]
     },
     "metadata": {},
     "execution_count": 100
    }
   ],
   "source": [
    "avg_loss = 0\n",
    "# unet.eval()\n",
    "# unet.train()\n",
    "preds = []\n",
    "i = 0\n",
    "test_anomaly_losses = []\n",
    "test_normal_losses = []\n",
    "test_losses = []\n",
    "test_real = y_test.tolist()\n",
    "for batch in test_data:\n",
    "  with torch.no_grad():\n",
    "    # unet.train()\n",
    "    predictions = unet(batch.to(device)).cpu()\n",
    "    preds.append(predictions)\n",
    "  loss = criterion(predictions, batch.cpu())\n",
    "  for j in range(len(predictions)):\n",
    "    if int(y_test[i]) == 1:\n",
    "      test_anomaly_losses.append(float(criterion(predictions[j], batch[j])))\n",
    "    else:\n",
    "      test_normal_losses.append(float(criterion(predictions[j], batch[j])))\n",
    "    i += 1\n",
    "    test_losses.append(criterion(predictions[j], batch[j]))\n",
    "  # print(loss)\n",
    "  # print(loss)\n",
    "  avg_loss += loss / len(test_data)\n",
    "# avg_loss\n",
    "\n",
    "avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "VpDKorrRso9o",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "538d40fc-5f32-418e-ab61-7c7e8366b448"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(403.0717420038187, 15.858038498333523)"
      ]
     },
     "metadata": {},
     "execution_count": 101
    }
   ],
   "source": [
    "sum(test_anomaly_losses)/len(test_anomaly_losses) , sum(test_normal_losses)/len(test_normal_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5LEvbZKYuh7J",
    "outputId": "3f76d5ef-3aa0-49a9-8866-aeb6bc14384f"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.9801078167115904\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "vals = np.arange(40, 900, 0.5).tolist()\n",
    "for threshold in vals:\n",
    "  preds = []\n",
    "  for j in range(len(test_losses)):\n",
    "    if test_losses[j] > threshold:\n",
    "      preds.append(1)\n",
    "    else:\n",
    "      preds.append(0)\n",
    "  \n",
    "  results.append(roc_auc_score(test_real,preds))\n",
    "\n",
    "print(max(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QaSSqG8SbAw2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
