{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9SStKf4G0V5H"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim import Adam\n",
        "import torchaudio\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from skimage.util import img_as_ubyte\n",
        "import pandas as pd\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import librosa\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import io\n",
        "from sklearn.metrics import roc_auc_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XtxbKLZq5KX",
        "outputId": "0c6de58c-f736-440b-85ce-a6ad2ae77c29"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYxHegIM0Z4i",
        "outputId": "8b35e1ba-3d36-4aca-c569-e94fbad47a14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "h9DATQwS0ivD"
      },
      "outputs": [],
      "source": [
        "class MimiiDataset(Dataset):\n",
        "    def __init__(self,audio_dir, n_fft = 1024, win_length = 1024,\n",
        "                 hop_length = 512,power = 2,n_mels = 128,pad_mode = 'reflect',\n",
        "                 sr = 16000,center = True,norm = None):\n",
        "      \n",
        "        super(MimiiDataset, self).__init__()\n",
        "        self.audio_dir = audio_dir\n",
        "        self.n_mels = n_mels\n",
        "        self.n_fft = n_fft\n",
        "        self.win_length = win_length\n",
        "        self.hop_length = hop_length\n",
        "        self.power = power\n",
        "        self.pad_mode = pad_mode\n",
        "        self.sr = sr\n",
        "        self.center = center\n",
        "        self.norm = norm\n",
        "\n",
        "    def get_files(self):\n",
        "       return self.train_files, self.test_files\n",
        "    \n",
        "    def get_data(self,device, id):\n",
        "        \n",
        "        self.train_files, self.train_labels = self._train_file_list(device, id)\n",
        "        self.test_files, self.test_labels = self._test_file_list(device, id)\n",
        "        \n",
        "        self.train_data = self.get_audios(self.train_files)\n",
        "        self.test_data = self.get_audios(self.test_files)\n",
        "        \n",
        "        return self.train_data, self.test_data, self.train_labels, self.test_labels\n",
        "    \n",
        "    def _train_file_list(self, device, id):\n",
        "        query = os.path.abspath(\n",
        "            f\"{self.audio_dir}/{device}/train/normal_id_0{id}*.wav\"\n",
        "        )\n",
        "        train_normal_files = sorted(glob.glob(query))\n",
        "        train_normal_labels = np.zeros(len(train_normal_files))\n",
        "        \n",
        "        query = os.path.abspath(\n",
        "                f\"{self.audio_dir}/{device}/train/anomaly_id_0{id}*.wav\"\n",
        "            )\n",
        "        train_anomaly_files = sorted(glob.glob(query))\n",
        "        train_anomaly_labels = np.ones(len(train_anomaly_files))\n",
        "        \n",
        "        train_file_list = np.concatenate((train_normal_files, train_anomaly_files), axis=0)\n",
        "        train_labels = np.concatenate((train_normal_labels, train_anomaly_labels), axis=0)\n",
        "        \n",
        "        return train_file_list, train_labels\n",
        "    \n",
        "    def _test_file_list(self, device, id):     \n",
        "        query = os.path.abspath(\n",
        "            f\"{self.audio_dir}/{device}/test/normal_id_0{id}*.wav\"\n",
        "            )\n",
        "        test_normal_files = sorted(glob.glob(query))\n",
        "        test_normal_labels = np.zeros(len(test_normal_files))\n",
        "        \n",
        "        query = os.path.abspath(\n",
        "            f\"{self.audio_dir}/{device}/test/anomaly_id_0{id}*.wav\"\n",
        "            )\n",
        "        test_anomaly_files = sorted(glob.glob(query))\n",
        "        test_anomaly_labels = np.ones(len(test_anomaly_files))\n",
        "        \n",
        "        test_file_list = np.concatenate((test_normal_files, \n",
        "                                          test_anomaly_files), axis=0)\n",
        "        test_labels = np.concatenate((test_normal_labels,\n",
        "                                      test_anomaly_labels), axis=0)\n",
        "          \n",
        "        return test_file_list, test_labels\n",
        "\n",
        "    def normalize(self,tensor):\n",
        "        tensor_minusmean = tensor - tensor.mean()\n",
        "        return tensor_minusmean/np.absolute(tensor_minusmean).max()\n",
        "\n",
        "    def make0min(self,tensornd):\n",
        "        tensor = tensornd.numpy()\n",
        "        res = np.where(tensor == 0, 1E-19 , tensor)\n",
        "        return torch.from_numpy(res)\n",
        "\n",
        "    def spectrogrameToImage(self,specgram):\n",
        "        # specgram = torchaudio.transforms.MelSpectrogram(n_fft=1024, win_length=1024, \n",
        "        #                                                 hop_length=512, power=2, \n",
        "        #                                                 normalized=True, n_mels=128)(waveform )\n",
        "        specgram= self.make0min(specgram)\n",
        "        specgram = specgram.log2()[0,:,:].numpy()\n",
        "        \n",
        "        tr2image = transforms.Compose([transforms.ToPILImage()])\n",
        "\n",
        "        specgram= self.normalize(specgram)\n",
        "        # specgram = img_as_ubyte(specgram)\n",
        "        specgramImage = tr2image(specgram)\n",
        "        return specgramImage\n",
        "\n",
        "    def get_logmelspectrogram(self, waveform):\n",
        "        melspec = librosa.feature.melspectrogram(\n",
        "          n_fft=self.n_fft, win_length=self.win_length, hop_length=self.hop_length,\n",
        "          power=self.power,n_mels=self.n_mels,pad_mode=self.pad_mode,sr=self.sr,\n",
        "          center=self.center,norm=self.norm,htk=True,\n",
        "          y = waveform.numpy()\n",
        "        )\n",
        "\n",
        "        logmelspec = librosa.power_to_db(melspec)\n",
        "\n",
        "        return logmelspec\n",
        "\n",
        "    def get_melspectrogram(self,waveform):\n",
        "        melspec = librosa.feature.melspectrogram(\n",
        "            n_fft=self.n_fft, win_length=self.win_length, hop_length=self.hop_length,\n",
        "            power=self.power,n_mels=self.n_mels,pad_mode=self.pad_mode,sr=self.sr,\n",
        "            center=self.center,norm=self.norm,htk=True,\n",
        "            y = waveform.numpy()\n",
        "        )\n",
        "\n",
        "        return melspec\n",
        "    \n",
        "    def get_mfcc(self,waveform):\n",
        "        mfcc = librosa.feature.mfcc(    \n",
        "            n_fft=self.n_fft, win_length=self.win_length, \n",
        "            hop_length=self.hop_length,pad_mode=self.pad_mode,sr=self.sr,\n",
        "            center=self.center,norm=self.norm,n_mfcc=40,\n",
        "            y = waveform.numpy()\n",
        "        )\n",
        "\n",
        "        return mfcc\n",
        "\n",
        "    def get_chroma_stft(self,waveform):\n",
        "        stft = librosa.feature.chroma_stft(\n",
        "            n_fft=self.n_fft, win_length=self.win_length, \n",
        "            hop_length=self.hop_length,pad_mode=self.pad_mode,sr=self.sr,\n",
        "            center=self.center,norm=self.norm,n_chroma=12,\n",
        "            y=waveform.numpy()\n",
        "        )\n",
        "\n",
        "        return stft\n",
        "\n",
        "    def get_spectral_contrast(self,waveform):\n",
        "        spec_contrast = librosa.feature.spectral_contrast(    \n",
        "            n_fft=self.n_fft, win_length=self.win_length,center=self.center,\n",
        "            hop_length=self.hop_length,pad_mode=self.pad_mode,sr=self.sr,\n",
        "            y = waveform.numpy()\n",
        "        )\n",
        "\n",
        "        return spec_contrast\n",
        "    \n",
        "    def get_tonnetz(self,waveform):\n",
        "        harmonic = librosa.effects.harmonic(waveform.numpy())\n",
        "        tonnetz = librosa.feature.tonnetz(y=harmonic,sr=self.sr)\n",
        "\n",
        "        return tonnetz\n",
        "\n",
        "    def get_audios(self, file_list):\n",
        "        data = []\n",
        "        for i in range(len(file_list)):\n",
        "          y, sr = torchaudio.load(file_list[i])  \n",
        "          data.append(y)\n",
        "\n",
        "        return data\n",
        "    def _derive_data(self, file_list):\n",
        "        train_data = []\n",
        "        test_data = []\n",
        "        train_mode = True\n",
        "        for file_list in [self.train_files, self.test_files]:\n",
        "          tr2tensor = transforms.Compose([transforms.PILToTensor()])\n",
        "          data = []\n",
        "          for j in range(len(file_list)):\n",
        "            y, sr = torchaudio.load(file_list[j])  \n",
        "            spec = self.get_melspectrogram(y)\n",
        "            spec = self.spectrogrameToImage(spec)\n",
        "            spec = spec.convert('RGB')\n",
        "            vectors = tr2tensor(spec)\n",
        "            if train_mode:     \n",
        "              train_data.append(vectors)\n",
        "            else:\n",
        "              test_data.append(vectors)\n",
        "            \n",
        "          train_mode = False\n",
        "                \n",
        "        return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "S96soeIc0o13"
      },
      "outputs": [],
      "source": [
        "dataset = MimiiDataset('/content/drive/MyDrive/mimii')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "Gn2zdn92doi1"
      },
      "outputs": [],
      "source": [
        "_, _, y_train, y_test = dataset.get_data('ToyConveyor', 1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_mf, test_mf = torch.load('/content/drive/MyDrive/mixed_features/train_mf_toycar4.pt'), torch.load('/content/drive/MyDrive/mixed_features/test_mf_toycar4.pt')"
      ],
      "metadata": {
        "id": "MY5guG8kXa-Z"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "SgjpeWy_RV1C"
      },
      "outputs": [],
      "source": [
        "train_mixed_f = torch.load('/content/drive/MyDrive/mixed_features/train_mf_toyconveyor1.pt')\n",
        "test_mixed_f = torch.load('/content/drive/MyDrive/mixed_features/test_mf_toyconveyor1.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEl3qOh-mZVK",
        "outputId": "1b337aca-468c-4f98-b80c-20c92d234d97"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1000, 193])"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ],
      "source": [
        "train_mixed_f.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "jWMPVGu1qiEq"
      },
      "outputs": [],
      "source": [
        "train_data = DataLoader(train_mixed_f, batch_size=64, shuffle = True)\n",
        "test_data = DataLoader(test_mixed_f, batch_size = 64, shuffle= False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "vNTBTRe6qnBq"
      },
      "outputs": [],
      "source": [
        "class UNet_FC(nn.Module):\n",
        "\n",
        "  def __init__(self, in_features):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.bn = nn.BatchNorm1d(128)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "    self.fc0 = nn.Linear(in_features=in_features,out_features=in_features)\n",
        "\n",
        "    # encoder\n",
        "    self.fc1 = nn.Linear(in_features=in_features, out_features=128)\n",
        "    self.fc2 = nn.Linear(in_features=128,out_features=128)\n",
        "    self.fc3 = nn.Linear(in_features=128, out_features=128)\n",
        "    self.fc4 = nn.Linear(in_features=128, out_features=128)\n",
        "    self.fc5 = nn.Linear(in_features=128, out_features=8)\n",
        "\n",
        "    # decoder\n",
        "    self.fc6 = nn.Linear(in_features=8, out_features=128)\n",
        "    self.fc7 = nn.Linear(in_features=128*2, out_features=128)\n",
        "    self.fc8 = nn.Linear(in_features=128*2, out_features=128)\n",
        "    self.fc9 = nn.Linear(in_features=128*2, out_features=128)\n",
        "\n",
        "    self.out = nn.Linear(in_features=128*2, out_features=in_features)\n",
        "\n",
        "  def encoder(self, x):\n",
        "    input = self.fc0(x)\n",
        "\n",
        "    x1 = self.relu(self.bn(self.fc1(input)))\n",
        "    x2 = self.relu(self.bn(self.fc2(x1)))\n",
        "    x3 = self.relu(self.bn(self.fc3(x2)))\n",
        "    x4 = self.relu(self.bn(self.fc4(x3)))\n",
        "    x5 = self.relu(self.fc5(x4))\n",
        "\n",
        "    return [x5, x4, x3, x2, x1]\n",
        "\n",
        "  def decoder(self, x):\n",
        "    x6 = self.relu(self.fc6(x[0]))\n",
        "    con1 = torch.cat((x6,x[1]), 1) \n",
        "    x7 = self.relu(self.bn(self.fc7(con1)))\n",
        "    con2 = torch.cat((x7,x[2]), 1)\n",
        "    x8 = self.relu(self.bn(self.fc8(con2)))\n",
        "    con3 = torch.cat((x8,x[3]), 1)\n",
        "    x9 = self.relu(self.bn(self.fc9(con3)))\n",
        "    con4 = torch.cat((x9,x[4]), 1)\n",
        "\n",
        "    x10 = self.out(con4)\n",
        "\n",
        "    return x10\n",
        "  \n",
        "  def forward(self, x):\n",
        "    # encoded = self.encoder(x)\n",
        "\n",
        "    # decoded = self.decoder(encoded)\n",
        "    input = self.fc0(x)\n",
        "\n",
        "    x1 = self.relu(self.bn(self.fc1(input)))\n",
        "    x2 = self.relu(self.bn(self.fc2(x1)))\n",
        "    x3 = self.relu(self.bn(self.fc3(x2)))\n",
        "    x4 = self.relu(self.bn(self.fc4(x3)))\n",
        "    x5 = self.relu(self.fc5(x4))\n",
        "\n",
        "    xy = [x5, x4, x3, x2, x1]\n",
        "\n",
        "    x6 = self.relu(self.fc6(xy[0]))\n",
        "    con1 = torch.cat((x6,xy[1]), 1) \n",
        "    x7 = self.relu(self.bn(self.fc7(con1)))\n",
        "    con2 = torch.cat((x7,xy[2]), 1)\n",
        "    x8 = self.relu(self.bn(self.fc8(con2)))\n",
        "    con3 = torch.cat((x8,xy[3]), 1)\n",
        "    x9 = self.relu(self.bn(self.fc9(con3)))\n",
        "    con4 = torch.cat((x9,xy[4]), 1)\n",
        "\n",
        "    x10 = self.out(con4)\n",
        "\n",
        "    # return decoded\n",
        "    return x10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "ZfgcBtQ3qn5l"
      },
      "outputs": [],
      "source": [
        "def train(model, optimizer, criterion, data_tr, data_val, scheduler = None,\n",
        "          epochs = 3000, device = 'cpu'):\n",
        "    # X_val, Y_val = next(iter(data_val))\n",
        "    losses = []\n",
        "    prev_avg_loss = 100000\n",
        "    for epoch in range(epochs):\n",
        "        train_avg_loss = 0\n",
        "        test_avg_loss = 0\n",
        "        # model.train()  # train mode\n",
        "        for batch in data_tr:\n",
        "          # data to device\n",
        "          batch = batch.to(device)\n",
        "          # set parameter gradients to zero\n",
        "          optimizer.zero_grad()\n",
        "          # forward\n",
        "          # print(Y_batch.shape)\n",
        "          predictions = model(batch)\n",
        "          loss = criterion(predictions, batch)\n",
        "          loss.backward() # backward-pass\n",
        "          optimizer.step()  # update weights\n",
        "          # calculate loss to show the user\n",
        "          if scheduler:\n",
        "            scheduler.step(loss)\n",
        "          train_avg_loss += loss / len(data_tr)\n",
        "\n",
        "        # model.eval()\n",
        "        for batch in data_val:\n",
        "          with torch.no_grad():\n",
        "            preds = model(batch.to(device)).cpu()\n",
        "            loss = criterion(preds,batch)\n",
        "            test_avg_loss += loss / len(data_val)\n",
        "                    \n",
        "        losses.append(train_avg_loss.item())\n",
        "        # if (epoch+1)%50 == 0:\n",
        "        print(\"{}/{} train_loss: {} test_loss:{}\".format(epoch+1, epochs, train_avg_loss, test_avg_loss))\n",
        "        # if test_avg_loss < 70:\n",
        "        #   break\n",
        "    return losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "ptkVTF55quOL"
      },
      "outputs": [],
      "source": [
        "unet = UNet_FC(in_features=193).to(device)\n",
        "optimizer = Adam(params = unet.parameters(), lr = 10e-3)\n",
        "# optimizer = Adam(params = unet.parameters())\n",
        "criterion = nn.MSELoss()\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, \n",
        "                                                       min_lr=10e-4, mode = 'min',\n",
        "                                                       patience = 30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkfmYl9oXhcB",
        "outputId": "31895e61-19ba-419c-9d56-fa62ac97ad68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/3000 train_loss: 192830.21875 test_loss:183578.109375\n",
            "2/3000 train_loss: 188485.265625 test_loss:178800.21875\n",
            "3/3000 train_loss: 183251.46875 test_loss:172817.390625\n",
            "4/3000 train_loss: 176227.734375 test_loss:165025.09375\n",
            "5/3000 train_loss: 167169.3125 test_loss:155102.859375\n",
            "6/3000 train_loss: 156369.640625 test_loss:143367.546875\n",
            "7/3000 train_loss: 143577.453125 test_loss:129848.25\n",
            "8/3000 train_loss: 129536.6171875 test_loss:115286.2109375\n",
            "9/3000 train_loss: 113364.453125 test_loss:99330.6640625\n",
            "10/3000 train_loss: 97094.5078125 test_loss:83877.8046875\n",
            "11/3000 train_loss: 81835.796875 test_loss:70121.875\n",
            "12/3000 train_loss: 67833.546875 test_loss:56991.7890625\n",
            "13/3000 train_loss: 55354.859375 test_loss:45414.30859375\n",
            "14/3000 train_loss: 43937.85546875 test_loss:36221.94921875\n",
            "15/3000 train_loss: 34704.890625 test_loss:27901.791015625\n",
            "16/3000 train_loss: 26819.681640625 test_loss:20857.37890625\n",
            "17/3000 train_loss: 20065.15625 test_loss:15139.126953125\n",
            "18/3000 train_loss: 14880.205078125 test_loss:10916.3046875\n",
            "19/3000 train_loss: 13896.59375 test_loss:11270.2900390625\n",
            "20/3000 train_loss: 9746.5859375 test_loss:8363.40234375\n",
            "21/3000 train_loss: 7169.27294921875 test_loss:6502.86083984375\n",
            "22/3000 train_loss: 5341.31689453125 test_loss:5125.3955078125\n",
            "23/3000 train_loss: 3922.221923828125 test_loss:4188.11474609375\n",
            "24/3000 train_loss: 3553.3701171875 test_loss:3598.0908203125\n",
            "25/3000 train_loss: 2256.679443359375 test_loss:3168.12744140625\n",
            "26/3000 train_loss: 2426.3505859375 test_loss:2916.000732421875\n",
            "27/3000 train_loss: 1747.7874755859375 test_loss:2695.189453125\n",
            "28/3000 train_loss: 1528.7476806640625 test_loss:2561.268798828125\n",
            "29/3000 train_loss: 1231.837646484375 test_loss:2421.289306640625\n",
            "30/3000 train_loss: 1420.1461181640625 test_loss:2308.30810546875\n",
            "31/3000 train_loss: 1364.7021484375 test_loss:2218.40673828125\n",
            "32/3000 train_loss: 911.1310424804688 test_loss:2119.879638671875\n",
            "33/3000 train_loss: 1057.5982666015625 test_loss:2041.3367919921875\n",
            "34/3000 train_loss: 1066.9561767578125 test_loss:1792.825927734375\n",
            "35/3000 train_loss: 1050.778076171875 test_loss:1661.1072998046875\n",
            "36/3000 train_loss: 965.184326171875 test_loss:1602.2100830078125\n",
            "37/3000 train_loss: 816.4889526367188 test_loss:1490.7950439453125\n",
            "38/3000 train_loss: 815.91064453125 test_loss:1449.122314453125\n",
            "39/3000 train_loss: 920.233154296875 test_loss:1454.3525390625\n",
            "40/3000 train_loss: 951.2140502929688 test_loss:1447.97314453125\n",
            "41/3000 train_loss: 940.8458862304688 test_loss:1457.2540283203125\n",
            "42/3000 train_loss: 1063.78125 test_loss:1458.0419921875\n",
            "43/3000 train_loss: 822.8948974609375 test_loss:1436.6077880859375\n",
            "44/3000 train_loss: 858.47998046875 test_loss:1429.103759765625\n",
            "45/3000 train_loss: 826.1602172851562 test_loss:1393.55859375\n",
            "46/3000 train_loss: 811.2809448242188 test_loss:1348.9404296875\n",
            "47/3000 train_loss: 708.4492797851562 test_loss:1332.644775390625\n",
            "48/3000 train_loss: 706.4981079101562 test_loss:1331.321533203125\n",
            "49/3000 train_loss: 660.2281494140625 test_loss:1317.768798828125\n",
            "50/3000 train_loss: 770.8928833007812 test_loss:1315.4132080078125\n",
            "51/3000 train_loss: 780.576171875 test_loss:1306.33154296875\n",
            "52/3000 train_loss: 792.836181640625 test_loss:1302.38671875\n",
            "53/3000 train_loss: 680.723388671875 test_loss:1281.129638671875\n",
            "54/3000 train_loss: 657.5570678710938 test_loss:1266.4735107421875\n",
            "55/3000 train_loss: 872.7493896484375 test_loss:1233.8177490234375\n",
            "56/3000 train_loss: 843.28564453125 test_loss:1158.2374267578125\n",
            "57/3000 train_loss: 707.3968505859375 test_loss:1160.6036376953125\n",
            "58/3000 train_loss: 645.2693481445312 test_loss:1151.36669921875\n",
            "59/3000 train_loss: 719.5189819335938 test_loss:1127.0531005859375\n",
            "60/3000 train_loss: 757.7936401367188 test_loss:1065.4853515625\n",
            "61/3000 train_loss: 674.4056396484375 test_loss:1033.3076171875\n",
            "62/3000 train_loss: 612.7918701171875 test_loss:1020.8178100585938\n",
            "63/3000 train_loss: 690.28173828125 test_loss:1004.0426025390625\n",
            "64/3000 train_loss: 521.361083984375 test_loss:996.55029296875\n",
            "65/3000 train_loss: 655.9884643554688 test_loss:973.592529296875\n",
            "66/3000 train_loss: 579.3087158203125 test_loss:955.2640380859375\n",
            "67/3000 train_loss: 580.5573120117188 test_loss:907.302490234375\n",
            "68/3000 train_loss: 715.9512329101562 test_loss:800.3130493164062\n",
            "69/3000 train_loss: 612.6096801757812 test_loss:737.19189453125\n",
            "70/3000 train_loss: 549.0017700195312 test_loss:721.830078125\n",
            "71/3000 train_loss: 495.9385986328125 test_loss:721.8065795898438\n",
            "72/3000 train_loss: 508.7793884277344 test_loss:715.5189208984375\n",
            "73/3000 train_loss: 545.4103393554688 test_loss:685.571533203125\n",
            "74/3000 train_loss: 466.5339050292969 test_loss:687.7045288085938\n",
            "75/3000 train_loss: 443.8188171386719 test_loss:693.255615234375\n",
            "76/3000 train_loss: 483.2584533691406 test_loss:656.8936767578125\n",
            "77/3000 train_loss: 490.6162109375 test_loss:605.6755981445312\n",
            "78/3000 train_loss: 430.783935546875 test_loss:614.1181030273438\n",
            "79/3000 train_loss: 446.66192626953125 test_loss:621.3971557617188\n",
            "80/3000 train_loss: 477.99658203125 test_loss:591.0250244140625\n",
            "81/3000 train_loss: 442.58355712890625 test_loss:616.572998046875\n",
            "82/3000 train_loss: 451.2935791015625 test_loss:600.3706665039062\n",
            "83/3000 train_loss: 446.27337646484375 test_loss:595.2003784179688\n",
            "84/3000 train_loss: 386.0856628417969 test_loss:570.97900390625\n",
            "85/3000 train_loss: 400.1297302246094 test_loss:556.7981567382812\n",
            "86/3000 train_loss: 452.9358215332031 test_loss:543.32568359375\n",
            "87/3000 train_loss: 396.1170654296875 test_loss:531.2312622070312\n",
            "88/3000 train_loss: 385.513427734375 test_loss:567.18017578125\n",
            "89/3000 train_loss: 410.8374938964844 test_loss:511.082275390625\n",
            "90/3000 train_loss: 375.7085266113281 test_loss:491.951904296875\n",
            "91/3000 train_loss: 369.76446533203125 test_loss:504.2140808105469\n",
            "92/3000 train_loss: 403.42413330078125 test_loss:521.194580078125\n",
            "93/3000 train_loss: 363.7758483886719 test_loss:481.0204772949219\n",
            "94/3000 train_loss: 349.08465576171875 test_loss:501.610595703125\n",
            "95/3000 train_loss: 424.3625183105469 test_loss:511.5495300292969\n",
            "96/3000 train_loss: 380.70147705078125 test_loss:495.9334411621094\n",
            "97/3000 train_loss: 342.831787109375 test_loss:468.5150451660156\n",
            "98/3000 train_loss: 351.98309326171875 test_loss:487.71832275390625\n",
            "99/3000 train_loss: 378.57421875 test_loss:477.8422546386719\n",
            "100/3000 train_loss: 370.27105712890625 test_loss:486.0812683105469\n",
            "101/3000 train_loss: 344.241455078125 test_loss:460.24383544921875\n",
            "102/3000 train_loss: 344.8656921386719 test_loss:446.05511474609375\n",
            "103/3000 train_loss: 310.5372009277344 test_loss:445.503662109375\n",
            "104/3000 train_loss: 309.1165466308594 test_loss:478.21551513671875\n",
            "105/3000 train_loss: 359.0222473144531 test_loss:485.73223876953125\n",
            "106/3000 train_loss: 352.0250244140625 test_loss:447.28887939453125\n",
            "107/3000 train_loss: 306.5878601074219 test_loss:483.5395812988281\n",
            "108/3000 train_loss: 312.0268859863281 test_loss:448.979248046875\n",
            "109/3000 train_loss: 319.5358581542969 test_loss:433.11370849609375\n",
            "110/3000 train_loss: 314.6103210449219 test_loss:435.14276123046875\n",
            "111/3000 train_loss: 311.53118896484375 test_loss:424.2280578613281\n",
            "112/3000 train_loss: 290.87060546875 test_loss:446.7894592285156\n",
            "113/3000 train_loss: 300.95831298828125 test_loss:422.5775451660156\n",
            "114/3000 train_loss: 282.40478515625 test_loss:457.0751953125\n",
            "115/3000 train_loss: 306.4090576171875 test_loss:425.3006896972656\n",
            "116/3000 train_loss: 302.7209167480469 test_loss:411.4667663574219\n",
            "117/3000 train_loss: 311.5087890625 test_loss:418.44970703125\n",
            "118/3000 train_loss: 321.01824951171875 test_loss:410.69140625\n",
            "119/3000 train_loss: 271.80615234375 test_loss:423.738037109375\n",
            "120/3000 train_loss: 329.7298583984375 test_loss:413.9801330566406\n",
            "121/3000 train_loss: 307.2937927246094 test_loss:418.35736083984375\n",
            "122/3000 train_loss: 283.5719299316406 test_loss:447.9878234863281\n",
            "123/3000 train_loss: 288.07470703125 test_loss:420.6781005859375\n",
            "124/3000 train_loss: 321.3920593261719 test_loss:426.7656555175781\n",
            "125/3000 train_loss: 287.0963134765625 test_loss:406.36175537109375\n",
            "126/3000 train_loss: 303.1585388183594 test_loss:414.5440979003906\n",
            "127/3000 train_loss: 303.76776123046875 test_loss:408.9901428222656\n",
            "128/3000 train_loss: 291.9951171875 test_loss:408.6292724609375\n",
            "129/3000 train_loss: 274.8095397949219 test_loss:418.0113525390625\n",
            "130/3000 train_loss: 304.14202880859375 test_loss:403.7470397949219\n",
            "131/3000 train_loss: 283.7138671875 test_loss:400.2579345703125\n",
            "132/3000 train_loss: 266.3168640136719 test_loss:410.6394958496094\n",
            "133/3000 train_loss: 291.2038269042969 test_loss:401.1656494140625\n",
            "134/3000 train_loss: 275.5929870605469 test_loss:408.7253723144531\n",
            "135/3000 train_loss: 285.4144592285156 test_loss:391.52178955078125\n",
            "136/3000 train_loss: 270.94805908203125 test_loss:408.97760009765625\n",
            "137/3000 train_loss: 285.9267272949219 test_loss:392.62255859375\n",
            "138/3000 train_loss: 261.34814453125 test_loss:411.60150146484375\n",
            "139/3000 train_loss: 314.5419921875 test_loss:390.9432373046875\n",
            "140/3000 train_loss: 288.8305358886719 test_loss:394.4932861328125\n",
            "141/3000 train_loss: 275.40606689453125 test_loss:393.5706787109375\n",
            "142/3000 train_loss: 263.83587646484375 test_loss:392.08148193359375\n",
            "143/3000 train_loss: 270.6683044433594 test_loss:382.9342346191406\n",
            "144/3000 train_loss: 258.11175537109375 test_loss:409.47113037109375\n",
            "145/3000 train_loss: 281.58056640625 test_loss:398.40771484375\n",
            "146/3000 train_loss: 282.7442321777344 test_loss:380.9471740722656\n",
            "147/3000 train_loss: 316.0338134765625 test_loss:386.36627197265625\n",
            "148/3000 train_loss: 273.1595153808594 test_loss:399.6062927246094\n",
            "149/3000 train_loss: 314.5224304199219 test_loss:405.7576599121094\n",
            "150/3000 train_loss: 259.9688415527344 test_loss:387.7347717285156\n",
            "151/3000 train_loss: 248.78756713867188 test_loss:397.6468505859375\n",
            "152/3000 train_loss: 281.5190734863281 test_loss:420.9419250488281\n",
            "153/3000 train_loss: 279.9225158691406 test_loss:379.2570495605469\n",
            "154/3000 train_loss: 249.36448669433594 test_loss:415.29351806640625\n",
            "155/3000 train_loss: 269.8633728027344 test_loss:386.6585998535156\n",
            "156/3000 train_loss: 262.2579650878906 test_loss:373.7801818847656\n",
            "157/3000 train_loss: 270.22137451171875 test_loss:381.7773742675781\n",
            "158/3000 train_loss: 273.4171142578125 test_loss:382.95928955078125\n",
            "159/3000 train_loss: 257.732666015625 test_loss:384.9297180175781\n",
            "160/3000 train_loss: 254.02349853515625 test_loss:377.0913391113281\n",
            "161/3000 train_loss: 260.0898742675781 test_loss:374.650390625\n",
            "162/3000 train_loss: 253.47242736816406 test_loss:367.12530517578125\n",
            "163/3000 train_loss: 246.38052368164062 test_loss:375.44024658203125\n",
            "164/3000 train_loss: 253.35678100585938 test_loss:365.3713073730469\n",
            "165/3000 train_loss: 266.06903076171875 test_loss:372.6063232421875\n",
            "166/3000 train_loss: 270.9111328125 test_loss:390.4228210449219\n",
            "167/3000 train_loss: 252.22402954101562 test_loss:372.46746826171875\n",
            "168/3000 train_loss: 256.8929443359375 test_loss:380.1910400390625\n",
            "169/3000 train_loss: 261.03277587890625 test_loss:362.62640380859375\n",
            "170/3000 train_loss: 258.0560607910156 test_loss:361.51922607421875\n",
            "171/3000 train_loss: 255.0899658203125 test_loss:372.13580322265625\n",
            "172/3000 train_loss: 249.67910766601562 test_loss:401.9114990234375\n",
            "173/3000 train_loss: 271.3641662597656 test_loss:371.8890686035156\n",
            "174/3000 train_loss: 226.3944091796875 test_loss:367.46087646484375\n",
            "175/3000 train_loss: 225.96334838867188 test_loss:371.58135986328125\n",
            "176/3000 train_loss: 258.98736572265625 test_loss:362.3526611328125\n",
            "177/3000 train_loss: 238.28575134277344 test_loss:357.51617431640625\n",
            "178/3000 train_loss: 248.3171844482422 test_loss:359.5489196777344\n",
            "179/3000 train_loss: 255.74285888671875 test_loss:358.2100524902344\n",
            "180/3000 train_loss: 237.01780700683594 test_loss:356.4769287109375\n",
            "181/3000 train_loss: 241.3630828857422 test_loss:368.1639404296875\n",
            "182/3000 train_loss: 258.4290466308594 test_loss:355.9315185546875\n",
            "183/3000 train_loss: 249.69891357421875 test_loss:352.84893798828125\n",
            "184/3000 train_loss: 245.95237731933594 test_loss:369.8265380859375\n",
            "185/3000 train_loss: 235.2091522216797 test_loss:364.789306640625\n",
            "186/3000 train_loss: 283.1847839355469 test_loss:349.95501708984375\n",
            "187/3000 train_loss: 247.23670959472656 test_loss:353.5424499511719\n",
            "188/3000 train_loss: 245.9476318359375 test_loss:353.0428466796875\n",
            "189/3000 train_loss: 236.66856384277344 test_loss:354.5908203125\n",
            "190/3000 train_loss: 250.9940643310547 test_loss:353.16033935546875\n",
            "191/3000 train_loss: 258.58447265625 test_loss:355.8075866699219\n",
            "192/3000 train_loss: 227.42803955078125 test_loss:348.9710998535156\n",
            "193/3000 train_loss: 265.658935546875 test_loss:349.66357421875\n",
            "194/3000 train_loss: 250.2928466796875 test_loss:354.1153564453125\n",
            "195/3000 train_loss: 245.14491271972656 test_loss:357.91448974609375\n",
            "196/3000 train_loss: 236.1202850341797 test_loss:362.3779296875\n",
            "197/3000 train_loss: 230.57994079589844 test_loss:343.7486572265625\n",
            "198/3000 train_loss: 273.00732421875 test_loss:345.31854248046875\n",
            "199/3000 train_loss: 272.5242004394531 test_loss:343.38983154296875\n",
            "200/3000 train_loss: 249.00526428222656 test_loss:358.0745849609375\n",
            "201/3000 train_loss: 249.5139617919922 test_loss:356.22637939453125\n",
            "202/3000 train_loss: 254.30067443847656 test_loss:363.30657958984375\n",
            "203/3000 train_loss: 251.09713745117188 test_loss:357.3296813964844\n",
            "204/3000 train_loss: 212.04660034179688 test_loss:343.1403503417969\n",
            "205/3000 train_loss: 234.51947021484375 test_loss:339.85186767578125\n",
            "206/3000 train_loss: 234.2678680419922 test_loss:347.1402587890625\n",
            "207/3000 train_loss: 263.22222900390625 test_loss:355.62274169921875\n",
            "208/3000 train_loss: 220.23941040039062 test_loss:345.41424560546875\n",
            "209/3000 train_loss: 219.21221923828125 test_loss:342.5946044921875\n",
            "210/3000 train_loss: 227.91421508789062 test_loss:344.7107238769531\n",
            "211/3000 train_loss: 206.4622802734375 test_loss:345.38739013671875\n",
            "212/3000 train_loss: 238.0312957763672 test_loss:340.6771545410156\n",
            "213/3000 train_loss: 222.73402404785156 test_loss:333.0953063964844\n",
            "214/3000 train_loss: 262.69549560546875 test_loss:337.1528015136719\n",
            "215/3000 train_loss: 227.6587677001953 test_loss:341.604248046875\n",
            "216/3000 train_loss: 229.24717712402344 test_loss:336.982666015625\n",
            "217/3000 train_loss: 278.8490905761719 test_loss:357.9603576660156\n",
            "218/3000 train_loss: 299.9827880859375 test_loss:361.9818420410156\n",
            "219/3000 train_loss: 263.2390441894531 test_loss:349.7310485839844\n",
            "220/3000 train_loss: 247.17843627929688 test_loss:334.8375244140625\n",
            "221/3000 train_loss: 241.82037353515625 test_loss:329.56695556640625\n",
            "222/3000 train_loss: 240.42770385742188 test_loss:324.9538879394531\n",
            "223/3000 train_loss: 206.84188842773438 test_loss:332.4824523925781\n",
            "224/3000 train_loss: 246.00917053222656 test_loss:324.1377868652344\n",
            "225/3000 train_loss: 230.23681640625 test_loss:331.9373779296875\n",
            "226/3000 train_loss: 228.8062744140625 test_loss:319.3749084472656\n",
            "227/3000 train_loss: 226.981689453125 test_loss:332.44708251953125\n",
            "228/3000 train_loss: 228.63209533691406 test_loss:343.5795593261719\n",
            "229/3000 train_loss: 220.18800354003906 test_loss:335.2420959472656\n",
            "230/3000 train_loss: 221.52621459960938 test_loss:335.4288024902344\n",
            "231/3000 train_loss: 208.3146209716797 test_loss:326.8325500488281\n",
            "232/3000 train_loss: 232.78469848632812 test_loss:319.61767578125\n",
            "233/3000 train_loss: 221.16900634765625 test_loss:324.56610107421875\n",
            "234/3000 train_loss: 226.66844177246094 test_loss:324.13818359375\n",
            "235/3000 train_loss: 226.90701293945312 test_loss:320.6179504394531\n",
            "236/3000 train_loss: 234.00982666015625 test_loss:329.3307189941406\n",
            "237/3000 train_loss: 230.34693908691406 test_loss:326.41534423828125\n",
            "238/3000 train_loss: 261.16448974609375 test_loss:321.7433776855469\n",
            "239/3000 train_loss: 214.43923950195312 test_loss:324.5389404296875\n",
            "240/3000 train_loss: 209.27401733398438 test_loss:314.4020690917969\n",
            "241/3000 train_loss: 208.63404846191406 test_loss:316.3454284667969\n",
            "242/3000 train_loss: 203.1060791015625 test_loss:316.9097900390625\n",
            "243/3000 train_loss: 201.08763122558594 test_loss:328.1883544921875\n",
            "244/3000 train_loss: 209.09927368164062 test_loss:319.96026611328125\n",
            "245/3000 train_loss: 214.1640625 test_loss:312.93023681640625\n",
            "246/3000 train_loss: 231.671630859375 test_loss:319.2344665527344\n",
            "247/3000 train_loss: 206.41566467285156 test_loss:311.8979797363281\n",
            "248/3000 train_loss: 226.17066955566406 test_loss:314.61895751953125\n",
            "249/3000 train_loss: 199.25616455078125 test_loss:315.572998046875\n",
            "250/3000 train_loss: 207.2876739501953 test_loss:327.703125\n",
            "251/3000 train_loss: 190.08497619628906 test_loss:312.1466979980469\n",
            "252/3000 train_loss: 382.173583984375 test_loss:318.6315612792969\n",
            "253/3000 train_loss: 247.07041931152344 test_loss:432.7559509277344\n",
            "254/3000 train_loss: 251.80845642089844 test_loss:363.209228515625\n",
            "255/3000 train_loss: 234.68875122070312 test_loss:328.7752685546875\n",
            "256/3000 train_loss: 215.79843139648438 test_loss:315.23907470703125\n",
            "257/3000 train_loss: 197.37921142578125 test_loss:314.838623046875\n",
            "258/3000 train_loss: 199.2852325439453 test_loss:314.000244140625\n",
            "259/3000 train_loss: 177.92681884765625 test_loss:311.18408203125\n",
            "260/3000 train_loss: 206.25047302246094 test_loss:305.0633239746094\n",
            "261/3000 train_loss: 231.8993377685547 test_loss:306.91455078125\n",
            "262/3000 train_loss: 224.3752899169922 test_loss:323.03619384765625\n",
            "263/3000 train_loss: 221.92633056640625 test_loss:309.5765686035156\n",
            "264/3000 train_loss: 204.83355712890625 test_loss:313.212890625\n",
            "265/3000 train_loss: 203.21871948242188 test_loss:309.46270751953125\n",
            "266/3000 train_loss: 198.18466186523438 test_loss:305.7121887207031\n",
            "267/3000 train_loss: 212.2660675048828 test_loss:306.6131286621094\n",
            "268/3000 train_loss: 195.30262756347656 test_loss:301.89617919921875\n",
            "269/3000 train_loss: 193.85983276367188 test_loss:304.890380859375\n",
            "270/3000 train_loss: 198.08387756347656 test_loss:308.78521728515625\n",
            "271/3000 train_loss: 201.65869140625 test_loss:305.2381591796875\n",
            "272/3000 train_loss: 228.87510681152344 test_loss:298.703125\n",
            "273/3000 train_loss: 189.52406311035156 test_loss:303.8380126953125\n",
            "274/3000 train_loss: 188.7958984375 test_loss:316.00341796875\n",
            "275/3000 train_loss: 195.01632690429688 test_loss:308.72039794921875\n",
            "276/3000 train_loss: 205.96249389648438 test_loss:302.52783203125\n",
            "277/3000 train_loss: 187.1474609375 test_loss:296.9862060546875\n",
            "278/3000 train_loss: 203.89727783203125 test_loss:308.1431884765625\n",
            "279/3000 train_loss: 205.28367614746094 test_loss:308.522705078125\n",
            "280/3000 train_loss: 185.99935913085938 test_loss:300.86346435546875\n",
            "281/3000 train_loss: 187.04147338867188 test_loss:301.64483642578125\n",
            "282/3000 train_loss: 208.17507934570312 test_loss:310.1610412597656\n",
            "283/3000 train_loss: 204.2716827392578 test_loss:308.0475769042969\n",
            "284/3000 train_loss: 187.79344177246094 test_loss:302.45037841796875\n",
            "285/3000 train_loss: 196.4217987060547 test_loss:301.404052734375\n",
            "286/3000 train_loss: 191.49693298339844 test_loss:305.14947509765625\n",
            "287/3000 train_loss: 205.82577514648438 test_loss:296.6679382324219\n",
            "288/3000 train_loss: 193.046142578125 test_loss:298.2281799316406\n",
            "289/3000 train_loss: 213.31832885742188 test_loss:291.0220947265625\n",
            "290/3000 train_loss: 187.6013641357422 test_loss:298.18756103515625\n",
            "291/3000 train_loss: 216.7123565673828 test_loss:295.17205810546875\n",
            "292/3000 train_loss: 196.54690551757812 test_loss:302.50238037109375\n",
            "293/3000 train_loss: 184.7865753173828 test_loss:289.8780822753906\n",
            "294/3000 train_loss: 173.560791015625 test_loss:292.7822570800781\n",
            "295/3000 train_loss: 183.00424194335938 test_loss:295.6699523925781\n",
            "296/3000 train_loss: 180.90296936035156 test_loss:300.0828857421875\n",
            "297/3000 train_loss: 181.47836303710938 test_loss:303.5750732421875\n",
            "298/3000 train_loss: 167.7396697998047 test_loss:289.4925231933594\n",
            "299/3000 train_loss: 199.8104248046875 test_loss:288.3162536621094\n",
            "300/3000 train_loss: 199.7288360595703 test_loss:295.9833679199219\n",
            "301/3000 train_loss: 184.0565185546875 test_loss:300.2251281738281\n",
            "302/3000 train_loss: 168.47589111328125 test_loss:297.5421142578125\n",
            "303/3000 train_loss: 197.10955810546875 test_loss:292.3332214355469\n",
            "304/3000 train_loss: 189.5308380126953 test_loss:291.4682922363281\n",
            "305/3000 train_loss: 213.1475830078125 test_loss:293.9742431640625\n",
            "306/3000 train_loss: 194.1623077392578 test_loss:299.39605712890625\n",
            "307/3000 train_loss: 203.66236877441406 test_loss:289.37628173828125\n",
            "308/3000 train_loss: 202.56678771972656 test_loss:291.1457824707031\n",
            "309/3000 train_loss: 184.1878204345703 test_loss:288.43817138671875\n",
            "310/3000 train_loss: 173.758056640625 test_loss:288.3770751953125\n",
            "311/3000 train_loss: 192.72813415527344 test_loss:282.1950988769531\n",
            "312/3000 train_loss: 188.79368591308594 test_loss:280.6436767578125\n",
            "313/3000 train_loss: 205.61935424804688 test_loss:294.79412841796875\n",
            "314/3000 train_loss: 201.435791015625 test_loss:292.4745178222656\n",
            "315/3000 train_loss: 180.0807342529297 test_loss:302.130859375\n",
            "316/3000 train_loss: 195.81056213378906 test_loss:287.5987548828125\n",
            "317/3000 train_loss: 175.72860717773438 test_loss:293.485595703125\n",
            "318/3000 train_loss: 166.0543670654297 test_loss:283.2095642089844\n",
            "319/3000 train_loss: 175.0325927734375 test_loss:278.8898620605469\n",
            "320/3000 train_loss: 189.651611328125 test_loss:283.79119873046875\n",
            "321/3000 train_loss: 158.66925048828125 test_loss:284.677490234375\n",
            "322/3000 train_loss: 196.98434448242188 test_loss:284.9806213378906\n",
            "323/3000 train_loss: 192.58267211914062 test_loss:281.70928955078125\n",
            "324/3000 train_loss: 190.1739501953125 test_loss:281.7672424316406\n",
            "325/3000 train_loss: 154.17318725585938 test_loss:277.8193359375\n",
            "326/3000 train_loss: 171.4225616455078 test_loss:285.2864074707031\n",
            "327/3000 train_loss: 174.61073303222656 test_loss:289.9897766113281\n",
            "328/3000 train_loss: 180.14035034179688 test_loss:285.8476867675781\n",
            "329/3000 train_loss: 180.3610076904297 test_loss:275.25390625\n",
            "330/3000 train_loss: 169.15261840820312 test_loss:281.400390625\n",
            "331/3000 train_loss: 183.712646484375 test_loss:289.2590026855469\n",
            "332/3000 train_loss: 182.08816528320312 test_loss:286.0344543457031\n",
            "333/3000 train_loss: 190.26943969726562 test_loss:278.3052062988281\n",
            "334/3000 train_loss: 186.606201171875 test_loss:280.4301452636719\n",
            "335/3000 train_loss: 164.388671875 test_loss:288.5895690917969\n",
            "336/3000 train_loss: 173.10191345214844 test_loss:273.0785217285156\n",
            "337/3000 train_loss: 157.08389282226562 test_loss:275.8408203125\n",
            "338/3000 train_loss: 181.53746032714844 test_loss:276.47003173828125\n",
            "339/3000 train_loss: 175.85337829589844 test_loss:279.5065002441406\n",
            "340/3000 train_loss: 165.01657104492188 test_loss:277.0889587402344\n",
            "341/3000 train_loss: 167.73414611816406 test_loss:280.8317565917969\n",
            "342/3000 train_loss: 182.9512939453125 test_loss:276.39642333984375\n",
            "343/3000 train_loss: 169.49632263183594 test_loss:273.7166442871094\n",
            "344/3000 train_loss: 161.1212615966797 test_loss:272.5787048339844\n",
            "345/3000 train_loss: 159.0415802001953 test_loss:278.73736572265625\n",
            "346/3000 train_loss: 180.80226135253906 test_loss:275.58184814453125\n",
            "347/3000 train_loss: 173.4552001953125 test_loss:277.425048828125\n",
            "348/3000 train_loss: 164.77284240722656 test_loss:273.9354248046875\n",
            "349/3000 train_loss: 150.69876098632812 test_loss:273.979248046875\n",
            "350/3000 train_loss: 159.9821319580078 test_loss:271.9866943359375\n",
            "351/3000 train_loss: 159.4732208251953 test_loss:270.363037109375\n",
            "352/3000 train_loss: 169.66668701171875 test_loss:283.24713134765625\n",
            "353/3000 train_loss: 170.154052734375 test_loss:274.0218505859375\n",
            "354/3000 train_loss: 168.75762939453125 test_loss:279.5815734863281\n",
            "355/3000 train_loss: 180.1653289794922 test_loss:274.84368896484375\n",
            "356/3000 train_loss: 144.86419677734375 test_loss:277.95367431640625\n",
            "357/3000 train_loss: 144.4932403564453 test_loss:276.89337158203125\n",
            "358/3000 train_loss: 153.58572387695312 test_loss:269.9915466308594\n",
            "359/3000 train_loss: 187.8766632080078 test_loss:276.56060791015625\n",
            "360/3000 train_loss: 158.54229736328125 test_loss:266.2424621582031\n",
            "361/3000 train_loss: 170.8243408203125 test_loss:276.82855224609375\n",
            "362/3000 train_loss: 193.4939727783203 test_loss:264.89825439453125\n",
            "363/3000 train_loss: 176.37112426757812 test_loss:277.73541259765625\n",
            "364/3000 train_loss: 172.45335388183594 test_loss:261.610107421875\n",
            "365/3000 train_loss: 201.94400024414062 test_loss:268.20806884765625\n",
            "366/3000 train_loss: 187.0052032470703 test_loss:276.0428161621094\n",
            "367/3000 train_loss: 159.75527954101562 test_loss:267.5339050292969\n",
            "368/3000 train_loss: 158.514892578125 test_loss:267.6864318847656\n",
            "369/3000 train_loss: 153.088623046875 test_loss:264.29083251953125\n",
            "370/3000 train_loss: 179.0051727294922 test_loss:263.2073059082031\n",
            "371/3000 train_loss: 157.4643096923828 test_loss:266.09478759765625\n",
            "372/3000 train_loss: 154.2611846923828 test_loss:271.84185791015625\n",
            "373/3000 train_loss: 162.9563751220703 test_loss:262.8605041503906\n",
            "374/3000 train_loss: 160.45355224609375 test_loss:261.7109375\n",
            "375/3000 train_loss: 159.53146362304688 test_loss:267.29998779296875\n",
            "376/3000 train_loss: 166.013671875 test_loss:263.5259704589844\n",
            "377/3000 train_loss: 149.83465576171875 test_loss:259.64215087890625\n",
            "378/3000 train_loss: 192.6642608642578 test_loss:261.072265625\n",
            "379/3000 train_loss: 176.53121948242188 test_loss:258.69378662109375\n",
            "380/3000 train_loss: 162.53073120117188 test_loss:258.4478454589844\n",
            "381/3000 train_loss: 161.7325439453125 test_loss:250.55239868164062\n",
            "382/3000 train_loss: 134.92982482910156 test_loss:262.630126953125\n",
            "383/3000 train_loss: 145.68685913085938 test_loss:269.0714111328125\n",
            "384/3000 train_loss: 148.09161376953125 test_loss:262.7005310058594\n",
            "385/3000 train_loss: 159.5111846923828 test_loss:263.31298828125\n",
            "386/3000 train_loss: 152.95582580566406 test_loss:267.3055725097656\n",
            "387/3000 train_loss: 187.72447204589844 test_loss:261.71051025390625\n",
            "388/3000 train_loss: 129.9071807861328 test_loss:248.64474487304688\n",
            "389/3000 train_loss: 143.61317443847656 test_loss:265.26190185546875\n",
            "390/3000 train_loss: 156.60975646972656 test_loss:258.07275390625\n",
            "391/3000 train_loss: 154.12733459472656 test_loss:259.0338134765625\n",
            "392/3000 train_loss: 164.10992431640625 test_loss:248.74244689941406\n",
            "393/3000 train_loss: 159.7884063720703 test_loss:247.06936645507812\n",
            "394/3000 train_loss: 138.22962951660156 test_loss:256.4072570800781\n",
            "395/3000 train_loss: 144.03042602539062 test_loss:255.8536376953125\n",
            "396/3000 train_loss: 168.46177673339844 test_loss:251.53834533691406\n",
            "397/3000 train_loss: 154.01998901367188 test_loss:254.8282470703125\n",
            "398/3000 train_loss: 144.29490661621094 test_loss:251.41165161132812\n",
            "399/3000 train_loss: 165.18650817871094 test_loss:249.42752075195312\n",
            "400/3000 train_loss: 151.51490783691406 test_loss:247.85833740234375\n",
            "401/3000 train_loss: 160.95465087890625 test_loss:252.760009765625\n",
            "402/3000 train_loss: 154.51589965820312 test_loss:249.3795928955078\n",
            "403/3000 train_loss: 154.8120880126953 test_loss:254.23724365234375\n",
            "404/3000 train_loss: 138.861083984375 test_loss:253.71896362304688\n",
            "405/3000 train_loss: 143.7052764892578 test_loss:257.9249572753906\n",
            "406/3000 train_loss: 167.67481994628906 test_loss:250.88327026367188\n",
            "407/3000 train_loss: 144.69146728515625 test_loss:243.45913696289062\n",
            "408/3000 train_loss: 154.62503051757812 test_loss:250.93531799316406\n",
            "409/3000 train_loss: 134.89881896972656 test_loss:256.26361083984375\n",
            "410/3000 train_loss: 134.94361877441406 test_loss:256.4054260253906\n",
            "411/3000 train_loss: 134.25390625 test_loss:247.54441833496094\n",
            "412/3000 train_loss: 127.37854766845703 test_loss:248.87950134277344\n",
            "413/3000 train_loss: 166.45742797851562 test_loss:248.46331787109375\n",
            "414/3000 train_loss: 135.41586303710938 test_loss:241.24017333984375\n",
            "415/3000 train_loss: 165.41729736328125 test_loss:249.74151611328125\n",
            "416/3000 train_loss: 138.80474853515625 test_loss:249.232666015625\n",
            "417/3000 train_loss: 147.34852600097656 test_loss:237.725341796875\n",
            "418/3000 train_loss: 131.78582763671875 test_loss:256.165283203125\n",
            "419/3000 train_loss: 138.36965942382812 test_loss:239.58139038085938\n",
            "420/3000 train_loss: 138.5640106201172 test_loss:238.99240112304688\n",
            "421/3000 train_loss: 152.14068603515625 test_loss:243.12887573242188\n",
            "422/3000 train_loss: 151.1392822265625 test_loss:238.3701171875\n",
            "423/3000 train_loss: 138.38414001464844 test_loss:249.6338653564453\n",
            "424/3000 train_loss: 132.9415283203125 test_loss:240.05718994140625\n",
            "425/3000 train_loss: 167.37136840820312 test_loss:253.96444702148438\n",
            "426/3000 train_loss: 146.64076232910156 test_loss:250.01515197753906\n",
            "427/3000 train_loss: 141.38536071777344 test_loss:248.5869903564453\n",
            "428/3000 train_loss: 142.91513061523438 test_loss:240.96238708496094\n",
            "429/3000 train_loss: 136.2678680419922 test_loss:241.69290161132812\n",
            "430/3000 train_loss: 139.01846313476562 test_loss:243.99591064453125\n",
            "431/3000 train_loss: 136.19496154785156 test_loss:236.79397583007812\n",
            "432/3000 train_loss: 137.82681274414062 test_loss:238.1183624267578\n",
            "433/3000 train_loss: 133.52967834472656 test_loss:243.8267059326172\n",
            "434/3000 train_loss: 147.7395477294922 test_loss:235.870361328125\n",
            "435/3000 train_loss: 162.86383056640625 test_loss:239.67019653320312\n",
            "436/3000 train_loss: 134.48117065429688 test_loss:236.8249053955078\n",
            "437/3000 train_loss: 130.22137451171875 test_loss:249.4220428466797\n",
            "438/3000 train_loss: 131.7330780029297 test_loss:240.61866760253906\n",
            "439/3000 train_loss: 131.3587646484375 test_loss:245.88262939453125\n",
            "440/3000 train_loss: 156.57241821289062 test_loss:242.42556762695312\n",
            "441/3000 train_loss: 138.6682586669922 test_loss:248.52923583984375\n",
            "442/3000 train_loss: 138.26626586914062 test_loss:235.4065399169922\n",
            "443/3000 train_loss: 129.09506225585938 test_loss:230.0474090576172\n",
            "444/3000 train_loss: 126.00022888183594 test_loss:227.00721740722656\n",
            "445/3000 train_loss: 133.46356201171875 test_loss:231.75079345703125\n",
            "446/3000 train_loss: 144.21762084960938 test_loss:243.07977294921875\n",
            "447/3000 train_loss: 134.10618591308594 test_loss:231.06256103515625\n",
            "448/3000 train_loss: 150.9029083251953 test_loss:243.02212524414062\n",
            "449/3000 train_loss: 147.3806610107422 test_loss:243.11734008789062\n",
            "450/3000 train_loss: 137.61651611328125 test_loss:229.7217254638672\n",
            "451/3000 train_loss: 145.64181518554688 test_loss:243.90225219726562\n",
            "452/3000 train_loss: 133.04493713378906 test_loss:234.4371337890625\n",
            "453/3000 train_loss: 152.01841735839844 test_loss:231.4130401611328\n",
            "454/3000 train_loss: 142.78518676757812 test_loss:234.18809509277344\n",
            "455/3000 train_loss: 151.22943115234375 test_loss:239.0399169921875\n",
            "456/3000 train_loss: 130.0637664794922 test_loss:234.15354919433594\n",
            "457/3000 train_loss: 122.94988250732422 test_loss:227.6742401123047\n",
            "458/3000 train_loss: 154.03302001953125 test_loss:230.861328125\n",
            "459/3000 train_loss: 150.52859497070312 test_loss:232.0902557373047\n",
            "460/3000 train_loss: 131.6353302001953 test_loss:230.27444458007812\n",
            "461/3000 train_loss: 135.14825439453125 test_loss:229.4818572998047\n",
            "462/3000 train_loss: 137.88775634765625 test_loss:222.98818969726562\n",
            "463/3000 train_loss: 147.11944580078125 test_loss:236.47317504882812\n",
            "464/3000 train_loss: 122.23541259765625 test_loss:228.1708984375\n",
            "465/3000 train_loss: 150.054443359375 test_loss:230.41744995117188\n",
            "466/3000 train_loss: 125.12775421142578 test_loss:220.58860778808594\n",
            "467/3000 train_loss: 138.93112182617188 test_loss:236.27932739257812\n",
            "468/3000 train_loss: 156.36395263671875 test_loss:236.65382385253906\n",
            "469/3000 train_loss: 138.2167510986328 test_loss:230.96075439453125\n",
            "470/3000 train_loss: 139.6672821044922 test_loss:229.0625457763672\n",
            "471/3000 train_loss: 123.91058349609375 test_loss:220.7327117919922\n",
            "472/3000 train_loss: 140.36279296875 test_loss:233.62454223632812\n",
            "473/3000 train_loss: 126.376220703125 test_loss:235.4654541015625\n",
            "474/3000 train_loss: 143.2925262451172 test_loss:224.52288818359375\n",
            "475/3000 train_loss: 125.90991973876953 test_loss:229.5662078857422\n",
            "476/3000 train_loss: 116.13591766357422 test_loss:231.28189086914062\n",
            "477/3000 train_loss: 126.17544555664062 test_loss:226.412353515625\n",
            "478/3000 train_loss: 129.5172119140625 test_loss:224.10687255859375\n",
            "479/3000 train_loss: 125.67568969726562 test_loss:223.66452026367188\n",
            "480/3000 train_loss: 123.60901641845703 test_loss:236.96365356445312\n",
            "481/3000 train_loss: 132.6278076171875 test_loss:237.64718627929688\n",
            "482/3000 train_loss: 139.72265625 test_loss:231.88795471191406\n",
            "483/3000 train_loss: 134.6315460205078 test_loss:228.77606201171875\n",
            "484/3000 train_loss: 140.0977020263672 test_loss:223.30520629882812\n",
            "485/3000 train_loss: 117.79402160644531 test_loss:220.7415771484375\n",
            "486/3000 train_loss: 122.41985321044922 test_loss:227.01119995117188\n",
            "487/3000 train_loss: 134.5084686279297 test_loss:229.41635131835938\n",
            "488/3000 train_loss: 128.5625457763672 test_loss:221.5531005859375\n",
            "489/3000 train_loss: 118.39192962646484 test_loss:224.1605987548828\n",
            "490/3000 train_loss: 127.12190246582031 test_loss:219.43399047851562\n",
            "491/3000 train_loss: 124.44438934326172 test_loss:217.51773071289062\n",
            "492/3000 train_loss: 135.64669799804688 test_loss:219.28343200683594\n",
            "493/3000 train_loss: 117.9783935546875 test_loss:220.15052795410156\n",
            "494/3000 train_loss: 137.58409118652344 test_loss:227.16897583007812\n",
            "495/3000 train_loss: 124.84393310546875 test_loss:222.84498596191406\n",
            "496/3000 train_loss: 121.47059631347656 test_loss:213.78282165527344\n",
            "497/3000 train_loss: 146.3228759765625 test_loss:223.8994598388672\n",
            "498/3000 train_loss: 135.5401611328125 test_loss:220.66542053222656\n",
            "499/3000 train_loss: 128.48941040039062 test_loss:222.62559509277344\n",
            "500/3000 train_loss: 145.11318969726562 test_loss:245.38323974609375\n",
            "501/3000 train_loss: 157.7125701904297 test_loss:236.65318298339844\n",
            "502/3000 train_loss: 146.05760192871094 test_loss:222.00811767578125\n",
            "503/3000 train_loss: 147.6580047607422 test_loss:228.52081298828125\n",
            "504/3000 train_loss: 138.58457946777344 test_loss:240.37937927246094\n",
            "505/3000 train_loss: 139.4198760986328 test_loss:231.5890655517578\n",
            "506/3000 train_loss: 120.98983764648438 test_loss:215.90487670898438\n",
            "507/3000 train_loss: 139.52406311035156 test_loss:218.5385284423828\n",
            "508/3000 train_loss: 143.3470916748047 test_loss:227.08412170410156\n",
            "509/3000 train_loss: 130.82801818847656 test_loss:216.7738037109375\n",
            "510/3000 train_loss: 124.17479705810547 test_loss:219.78717041015625\n",
            "511/3000 train_loss: 106.8758544921875 test_loss:212.23818969726562\n",
            "512/3000 train_loss: 118.15987396240234 test_loss:211.03794860839844\n",
            "513/3000 train_loss: 121.79885864257812 test_loss:206.0654296875\n",
            "514/3000 train_loss: 127.19928741455078 test_loss:210.87530517578125\n",
            "515/3000 train_loss: 140.83726501464844 test_loss:217.79953002929688\n",
            "516/3000 train_loss: 118.51811218261719 test_loss:205.79884338378906\n",
            "517/3000 train_loss: 106.54596710205078 test_loss:203.05308532714844\n",
            "518/3000 train_loss: 137.86827087402344 test_loss:219.76126098632812\n",
            "519/3000 train_loss: 127.18362426757812 test_loss:212.95867919921875\n",
            "520/3000 train_loss: 131.62184143066406 test_loss:219.09971618652344\n",
            "521/3000 train_loss: 116.72932434082031 test_loss:210.45565795898438\n",
            "522/3000 train_loss: 109.92388153076172 test_loss:209.50949096679688\n",
            "523/3000 train_loss: 123.93045806884766 test_loss:219.75570678710938\n",
            "524/3000 train_loss: 142.23162841796875 test_loss:212.84783935546875\n",
            "525/3000 train_loss: 120.57170867919922 test_loss:211.9479217529297\n",
            "526/3000 train_loss: 119.062744140625 test_loss:210.74801635742188\n",
            "527/3000 train_loss: 119.05216979980469 test_loss:216.63108825683594\n",
            "528/3000 train_loss: 108.7430419921875 test_loss:208.78692626953125\n",
            "529/3000 train_loss: 116.97240447998047 test_loss:204.42970275878906\n",
            "530/3000 train_loss: 124.41522216796875 test_loss:207.07418823242188\n",
            "531/3000 train_loss: 119.57796478271484 test_loss:210.7040252685547\n",
            "532/3000 train_loss: 119.04411315917969 test_loss:210.68670654296875\n",
            "533/3000 train_loss: 131.62767028808594 test_loss:207.68963623046875\n",
            "534/3000 train_loss: 122.23171997070312 test_loss:207.95004272460938\n",
            "535/3000 train_loss: 120.50697326660156 test_loss:211.078125\n",
            "536/3000 train_loss: 114.05707550048828 test_loss:204.26754760742188\n",
            "537/3000 train_loss: 206.27232360839844 test_loss:212.69894409179688\n",
            "538/3000 train_loss: 127.24545288085938 test_loss:210.96994018554688\n",
            "539/3000 train_loss: 129.1865997314453 test_loss:207.8202362060547\n",
            "540/3000 train_loss: 115.03315734863281 test_loss:217.8216552734375\n",
            "541/3000 train_loss: 110.21763610839844 test_loss:201.82101440429688\n",
            "542/3000 train_loss: 127.18141174316406 test_loss:213.42996215820312\n",
            "543/3000 train_loss: 123.6395492553711 test_loss:210.54530334472656\n",
            "544/3000 train_loss: 116.58297729492188 test_loss:199.84521484375\n",
            "545/3000 train_loss: 116.84803771972656 test_loss:200.88677978515625\n",
            "546/3000 train_loss: 133.75555419921875 test_loss:209.57388305664062\n",
            "547/3000 train_loss: 108.51482391357422 test_loss:202.96426391601562\n",
            "548/3000 train_loss: 112.87609100341797 test_loss:206.00328063964844\n",
            "549/3000 train_loss: 135.44996643066406 test_loss:207.2241973876953\n",
            "550/3000 train_loss: 116.34700775146484 test_loss:199.3229522705078\n",
            "551/3000 train_loss: 123.12631225585938 test_loss:195.50003051757812\n",
            "552/3000 train_loss: 125.59131622314453 test_loss:201.58058166503906\n",
            "553/3000 train_loss: 108.45032501220703 test_loss:208.5208740234375\n",
            "554/3000 train_loss: 107.45524597167969 test_loss:203.60939025878906\n",
            "555/3000 train_loss: 109.52913665771484 test_loss:209.69920349121094\n",
            "556/3000 train_loss: 114.6832275390625 test_loss:199.09796142578125\n",
            "557/3000 train_loss: 110.79057312011719 test_loss:200.86134338378906\n",
            "558/3000 train_loss: 110.04243469238281 test_loss:202.182861328125\n",
            "559/3000 train_loss: 111.9941177368164 test_loss:210.18507385253906\n",
            "560/3000 train_loss: 119.84529113769531 test_loss:195.7740020751953\n",
            "561/3000 train_loss: 117.20875549316406 test_loss:203.48489379882812\n",
            "562/3000 train_loss: 103.47543334960938 test_loss:198.9769287109375\n",
            "563/3000 train_loss: 116.5515365600586 test_loss:197.0107421875\n",
            "564/3000 train_loss: 119.98918151855469 test_loss:189.5538787841797\n",
            "565/3000 train_loss: 112.51825714111328 test_loss:193.55299377441406\n",
            "566/3000 train_loss: 114.25885009765625 test_loss:199.6011962890625\n",
            "567/3000 train_loss: 109.16802978515625 test_loss:190.9014434814453\n",
            "568/3000 train_loss: 123.48660278320312 test_loss:202.00648498535156\n",
            "569/3000 train_loss: 110.67025756835938 test_loss:201.14871215820312\n",
            "570/3000 train_loss: 119.87952423095703 test_loss:203.172119140625\n",
            "571/3000 train_loss: 122.3902816772461 test_loss:201.80360412597656\n",
            "572/3000 train_loss: 101.11752319335938 test_loss:196.91183471679688\n",
            "573/3000 train_loss: 114.12028503417969 test_loss:198.5852813720703\n",
            "574/3000 train_loss: 100.08672332763672 test_loss:189.26853942871094\n",
            "575/3000 train_loss: 104.43138885498047 test_loss:189.20233154296875\n",
            "576/3000 train_loss: 99.86351776123047 test_loss:191.5912628173828\n",
            "577/3000 train_loss: 115.26680755615234 test_loss:191.5420379638672\n",
            "578/3000 train_loss: 108.0893783569336 test_loss:189.95956420898438\n",
            "579/3000 train_loss: 104.48753356933594 test_loss:193.73228454589844\n",
            "580/3000 train_loss: 121.8390884399414 test_loss:190.90658569335938\n",
            "581/3000 train_loss: 98.7378158569336 test_loss:187.6119384765625\n",
            "582/3000 train_loss: 102.46666717529297 test_loss:185.89419555664062\n",
            "583/3000 train_loss: 104.1613540649414 test_loss:186.228271484375\n",
            "584/3000 train_loss: 118.3065185546875 test_loss:191.71572875976562\n",
            "585/3000 train_loss: 102.27027893066406 test_loss:202.18759155273438\n",
            "586/3000 train_loss: 117.47098541259766 test_loss:195.41226196289062\n",
            "587/3000 train_loss: 114.97181701660156 test_loss:194.44149780273438\n",
            "588/3000 train_loss: 108.01142883300781 test_loss:192.1196746826172\n",
            "589/3000 train_loss: 97.63343811035156 test_loss:200.81085205078125\n",
            "590/3000 train_loss: 99.11711883544922 test_loss:187.886474609375\n",
            "591/3000 train_loss: 102.76383209228516 test_loss:184.70138549804688\n",
            "592/3000 train_loss: 94.57731628417969 test_loss:202.5041961669922\n",
            "593/3000 train_loss: 124.76145935058594 test_loss:193.34402465820312\n",
            "594/3000 train_loss: 98.93098449707031 test_loss:191.28042602539062\n",
            "595/3000 train_loss: 100.38287353515625 test_loss:184.727294921875\n",
            "596/3000 train_loss: 109.40841674804688 test_loss:185.4925079345703\n",
            "597/3000 train_loss: 92.95967102050781 test_loss:184.18038940429688\n",
            "598/3000 train_loss: 95.16326141357422 test_loss:185.63926696777344\n",
            "599/3000 train_loss: 102.03779602050781 test_loss:186.98495483398438\n",
            "600/3000 train_loss: 97.4906234741211 test_loss:191.64234924316406\n",
            "601/3000 train_loss: 128.0613250732422 test_loss:194.87750244140625\n",
            "602/3000 train_loss: 105.69139099121094 test_loss:189.52078247070312\n",
            "603/3000 train_loss: 108.74535369873047 test_loss:182.4676055908203\n",
            "604/3000 train_loss: 96.52853393554688 test_loss:196.8915557861328\n",
            "605/3000 train_loss: 121.52225494384766 test_loss:188.4339141845703\n",
            "606/3000 train_loss: 92.7504653930664 test_loss:183.54994201660156\n",
            "607/3000 train_loss: 101.756103515625 test_loss:194.66722106933594\n",
            "608/3000 train_loss: 109.1989517211914 test_loss:182.69296264648438\n",
            "609/3000 train_loss: 109.08026123046875 test_loss:186.0418243408203\n",
            "610/3000 train_loss: 98.11357879638672 test_loss:184.0573272705078\n",
            "611/3000 train_loss: 101.57563781738281 test_loss:183.65127563476562\n",
            "612/3000 train_loss: 96.113525390625 test_loss:181.34359741210938\n",
            "613/3000 train_loss: 100.65086364746094 test_loss:186.0150909423828\n",
            "614/3000 train_loss: 100.5489501953125 test_loss:186.9913787841797\n",
            "615/3000 train_loss: 96.8336410522461 test_loss:197.48985290527344\n",
            "616/3000 train_loss: 114.3011703491211 test_loss:183.79330444335938\n",
            "617/3000 train_loss: 108.04341888427734 test_loss:185.10067749023438\n",
            "618/3000 train_loss: 98.126708984375 test_loss:188.63729858398438\n",
            "619/3000 train_loss: 98.28336334228516 test_loss:184.36097717285156\n",
            "620/3000 train_loss: 91.86363220214844 test_loss:184.02345275878906\n",
            "621/3000 train_loss: 117.59353637695312 test_loss:179.4669647216797\n",
            "622/3000 train_loss: 106.15509796142578 test_loss:184.21743774414062\n",
            "623/3000 train_loss: 97.24705505371094 test_loss:177.24453735351562\n",
            "624/3000 train_loss: 104.84024047851562 test_loss:179.33749389648438\n",
            "625/3000 train_loss: 93.74774169921875 test_loss:183.23727416992188\n",
            "626/3000 train_loss: 92.68817138671875 test_loss:177.259033203125\n",
            "627/3000 train_loss: 89.98600769042969 test_loss:171.0408935546875\n",
            "628/3000 train_loss: 124.32180786132812 test_loss:175.92723083496094\n",
            "629/3000 train_loss: 92.14056396484375 test_loss:181.283203125\n",
            "630/3000 train_loss: 116.79207611083984 test_loss:182.31747436523438\n",
            "631/3000 train_loss: 125.05269622802734 test_loss:189.3778533935547\n",
            "632/3000 train_loss: 106.89305114746094 test_loss:188.5599822998047\n",
            "633/3000 train_loss: 115.99723052978516 test_loss:185.38204956054688\n",
            "634/3000 train_loss: 119.59515380859375 test_loss:184.02598571777344\n",
            "635/3000 train_loss: 102.15074157714844 test_loss:187.22581481933594\n",
            "636/3000 train_loss: 101.864990234375 test_loss:182.59210205078125\n",
            "637/3000 train_loss: 95.42410278320312 test_loss:182.94140625\n",
            "638/3000 train_loss: 113.72776794433594 test_loss:178.21743774414062\n",
            "639/3000 train_loss: 89.81190490722656 test_loss:179.2094268798828\n",
            "640/3000 train_loss: 101.72886657714844 test_loss:179.64035034179688\n",
            "641/3000 train_loss: 93.59769439697266 test_loss:173.32791137695312\n",
            "642/3000 train_loss: 95.93120574951172 test_loss:197.18350219726562\n",
            "643/3000 train_loss: 96.42472839355469 test_loss:175.5399169921875\n",
            "644/3000 train_loss: 109.0347671508789 test_loss:184.4684295654297\n",
            "645/3000 train_loss: 92.21031188964844 test_loss:180.45689392089844\n",
            "646/3000 train_loss: 97.08514404296875 test_loss:180.61721801757812\n",
            "647/3000 train_loss: 96.99474334716797 test_loss:180.12744140625\n",
            "648/3000 train_loss: 113.05396270751953 test_loss:188.4173126220703\n",
            "649/3000 train_loss: 97.2901840209961 test_loss:184.11390686035156\n",
            "650/3000 train_loss: 99.31591796875 test_loss:179.42239379882812\n",
            "651/3000 train_loss: 93.79036712646484 test_loss:177.35946655273438\n",
            "652/3000 train_loss: 102.64533996582031 test_loss:176.4996337890625\n",
            "653/3000 train_loss: 93.59690856933594 test_loss:181.37527465820312\n",
            "654/3000 train_loss: 115.66476440429688 test_loss:179.8670654296875\n",
            "655/3000 train_loss: 103.42581176757812 test_loss:175.62403869628906\n",
            "656/3000 train_loss: 94.71247863769531 test_loss:175.024658203125\n",
            "657/3000 train_loss: 100.76316833496094 test_loss:180.23538208007812\n",
            "658/3000 train_loss: 94.69806671142578 test_loss:179.10487365722656\n",
            "659/3000 train_loss: 98.87836456298828 test_loss:178.13916015625\n",
            "660/3000 train_loss: 93.34967803955078 test_loss:182.43734741210938\n",
            "661/3000 train_loss: 86.72779083251953 test_loss:175.64971923828125\n",
            "662/3000 train_loss: 98.87749481201172 test_loss:181.6007537841797\n",
            "663/3000 train_loss: 102.49002838134766 test_loss:179.55368041992188\n",
            "664/3000 train_loss: 89.43097686767578 test_loss:170.6317138671875\n",
            "665/3000 train_loss: 101.51385498046875 test_loss:175.63035583496094\n",
            "666/3000 train_loss: 82.36605834960938 test_loss:170.1286163330078\n",
            "667/3000 train_loss: 104.79761505126953 test_loss:173.22299194335938\n",
            "668/3000 train_loss: 88.54278564453125 test_loss:170.98158264160156\n",
            "669/3000 train_loss: 99.8044662475586 test_loss:169.0203857421875\n",
            "670/3000 train_loss: 86.9780044555664 test_loss:170.49534606933594\n",
            "671/3000 train_loss: 101.0453872680664 test_loss:172.88145446777344\n",
            "672/3000 train_loss: 98.56233978271484 test_loss:180.36431884765625\n",
            "673/3000 train_loss: 89.6121597290039 test_loss:171.30899047851562\n",
            "674/3000 train_loss: 96.18651580810547 test_loss:172.18849182128906\n",
            "675/3000 train_loss: 84.78384399414062 test_loss:167.02565002441406\n",
            "676/3000 train_loss: 95.02276611328125 test_loss:163.7272491455078\n",
            "677/3000 train_loss: 90.25139617919922 test_loss:173.1103057861328\n",
            "678/3000 train_loss: 99.30957794189453 test_loss:174.23690795898438\n",
            "679/3000 train_loss: 103.22161865234375 test_loss:170.460693359375\n",
            "680/3000 train_loss: 89.94146728515625 test_loss:170.7478790283203\n",
            "681/3000 train_loss: 88.96541595458984 test_loss:168.10250854492188\n",
            "682/3000 train_loss: 97.01061248779297 test_loss:168.81410217285156\n",
            "683/3000 train_loss: 99.59613037109375 test_loss:172.3854217529297\n",
            "684/3000 train_loss: 117.35354614257812 test_loss:164.96298217773438\n",
            "685/3000 train_loss: 96.74536895751953 test_loss:175.0761260986328\n",
            "686/3000 train_loss: 98.66988372802734 test_loss:174.16415405273438\n",
            "687/3000 train_loss: 96.08827209472656 test_loss:171.93338012695312\n",
            "688/3000 train_loss: 99.6541519165039 test_loss:177.53736877441406\n",
            "689/3000 train_loss: 99.64476776123047 test_loss:168.4562225341797\n",
            "690/3000 train_loss: 92.6668930053711 test_loss:175.89669799804688\n",
            "691/3000 train_loss: 98.92245483398438 test_loss:176.39163208007812\n",
            "692/3000 train_loss: 104.27023315429688 test_loss:166.21078491210938\n",
            "693/3000 train_loss: 89.85392761230469 test_loss:172.4213409423828\n",
            "694/3000 train_loss: 94.95680236816406 test_loss:163.66400146484375\n",
            "695/3000 train_loss: 88.3609848022461 test_loss:164.46543884277344\n",
            "696/3000 train_loss: 85.82182312011719 test_loss:166.04791259765625\n",
            "697/3000 train_loss: 98.10325622558594 test_loss:171.45269775390625\n",
            "698/3000 train_loss: 88.87673950195312 test_loss:158.22715759277344\n",
            "699/3000 train_loss: 86.65155792236328 test_loss:160.73788452148438\n",
            "700/3000 train_loss: 98.38602447509766 test_loss:168.19496154785156\n",
            "701/3000 train_loss: 89.11058044433594 test_loss:171.70632934570312\n",
            "702/3000 train_loss: 97.5763168334961 test_loss:164.29641723632812\n",
            "703/3000 train_loss: 104.35354614257812 test_loss:168.56373596191406\n",
            "704/3000 train_loss: 107.36123657226562 test_loss:161.77536010742188\n",
            "705/3000 train_loss: 86.52508544921875 test_loss:167.07763671875\n",
            "706/3000 train_loss: 89.99839782714844 test_loss:175.23196411132812\n",
            "707/3000 train_loss: 106.89817810058594 test_loss:179.63905334472656\n",
            "708/3000 train_loss: 92.8341293334961 test_loss:172.6150360107422\n",
            "709/3000 train_loss: 86.48504638671875 test_loss:179.27618408203125\n",
            "710/3000 train_loss: 102.31029510498047 test_loss:164.17543029785156\n",
            "711/3000 train_loss: 92.72254943847656 test_loss:175.900634765625\n",
            "712/3000 train_loss: 95.14714813232422 test_loss:166.44554138183594\n",
            "713/3000 train_loss: 83.09454345703125 test_loss:169.52940368652344\n",
            "714/3000 train_loss: 82.0154800415039 test_loss:171.2810821533203\n",
            "715/3000 train_loss: 90.9031982421875 test_loss:161.3002471923828\n",
            "716/3000 train_loss: 84.24736022949219 test_loss:165.18946838378906\n",
            "717/3000 train_loss: 83.94610595703125 test_loss:160.64947509765625\n",
            "718/3000 train_loss: 86.90025329589844 test_loss:169.82156372070312\n",
            "719/3000 train_loss: 94.95226287841797 test_loss:160.85647583007812\n",
            "720/3000 train_loss: 94.92915344238281 test_loss:163.94761657714844\n",
            "721/3000 train_loss: 93.95185852050781 test_loss:173.31716918945312\n",
            "722/3000 train_loss: 94.44185638427734 test_loss:164.8828887939453\n",
            "723/3000 train_loss: 91.44426727294922 test_loss:159.68711853027344\n",
            "724/3000 train_loss: 90.05021667480469 test_loss:166.904052734375\n",
            "725/3000 train_loss: 76.09825134277344 test_loss:159.4583740234375\n",
            "726/3000 train_loss: 77.16246032714844 test_loss:159.21884155273438\n",
            "727/3000 train_loss: 95.40946197509766 test_loss:164.65011596679688\n",
            "728/3000 train_loss: 83.26905822753906 test_loss:161.54766845703125\n",
            "729/3000 train_loss: 88.36717987060547 test_loss:158.128662109375\n",
            "730/3000 train_loss: 83.40193939208984 test_loss:162.3677520751953\n",
            "731/3000 train_loss: 90.02794647216797 test_loss:160.8211669921875\n",
            "732/3000 train_loss: 90.51165008544922 test_loss:164.29493713378906\n",
            "733/3000 train_loss: 90.07357025146484 test_loss:166.1528778076172\n",
            "734/3000 train_loss: 99.4845199584961 test_loss:163.96728515625\n",
            "735/3000 train_loss: 111.27906036376953 test_loss:176.90435791015625\n",
            "736/3000 train_loss: 86.20149230957031 test_loss:160.51063537597656\n",
            "737/3000 train_loss: 86.21866607666016 test_loss:156.88800048828125\n",
            "738/3000 train_loss: 95.04227447509766 test_loss:166.64959716796875\n",
            "739/3000 train_loss: 83.50424194335938 test_loss:164.74197387695312\n",
            "740/3000 train_loss: 79.99102783203125 test_loss:166.65711975097656\n",
            "741/3000 train_loss: 90.6362075805664 test_loss:169.4208984375\n",
            "742/3000 train_loss: 92.62057495117188 test_loss:175.0489044189453\n",
            "743/3000 train_loss: 90.99144744873047 test_loss:163.09873962402344\n",
            "744/3000 train_loss: 92.56718444824219 test_loss:176.85009765625\n",
            "745/3000 train_loss: 89.02471923828125 test_loss:167.10543823242188\n",
            "746/3000 train_loss: 86.11453247070312 test_loss:159.29417419433594\n",
            "747/3000 train_loss: 101.53567504882812 test_loss:172.9722137451172\n",
            "748/3000 train_loss: 88.97713470458984 test_loss:174.09326171875\n",
            "749/3000 train_loss: 85.27359771728516 test_loss:164.53854370117188\n",
            "750/3000 train_loss: 79.5751953125 test_loss:172.37725830078125\n",
            "751/3000 train_loss: 85.98809051513672 test_loss:171.67904663085938\n",
            "752/3000 train_loss: 84.71930694580078 test_loss:162.91932678222656\n",
            "753/3000 train_loss: 87.49756622314453 test_loss:165.1399383544922\n",
            "754/3000 train_loss: 96.32794189453125 test_loss:171.49122619628906\n",
            "755/3000 train_loss: 105.54706573486328 test_loss:173.79808044433594\n",
            "756/3000 train_loss: 94.07571411132812 test_loss:159.83975219726562\n",
            "757/3000 train_loss: 75.76734161376953 test_loss:170.69232177734375\n",
            "758/3000 train_loss: 79.81072998046875 test_loss:156.2376251220703\n",
            "759/3000 train_loss: 94.51248168945312 test_loss:163.19427490234375\n",
            "760/3000 train_loss: 87.49886322021484 test_loss:165.74972534179688\n",
            "761/3000 train_loss: 83.03174591064453 test_loss:155.90420532226562\n",
            "762/3000 train_loss: 83.2219009399414 test_loss:170.37828063964844\n",
            "763/3000 train_loss: 78.78253173828125 test_loss:159.01222229003906\n",
            "764/3000 train_loss: 80.60652160644531 test_loss:161.54739379882812\n",
            "765/3000 train_loss: 96.28733825683594 test_loss:158.98477172851562\n",
            "766/3000 train_loss: 83.99075317382812 test_loss:161.0887451171875\n",
            "767/3000 train_loss: 81.6563949584961 test_loss:164.58364868164062\n",
            "768/3000 train_loss: 84.52485656738281 test_loss:168.08753967285156\n",
            "769/3000 train_loss: 81.36265563964844 test_loss:163.97105407714844\n",
            "770/3000 train_loss: 87.3411865234375 test_loss:156.24159240722656\n",
            "771/3000 train_loss: 91.8350830078125 test_loss:173.3073272705078\n",
            "772/3000 train_loss: 85.69589233398438 test_loss:170.43450927734375\n",
            "773/3000 train_loss: 73.81526184082031 test_loss:164.1913604736328\n",
            "774/3000 train_loss: 79.33840942382812 test_loss:165.32595825195312\n",
            "775/3000 train_loss: 74.16014862060547 test_loss:168.9222869873047\n",
            "776/3000 train_loss: 82.6103286743164 test_loss:162.38490295410156\n",
            "777/3000 train_loss: 91.04029083251953 test_loss:167.27081298828125\n",
            "778/3000 train_loss: 87.97867584228516 test_loss:166.1401824951172\n",
            "779/3000 train_loss: 87.62445831298828 test_loss:168.0655517578125\n",
            "780/3000 train_loss: 83.04666137695312 test_loss:158.54751586914062\n",
            "781/3000 train_loss: 88.41024780273438 test_loss:156.95223999023438\n",
            "782/3000 train_loss: 83.12110137939453 test_loss:157.89874267578125\n",
            "783/3000 train_loss: 89.18807983398438 test_loss:180.60826110839844\n",
            "784/3000 train_loss: 80.1527099609375 test_loss:157.94515991210938\n",
            "785/3000 train_loss: 77.44719696044922 test_loss:156.9820556640625\n",
            "786/3000 train_loss: 80.54839324951172 test_loss:167.3953857421875\n",
            "787/3000 train_loss: 93.62061309814453 test_loss:160.43838500976562\n",
            "788/3000 train_loss: 81.1398696899414 test_loss:166.30470275878906\n",
            "789/3000 train_loss: 83.38017272949219 test_loss:168.795654296875\n",
            "790/3000 train_loss: 75.80142211914062 test_loss:159.91966247558594\n",
            "791/3000 train_loss: 73.51583099365234 test_loss:156.35850524902344\n",
            "792/3000 train_loss: 72.55770874023438 test_loss:154.54644775390625\n",
            "793/3000 train_loss: 83.47166442871094 test_loss:154.01226806640625\n",
            "794/3000 train_loss: 82.18746185302734 test_loss:160.76390075683594\n",
            "795/3000 train_loss: 80.2856674194336 test_loss:158.08108520507812\n",
            "796/3000 train_loss: 89.21865844726562 test_loss:166.76327514648438\n",
            "797/3000 train_loss: 78.49431610107422 test_loss:158.11285400390625\n",
            "798/3000 train_loss: 79.60855865478516 test_loss:152.45228576660156\n",
            "799/3000 train_loss: 78.21553802490234 test_loss:163.33810424804688\n",
            "800/3000 train_loss: 73.99636840820312 test_loss:163.6014404296875\n",
            "801/3000 train_loss: 69.39076232910156 test_loss:158.9483184814453\n",
            "802/3000 train_loss: 78.04358673095703 test_loss:165.78353881835938\n",
            "803/3000 train_loss: 72.38997650146484 test_loss:155.33682250976562\n",
            "804/3000 train_loss: 76.9067153930664 test_loss:165.62400817871094\n",
            "805/3000 train_loss: 75.8299560546875 test_loss:156.4341583251953\n",
            "806/3000 train_loss: 80.15654754638672 test_loss:158.7130889892578\n",
            "807/3000 train_loss: 77.63961791992188 test_loss:155.08502197265625\n",
            "808/3000 train_loss: 66.07427215576172 test_loss:152.58387756347656\n",
            "809/3000 train_loss: 77.79508972167969 test_loss:163.64720153808594\n",
            "810/3000 train_loss: 83.68401336669922 test_loss:164.156982421875\n",
            "811/3000 train_loss: 79.90415954589844 test_loss:155.9857635498047\n",
            "812/3000 train_loss: 72.51271057128906 test_loss:157.26702880859375\n",
            "813/3000 train_loss: 76.62176513671875 test_loss:150.58718872070312\n",
            "814/3000 train_loss: 74.71013641357422 test_loss:158.10690307617188\n",
            "815/3000 train_loss: 81.12133026123047 test_loss:162.46974182128906\n",
            "816/3000 train_loss: 81.97956848144531 test_loss:156.39263916015625\n",
            "817/3000 train_loss: 76.71001434326172 test_loss:158.99093627929688\n",
            "818/3000 train_loss: 102.27262115478516 test_loss:164.71728515625\n",
            "819/3000 train_loss: 85.19355010986328 test_loss:159.65707397460938\n",
            "820/3000 train_loss: 81.45396423339844 test_loss:157.3444366455078\n",
            "821/3000 train_loss: 79.15095520019531 test_loss:153.5444793701172\n",
            "822/3000 train_loss: 72.94061279296875 test_loss:152.682861328125\n",
            "823/3000 train_loss: 71.81893920898438 test_loss:150.89930725097656\n",
            "824/3000 train_loss: 75.35478210449219 test_loss:155.4315643310547\n",
            "825/3000 train_loss: 88.65573120117188 test_loss:158.94564819335938\n",
            "826/3000 train_loss: 83.16934204101562 test_loss:153.06089782714844\n",
            "827/3000 train_loss: 69.5718002319336 test_loss:154.0530548095703\n",
            "828/3000 train_loss: 86.98904418945312 test_loss:148.18592834472656\n",
            "829/3000 train_loss: 80.2391128540039 test_loss:149.33529663085938\n",
            "830/3000 train_loss: 83.41053009033203 test_loss:160.86599731445312\n",
            "831/3000 train_loss: 73.05924224853516 test_loss:152.1878204345703\n",
            "832/3000 train_loss: 87.30934143066406 test_loss:147.5557403564453\n",
            "833/3000 train_loss: 71.76947021484375 test_loss:149.43702697753906\n",
            "834/3000 train_loss: 77.1308822631836 test_loss:165.5342254638672\n",
            "835/3000 train_loss: 79.51394653320312 test_loss:157.42864990234375\n",
            "836/3000 train_loss: 65.14523315429688 test_loss:153.02468872070312\n",
            "837/3000 train_loss: 65.41729736328125 test_loss:150.1422119140625\n",
            "838/3000 train_loss: 72.0059814453125 test_loss:159.40921020507812\n",
            "839/3000 train_loss: 75.86348724365234 test_loss:154.06092834472656\n",
            "840/3000 train_loss: 73.62991333007812 test_loss:148.83041381835938\n",
            "841/3000 train_loss: 73.4488525390625 test_loss:152.43150329589844\n",
            "842/3000 train_loss: 90.3872299194336 test_loss:151.41761779785156\n",
            "843/3000 train_loss: 84.64476013183594 test_loss:152.8549346923828\n",
            "844/3000 train_loss: 75.3856201171875 test_loss:144.0462188720703\n",
            "845/3000 train_loss: 79.37577056884766 test_loss:150.82125854492188\n",
            "846/3000 train_loss: 86.25786590576172 test_loss:146.4420928955078\n",
            "847/3000 train_loss: 86.51451110839844 test_loss:166.9494171142578\n",
            "848/3000 train_loss: 78.74696350097656 test_loss:151.49765014648438\n",
            "849/3000 train_loss: 77.39964294433594 test_loss:150.7576904296875\n",
            "850/3000 train_loss: 68.38063049316406 test_loss:153.73870849609375\n",
            "851/3000 train_loss: 74.32777404785156 test_loss:153.16632080078125\n",
            "852/3000 train_loss: 64.96311950683594 test_loss:156.08157348632812\n",
            "853/3000 train_loss: 84.53850555419922 test_loss:150.14755249023438\n",
            "854/3000 train_loss: 77.28845977783203 test_loss:164.99205017089844\n",
            "855/3000 train_loss: 71.54533386230469 test_loss:147.06639099121094\n",
            "856/3000 train_loss: 76.78973388671875 test_loss:168.38885498046875\n",
            "857/3000 train_loss: 74.40127563476562 test_loss:153.53903198242188\n",
            "858/3000 train_loss: 67.91610717773438 test_loss:158.65957641601562\n",
            "859/3000 train_loss: 74.38440704345703 test_loss:149.6450958251953\n",
            "860/3000 train_loss: 71.2900390625 test_loss:162.42489624023438\n",
            "861/3000 train_loss: 76.1593017578125 test_loss:150.59922790527344\n",
            "862/3000 train_loss: 72.45137023925781 test_loss:146.28628540039062\n",
            "863/3000 train_loss: 73.87289428710938 test_loss:150.0519256591797\n",
            "864/3000 train_loss: 77.71217346191406 test_loss:160.9514617919922\n",
            "865/3000 train_loss: 77.04522705078125 test_loss:150.55398559570312\n",
            "866/3000 train_loss: 71.86040496826172 test_loss:149.2810821533203\n",
            "867/3000 train_loss: 86.38088989257812 test_loss:154.61773681640625\n",
            "868/3000 train_loss: 75.9931869506836 test_loss:141.9761505126953\n",
            "869/3000 train_loss: 83.06072998046875 test_loss:148.58580017089844\n",
            "870/3000 train_loss: 83.9527816772461 test_loss:157.313232421875\n",
            "871/3000 train_loss: 73.90774536132812 test_loss:153.33889770507812\n",
            "872/3000 train_loss: 87.98298645019531 test_loss:156.33029174804688\n",
            "873/3000 train_loss: 77.0907974243164 test_loss:153.85861206054688\n",
            "874/3000 train_loss: 80.33377838134766 test_loss:148.28915405273438\n",
            "875/3000 train_loss: 75.55010223388672 test_loss:158.1158905029297\n",
            "876/3000 train_loss: 80.32072448730469 test_loss:149.09974670410156\n",
            "877/3000 train_loss: 90.2964096069336 test_loss:161.15455627441406\n",
            "878/3000 train_loss: 79.4334487915039 test_loss:144.82150268554688\n",
            "879/3000 train_loss: 67.0721664428711 test_loss:153.88807678222656\n",
            "880/3000 train_loss: 73.3870620727539 test_loss:148.30645751953125\n",
            "881/3000 train_loss: 76.75259399414062 test_loss:144.15542602539062\n",
            "882/3000 train_loss: 71.18730926513672 test_loss:150.3352813720703\n",
            "883/3000 train_loss: 70.52188110351562 test_loss:150.98353576660156\n",
            "884/3000 train_loss: 71.26022338867188 test_loss:148.86007690429688\n",
            "885/3000 train_loss: 67.00799560546875 test_loss:147.69622802734375\n",
            "886/3000 train_loss: 78.18000793457031 test_loss:148.79151916503906\n",
            "887/3000 train_loss: 75.81629180908203 test_loss:152.50721740722656\n",
            "888/3000 train_loss: 65.21147155761719 test_loss:141.14126586914062\n",
            "889/3000 train_loss: 82.44625091552734 test_loss:159.6746063232422\n",
            "890/3000 train_loss: 69.27949523925781 test_loss:150.89303588867188\n",
            "891/3000 train_loss: 78.16983032226562 test_loss:145.12081909179688\n",
            "892/3000 train_loss: 80.2373275756836 test_loss:145.56588745117188\n",
            "893/3000 train_loss: 79.38880157470703 test_loss:148.85263061523438\n",
            "894/3000 train_loss: 69.65117645263672 test_loss:150.3909454345703\n",
            "895/3000 train_loss: 87.4271011352539 test_loss:156.60923767089844\n",
            "896/3000 train_loss: 70.07442474365234 test_loss:149.45924377441406\n",
            "897/3000 train_loss: 76.70442199707031 test_loss:149.13629150390625\n",
            "898/3000 train_loss: 71.19862365722656 test_loss:142.26890563964844\n",
            "899/3000 train_loss: 81.98065948486328 test_loss:149.53732299804688\n",
            "900/3000 train_loss: 69.64451599121094 test_loss:150.19094848632812\n",
            "901/3000 train_loss: 80.066162109375 test_loss:150.13880920410156\n",
            "902/3000 train_loss: 71.84230041503906 test_loss:152.64285278320312\n",
            "903/3000 train_loss: 78.46488952636719 test_loss:152.94131469726562\n",
            "904/3000 train_loss: 73.07328796386719 test_loss:144.12564086914062\n",
            "905/3000 train_loss: 70.22755432128906 test_loss:148.3460235595703\n",
            "906/3000 train_loss: 72.61135864257812 test_loss:144.5892791748047\n",
            "907/3000 train_loss: 83.77572631835938 test_loss:157.4674072265625\n",
            "908/3000 train_loss: 72.57942199707031 test_loss:146.14772033691406\n",
            "909/3000 train_loss: 78.22410583496094 test_loss:150.2050018310547\n",
            "910/3000 train_loss: 62.14296340942383 test_loss:143.4200897216797\n",
            "911/3000 train_loss: 76.59886169433594 test_loss:151.97335815429688\n",
            "912/3000 train_loss: 76.42546081542969 test_loss:144.1042022705078\n",
            "913/3000 train_loss: 77.22400665283203 test_loss:146.84378051757812\n",
            "914/3000 train_loss: 79.7182388305664 test_loss:146.67019653320312\n",
            "915/3000 train_loss: 62.835914611816406 test_loss:152.76351928710938\n",
            "916/3000 train_loss: 69.671875 test_loss:139.03366088867188\n",
            "917/3000 train_loss: 72.25126647949219 test_loss:146.20863342285156\n",
            "918/3000 train_loss: 61.899723052978516 test_loss:145.9661865234375\n",
            "919/3000 train_loss: 78.86522674560547 test_loss:146.56158447265625\n",
            "920/3000 train_loss: 67.2119369506836 test_loss:144.8350372314453\n",
            "921/3000 train_loss: 68.4129867553711 test_loss:150.11801147460938\n",
            "922/3000 train_loss: 72.54411315917969 test_loss:137.64869689941406\n",
            "923/3000 train_loss: 75.09346771240234 test_loss:149.14593505859375\n",
            "924/3000 train_loss: 63.27970886230469 test_loss:138.59323120117188\n",
            "925/3000 train_loss: 72.01789093017578 test_loss:150.80545043945312\n",
            "926/3000 train_loss: 71.91168212890625 test_loss:140.6943359375\n",
            "927/3000 train_loss: 69.27032470703125 test_loss:146.76954650878906\n",
            "928/3000 train_loss: 70.17225646972656 test_loss:140.20407104492188\n",
            "929/3000 train_loss: 71.00480651855469 test_loss:143.17477416992188\n",
            "930/3000 train_loss: 66.89547729492188 test_loss:145.9508819580078\n",
            "931/3000 train_loss: 69.38632202148438 test_loss:154.13526916503906\n",
            "932/3000 train_loss: 66.34121704101562 test_loss:147.32781982421875\n",
            "933/3000 train_loss: 94.91312408447266 test_loss:151.98532104492188\n",
            "934/3000 train_loss: 75.77781677246094 test_loss:145.8018341064453\n",
            "935/3000 train_loss: 89.50656127929688 test_loss:147.36900329589844\n",
            "936/3000 train_loss: 73.97421264648438 test_loss:158.42205810546875\n",
            "937/3000 train_loss: 72.5621566772461 test_loss:142.0247344970703\n",
            "938/3000 train_loss: 66.8118667602539 test_loss:144.25149536132812\n",
            "939/3000 train_loss: 64.01878356933594 test_loss:151.14236450195312\n",
            "940/3000 train_loss: 69.23249053955078 test_loss:145.10690307617188\n",
            "941/3000 train_loss: 70.78619384765625 test_loss:142.8199462890625\n",
            "942/3000 train_loss: 83.25081634521484 test_loss:144.0325927734375\n",
            "943/3000 train_loss: 72.31553649902344 test_loss:148.69021606445312\n",
            "944/3000 train_loss: 95.17544555664062 test_loss:146.93125915527344\n",
            "945/3000 train_loss: 79.27774047851562 test_loss:149.3397674560547\n",
            "946/3000 train_loss: 71.11006927490234 test_loss:151.99945068359375\n",
            "947/3000 train_loss: 68.94459533691406 test_loss:148.42239379882812\n",
            "948/3000 train_loss: 68.87237548828125 test_loss:151.05438232421875\n",
            "949/3000 train_loss: 73.31857299804688 test_loss:135.75082397460938\n",
            "950/3000 train_loss: 86.79692077636719 test_loss:149.38966369628906\n",
            "951/3000 train_loss: 67.90312194824219 test_loss:145.35855102539062\n",
            "952/3000 train_loss: 73.40157318115234 test_loss:147.5683135986328\n",
            "953/3000 train_loss: 66.65827941894531 test_loss:135.99380493164062\n",
            "954/3000 train_loss: 65.50565338134766 test_loss:142.9836883544922\n",
            "955/3000 train_loss: 102.6676254272461 test_loss:141.8201904296875\n",
            "956/3000 train_loss: 68.89004516601562 test_loss:145.3032989501953\n",
            "957/3000 train_loss: 73.73499298095703 test_loss:136.40093994140625\n",
            "958/3000 train_loss: 75.08382415771484 test_loss:143.9967041015625\n",
            "959/3000 train_loss: 87.79864501953125 test_loss:140.26609802246094\n",
            "960/3000 train_loss: 67.15029907226562 test_loss:145.84056091308594\n",
            "961/3000 train_loss: 73.26513671875 test_loss:145.123046875\n",
            "962/3000 train_loss: 72.35173034667969 test_loss:148.8551025390625\n",
            "963/3000 train_loss: 74.30171966552734 test_loss:140.85000610351562\n",
            "964/3000 train_loss: 71.23902130126953 test_loss:148.08030700683594\n",
            "965/3000 train_loss: 69.68768310546875 test_loss:144.2227020263672\n",
            "966/3000 train_loss: 68.91898345947266 test_loss:140.41119384765625\n",
            "967/3000 train_loss: 82.14498138427734 test_loss:139.3555145263672\n",
            "968/3000 train_loss: 68.43072509765625 test_loss:136.49009704589844\n",
            "969/3000 train_loss: 64.84699249267578 test_loss:136.95901489257812\n",
            "970/3000 train_loss: 66.2298355102539 test_loss:139.7608184814453\n",
            "971/3000 train_loss: 72.55796813964844 test_loss:144.11209106445312\n",
            "972/3000 train_loss: 61.712276458740234 test_loss:143.6868896484375\n",
            "973/3000 train_loss: 62.57973861694336 test_loss:138.89968872070312\n",
            "974/3000 train_loss: 78.36553955078125 test_loss:144.81809997558594\n",
            "975/3000 train_loss: 68.80416870117188 test_loss:136.64535522460938\n",
            "976/3000 train_loss: 65.26915740966797 test_loss:137.79454040527344\n",
            "977/3000 train_loss: 63.39670181274414 test_loss:147.072021484375\n",
            "978/3000 train_loss: 72.79085540771484 test_loss:148.14151000976562\n",
            "979/3000 train_loss: 63.940773010253906 test_loss:137.20689392089844\n",
            "980/3000 train_loss: 59.65578079223633 test_loss:140.55462646484375\n",
            "981/3000 train_loss: 66.306396484375 test_loss:141.60543823242188\n",
            "982/3000 train_loss: 67.4330062866211 test_loss:135.15623474121094\n",
            "983/3000 train_loss: 74.91310119628906 test_loss:145.59295654296875\n",
            "984/3000 train_loss: 65.17206573486328 test_loss:132.40557861328125\n",
            "985/3000 train_loss: 77.25196075439453 test_loss:144.06500244140625\n",
            "986/3000 train_loss: 74.9228515625 test_loss:137.4722137451172\n",
            "987/3000 train_loss: 65.86345672607422 test_loss:144.79319763183594\n",
            "988/3000 train_loss: 60.57735061645508 test_loss:140.9667510986328\n",
            "989/3000 train_loss: 70.14492797851562 test_loss:145.78024291992188\n",
            "990/3000 train_loss: 73.5038070678711 test_loss:153.4571990966797\n",
            "991/3000 train_loss: 64.76602172851562 test_loss:138.23828125\n",
            "992/3000 train_loss: 66.0373764038086 test_loss:138.70172119140625\n",
            "993/3000 train_loss: 64.50331115722656 test_loss:140.93960571289062\n",
            "994/3000 train_loss: 68.83222961425781 test_loss:138.89743041992188\n",
            "995/3000 train_loss: 64.92878723144531 test_loss:143.92800903320312\n",
            "996/3000 train_loss: 66.20000457763672 test_loss:133.73175048828125\n",
            "997/3000 train_loss: 71.2564468383789 test_loss:137.11732482910156\n",
            "998/3000 train_loss: 66.25035858154297 test_loss:147.2850799560547\n",
            "999/3000 train_loss: 76.354248046875 test_loss:135.7922821044922\n",
            "1000/3000 train_loss: 62.38762283325195 test_loss:146.2624969482422\n",
            "1001/3000 train_loss: 64.58599090576172 test_loss:134.89366149902344\n",
            "1002/3000 train_loss: 61.75713348388672 test_loss:145.08889770507812\n",
            "1003/3000 train_loss: 80.80476379394531 test_loss:145.84994506835938\n",
            "1004/3000 train_loss: 66.53827667236328 test_loss:138.39883422851562\n",
            "1005/3000 train_loss: 69.12174224853516 test_loss:131.82962036132812\n",
            "1006/3000 train_loss: 59.279415130615234 test_loss:137.47874450683594\n",
            "1007/3000 train_loss: 65.24636840820312 test_loss:148.67465209960938\n",
            "1008/3000 train_loss: 67.65220642089844 test_loss:136.8267364501953\n",
            "1009/3000 train_loss: 64.63287353515625 test_loss:135.26480102539062\n",
            "1010/3000 train_loss: 70.27400970458984 test_loss:132.83705139160156\n",
            "1011/3000 train_loss: 60.92817687988281 test_loss:138.27659606933594\n",
            "1012/3000 train_loss: 65.23206329345703 test_loss:141.2767333984375\n",
            "1013/3000 train_loss: 67.28470611572266 test_loss:134.78196716308594\n",
            "1014/3000 train_loss: 73.188232421875 test_loss:136.6790008544922\n",
            "1015/3000 train_loss: 62.11687088012695 test_loss:139.41160583496094\n",
            "1016/3000 train_loss: 62.34550857543945 test_loss:139.47361755371094\n",
            "1017/3000 train_loss: 62.44975280761719 test_loss:132.00682067871094\n",
            "1018/3000 train_loss: 67.25762939453125 test_loss:145.42584228515625\n",
            "1019/3000 train_loss: 62.83098602294922 test_loss:131.9121551513672\n",
            "1020/3000 train_loss: 60.04591369628906 test_loss:131.27691650390625\n",
            "1021/3000 train_loss: 59.88132095336914 test_loss:130.12869262695312\n",
            "1022/3000 train_loss: 69.65725708007812 test_loss:136.79396057128906\n",
            "1023/3000 train_loss: 64.16356658935547 test_loss:135.06805419921875\n",
            "1024/3000 train_loss: 63.74897766113281 test_loss:136.62765502929688\n",
            "1025/3000 train_loss: 68.56812286376953 test_loss:130.5018768310547\n",
            "1026/3000 train_loss: 54.567840576171875 test_loss:130.645263671875\n",
            "1027/3000 train_loss: 70.43184661865234 test_loss:130.17930603027344\n",
            "1028/3000 train_loss: 65.9895248413086 test_loss:132.15211486816406\n",
            "1029/3000 train_loss: 68.8584213256836 test_loss:126.0700912475586\n",
            "1030/3000 train_loss: 59.89054489135742 test_loss:133.10797119140625\n",
            "1031/3000 train_loss: 72.55677032470703 test_loss:130.65521240234375\n",
            "1032/3000 train_loss: 72.4867172241211 test_loss:141.94174194335938\n",
            "1033/3000 train_loss: 70.28746032714844 test_loss:145.8082275390625\n",
            "1034/3000 train_loss: 61.810367584228516 test_loss:131.50108337402344\n",
            "1035/3000 train_loss: 95.47193908691406 test_loss:144.45863342285156\n",
            "1036/3000 train_loss: 68.16853332519531 test_loss:141.63876342773438\n",
            "1037/3000 train_loss: 59.089324951171875 test_loss:136.38565063476562\n",
            "1038/3000 train_loss: 64.27852630615234 test_loss:129.26885986328125\n",
            "1039/3000 train_loss: 60.0994758605957 test_loss:130.63568115234375\n",
            "1040/3000 train_loss: 61.86409378051758 test_loss:134.24447631835938\n",
            "1041/3000 train_loss: 60.5679817199707 test_loss:130.88153076171875\n",
            "1042/3000 train_loss: 64.6369400024414 test_loss:129.34088134765625\n",
            "1043/3000 train_loss: 75.655517578125 test_loss:135.85452270507812\n",
            "1044/3000 train_loss: 60.513206481933594 test_loss:132.29052734375\n",
            "1045/3000 train_loss: 64.24675750732422 test_loss:140.69789123535156\n",
            "1046/3000 train_loss: 70.53627014160156 test_loss:142.1369171142578\n",
            "1047/3000 train_loss: 59.05429458618164 test_loss:134.4813232421875\n",
            "1048/3000 train_loss: 62.327964782714844 test_loss:129.2762451171875\n",
            "1049/3000 train_loss: 62.123939514160156 test_loss:133.4715118408203\n",
            "1050/3000 train_loss: 62.110687255859375 test_loss:135.68702697753906\n",
            "1051/3000 train_loss: 63.75822448730469 test_loss:133.08750915527344\n",
            "1052/3000 train_loss: 61.54099655151367 test_loss:148.02597045898438\n",
            "1053/3000 train_loss: 74.02815246582031 test_loss:136.21148681640625\n",
            "1054/3000 train_loss: 57.51514434814453 test_loss:129.44789123535156\n",
            "1055/3000 train_loss: 66.82991027832031 test_loss:130.4191131591797\n",
            "1056/3000 train_loss: 61.61250686645508 test_loss:124.17789459228516\n",
            "1057/3000 train_loss: 62.508766174316406 test_loss:141.43145751953125\n",
            "1058/3000 train_loss: 57.82378387451172 test_loss:125.7156753540039\n",
            "1059/3000 train_loss: 61.823726654052734 test_loss:150.8347930908203\n",
            "1060/3000 train_loss: 65.35812377929688 test_loss:131.14773559570312\n",
            "1061/3000 train_loss: 62.33098602294922 test_loss:138.828369140625\n",
            "1062/3000 train_loss: 55.04127502441406 test_loss:135.4514617919922\n",
            "1063/3000 train_loss: 60.581520080566406 test_loss:137.50497436523438\n",
            "1064/3000 train_loss: 62.999267578125 test_loss:143.3778533935547\n",
            "1065/3000 train_loss: 58.69560623168945 test_loss:134.23800659179688\n",
            "1066/3000 train_loss: 70.01998901367188 test_loss:131.74429321289062\n",
            "1067/3000 train_loss: 59.443241119384766 test_loss:128.818359375\n",
            "1068/3000 train_loss: 56.58243942260742 test_loss:143.18212890625\n",
            "1069/3000 train_loss: 81.5233383178711 test_loss:161.46824645996094\n",
            "1070/3000 train_loss: 67.77973175048828 test_loss:133.3402862548828\n",
            "1071/3000 train_loss: 62.63701629638672 test_loss:141.87295532226562\n",
            "1072/3000 train_loss: 67.49320983886719 test_loss:125.80052947998047\n",
            "1073/3000 train_loss: 62.662899017333984 test_loss:134.4551544189453\n",
            "1074/3000 train_loss: 68.97125244140625 test_loss:133.2372283935547\n",
            "1075/3000 train_loss: 62.00998306274414 test_loss:135.99920654296875\n",
            "1076/3000 train_loss: 65.31211853027344 test_loss:137.73536682128906\n",
            "1077/3000 train_loss: 73.14996337890625 test_loss:137.1979522705078\n",
            "1078/3000 train_loss: 66.5559310913086 test_loss:140.87322998046875\n",
            "1079/3000 train_loss: 68.61187744140625 test_loss:144.79248046875\n",
            "1080/3000 train_loss: 64.87726593017578 test_loss:133.00685119628906\n",
            "1081/3000 train_loss: 57.780311584472656 test_loss:128.5381317138672\n",
            "1082/3000 train_loss: 76.8648910522461 test_loss:139.7904815673828\n",
            "1083/3000 train_loss: 76.52851104736328 test_loss:142.66046142578125\n",
            "1084/3000 train_loss: 66.4965591430664 test_loss:137.05169677734375\n",
            "1085/3000 train_loss: 64.35015869140625 test_loss:132.1165008544922\n",
            "1086/3000 train_loss: 61.91372299194336 test_loss:130.48548889160156\n",
            "1087/3000 train_loss: 66.8956298828125 test_loss:129.89080810546875\n",
            "1088/3000 train_loss: 67.80801391601562 test_loss:139.6992645263672\n",
            "1089/3000 train_loss: 62.660560607910156 test_loss:141.9088592529297\n",
            "1090/3000 train_loss: 63.78600311279297 test_loss:132.98968505859375\n",
            "1091/3000 train_loss: 62.73683547973633 test_loss:135.03346252441406\n",
            "1092/3000 train_loss: 69.7665786743164 test_loss:137.77442932128906\n",
            "1093/3000 train_loss: 69.94405364990234 test_loss:134.98057556152344\n",
            "1094/3000 train_loss: 62.039920806884766 test_loss:139.48963928222656\n",
            "1095/3000 train_loss: 66.68531036376953 test_loss:137.63189697265625\n",
            "1096/3000 train_loss: 56.81196975708008 test_loss:131.4595947265625\n",
            "1097/3000 train_loss: 63.423797607421875 test_loss:133.37013244628906\n",
            "1098/3000 train_loss: 60.56998825073242 test_loss:129.43719482421875\n",
            "1099/3000 train_loss: 65.51657104492188 test_loss:138.10528564453125\n",
            "1100/3000 train_loss: 57.94868469238281 test_loss:129.52432250976562\n",
            "1101/3000 train_loss: 64.51229095458984 test_loss:131.8928680419922\n",
            "1102/3000 train_loss: 56.821590423583984 test_loss:127.63610076904297\n",
            "1103/3000 train_loss: 56.859745025634766 test_loss:140.3218231201172\n",
            "1104/3000 train_loss: 62.147647857666016 test_loss:127.3427734375\n",
            "1105/3000 train_loss: 61.9721794128418 test_loss:132.42974853515625\n",
            "1106/3000 train_loss: 67.27616119384766 test_loss:133.0731201171875\n",
            "1107/3000 train_loss: 64.6931381225586 test_loss:138.44000244140625\n",
            "1108/3000 train_loss: 63.10989761352539 test_loss:127.12169647216797\n",
            "1109/3000 train_loss: 60.30194854736328 test_loss:131.9374542236328\n",
            "1110/3000 train_loss: 53.292720794677734 test_loss:134.08944702148438\n",
            "1111/3000 train_loss: 56.35642623901367 test_loss:134.1192626953125\n",
            "1112/3000 train_loss: 75.07759857177734 test_loss:127.09390258789062\n",
            "1113/3000 train_loss: 63.79036331176758 test_loss:135.84280395507812\n",
            "1114/3000 train_loss: 55.45479202270508 test_loss:131.87892150878906\n",
            "1115/3000 train_loss: 75.4736099243164 test_loss:125.650634765625\n",
            "1116/3000 train_loss: 58.44697570800781 test_loss:137.3712158203125\n",
            "1117/3000 train_loss: 61.57544708251953 test_loss:144.52359008789062\n",
            "1118/3000 train_loss: 59.344173431396484 test_loss:132.41636657714844\n",
            "1119/3000 train_loss: 65.18473052978516 test_loss:144.96754455566406\n",
            "1120/3000 train_loss: 62.117713928222656 test_loss:125.050537109375\n",
            "1121/3000 train_loss: 65.76310729980469 test_loss:138.1092071533203\n",
            "1122/3000 train_loss: 62.748435974121094 test_loss:137.5796356201172\n",
            "1123/3000 train_loss: 59.56512451171875 test_loss:124.7392349243164\n",
            "1124/3000 train_loss: 60.56283950805664 test_loss:139.29989624023438\n",
            "1125/3000 train_loss: 61.65027618408203 test_loss:125.77828979492188\n",
            "1126/3000 train_loss: 58.40787124633789 test_loss:135.8918914794922\n",
            "1127/3000 train_loss: 63.3764762878418 test_loss:131.26995849609375\n",
            "1128/3000 train_loss: 61.121036529541016 test_loss:130.33900451660156\n",
            "1129/3000 train_loss: 51.47159194946289 test_loss:127.48672485351562\n",
            "1130/3000 train_loss: 67.61615753173828 test_loss:123.457763671875\n",
            "1131/3000 train_loss: 53.2730827331543 test_loss:128.76922607421875\n",
            "1132/3000 train_loss: 58.53269577026367 test_loss:131.15553283691406\n",
            "1133/3000 train_loss: 61.99124526977539 test_loss:131.614501953125\n",
            "1134/3000 train_loss: 59.077423095703125 test_loss:134.35888671875\n",
            "1135/3000 train_loss: 62.215702056884766 test_loss:139.70513916015625\n",
            "1136/3000 train_loss: 71.01631164550781 test_loss:135.77081298828125\n",
            "1137/3000 train_loss: 60.600807189941406 test_loss:130.42567443847656\n",
            "1138/3000 train_loss: 57.844181060791016 test_loss:123.12211608886719\n",
            "1139/3000 train_loss: 57.25935363769531 test_loss:140.71368408203125\n",
            "1140/3000 train_loss: 58.904212951660156 test_loss:123.34859466552734\n",
            "1141/3000 train_loss: 54.16468811035156 test_loss:132.3268280029297\n",
            "1142/3000 train_loss: 54.03229904174805 test_loss:124.44951629638672\n",
            "1143/3000 train_loss: 60.282264709472656 test_loss:118.40782928466797\n",
            "1144/3000 train_loss: 59.60988998413086 test_loss:144.901123046875\n",
            "1145/3000 train_loss: 57.40489196777344 test_loss:125.90998840332031\n",
            "1146/3000 train_loss: 60.69609069824219 test_loss:125.04963684082031\n",
            "1147/3000 train_loss: 57.91738510131836 test_loss:118.65090942382812\n",
            "1148/3000 train_loss: 56.055274963378906 test_loss:120.23123168945312\n",
            "1149/3000 train_loss: 66.89558410644531 test_loss:122.83158111572266\n",
            "1150/3000 train_loss: 59.66299057006836 test_loss:129.544189453125\n",
            "1151/3000 train_loss: 55.04022979736328 test_loss:123.51373291015625\n",
            "1152/3000 train_loss: 62.814544677734375 test_loss:142.5457000732422\n",
            "1153/3000 train_loss: 62.96746826171875 test_loss:138.15191650390625\n",
            "1154/3000 train_loss: 59.3651008605957 test_loss:129.45059204101562\n",
            "1155/3000 train_loss: 57.1247673034668 test_loss:131.00067138671875\n",
            "1156/3000 train_loss: 52.63042068481445 test_loss:134.2342987060547\n",
            "1157/3000 train_loss: 65.4928207397461 test_loss:130.5437774658203\n",
            "1158/3000 train_loss: 62.40375518798828 test_loss:131.70713806152344\n",
            "1159/3000 train_loss: 57.57516860961914 test_loss:134.8982391357422\n",
            "1160/3000 train_loss: 56.734222412109375 test_loss:136.8798065185547\n",
            "1161/3000 train_loss: 56.318214416503906 test_loss:135.83091735839844\n",
            "1162/3000 train_loss: 65.36540985107422 test_loss:124.8142318725586\n",
            "1163/3000 train_loss: 55.85470962524414 test_loss:124.85629272460938\n",
            "1164/3000 train_loss: 61.97477722167969 test_loss:132.7399139404297\n",
            "1165/3000 train_loss: 55.27500534057617 test_loss:130.9853057861328\n",
            "1166/3000 train_loss: 56.49705505371094 test_loss:134.91481018066406\n",
            "1167/3000 train_loss: 58.91187286376953 test_loss:132.26963806152344\n",
            "1168/3000 train_loss: 56.84499740600586 test_loss:126.64968872070312\n",
            "1169/3000 train_loss: 53.73818588256836 test_loss:134.99998474121094\n",
            "1170/3000 train_loss: 53.54376220703125 test_loss:142.1653594970703\n",
            "1171/3000 train_loss: 64.39798736572266 test_loss:136.851318359375\n",
            "1172/3000 train_loss: 62.26246643066406 test_loss:124.72811126708984\n",
            "1173/3000 train_loss: 54.944557189941406 test_loss:135.00152587890625\n",
            "1174/3000 train_loss: 57.2553825378418 test_loss:123.9761962890625\n",
            "1175/3000 train_loss: 53.229454040527344 test_loss:121.50665283203125\n",
            "1176/3000 train_loss: 59.721614837646484 test_loss:126.52296447753906\n",
            "1177/3000 train_loss: 58.541622161865234 test_loss:121.22996520996094\n",
            "1178/3000 train_loss: 62.31124496459961 test_loss:128.18898010253906\n",
            "1179/3000 train_loss: 62.2639045715332 test_loss:126.11854553222656\n",
            "1180/3000 train_loss: 59.633583068847656 test_loss:129.41136169433594\n",
            "1181/3000 train_loss: 61.36760330200195 test_loss:125.088134765625\n",
            "1182/3000 train_loss: 56.206520080566406 test_loss:118.8216323852539\n",
            "1183/3000 train_loss: 70.58341217041016 test_loss:130.03196716308594\n",
            "1184/3000 train_loss: 64.30534362792969 test_loss:130.29367065429688\n",
            "1185/3000 train_loss: 61.96739959716797 test_loss:123.16840362548828\n",
            "1186/3000 train_loss: 58.038421630859375 test_loss:122.5559310913086\n",
            "1187/3000 train_loss: 60.27289581298828 test_loss:123.95052337646484\n",
            "1188/3000 train_loss: 61.07228088378906 test_loss:124.33586883544922\n",
            "1189/3000 train_loss: 61.807029724121094 test_loss:131.71661376953125\n",
            "1190/3000 train_loss: 61.49220657348633 test_loss:119.96125030517578\n",
            "1191/3000 train_loss: 68.96000671386719 test_loss:119.52479553222656\n",
            "1192/3000 train_loss: 61.56743240356445 test_loss:126.46112823486328\n",
            "1193/3000 train_loss: 50.04762649536133 test_loss:122.12361145019531\n",
            "1194/3000 train_loss: 58.59321594238281 test_loss:128.3614959716797\n",
            "1195/3000 train_loss: 65.65230560302734 test_loss:119.53001403808594\n",
            "1196/3000 train_loss: 64.95343017578125 test_loss:133.19699096679688\n",
            "1197/3000 train_loss: 60.346900939941406 test_loss:123.88558197021484\n",
            "1198/3000 train_loss: 56.302730560302734 test_loss:125.56434631347656\n",
            "1199/3000 train_loss: 64.14091491699219 test_loss:124.88343811035156\n",
            "1200/3000 train_loss: 49.97994613647461 test_loss:124.23332214355469\n",
            "1201/3000 train_loss: 55.79780960083008 test_loss:118.87866973876953\n",
            "1202/3000 train_loss: 49.58931350708008 test_loss:116.24837493896484\n",
            "1203/3000 train_loss: 61.09626770019531 test_loss:136.1963348388672\n",
            "1204/3000 train_loss: 61.540531158447266 test_loss:121.89633178710938\n",
            "1205/3000 train_loss: 51.70632553100586 test_loss:130.2816619873047\n",
            "1206/3000 train_loss: 63.802467346191406 test_loss:116.57435607910156\n",
            "1207/3000 train_loss: 59.55842208862305 test_loss:134.988525390625\n",
            "1208/3000 train_loss: 55.72862243652344 test_loss:117.49082946777344\n",
            "1209/3000 train_loss: 59.293861389160156 test_loss:126.41555786132812\n",
            "1210/3000 train_loss: 60.19553756713867 test_loss:125.84568786621094\n",
            "1211/3000 train_loss: 65.11373901367188 test_loss:121.19912719726562\n",
            "1212/3000 train_loss: 56.49187469482422 test_loss:124.74931335449219\n",
            "1213/3000 train_loss: 50.90882110595703 test_loss:125.56140899658203\n",
            "1214/3000 train_loss: 61.07755661010742 test_loss:124.91432189941406\n",
            "1215/3000 train_loss: 50.22636413574219 test_loss:127.64990234375\n",
            "1216/3000 train_loss: 52.474517822265625 test_loss:118.76423645019531\n",
            "1217/3000 train_loss: 54.9826774597168 test_loss:137.87596130371094\n",
            "1218/3000 train_loss: 60.28620147705078 test_loss:127.5362548828125\n",
            "1219/3000 train_loss: 58.41413116455078 test_loss:133.41867065429688\n",
            "1220/3000 train_loss: 56.42396545410156 test_loss:119.27226257324219\n",
            "1221/3000 train_loss: 57.81157302856445 test_loss:123.95317840576172\n",
            "1222/3000 train_loss: 56.67772674560547 test_loss:120.68995666503906\n",
            "1223/3000 train_loss: 56.7208366394043 test_loss:130.56446838378906\n",
            "1224/3000 train_loss: 52.635009765625 test_loss:120.61981201171875\n",
            "1225/3000 train_loss: 54.14576721191406 test_loss:117.11068725585938\n",
            "1226/3000 train_loss: 54.91854476928711 test_loss:138.07363891601562\n",
            "1227/3000 train_loss: 69.01679992675781 test_loss:121.53678131103516\n",
            "1228/3000 train_loss: 59.8399658203125 test_loss:142.5724639892578\n",
            "1229/3000 train_loss: 52.38615036010742 test_loss:118.00365447998047\n",
            "1230/3000 train_loss: 59.12109375 test_loss:124.33839416503906\n",
            "1231/3000 train_loss: 49.95978927612305 test_loss:121.73951721191406\n",
            "1232/3000 train_loss: 61.7290153503418 test_loss:131.84918212890625\n",
            "1233/3000 train_loss: 55.474884033203125 test_loss:120.42303466796875\n",
            "1234/3000 train_loss: 57.25169372558594 test_loss:133.89743041992188\n",
            "1235/3000 train_loss: 61.26045227050781 test_loss:125.47537231445312\n",
            "1236/3000 train_loss: 58.53316879272461 test_loss:119.56248474121094\n",
            "1237/3000 train_loss: 52.87034225463867 test_loss:133.9704132080078\n",
            "1238/3000 train_loss: 63.51841735839844 test_loss:119.84765625\n",
            "1239/3000 train_loss: 52.38154602050781 test_loss:123.21790313720703\n",
            "1240/3000 train_loss: 51.52383041381836 test_loss:131.7703857421875\n",
            "1241/3000 train_loss: 63.449676513671875 test_loss:126.74626159667969\n",
            "1242/3000 train_loss: 52.89985656738281 test_loss:126.85503387451172\n",
            "1243/3000 train_loss: 58.73873519897461 test_loss:135.25689697265625\n",
            "1244/3000 train_loss: 56.57069396972656 test_loss:125.96182250976562\n",
            "1245/3000 train_loss: 56.7633056640625 test_loss:129.42660522460938\n",
            "1246/3000 train_loss: 57.152957916259766 test_loss:134.47463989257812\n",
            "1247/3000 train_loss: 68.38227081298828 test_loss:121.93221282958984\n",
            "1248/3000 train_loss: 56.41057586669922 test_loss:124.54226684570312\n",
            "1249/3000 train_loss: 54.536705017089844 test_loss:126.37908935546875\n",
            "1250/3000 train_loss: 58.39241409301758 test_loss:127.19132232666016\n",
            "1251/3000 train_loss: 74.76531982421875 test_loss:132.6418914794922\n",
            "1252/3000 train_loss: 68.00775146484375 test_loss:136.24740600585938\n",
            "1253/3000 train_loss: 64.22803497314453 test_loss:126.21357727050781\n",
            "1254/3000 train_loss: 61.25898361206055 test_loss:132.00816345214844\n",
            "1255/3000 train_loss: 60.70601272583008 test_loss:122.23757934570312\n",
            "1256/3000 train_loss: 59.165077209472656 test_loss:129.91415405273438\n",
            "1257/3000 train_loss: 57.7716064453125 test_loss:119.9476318359375\n",
            "1258/3000 train_loss: 63.63740921020508 test_loss:124.13406372070312\n",
            "1259/3000 train_loss: 64.5841293334961 test_loss:135.05828857421875\n",
            "1260/3000 train_loss: 52.98531723022461 test_loss:118.62908172607422\n",
            "1261/3000 train_loss: 52.488311767578125 test_loss:120.52166748046875\n",
            "1262/3000 train_loss: 63.31571578979492 test_loss:124.5817642211914\n",
            "1263/3000 train_loss: 55.06095886230469 test_loss:119.40128326416016\n",
            "1264/3000 train_loss: 59.6165771484375 test_loss:124.25857543945312\n",
            "1265/3000 train_loss: 54.84844207763672 test_loss:119.49536895751953\n",
            "1266/3000 train_loss: 49.128902435302734 test_loss:129.54837036132812\n",
            "1267/3000 train_loss: 55.85236358642578 test_loss:129.2935333251953\n",
            "1268/3000 train_loss: 52.30211639404297 test_loss:125.18630981445312\n",
            "1269/3000 train_loss: 53.62403106689453 test_loss:125.99342346191406\n",
            "1270/3000 train_loss: 61.93948745727539 test_loss:118.3132553100586\n",
            "1271/3000 train_loss: 75.17674255371094 test_loss:124.3468246459961\n",
            "1272/3000 train_loss: 58.4051628112793 test_loss:126.42050170898438\n",
            "1273/3000 train_loss: 53.68461227416992 test_loss:131.50083923339844\n",
            "1274/3000 train_loss: 55.20842361450195 test_loss:126.21443939208984\n",
            "1275/3000 train_loss: 54.913673400878906 test_loss:121.31549072265625\n",
            "1276/3000 train_loss: 59.27074432373047 test_loss:116.43862915039062\n",
            "1277/3000 train_loss: 57.7203254699707 test_loss:119.40693664550781\n",
            "1278/3000 train_loss: 57.146644592285156 test_loss:118.72721862792969\n",
            "1279/3000 train_loss: 49.90034866333008 test_loss:123.72112274169922\n",
            "1280/3000 train_loss: 64.29689025878906 test_loss:122.03028869628906\n",
            "1281/3000 train_loss: 51.217041015625 test_loss:125.00270080566406\n",
            "1282/3000 train_loss: 50.34517288208008 test_loss:124.36193084716797\n",
            "1283/3000 train_loss: 56.54804229736328 test_loss:112.4251708984375\n",
            "1284/3000 train_loss: 58.1689567565918 test_loss:124.94918060302734\n",
            "1285/3000 train_loss: 49.623374938964844 test_loss:118.7560043334961\n",
            "1286/3000 train_loss: 56.16343688964844 test_loss:116.5269546508789\n",
            "1287/3000 train_loss: 49.22682571411133 test_loss:118.82218170166016\n",
            "1288/3000 train_loss: 53.47834396362305 test_loss:124.82402801513672\n",
            "1289/3000 train_loss: 50.870784759521484 test_loss:121.12413024902344\n",
            "1290/3000 train_loss: 52.96387481689453 test_loss:125.99820709228516\n",
            "1291/3000 train_loss: 51.4822883605957 test_loss:126.89105224609375\n",
            "1292/3000 train_loss: 54.51603698730469 test_loss:125.79161071777344\n",
            "1293/3000 train_loss: 52.029884338378906 test_loss:121.59505462646484\n",
            "1294/3000 train_loss: 55.14189529418945 test_loss:126.52770233154297\n",
            "1295/3000 train_loss: 57.49641418457031 test_loss:120.17955780029297\n",
            "1296/3000 train_loss: 54.021270751953125 test_loss:117.7792739868164\n",
            "1297/3000 train_loss: 63.48846435546875 test_loss:112.42982482910156\n",
            "1298/3000 train_loss: 61.84811782836914 test_loss:127.05243682861328\n",
            "1299/3000 train_loss: 51.426151275634766 test_loss:129.36105346679688\n",
            "1300/3000 train_loss: 51.32848358154297 test_loss:119.43897247314453\n",
            "1301/3000 train_loss: 52.2834587097168 test_loss:126.07107543945312\n",
            "1302/3000 train_loss: 55.905765533447266 test_loss:122.68263244628906\n",
            "1303/3000 train_loss: 60.898319244384766 test_loss:129.98072814941406\n",
            "1304/3000 train_loss: 51.93859100341797 test_loss:120.60736846923828\n",
            "1305/3000 train_loss: 60.033241271972656 test_loss:120.2341079711914\n",
            "1306/3000 train_loss: 53.71089172363281 test_loss:120.39234924316406\n",
            "1307/3000 train_loss: 53.0533447265625 test_loss:128.00648498535156\n",
            "1308/3000 train_loss: 52.20754623413086 test_loss:120.25528717041016\n",
            "1309/3000 train_loss: 50.628318786621094 test_loss:128.8845672607422\n",
            "1310/3000 train_loss: 45.65639114379883 test_loss:117.24114227294922\n",
            "1311/3000 train_loss: 53.511268615722656 test_loss:118.10664367675781\n",
            "1312/3000 train_loss: 56.13969802856445 test_loss:136.23716735839844\n",
            "1313/3000 train_loss: 53.114662170410156 test_loss:124.08267974853516\n",
            "1314/3000 train_loss: 70.24658966064453 test_loss:121.75067901611328\n",
            "1315/3000 train_loss: 47.61079025268555 test_loss:124.07962799072266\n",
            "1316/3000 train_loss: 49.56284713745117 test_loss:117.9593734741211\n",
            "1317/3000 train_loss: 52.30607223510742 test_loss:126.03305053710938\n",
            "1318/3000 train_loss: 56.87017059326172 test_loss:115.15074920654297\n",
            "1319/3000 train_loss: 53.70418930053711 test_loss:130.93374633789062\n",
            "1320/3000 train_loss: 50.281925201416016 test_loss:114.30628967285156\n",
            "1321/3000 train_loss: 55.88740921020508 test_loss:130.58090209960938\n",
            "1322/3000 train_loss: 53.432647705078125 test_loss:124.19727325439453\n",
            "1323/3000 train_loss: 53.04857635498047 test_loss:119.81916046142578\n",
            "1324/3000 train_loss: 54.10081481933594 test_loss:118.85755920410156\n",
            "1325/3000 train_loss: 54.0228271484375 test_loss:122.75078582763672\n",
            "1326/3000 train_loss: 52.92603302001953 test_loss:112.50041198730469\n",
            "1327/3000 train_loss: 47.97754669189453 test_loss:115.18669891357422\n",
            "1328/3000 train_loss: 50.16307067871094 test_loss:126.3050765991211\n",
            "1329/3000 train_loss: 51.12995529174805 test_loss:121.79963684082031\n",
            "1330/3000 train_loss: 53.5062255859375 test_loss:126.67730712890625\n",
            "1331/3000 train_loss: 49.03708267211914 test_loss:124.23776245117188\n",
            "1332/3000 train_loss: 53.58181381225586 test_loss:128.15032958984375\n",
            "1333/3000 train_loss: 58.57197189331055 test_loss:118.20443725585938\n",
            "1334/3000 train_loss: 55.77663040161133 test_loss:138.45001220703125\n",
            "1335/3000 train_loss: 50.626678466796875 test_loss:115.47834777832031\n",
            "1336/3000 train_loss: 51.72673797607422 test_loss:124.48318481445312\n",
            "1337/3000 train_loss: 49.881561279296875 test_loss:131.15538024902344\n",
            "1338/3000 train_loss: 64.99530792236328 test_loss:115.90673065185547\n",
            "1339/3000 train_loss: 44.88224411010742 test_loss:128.97100830078125\n",
            "1340/3000 train_loss: 49.42536163330078 test_loss:116.59603881835938\n",
            "1341/3000 train_loss: 51.525726318359375 test_loss:116.22236633300781\n",
            "1342/3000 train_loss: 69.55074310302734 test_loss:127.56312561035156\n",
            "1343/3000 train_loss: 58.062129974365234 test_loss:115.29438781738281\n",
            "1344/3000 train_loss: 48.990108489990234 test_loss:134.6522216796875\n",
            "1345/3000 train_loss: 54.2564811706543 test_loss:113.80498504638672\n",
            "1346/3000 train_loss: 50.498043060302734 test_loss:116.7694091796875\n",
            "1347/3000 train_loss: 48.3902587890625 test_loss:114.14677429199219\n",
            "1348/3000 train_loss: 52.379215240478516 test_loss:124.12905883789062\n",
            "1349/3000 train_loss: 58.10133361816406 test_loss:125.09733581542969\n",
            "1350/3000 train_loss: 55.39320755004883 test_loss:121.49642944335938\n",
            "1351/3000 train_loss: 57.34147262573242 test_loss:128.80047607421875\n",
            "1352/3000 train_loss: 53.059974670410156 test_loss:117.854248046875\n",
            "1353/3000 train_loss: 51.0269775390625 test_loss:116.1380844116211\n",
            "1354/3000 train_loss: 56.62584686279297 test_loss:124.07762908935547\n",
            "1355/3000 train_loss: 48.34718322753906 test_loss:117.1711196899414\n",
            "1356/3000 train_loss: 54.2433967590332 test_loss:123.20349884033203\n",
            "1357/3000 train_loss: 53.046749114990234 test_loss:123.3127670288086\n",
            "1358/3000 train_loss: 61.68803024291992 test_loss:114.98786926269531\n",
            "1359/3000 train_loss: 60.582664489746094 test_loss:129.03018188476562\n",
            "1360/3000 train_loss: 50.13680648803711 test_loss:132.41213989257812\n",
            "1361/3000 train_loss: 55.88040542602539 test_loss:116.1183090209961\n",
            "1362/3000 train_loss: 54.92531204223633 test_loss:119.75867462158203\n",
            "1363/3000 train_loss: 60.15621566772461 test_loss:122.00006103515625\n",
            "1364/3000 train_loss: 51.82974624633789 test_loss:117.1537094116211\n",
            "1365/3000 train_loss: 46.161773681640625 test_loss:118.28269958496094\n",
            "1366/3000 train_loss: 47.619651794433594 test_loss:116.11091613769531\n",
            "1367/3000 train_loss: 56.48173141479492 test_loss:115.69078063964844\n",
            "1368/3000 train_loss: 52.268638610839844 test_loss:122.42158508300781\n",
            "1369/3000 train_loss: 53.39524459838867 test_loss:115.02022552490234\n",
            "1370/3000 train_loss: 49.189186096191406 test_loss:121.87810516357422\n",
            "1371/3000 train_loss: 57.825748443603516 test_loss:113.38203430175781\n",
            "1372/3000 train_loss: 50.2075309753418 test_loss:120.19512939453125\n",
            "1373/3000 train_loss: 58.848941802978516 test_loss:136.8386993408203\n",
            "1374/3000 train_loss: 61.91463851928711 test_loss:120.08293914794922\n",
            "1375/3000 train_loss: 55.470272064208984 test_loss:112.09375\n",
            "1376/3000 train_loss: 49.55301284790039 test_loss:116.83641815185547\n",
            "1377/3000 train_loss: 58.324989318847656 test_loss:129.08383178710938\n",
            "1378/3000 train_loss: 56.184608459472656 test_loss:115.11424255371094\n",
            "1379/3000 train_loss: 48.45659255981445 test_loss:120.69507598876953\n",
            "1380/3000 train_loss: 57.7176513671875 test_loss:115.2938461303711\n",
            "1381/3000 train_loss: 53.71337890625 test_loss:115.52769470214844\n",
            "1382/3000 train_loss: 46.014163970947266 test_loss:130.0642852783203\n",
            "1383/3000 train_loss: 55.89558792114258 test_loss:119.15084075927734\n",
            "1384/3000 train_loss: 52.180484771728516 test_loss:127.32796478271484\n",
            "1385/3000 train_loss: 52.60355758666992 test_loss:125.17619323730469\n",
            "1386/3000 train_loss: 54.31531524658203 test_loss:136.696044921875\n",
            "1387/3000 train_loss: 48.99256896972656 test_loss:119.17422485351562\n",
            "1388/3000 train_loss: 52.84467697143555 test_loss:127.75004577636719\n",
            "1389/3000 train_loss: 45.51107406616211 test_loss:124.18576049804688\n",
            "1390/3000 train_loss: 56.828556060791016 test_loss:136.85107421875\n",
            "1391/3000 train_loss: 52.19424057006836 test_loss:115.65592956542969\n",
            "1392/3000 train_loss: 49.08114242553711 test_loss:130.74566650390625\n",
            "1393/3000 train_loss: 46.2240104675293 test_loss:122.1573715209961\n",
            "1394/3000 train_loss: 55.677024841308594 test_loss:126.0091552734375\n",
            "1395/3000 train_loss: 53.20201110839844 test_loss:116.8760757446289\n",
            "1396/3000 train_loss: 64.14278411865234 test_loss:118.55733489990234\n",
            "1397/3000 train_loss: 57.23085403442383 test_loss:120.40283966064453\n",
            "1398/3000 train_loss: 48.41400909423828 test_loss:114.43815612792969\n",
            "1399/3000 train_loss: 58.28603744506836 test_loss:114.04635620117188\n",
            "1400/3000 train_loss: 49.26828384399414 test_loss:119.66832733154297\n",
            "1401/3000 train_loss: 48.92119598388672 test_loss:116.44011688232422\n",
            "1402/3000 train_loss: 48.14600372314453 test_loss:113.411865234375\n",
            "1403/3000 train_loss: 49.45503616333008 test_loss:123.7329330444336\n",
            "1404/3000 train_loss: 52.73012924194336 test_loss:129.73536682128906\n",
            "1405/3000 train_loss: 47.925811767578125 test_loss:115.67662048339844\n",
            "1406/3000 train_loss: 48.54393005371094 test_loss:113.59747314453125\n",
            "1407/3000 train_loss: 59.28489303588867 test_loss:121.773681640625\n",
            "1408/3000 train_loss: 46.02333068847656 test_loss:113.52523040771484\n",
            "1409/3000 train_loss: 49.523948669433594 test_loss:112.46582794189453\n",
            "1410/3000 train_loss: 56.5137825012207 test_loss:117.52918243408203\n",
            "1411/3000 train_loss: 54.733299255371094 test_loss:134.99609375\n",
            "1412/3000 train_loss: 53.57823181152344 test_loss:114.67213439941406\n",
            "1413/3000 train_loss: 48.53310012817383 test_loss:115.0708236694336\n",
            "1414/3000 train_loss: 56.9821662902832 test_loss:119.30239868164062\n",
            "1415/3000 train_loss: 59.4122200012207 test_loss:109.20232391357422\n",
            "1416/3000 train_loss: 51.13542938232422 test_loss:115.55787658691406\n",
            "1417/3000 train_loss: 53.208946228027344 test_loss:118.90621948242188\n",
            "1418/3000 train_loss: 61.3543701171875 test_loss:115.42143249511719\n",
            "1419/3000 train_loss: 48.13501739501953 test_loss:120.72581481933594\n",
            "1420/3000 train_loss: 49.55358123779297 test_loss:110.22735595703125\n",
            "1421/3000 train_loss: 58.22587966918945 test_loss:126.67515563964844\n",
            "1422/3000 train_loss: 48.35496520996094 test_loss:116.75685119628906\n",
            "1423/3000 train_loss: 54.5638427734375 test_loss:117.49968719482422\n",
            "1424/3000 train_loss: 50.87620544433594 test_loss:129.33648681640625\n",
            "1425/3000 train_loss: 53.72407531738281 test_loss:114.6212158203125\n",
            "1426/3000 train_loss: 53.01957702636719 test_loss:126.87255096435547\n",
            "1427/3000 train_loss: 50.553245544433594 test_loss:115.0406723022461\n",
            "1428/3000 train_loss: 53.016876220703125 test_loss:111.49845886230469\n",
            "1429/3000 train_loss: 48.1334114074707 test_loss:129.63294982910156\n",
            "1430/3000 train_loss: 52.62311553955078 test_loss:111.92150115966797\n",
            "1431/3000 train_loss: 50.3209228515625 test_loss:116.78053283691406\n",
            "1432/3000 train_loss: 45.62570571899414 test_loss:114.81625366210938\n",
            "1433/3000 train_loss: 49.80508804321289 test_loss:114.79473876953125\n",
            "1434/3000 train_loss: 52.403011322021484 test_loss:114.0217514038086\n",
            "1435/3000 train_loss: 54.313358306884766 test_loss:118.22694396972656\n",
            "1436/3000 train_loss: 49.9129753112793 test_loss:116.7993392944336\n",
            "1437/3000 train_loss: 47.4691162109375 test_loss:118.12320709228516\n",
            "1438/3000 train_loss: 45.03215789794922 test_loss:120.606689453125\n",
            "1439/3000 train_loss: 53.269344329833984 test_loss:122.62664794921875\n",
            "1440/3000 train_loss: 46.92851257324219 test_loss:118.6538314819336\n",
            "1441/3000 train_loss: 53.19315719604492 test_loss:115.94036102294922\n",
            "1442/3000 train_loss: 51.481590270996094 test_loss:116.66206359863281\n",
            "1443/3000 train_loss: 55.31891632080078 test_loss:123.12884521484375\n",
            "1444/3000 train_loss: 45.02766036987305 test_loss:119.06864929199219\n",
            "1445/3000 train_loss: 57.41698455810547 test_loss:116.19172668457031\n",
            "1446/3000 train_loss: 52.02833557128906 test_loss:120.95479583740234\n",
            "1447/3000 train_loss: 45.79309844970703 test_loss:113.59637451171875\n",
            "1448/3000 train_loss: 52.68893814086914 test_loss:118.23221588134766\n",
            "1449/3000 train_loss: 46.47520446777344 test_loss:113.35317993164062\n",
            "1450/3000 train_loss: 49.36875534057617 test_loss:118.89306640625\n",
            "1451/3000 train_loss: 51.68088150024414 test_loss:124.51377868652344\n",
            "1452/3000 train_loss: 49.264930725097656 test_loss:121.61532592773438\n",
            "1453/3000 train_loss: 62.53467559814453 test_loss:114.28633117675781\n",
            "1454/3000 train_loss: 51.360618591308594 test_loss:113.3996353149414\n",
            "1455/3000 train_loss: 48.30881881713867 test_loss:120.53392791748047\n",
            "1456/3000 train_loss: 49.576446533203125 test_loss:110.29745483398438\n",
            "1457/3000 train_loss: 47.48688507080078 test_loss:117.65472412109375\n",
            "1458/3000 train_loss: 50.289730072021484 test_loss:111.23385620117188\n",
            "1459/3000 train_loss: 53.70542907714844 test_loss:112.68668365478516\n",
            "1460/3000 train_loss: 46.136138916015625 test_loss:115.97914123535156\n",
            "1461/3000 train_loss: 50.56754684448242 test_loss:124.56095886230469\n",
            "1462/3000 train_loss: 55.98320007324219 test_loss:116.34361267089844\n",
            "1463/3000 train_loss: 50.606407165527344 test_loss:125.86756896972656\n",
            "1464/3000 train_loss: 46.747642517089844 test_loss:118.92263793945312\n",
            "1465/3000 train_loss: 46.85780715942383 test_loss:114.84542846679688\n",
            "1466/3000 train_loss: 46.2197151184082 test_loss:108.44204711914062\n",
            "1467/3000 train_loss: 45.089874267578125 test_loss:110.85062408447266\n",
            "1468/3000 train_loss: 46.62983703613281 test_loss:113.69335174560547\n",
            "1469/3000 train_loss: 52.00019836425781 test_loss:109.82124328613281\n",
            "1470/3000 train_loss: 53.16954040527344 test_loss:113.13041687011719\n",
            "1471/3000 train_loss: 44.210533142089844 test_loss:121.75643920898438\n",
            "1472/3000 train_loss: 75.0689697265625 test_loss:124.36590576171875\n",
            "1473/3000 train_loss: 57.80149459838867 test_loss:116.8099365234375\n",
            "1474/3000 train_loss: 45.23504638671875 test_loss:112.16551971435547\n",
            "1475/3000 train_loss: 46.589359283447266 test_loss:112.220458984375\n",
            "1476/3000 train_loss: 44.42401123046875 test_loss:120.4537124633789\n",
            "1477/3000 train_loss: 46.02709197998047 test_loss:110.12806701660156\n",
            "1478/3000 train_loss: 52.367584228515625 test_loss:110.97757720947266\n",
            "1479/3000 train_loss: 51.899497985839844 test_loss:110.0484390258789\n",
            "1480/3000 train_loss: 54.25492477416992 test_loss:118.06578826904297\n",
            "1481/3000 train_loss: 45.660072326660156 test_loss:124.81747436523438\n",
            "1482/3000 train_loss: 53.35227966308594 test_loss:117.52425384521484\n",
            "1483/3000 train_loss: 49.32160949707031 test_loss:116.41119384765625\n",
            "1484/3000 train_loss: 44.603973388671875 test_loss:121.03007507324219\n",
            "1485/3000 train_loss: 47.336402893066406 test_loss:109.62907409667969\n",
            "1486/3000 train_loss: 46.824764251708984 test_loss:112.67816925048828\n",
            "1487/3000 train_loss: 41.860595703125 test_loss:112.37686920166016\n",
            "1488/3000 train_loss: 40.77306365966797 test_loss:120.6986312866211\n",
            "1489/3000 train_loss: 49.74787902832031 test_loss:113.84147644042969\n",
            "1490/3000 train_loss: 46.34999465942383 test_loss:125.07102966308594\n",
            "1491/3000 train_loss: 50.24481201171875 test_loss:117.91631317138672\n",
            "1492/3000 train_loss: 49.89290237426758 test_loss:112.4023666381836\n",
            "1493/3000 train_loss: 44.41337585449219 test_loss:120.15032958984375\n",
            "1494/3000 train_loss: 47.41522979736328 test_loss:118.89444732666016\n",
            "1495/3000 train_loss: 48.707603454589844 test_loss:112.11225891113281\n",
            "1496/3000 train_loss: 52.255706787109375 test_loss:115.0550537109375\n",
            "1497/3000 train_loss: 55.4129753112793 test_loss:116.6203384399414\n",
            "1498/3000 train_loss: 50.47209930419922 test_loss:128.90960693359375\n",
            "1499/3000 train_loss: 53.81192398071289 test_loss:115.10872650146484\n",
            "1500/3000 train_loss: 50.471397399902344 test_loss:115.82998657226562\n",
            "1501/3000 train_loss: 54.0234260559082 test_loss:121.69062805175781\n",
            "1502/3000 train_loss: 47.263004302978516 test_loss:109.17794799804688\n",
            "1503/3000 train_loss: 44.99876022338867 test_loss:113.50474548339844\n",
            "1504/3000 train_loss: 42.132102966308594 test_loss:108.9336166381836\n",
            "1505/3000 train_loss: 49.35653305053711 test_loss:118.60884857177734\n",
            "1506/3000 train_loss: 53.099876403808594 test_loss:107.91670227050781\n",
            "1507/3000 train_loss: 44.28471755981445 test_loss:114.29241943359375\n",
            "1508/3000 train_loss: 42.7440299987793 test_loss:108.77386474609375\n",
            "1509/3000 train_loss: 47.7905158996582 test_loss:112.19364929199219\n",
            "1510/3000 train_loss: 43.129024505615234 test_loss:107.80579376220703\n",
            "1511/3000 train_loss: 44.385475158691406 test_loss:113.59102630615234\n",
            "1512/3000 train_loss: 42.37008285522461 test_loss:119.50315856933594\n",
            "1513/3000 train_loss: 46.75996017456055 test_loss:113.34649658203125\n",
            "1514/3000 train_loss: 45.750972747802734 test_loss:110.93563842773438\n",
            "1515/3000 train_loss: 47.04298782348633 test_loss:116.9364242553711\n",
            "1516/3000 train_loss: 47.25047302246094 test_loss:115.00875091552734\n",
            "1517/3000 train_loss: 41.570213317871094 test_loss:106.7001953125\n",
            "1518/3000 train_loss: 57.89231872558594 test_loss:111.10942077636719\n",
            "1519/3000 train_loss: 47.635292053222656 test_loss:118.47874450683594\n",
            "1520/3000 train_loss: 52.752132415771484 test_loss:110.26515197753906\n",
            "1521/3000 train_loss: 41.12920379638672 test_loss:106.34246826171875\n",
            "1522/3000 train_loss: 47.70029830932617 test_loss:127.12088012695312\n",
            "1523/3000 train_loss: 66.54574584960938 test_loss:123.23529052734375\n",
            "1524/3000 train_loss: 50.043434143066406 test_loss:113.2352066040039\n",
            "1525/3000 train_loss: 42.83416748046875 test_loss:115.43246459960938\n",
            "1526/3000 train_loss: 56.12480163574219 test_loss:123.18069458007812\n",
            "1527/3000 train_loss: 46.665164947509766 test_loss:113.27810668945312\n",
            "1528/3000 train_loss: 46.331356048583984 test_loss:110.46466827392578\n",
            "1529/3000 train_loss: 48.19509506225586 test_loss:109.39263916015625\n",
            "1530/3000 train_loss: 42.00860595703125 test_loss:112.64331817626953\n",
            "1531/3000 train_loss: 47.02876663208008 test_loss:122.79127502441406\n",
            "1532/3000 train_loss: 50.64868927001953 test_loss:118.1417236328125\n",
            "1533/3000 train_loss: 44.57352828979492 test_loss:106.81563568115234\n",
            "1534/3000 train_loss: 48.774810791015625 test_loss:131.87025451660156\n",
            "1535/3000 train_loss: 45.304527282714844 test_loss:107.3324203491211\n",
            "1536/3000 train_loss: 46.18341827392578 test_loss:122.94548034667969\n",
            "1537/3000 train_loss: 43.94329833984375 test_loss:111.22346496582031\n",
            "1538/3000 train_loss: 46.40134048461914 test_loss:116.65829467773438\n",
            "1539/3000 train_loss: 47.263980865478516 test_loss:117.29444122314453\n",
            "1540/3000 train_loss: 47.98465347290039 test_loss:110.26516723632812\n",
            "1541/3000 train_loss: 48.899986267089844 test_loss:106.05567932128906\n",
            "1542/3000 train_loss: 41.98288345336914 test_loss:107.83618927001953\n",
            "1543/3000 train_loss: 50.2783203125 test_loss:109.64351654052734\n",
            "1544/3000 train_loss: 39.84414291381836 test_loss:116.72256469726562\n",
            "1545/3000 train_loss: 46.85033416748047 test_loss:116.56681060791016\n",
            "1546/3000 train_loss: 41.83470153808594 test_loss:111.40180969238281\n",
            "1547/3000 train_loss: 48.613094329833984 test_loss:122.14376831054688\n",
            "1548/3000 train_loss: 46.6358642578125 test_loss:107.37259674072266\n",
            "1549/3000 train_loss: 47.20969009399414 test_loss:116.04257202148438\n",
            "1550/3000 train_loss: 48.81346893310547 test_loss:111.1074447631836\n",
            "1551/3000 train_loss: 42.49451446533203 test_loss:118.12428283691406\n",
            "1552/3000 train_loss: 47.92151641845703 test_loss:114.11658477783203\n",
            "1553/3000 train_loss: 42.684242248535156 test_loss:119.19175720214844\n",
            "1554/3000 train_loss: 53.107154846191406 test_loss:112.26073455810547\n",
            "1555/3000 train_loss: 51.408119201660156 test_loss:116.90408325195312\n",
            "1556/3000 train_loss: 43.40538024902344 test_loss:105.96971130371094\n",
            "1557/3000 train_loss: 41.432098388671875 test_loss:119.34782409667969\n",
            "1558/3000 train_loss: 43.407470703125 test_loss:105.80377960205078\n",
            "1559/3000 train_loss: 43.206626892089844 test_loss:117.12075805664062\n",
            "1560/3000 train_loss: 47.433082580566406 test_loss:128.29420471191406\n",
            "1561/3000 train_loss: 54.14577865600586 test_loss:105.72207641601562\n",
            "1562/3000 train_loss: 50.71006393432617 test_loss:121.03214263916016\n",
            "1563/3000 train_loss: 51.240936279296875 test_loss:109.95901489257812\n",
            "1564/3000 train_loss: 46.01020050048828 test_loss:106.27680969238281\n",
            "1565/3000 train_loss: 47.33502197265625 test_loss:109.11543273925781\n",
            "1566/3000 train_loss: 56.08976745605469 test_loss:114.49622344970703\n",
            "1567/3000 train_loss: 55.42596435546875 test_loss:106.75057220458984\n",
            "1568/3000 train_loss: 45.05322265625 test_loss:119.8840560913086\n",
            "1569/3000 train_loss: 47.991004943847656 test_loss:108.56961822509766\n",
            "1570/3000 train_loss: 59.91692352294922 test_loss:110.82788848876953\n",
            "1571/3000 train_loss: 48.48759078979492 test_loss:116.1982421875\n",
            "1572/3000 train_loss: 41.800498962402344 test_loss:113.43727111816406\n",
            "1573/3000 train_loss: 48.21525955200195 test_loss:106.84076690673828\n",
            "1574/3000 train_loss: 43.75067901611328 test_loss:104.96593475341797\n",
            "1575/3000 train_loss: 48.506771087646484 test_loss:110.80816650390625\n",
            "1576/3000 train_loss: 39.47298049926758 test_loss:106.53955841064453\n",
            "1577/3000 train_loss: 52.84601974487305 test_loss:118.26020812988281\n",
            "1578/3000 train_loss: 49.84565734863281 test_loss:112.48487091064453\n",
            "1579/3000 train_loss: 46.6543083190918 test_loss:108.12200164794922\n",
            "1580/3000 train_loss: 42.78508758544922 test_loss:106.72335052490234\n",
            "1581/3000 train_loss: 47.82619857788086 test_loss:112.757568359375\n",
            "1582/3000 train_loss: 46.02989196777344 test_loss:121.83821868896484\n",
            "1583/3000 train_loss: 47.835323333740234 test_loss:115.60655975341797\n",
            "1584/3000 train_loss: 48.88881301879883 test_loss:106.80198669433594\n",
            "1585/3000 train_loss: 38.0327033996582 test_loss:115.27714538574219\n",
            "1586/3000 train_loss: 43.20708465576172 test_loss:110.26944732666016\n",
            "1587/3000 train_loss: 43.535072326660156 test_loss:109.06836700439453\n",
            "1588/3000 train_loss: 42.36732864379883 test_loss:117.58552551269531\n",
            "1589/3000 train_loss: 49.933494567871094 test_loss:113.31675720214844\n",
            "1590/3000 train_loss: 42.9111442565918 test_loss:118.36162567138672\n",
            "1591/3000 train_loss: 49.26100158691406 test_loss:112.14644622802734\n",
            "1592/3000 train_loss: 38.26618194580078 test_loss:116.1784439086914\n",
            "1593/3000 train_loss: 44.306884765625 test_loss:112.78014373779297\n",
            "1594/3000 train_loss: 51.83112335205078 test_loss:116.17794036865234\n",
            "1595/3000 train_loss: 45.398860931396484 test_loss:121.01177215576172\n",
            "1596/3000 train_loss: 44.45314025878906 test_loss:111.91127014160156\n",
            "1597/3000 train_loss: 40.17412185668945 test_loss:104.33707427978516\n",
            "1598/3000 train_loss: 50.542720794677734 test_loss:116.05716705322266\n",
            "1599/3000 train_loss: 42.889739990234375 test_loss:117.30206298828125\n",
            "1600/3000 train_loss: 44.72256851196289 test_loss:112.15587615966797\n",
            "1601/3000 train_loss: 39.552242279052734 test_loss:109.77490234375\n",
            "1602/3000 train_loss: 44.036224365234375 test_loss:111.87239074707031\n",
            "1603/3000 train_loss: 49.194156646728516 test_loss:118.57496643066406\n",
            "1604/3000 train_loss: 44.268245697021484 test_loss:105.02024841308594\n",
            "1605/3000 train_loss: 47.549293518066406 test_loss:114.61579132080078\n",
            "1606/3000 train_loss: 40.950469970703125 test_loss:108.62835693359375\n",
            "1607/3000 train_loss: 38.66636276245117 test_loss:107.04734802246094\n",
            "1608/3000 train_loss: 41.04744338989258 test_loss:111.2448501586914\n",
            "1609/3000 train_loss: 40.79290771484375 test_loss:114.86018371582031\n",
            "1610/3000 train_loss: 46.06591033935547 test_loss:115.99847412109375\n",
            "1611/3000 train_loss: 46.760231018066406 test_loss:122.40837097167969\n",
            "1612/3000 train_loss: 58.352996826171875 test_loss:110.78826904296875\n",
            "1613/3000 train_loss: 47.50355911254883 test_loss:111.62549591064453\n",
            "1614/3000 train_loss: 42.62480926513672 test_loss:113.52729034423828\n",
            "1615/3000 train_loss: 43.49553298950195 test_loss:104.5337905883789\n",
            "1616/3000 train_loss: 42.60884094238281 test_loss:118.59503173828125\n",
            "1617/3000 train_loss: 44.99803924560547 test_loss:112.22118377685547\n",
            "1618/3000 train_loss: 53.17097473144531 test_loss:114.49498748779297\n",
            "1619/3000 train_loss: 44.494590759277344 test_loss:105.7425765991211\n",
            "1620/3000 train_loss: 46.60801696777344 test_loss:118.48628997802734\n",
            "1621/3000 train_loss: 48.89922332763672 test_loss:104.74662017822266\n",
            "1622/3000 train_loss: 38.85265350341797 test_loss:103.8584976196289\n",
            "1623/3000 train_loss: 39.8684196472168 test_loss:100.86529541015625\n",
            "1624/3000 train_loss: 48.542057037353516 test_loss:115.03199005126953\n",
            "1625/3000 train_loss: 41.28392028808594 test_loss:110.63072204589844\n",
            "1626/3000 train_loss: 42.626991271972656 test_loss:106.48500061035156\n",
            "1627/3000 train_loss: 48.24696350097656 test_loss:114.20011901855469\n",
            "1628/3000 train_loss: 46.549842834472656 test_loss:111.37417602539062\n",
            "1629/3000 train_loss: 41.46365737915039 test_loss:106.67681884765625\n",
            "1630/3000 train_loss: 46.18557357788086 test_loss:109.35863494873047\n",
            "1631/3000 train_loss: 39.52348709106445 test_loss:108.0562515258789\n",
            "1632/3000 train_loss: 49.58572769165039 test_loss:104.61018371582031\n",
            "1633/3000 train_loss: 47.547645568847656 test_loss:118.38375854492188\n",
            "1634/3000 train_loss: 48.02907180786133 test_loss:104.52983093261719\n",
            "1635/3000 train_loss: 45.6433219909668 test_loss:111.05382537841797\n",
            "1636/3000 train_loss: 47.786502838134766 test_loss:109.47775268554688\n",
            "1637/3000 train_loss: 47.259063720703125 test_loss:108.22105407714844\n",
            "1638/3000 train_loss: 40.48296356201172 test_loss:106.23731231689453\n",
            "1639/3000 train_loss: 40.06096267700195 test_loss:108.6541519165039\n",
            "1640/3000 train_loss: 52.088592529296875 test_loss:112.76077270507812\n",
            "1641/3000 train_loss: 52.050689697265625 test_loss:108.88646697998047\n",
            "1642/3000 train_loss: 48.433197021484375 test_loss:118.9954833984375\n",
            "1643/3000 train_loss: 38.043514251708984 test_loss:105.25880432128906\n",
            "1644/3000 train_loss: 40.9598274230957 test_loss:121.02838134765625\n",
            "1645/3000 train_loss: 43.336639404296875 test_loss:106.82828521728516\n",
            "1646/3000 train_loss: 44.192054748535156 test_loss:107.896484375\n",
            "1647/3000 train_loss: 40.91305160522461 test_loss:120.12445831298828\n",
            "1648/3000 train_loss: 38.80862808227539 test_loss:107.02449798583984\n",
            "1649/3000 train_loss: 44.27815628051758 test_loss:108.64412689208984\n",
            "1650/3000 train_loss: 42.704830169677734 test_loss:103.62825775146484\n",
            "1651/3000 train_loss: 47.38161849975586 test_loss:126.24411010742188\n",
            "1652/3000 train_loss: 49.83685302734375 test_loss:112.84954833984375\n",
            "1653/3000 train_loss: 40.98906326293945 test_loss:112.83822631835938\n",
            "1654/3000 train_loss: 41.282405853271484 test_loss:106.59379577636719\n",
            "1655/3000 train_loss: 42.22589874267578 test_loss:117.9051284790039\n",
            "1656/3000 train_loss: 44.77243423461914 test_loss:109.11249542236328\n",
            "1657/3000 train_loss: 42.67020034790039 test_loss:119.5470199584961\n",
            "1658/3000 train_loss: 38.83222198486328 test_loss:118.26068115234375\n",
            "1659/3000 train_loss: 46.94464874267578 test_loss:106.17039489746094\n",
            "1660/3000 train_loss: 41.1077766418457 test_loss:108.87354278564453\n",
            "1661/3000 train_loss: 41.450408935546875 test_loss:103.33477020263672\n",
            "1662/3000 train_loss: 41.9650764465332 test_loss:107.63081359863281\n",
            "1663/3000 train_loss: 43.25788116455078 test_loss:111.81895446777344\n",
            "1664/3000 train_loss: 46.5290641784668 test_loss:112.80597686767578\n",
            "1665/3000 train_loss: 43.33956527709961 test_loss:113.7161865234375\n",
            "1666/3000 train_loss: 47.10498046875 test_loss:110.63378143310547\n",
            "1667/3000 train_loss: 40.99551773071289 test_loss:110.1694564819336\n",
            "1668/3000 train_loss: 42.17615509033203 test_loss:110.1265640258789\n",
            "1669/3000 train_loss: 44.13666534423828 test_loss:111.94571685791016\n",
            "1670/3000 train_loss: 39.9774055480957 test_loss:118.02347564697266\n",
            "1671/3000 train_loss: 47.59355545043945 test_loss:110.5030517578125\n",
            "1672/3000 train_loss: 45.33717346191406 test_loss:124.08151245117188\n",
            "1673/3000 train_loss: 48.58327102661133 test_loss:121.92383575439453\n",
            "1674/3000 train_loss: 41.712806701660156 test_loss:119.76475524902344\n",
            "1675/3000 train_loss: 55.48408889770508 test_loss:108.13961029052734\n",
            "1676/3000 train_loss: 48.149105072021484 test_loss:106.31905364990234\n",
            "1677/3000 train_loss: 47.15342330932617 test_loss:107.28986358642578\n",
            "1678/3000 train_loss: 45.625919342041016 test_loss:101.86532592773438\n",
            "1679/3000 train_loss: 46.01218795776367 test_loss:106.4250259399414\n",
            "1680/3000 train_loss: 40.50701141357422 test_loss:107.13197326660156\n",
            "1681/3000 train_loss: 36.87923049926758 test_loss:106.5450210571289\n",
            "1682/3000 train_loss: 41.74985885620117 test_loss:108.13992309570312\n",
            "1683/3000 train_loss: 41.138710021972656 test_loss:108.90157318115234\n",
            "1684/3000 train_loss: 44.82017517089844 test_loss:111.82164764404297\n",
            "1685/3000 train_loss: 42.64335250854492 test_loss:114.49200439453125\n",
            "1686/3000 train_loss: 51.142452239990234 test_loss:103.74077606201172\n",
            "1687/3000 train_loss: 46.574501037597656 test_loss:99.10684967041016\n",
            "1688/3000 train_loss: 44.73713684082031 test_loss:114.27555847167969\n",
            "1689/3000 train_loss: 37.54499435424805 test_loss:100.00172424316406\n",
            "1690/3000 train_loss: 45.310516357421875 test_loss:104.48853302001953\n",
            "1691/3000 train_loss: 45.041534423828125 test_loss:104.1413803100586\n",
            "1692/3000 train_loss: 44.99581527709961 test_loss:118.39664459228516\n",
            "1693/3000 train_loss: 40.42509460449219 test_loss:101.90718078613281\n",
            "1694/3000 train_loss: 39.3689079284668 test_loss:112.64106750488281\n",
            "1695/3000 train_loss: 56.4021110534668 test_loss:112.23013305664062\n",
            "1696/3000 train_loss: 47.63356018066406 test_loss:113.23301696777344\n",
            "1697/3000 train_loss: 42.68198013305664 test_loss:115.24967956542969\n",
            "1698/3000 train_loss: 41.37038803100586 test_loss:107.77702331542969\n",
            "1699/3000 train_loss: 48.24220657348633 test_loss:118.1431655883789\n",
            "1700/3000 train_loss: 43.418941497802734 test_loss:102.13789367675781\n",
            "1701/3000 train_loss: 43.23423767089844 test_loss:115.2796859741211\n",
            "1702/3000 train_loss: 39.09614562988281 test_loss:107.87757873535156\n",
            "1703/3000 train_loss: 43.51924133300781 test_loss:105.57585144042969\n",
            "1704/3000 train_loss: 40.97625732421875 test_loss:116.3514404296875\n",
            "1705/3000 train_loss: 37.96970748901367 test_loss:103.89266204833984\n",
            "1706/3000 train_loss: 42.29315185546875 test_loss:103.71080017089844\n",
            "1707/3000 train_loss: 50.69910430908203 test_loss:113.33425903320312\n",
            "1708/3000 train_loss: 36.39199447631836 test_loss:103.13651275634766\n",
            "1709/3000 train_loss: 42.87554168701172 test_loss:111.02384185791016\n",
            "1710/3000 train_loss: 40.000938415527344 test_loss:113.12401580810547\n",
            "1711/3000 train_loss: 37.94231414794922 test_loss:114.36459350585938\n",
            "1712/3000 train_loss: 46.36527633666992 test_loss:107.71977233886719\n",
            "1713/3000 train_loss: 37.51515579223633 test_loss:114.5743637084961\n",
            "1714/3000 train_loss: 40.40122604370117 test_loss:101.15736389160156\n",
            "1715/3000 train_loss: 39.42314529418945 test_loss:110.06974792480469\n",
            "1716/3000 train_loss: 42.86653137207031 test_loss:114.43067932128906\n",
            "1717/3000 train_loss: 38.77255630493164 test_loss:103.25422668457031\n",
            "1718/3000 train_loss: 46.89943313598633 test_loss:110.01252746582031\n",
            "1719/3000 train_loss: 49.26137161254883 test_loss:111.99551391601562\n",
            "1720/3000 train_loss: 43.484806060791016 test_loss:111.18270874023438\n",
            "1721/3000 train_loss: 45.00907897949219 test_loss:107.16422271728516\n",
            "1722/3000 train_loss: 42.339012145996094 test_loss:107.03266906738281\n",
            "1723/3000 train_loss: 50.10039520263672 test_loss:100.3473892211914\n",
            "1724/3000 train_loss: 44.52448272705078 test_loss:114.73641967773438\n",
            "1725/3000 train_loss: 44.62369918823242 test_loss:114.574951171875\n",
            "1726/3000 train_loss: 38.02705383300781 test_loss:116.80784606933594\n",
            "1727/3000 train_loss: 42.914310455322266 test_loss:105.7356948852539\n",
            "1728/3000 train_loss: 37.51777267456055 test_loss:115.63605499267578\n",
            "1729/3000 train_loss: 45.6866569519043 test_loss:102.00395202636719\n",
            "1730/3000 train_loss: 46.504825592041016 test_loss:98.85478210449219\n",
            "1731/3000 train_loss: 49.141109466552734 test_loss:120.41030883789062\n",
            "1732/3000 train_loss: 47.71461486816406 test_loss:114.45311737060547\n",
            "1733/3000 train_loss: 38.77347946166992 test_loss:106.49445343017578\n",
            "1734/3000 train_loss: 53.6031494140625 test_loss:101.5169448852539\n",
            "1735/3000 train_loss: 43.52851867675781 test_loss:110.2209243774414\n",
            "1736/3000 train_loss: 38.470367431640625 test_loss:107.02091217041016\n",
            "1737/3000 train_loss: 45.611507415771484 test_loss:115.92620086669922\n",
            "1738/3000 train_loss: 37.65883255004883 test_loss:102.84100341796875\n",
            "1739/3000 train_loss: 44.410858154296875 test_loss:115.29549407958984\n",
            "1740/3000 train_loss: 46.10483169555664 test_loss:108.81178283691406\n",
            "1741/3000 train_loss: 44.289794921875 test_loss:109.09126281738281\n",
            "1742/3000 train_loss: 39.931880950927734 test_loss:109.95763397216797\n",
            "1743/3000 train_loss: 39.17231369018555 test_loss:103.07250213623047\n",
            "1744/3000 train_loss: 36.838096618652344 test_loss:105.89070129394531\n",
            "1745/3000 train_loss: 37.064239501953125 test_loss:112.82952880859375\n",
            "1746/3000 train_loss: 39.004703521728516 test_loss:104.76763153076172\n",
            "1747/3000 train_loss: 43.670413970947266 test_loss:108.67220306396484\n",
            "1748/3000 train_loss: 46.7778434753418 test_loss:110.36701965332031\n",
            "1749/3000 train_loss: 43.9428596496582 test_loss:103.86949157714844\n",
            "1750/3000 train_loss: 46.812660217285156 test_loss:111.31829071044922\n",
            "1751/3000 train_loss: 39.994842529296875 test_loss:118.30384826660156\n",
            "1752/3000 train_loss: 36.47740936279297 test_loss:104.33055114746094\n",
            "1753/3000 train_loss: 40.680538177490234 test_loss:105.46207427978516\n",
            "1754/3000 train_loss: 40.60573196411133 test_loss:115.77777099609375\n",
            "1755/3000 train_loss: 37.08987808227539 test_loss:109.49320983886719\n",
            "1756/3000 train_loss: 41.69615173339844 test_loss:98.87226104736328\n",
            "1757/3000 train_loss: 38.44630432128906 test_loss:116.39817810058594\n",
            "1758/3000 train_loss: 37.94918441772461 test_loss:111.01473999023438\n",
            "1759/3000 train_loss: 46.123775482177734 test_loss:110.65438842773438\n",
            "1760/3000 train_loss: 37.60193634033203 test_loss:108.58228302001953\n",
            "1761/3000 train_loss: 38.660865783691406 test_loss:120.45088195800781\n",
            "1762/3000 train_loss: 38.66365432739258 test_loss:109.92847442626953\n",
            "1763/3000 train_loss: 42.73381423950195 test_loss:114.29629516601562\n",
            "1764/3000 train_loss: 49.30546569824219 test_loss:112.4806137084961\n",
            "1765/3000 train_loss: 45.71805191040039 test_loss:103.82941436767578\n",
            "1766/3000 train_loss: 39.9051628112793 test_loss:106.91736602783203\n",
            "1767/3000 train_loss: 39.959259033203125 test_loss:116.72218322753906\n",
            "1768/3000 train_loss: 37.06016540527344 test_loss:103.99783325195312\n",
            "1769/3000 train_loss: 41.64395523071289 test_loss:108.47169494628906\n",
            "1770/3000 train_loss: 40.717323303222656 test_loss:102.21155548095703\n",
            "1771/3000 train_loss: 43.639339447021484 test_loss:107.28045654296875\n",
            "1772/3000 train_loss: 43.69601821899414 test_loss:97.98275756835938\n",
            "1773/3000 train_loss: 39.94470977783203 test_loss:106.21226501464844\n",
            "1774/3000 train_loss: 39.058406829833984 test_loss:110.34634399414062\n",
            "1775/3000 train_loss: 39.19929122924805 test_loss:97.044677734375\n",
            "1776/3000 train_loss: 39.1906852722168 test_loss:95.74219512939453\n",
            "1777/3000 train_loss: 40.38189697265625 test_loss:114.37237548828125\n",
            "1778/3000 train_loss: 39.775028228759766 test_loss:100.40021514892578\n",
            "1779/3000 train_loss: 43.14149475097656 test_loss:111.88118743896484\n",
            "1780/3000 train_loss: 43.357452392578125 test_loss:107.94961547851562\n",
            "1781/3000 train_loss: 39.50996398925781 test_loss:105.26307678222656\n",
            "1782/3000 train_loss: 40.36628723144531 test_loss:108.385986328125\n",
            "1783/3000 train_loss: 37.22164535522461 test_loss:102.83554077148438\n",
            "1784/3000 train_loss: 39.00389862060547 test_loss:103.35344696044922\n",
            "1785/3000 train_loss: 48.468597412109375 test_loss:104.47051239013672\n",
            "1786/3000 train_loss: 46.222251892089844 test_loss:101.43455505371094\n",
            "1787/3000 train_loss: 45.763919830322266 test_loss:99.31848907470703\n",
            "1788/3000 train_loss: 38.95016860961914 test_loss:100.43397521972656\n",
            "1789/3000 train_loss: 47.493370056152344 test_loss:113.84748840332031\n",
            "1790/3000 train_loss: 43.53260040283203 test_loss:95.65444946289062\n",
            "1791/3000 train_loss: 43.522342681884766 test_loss:103.85296630859375\n",
            "1792/3000 train_loss: 42.02717208862305 test_loss:114.30795288085938\n",
            "1793/3000 train_loss: 42.489830017089844 test_loss:114.46284484863281\n",
            "1794/3000 train_loss: 48.45386505126953 test_loss:100.44414520263672\n",
            "1795/3000 train_loss: 39.74247741699219 test_loss:118.41056823730469\n",
            "1796/3000 train_loss: 41.07672882080078 test_loss:109.0372543334961\n",
            "1797/3000 train_loss: 37.16156768798828 test_loss:103.5924072265625\n",
            "1798/3000 train_loss: 37.29758834838867 test_loss:110.39879608154297\n",
            "1799/3000 train_loss: 36.79986572265625 test_loss:106.56979370117188\n",
            "1800/3000 train_loss: 45.31444549560547 test_loss:107.1142578125\n",
            "1801/3000 train_loss: 41.21388626098633 test_loss:124.90158081054688\n",
            "1802/3000 train_loss: 40.93421936035156 test_loss:100.63359069824219\n",
            "1803/3000 train_loss: 38.775516510009766 test_loss:108.43135070800781\n",
            "1804/3000 train_loss: 37.31035232543945 test_loss:104.70823669433594\n",
            "1805/3000 train_loss: 46.78434753417969 test_loss:105.70423889160156\n",
            "1806/3000 train_loss: 46.53277587890625 test_loss:106.19098663330078\n",
            "1807/3000 train_loss: 45.1920051574707 test_loss:114.80365753173828\n",
            "1808/3000 train_loss: 40.90275192260742 test_loss:103.1081314086914\n",
            "1809/3000 train_loss: 38.64970397949219 test_loss:115.37223052978516\n",
            "1810/3000 train_loss: 36.860679626464844 test_loss:105.50818634033203\n",
            "1811/3000 train_loss: 36.84381103515625 test_loss:113.9796142578125\n",
            "1812/3000 train_loss: 41.11075973510742 test_loss:101.52019500732422\n",
            "1813/3000 train_loss: 38.32152557373047 test_loss:110.25992584228516\n",
            "1814/3000 train_loss: 40.65386962890625 test_loss:103.10506439208984\n",
            "1815/3000 train_loss: 42.63150405883789 test_loss:119.43463897705078\n",
            "1816/3000 train_loss: 40.82547378540039 test_loss:111.67951202392578\n",
            "1817/3000 train_loss: 35.64452362060547 test_loss:107.01326751708984\n",
            "1818/3000 train_loss: 36.75973129272461 test_loss:108.40994262695312\n",
            "1819/3000 train_loss: 37.734249114990234 test_loss:106.21572875976562\n",
            "1820/3000 train_loss: 40.517677307128906 test_loss:112.1847915649414\n",
            "1821/3000 train_loss: 38.78532028198242 test_loss:109.03714752197266\n",
            "1822/3000 train_loss: 43.10896301269531 test_loss:100.60791778564453\n",
            "1823/3000 train_loss: 39.49970245361328 test_loss:98.6788330078125\n",
            "1824/3000 train_loss: 37.971954345703125 test_loss:102.96604919433594\n",
            "1825/3000 train_loss: 40.81893539428711 test_loss:98.85723876953125\n",
            "1826/3000 train_loss: 36.84745407104492 test_loss:98.35075378417969\n",
            "1827/3000 train_loss: 33.00172805786133 test_loss:106.7873764038086\n",
            "1828/3000 train_loss: 38.409603118896484 test_loss:99.93228149414062\n",
            "1829/3000 train_loss: 37.150848388671875 test_loss:103.74400329589844\n",
            "1830/3000 train_loss: 37.319313049316406 test_loss:102.59649658203125\n",
            "1831/3000 train_loss: 33.49515151977539 test_loss:115.32781982421875\n",
            "1832/3000 train_loss: 44.454566955566406 test_loss:103.44292449951172\n",
            "1833/3000 train_loss: 40.80016326904297 test_loss:103.723388671875\n",
            "1834/3000 train_loss: 43.11158752441406 test_loss:109.5067367553711\n",
            "1835/3000 train_loss: 44.3816032409668 test_loss:105.98412322998047\n",
            "1836/3000 train_loss: 38.263221740722656 test_loss:102.36331176757812\n",
            "1837/3000 train_loss: 41.55965042114258 test_loss:93.95108795166016\n",
            "1838/3000 train_loss: 41.85133743286133 test_loss:113.31451416015625\n",
            "1839/3000 train_loss: 40.47724151611328 test_loss:106.52997589111328\n",
            "1840/3000 train_loss: 42.56532287597656 test_loss:104.30838775634766\n",
            "1841/3000 train_loss: 43.81334686279297 test_loss:100.9359359741211\n",
            "1842/3000 train_loss: 43.1420783996582 test_loss:115.96614074707031\n",
            "1843/3000 train_loss: 38.63101577758789 test_loss:105.47771453857422\n",
            "1844/3000 train_loss: 45.14307403564453 test_loss:113.73320007324219\n",
            "1845/3000 train_loss: 40.20631408691406 test_loss:114.01782989501953\n",
            "1846/3000 train_loss: 38.025421142578125 test_loss:106.67801666259766\n",
            "1847/3000 train_loss: 36.328697204589844 test_loss:105.43643188476562\n",
            "1848/3000 train_loss: 44.95062255859375 test_loss:106.3822250366211\n",
            "1849/3000 train_loss: 41.03493881225586 test_loss:102.1022720336914\n",
            "1850/3000 train_loss: 43.64112091064453 test_loss:106.00968170166016\n",
            "1851/3000 train_loss: 41.989017486572266 test_loss:108.30402374267578\n",
            "1852/3000 train_loss: 43.28692626953125 test_loss:112.89983367919922\n",
            "1853/3000 train_loss: 48.118080139160156 test_loss:109.9348373413086\n",
            "1854/3000 train_loss: 36.43849563598633 test_loss:107.98713684082031\n",
            "1855/3000 train_loss: 37.79765701293945 test_loss:102.7585678100586\n",
            "1856/3000 train_loss: 41.60514450073242 test_loss:107.9749526977539\n",
            "1857/3000 train_loss: 39.511802673339844 test_loss:104.15808868408203\n",
            "1858/3000 train_loss: 37.95866775512695 test_loss:101.53804016113281\n",
            "1859/3000 train_loss: 37.45914840698242 test_loss:101.38166809082031\n",
            "1860/3000 train_loss: 37.167076110839844 test_loss:110.9757308959961\n",
            "1861/3000 train_loss: 37.06514358520508 test_loss:102.72421264648438\n",
            "1862/3000 train_loss: 40.5362663269043 test_loss:107.2496566772461\n",
            "1863/3000 train_loss: 38.34678649902344 test_loss:111.54774475097656\n",
            "1864/3000 train_loss: 40.14879608154297 test_loss:120.68363189697266\n",
            "1865/3000 train_loss: 46.67264175415039 test_loss:111.0873794555664\n",
            "1866/3000 train_loss: 37.321773529052734 test_loss:99.36077117919922\n",
            "1867/3000 train_loss: 33.26191711425781 test_loss:104.42073059082031\n",
            "1868/3000 train_loss: 46.77040100097656 test_loss:99.1196517944336\n",
            "1869/3000 train_loss: 41.1322135925293 test_loss:105.17621612548828\n",
            "1870/3000 train_loss: 35.85350036621094 test_loss:106.5306625366211\n",
            "1871/3000 train_loss: 37.748268127441406 test_loss:116.25395202636719\n",
            "1872/3000 train_loss: 37.66305923461914 test_loss:101.57889556884766\n",
            "1873/3000 train_loss: 46.88772201538086 test_loss:103.35616302490234\n",
            "1874/3000 train_loss: 35.27425765991211 test_loss:109.0567398071289\n",
            "1875/3000 train_loss: 40.12854766845703 test_loss:107.84612274169922\n",
            "1876/3000 train_loss: 43.60689163208008 test_loss:106.27867889404297\n",
            "1877/3000 train_loss: 45.57417297363281 test_loss:112.96937561035156\n",
            "1878/3000 train_loss: 51.22378921508789 test_loss:100.1168441772461\n",
            "1879/3000 train_loss: 33.921504974365234 test_loss:95.42472076416016\n",
            "1880/3000 train_loss: 35.85209274291992 test_loss:114.23953247070312\n",
            "1881/3000 train_loss: 34.95496368408203 test_loss:96.60694122314453\n",
            "1882/3000 train_loss: 39.9282341003418 test_loss:111.49249267578125\n",
            "1883/3000 train_loss: 38.900718688964844 test_loss:102.7402572631836\n",
            "1884/3000 train_loss: 34.992286682128906 test_loss:101.47526550292969\n",
            "1885/3000 train_loss: 48.652183532714844 test_loss:94.66896057128906\n",
            "1886/3000 train_loss: 41.48466110229492 test_loss:111.32933044433594\n",
            "1887/3000 train_loss: 35.65424346923828 test_loss:99.96806335449219\n",
            "1888/3000 train_loss: 35.86555480957031 test_loss:97.47968292236328\n",
            "1889/3000 train_loss: 36.9510498046875 test_loss:106.47220611572266\n",
            "1890/3000 train_loss: 35.064735412597656 test_loss:96.99515533447266\n",
            "1891/3000 train_loss: 43.18126678466797 test_loss:101.99893951416016\n",
            "1892/3000 train_loss: 36.44379425048828 test_loss:103.14189147949219\n",
            "1893/3000 train_loss: 40.59212112426758 test_loss:98.46267700195312\n",
            "1894/3000 train_loss: 38.798980712890625 test_loss:107.94725036621094\n",
            "1895/3000 train_loss: 36.10757827758789 test_loss:109.80856323242188\n",
            "1896/3000 train_loss: 41.523983001708984 test_loss:103.27169799804688\n",
            "1897/3000 train_loss: 34.80083465576172 test_loss:94.37539672851562\n",
            "1898/3000 train_loss: 37.84532928466797 test_loss:102.64204406738281\n",
            "1899/3000 train_loss: 36.52720642089844 test_loss:103.06126403808594\n",
            "1900/3000 train_loss: 40.931251525878906 test_loss:98.0524673461914\n",
            "1901/3000 train_loss: 38.551090240478516 test_loss:110.48064422607422\n",
            "1902/3000 train_loss: 39.12702178955078 test_loss:106.84770202636719\n",
            "1903/3000 train_loss: 50.03995895385742 test_loss:98.52311706542969\n",
            "1904/3000 train_loss: 35.8576774597168 test_loss:108.16338348388672\n",
            "1905/3000 train_loss: 36.775657653808594 test_loss:103.33500671386719\n",
            "1906/3000 train_loss: 38.916595458984375 test_loss:115.7551498413086\n",
            "1907/3000 train_loss: 39.57676315307617 test_loss:107.50381469726562\n",
            "1908/3000 train_loss: 43.3572998046875 test_loss:97.889892578125\n",
            "1909/3000 train_loss: 41.359039306640625 test_loss:105.13172149658203\n",
            "1910/3000 train_loss: 40.2180290222168 test_loss:111.33395385742188\n",
            "1911/3000 train_loss: 32.694644927978516 test_loss:99.02930450439453\n",
            "1912/3000 train_loss: 36.30991744995117 test_loss:101.57543182373047\n",
            "1913/3000 train_loss: 43.591670989990234 test_loss:101.52379608154297\n",
            "1914/3000 train_loss: 35.837890625 test_loss:99.96102142333984\n",
            "1915/3000 train_loss: 38.45488739013672 test_loss:105.57302856445312\n",
            "1916/3000 train_loss: 37.083038330078125 test_loss:102.23604583740234\n",
            "1917/3000 train_loss: 35.18170166015625 test_loss:109.1666030883789\n",
            "1918/3000 train_loss: 46.53010177612305 test_loss:107.1533203125\n",
            "1919/3000 train_loss: 35.875938415527344 test_loss:103.82311248779297\n",
            "1920/3000 train_loss: 48.55534362792969 test_loss:117.1182861328125\n",
            "1921/3000 train_loss: 42.572139739990234 test_loss:106.97160339355469\n",
            "1922/3000 train_loss: 44.95842742919922 test_loss:108.63736724853516\n",
            "1923/3000 train_loss: 39.9742546081543 test_loss:105.70539855957031\n",
            "1924/3000 train_loss: 40.98696517944336 test_loss:105.42134857177734\n",
            "1925/3000 train_loss: 34.826263427734375 test_loss:105.24749755859375\n",
            "1926/3000 train_loss: 34.44650650024414 test_loss:113.69904327392578\n",
            "1927/3000 train_loss: 43.680843353271484 test_loss:99.26897430419922\n",
            "1928/3000 train_loss: 38.64017868041992 test_loss:105.96671295166016\n",
            "1929/3000 train_loss: 42.02654266357422 test_loss:108.95616912841797\n",
            "1930/3000 train_loss: 50.77395248413086 test_loss:120.83828735351562\n",
            "1931/3000 train_loss: 44.880245208740234 test_loss:107.88754272460938\n",
            "1932/3000 train_loss: 34.495662689208984 test_loss:106.94310760498047\n",
            "1933/3000 train_loss: 44.511985778808594 test_loss:106.84367370605469\n",
            "1934/3000 train_loss: 41.55224609375 test_loss:111.4842300415039\n",
            "1935/3000 train_loss: 38.82073974609375 test_loss:107.77083587646484\n",
            "1936/3000 train_loss: 34.75056076049805 test_loss:105.39057159423828\n",
            "1937/3000 train_loss: 35.435752868652344 test_loss:117.06098937988281\n",
            "1938/3000 train_loss: 40.53611373901367 test_loss:100.91537475585938\n",
            "1939/3000 train_loss: 45.02408981323242 test_loss:111.48822021484375\n",
            "1940/3000 train_loss: 42.556297302246094 test_loss:113.26012420654297\n",
            "1941/3000 train_loss: 46.86807632446289 test_loss:101.4239501953125\n",
            "1942/3000 train_loss: 41.89260482788086 test_loss:127.32210540771484\n",
            "1943/3000 train_loss: 47.99417495727539 test_loss:118.23511505126953\n",
            "1944/3000 train_loss: 38.12302780151367 test_loss:100.01173400878906\n",
            "1945/3000 train_loss: 38.883079528808594 test_loss:117.56259155273438\n",
            "1946/3000 train_loss: 38.218589782714844 test_loss:107.39530181884766\n",
            "1947/3000 train_loss: 38.6361198425293 test_loss:105.93267822265625\n",
            "1948/3000 train_loss: 43.59918975830078 test_loss:109.88265228271484\n",
            "1949/3000 train_loss: 41.296104431152344 test_loss:97.10694885253906\n",
            "1950/3000 train_loss: 40.75249481201172 test_loss:109.70569610595703\n",
            "1951/3000 train_loss: 38.33538055419922 test_loss:102.63626098632812\n",
            "1952/3000 train_loss: 45.34402084350586 test_loss:101.98619842529297\n",
            "1953/3000 train_loss: 35.01760482788086 test_loss:99.73822021484375\n",
            "1954/3000 train_loss: 40.6663818359375 test_loss:99.10043334960938\n",
            "1955/3000 train_loss: 34.590476989746094 test_loss:99.27470397949219\n",
            "1956/3000 train_loss: 34.548797607421875 test_loss:96.99836730957031\n",
            "1957/3000 train_loss: 33.824607849121094 test_loss:100.02558898925781\n",
            "1958/3000 train_loss: 37.101375579833984 test_loss:106.97134399414062\n",
            "1959/3000 train_loss: 44.24995803833008 test_loss:93.68778991699219\n",
            "1960/3000 train_loss: 39.249759674072266 test_loss:130.75759887695312\n",
            "1961/3000 train_loss: 40.54313659667969 test_loss:106.90385437011719\n",
            "1962/3000 train_loss: 33.48410415649414 test_loss:100.69450378417969\n",
            "1963/3000 train_loss: 37.89448928833008 test_loss:110.30194854736328\n",
            "1964/3000 train_loss: 36.018253326416016 test_loss:105.35037231445312\n",
            "1965/3000 train_loss: 33.96321487426758 test_loss:104.53404235839844\n",
            "1966/3000 train_loss: 30.125871658325195 test_loss:108.61946105957031\n",
            "1967/3000 train_loss: 35.00007629394531 test_loss:108.34557342529297\n",
            "1968/3000 train_loss: 36.592857360839844 test_loss:104.93356323242188\n",
            "1969/3000 train_loss: 35.42794418334961 test_loss:93.94290924072266\n",
            "1970/3000 train_loss: 37.038909912109375 test_loss:102.32318115234375\n",
            "1971/3000 train_loss: 36.409645080566406 test_loss:105.5290298461914\n",
            "1972/3000 train_loss: 36.43798828125 test_loss:98.28079986572266\n",
            "1973/3000 train_loss: 35.39804458618164 test_loss:94.83934783935547\n",
            "1974/3000 train_loss: 39.423954010009766 test_loss:106.07846069335938\n",
            "1975/3000 train_loss: 34.67314147949219 test_loss:97.9124755859375\n",
            "1976/3000 train_loss: 42.679405212402344 test_loss:105.14031982421875\n",
            "1977/3000 train_loss: 34.07571029663086 test_loss:95.8232421875\n",
            "1978/3000 train_loss: 37.62746047973633 test_loss:101.5279312133789\n",
            "1979/3000 train_loss: 34.957244873046875 test_loss:94.04414367675781\n",
            "1980/3000 train_loss: 41.32670211791992 test_loss:99.15829467773438\n",
            "1981/3000 train_loss: 36.624298095703125 test_loss:102.18573760986328\n",
            "1982/3000 train_loss: 44.555946350097656 test_loss:96.91064453125\n",
            "1983/3000 train_loss: 33.03891372680664 test_loss:96.68148040771484\n",
            "1984/3000 train_loss: 32.80040740966797 test_loss:92.76155853271484\n",
            "1985/3000 train_loss: 35.79350280761719 test_loss:101.31379699707031\n",
            "1986/3000 train_loss: 36.547977447509766 test_loss:90.7492446899414\n",
            "1987/3000 train_loss: 33.98798751831055 test_loss:103.22071075439453\n",
            "1988/3000 train_loss: 38.798667907714844 test_loss:92.6214828491211\n",
            "1989/3000 train_loss: 36.87413024902344 test_loss:101.44108581542969\n",
            "1990/3000 train_loss: 32.14195251464844 test_loss:96.83772277832031\n",
            "1991/3000 train_loss: 38.42891311645508 test_loss:105.81002044677734\n",
            "1992/3000 train_loss: 40.17521667480469 test_loss:101.48169708251953\n",
            "1993/3000 train_loss: 32.17107391357422 test_loss:94.33284759521484\n",
            "1994/3000 train_loss: 35.80144119262695 test_loss:101.01664733886719\n",
            "1995/3000 train_loss: 34.782691955566406 test_loss:99.24858856201172\n",
            "1996/3000 train_loss: 40.386722564697266 test_loss:94.58406829833984\n",
            "1997/3000 train_loss: 36.74616622924805 test_loss:99.23342895507812\n",
            "1998/3000 train_loss: 35.366493225097656 test_loss:97.85564422607422\n",
            "1999/3000 train_loss: 37.58576202392578 test_loss:99.79972839355469\n",
            "2000/3000 train_loss: 43.82213592529297 test_loss:98.35562133789062\n",
            "2001/3000 train_loss: 35.9087028503418 test_loss:97.90540313720703\n",
            "2002/3000 train_loss: 38.821128845214844 test_loss:107.60635375976562\n",
            "2003/3000 train_loss: 38.54899978637695 test_loss:94.83274841308594\n",
            "2004/3000 train_loss: 38.11831283569336 test_loss:97.79072570800781\n",
            "2005/3000 train_loss: 37.2379150390625 test_loss:101.41017150878906\n",
            "2006/3000 train_loss: 34.53260803222656 test_loss:103.4051742553711\n",
            "2007/3000 train_loss: 33.43539047241211 test_loss:99.35584259033203\n",
            "2008/3000 train_loss: 36.87611389160156 test_loss:102.88723754882812\n",
            "2009/3000 train_loss: 32.732208251953125 test_loss:100.26471710205078\n",
            "2010/3000 train_loss: 37.83296585083008 test_loss:102.1283187866211\n",
            "2011/3000 train_loss: 32.994911193847656 test_loss:105.4024658203125\n",
            "2012/3000 train_loss: 40.397705078125 test_loss:93.15379333496094\n",
            "2013/3000 train_loss: 31.242876052856445 test_loss:107.38478088378906\n",
            "2014/3000 train_loss: 38.048492431640625 test_loss:102.91717529296875\n",
            "2015/3000 train_loss: 38.069766998291016 test_loss:92.20211029052734\n",
            "2016/3000 train_loss: 41.3885498046875 test_loss:109.47607421875\n",
            "2017/3000 train_loss: 40.45341110229492 test_loss:105.82395935058594\n",
            "2018/3000 train_loss: 34.571659088134766 test_loss:98.30955505371094\n",
            "2019/3000 train_loss: 33.62942886352539 test_loss:105.73664855957031\n",
            "2020/3000 train_loss: 34.50566101074219 test_loss:93.40580749511719\n",
            "2021/3000 train_loss: 35.925392150878906 test_loss:104.87430572509766\n",
            "2022/3000 train_loss: 33.0867805480957 test_loss:104.24848937988281\n",
            "2023/3000 train_loss: 39.48011016845703 test_loss:92.5728530883789\n",
            "2024/3000 train_loss: 41.98059844970703 test_loss:101.34119415283203\n",
            "2025/3000 train_loss: 42.05203628540039 test_loss:90.5201187133789\n",
            "2026/3000 train_loss: 33.650821685791016 test_loss:88.09074401855469\n",
            "2027/3000 train_loss: 34.03739929199219 test_loss:101.48444366455078\n",
            "2028/3000 train_loss: 34.47761535644531 test_loss:91.02493286132812\n",
            "2029/3000 train_loss: 30.833925247192383 test_loss:95.40235900878906\n",
            "2030/3000 train_loss: 35.36444091796875 test_loss:98.13077545166016\n",
            "2031/3000 train_loss: 34.347747802734375 test_loss:96.74500274658203\n",
            "2032/3000 train_loss: 36.6063346862793 test_loss:101.66938781738281\n",
            "2033/3000 train_loss: 32.56271743774414 test_loss:93.7643814086914\n",
            "2034/3000 train_loss: 36.7496337890625 test_loss:97.30277252197266\n",
            "2035/3000 train_loss: 34.6639289855957 test_loss:96.3660659790039\n",
            "2036/3000 train_loss: 43.396881103515625 test_loss:97.50851440429688\n",
            "2037/3000 train_loss: 38.67839050292969 test_loss:100.88684844970703\n",
            "2038/3000 train_loss: 32.342594146728516 test_loss:99.39851379394531\n",
            "2039/3000 train_loss: 35.583133697509766 test_loss:102.38468170166016\n",
            "2040/3000 train_loss: 37.2789192199707 test_loss:99.28724670410156\n",
            "2041/3000 train_loss: 34.86534118652344 test_loss:96.18376922607422\n",
            "2042/3000 train_loss: 37.97461700439453 test_loss:112.12301635742188\n",
            "2043/3000 train_loss: 30.29029083251953 test_loss:91.04985809326172\n",
            "2044/3000 train_loss: 39.373661041259766 test_loss:107.03167724609375\n",
            "2045/3000 train_loss: 30.362762451171875 test_loss:95.42356872558594\n",
            "2046/3000 train_loss: 29.789033889770508 test_loss:96.12967681884766\n",
            "2047/3000 train_loss: 35.80520248413086 test_loss:95.99261474609375\n",
            "2048/3000 train_loss: 42.702552795410156 test_loss:92.26846313476562\n",
            "2049/3000 train_loss: 34.08887481689453 test_loss:97.57395935058594\n",
            "2050/3000 train_loss: 36.8559455871582 test_loss:91.6318130493164\n",
            "2051/3000 train_loss: 41.26655578613281 test_loss:97.81962585449219\n",
            "2052/3000 train_loss: 40.76969528198242 test_loss:103.45671844482422\n",
            "2053/3000 train_loss: 36.51811599731445 test_loss:99.94418334960938\n",
            "2054/3000 train_loss: 31.651153564453125 test_loss:97.1986083984375\n",
            "2055/3000 train_loss: 34.48576736450195 test_loss:104.96736907958984\n",
            "2056/3000 train_loss: 35.98419952392578 test_loss:99.85563659667969\n",
            "2057/3000 train_loss: 34.705135345458984 test_loss:97.65044403076172\n",
            "2058/3000 train_loss: 32.77018356323242 test_loss:96.22406768798828\n",
            "2059/3000 train_loss: 38.336368560791016 test_loss:96.0193862915039\n",
            "2060/3000 train_loss: 39.16996765136719 test_loss:89.71900177001953\n",
            "2061/3000 train_loss: 41.92976760864258 test_loss:124.22900390625\n",
            "2062/3000 train_loss: 36.304168701171875 test_loss:97.41241455078125\n",
            "2063/3000 train_loss: 33.63964080810547 test_loss:97.05825805664062\n",
            "2064/3000 train_loss: 32.72471618652344 test_loss:94.40504455566406\n",
            "2065/3000 train_loss: 36.52274703979492 test_loss:98.82270812988281\n",
            "2066/3000 train_loss: 45.25054931640625 test_loss:95.25849914550781\n",
            "2067/3000 train_loss: 35.32645034790039 test_loss:92.70609283447266\n",
            "2068/3000 train_loss: 32.66622543334961 test_loss:102.50764465332031\n",
            "2069/3000 train_loss: 34.71336364746094 test_loss:91.95266723632812\n",
            "2070/3000 train_loss: 37.9815559387207 test_loss:98.53890991210938\n",
            "2071/3000 train_loss: 31.83267593383789 test_loss:93.79103088378906\n",
            "2072/3000 train_loss: 28.86461067199707 test_loss:95.53093719482422\n",
            "2073/3000 train_loss: 36.996150970458984 test_loss:91.82738494873047\n",
            "2074/3000 train_loss: 33.668975830078125 test_loss:92.25748443603516\n",
            "2075/3000 train_loss: 32.67949295043945 test_loss:92.45729064941406\n",
            "2076/3000 train_loss: 37.49845886230469 test_loss:94.8085708618164\n",
            "2077/3000 train_loss: 35.858558654785156 test_loss:100.67282104492188\n",
            "2078/3000 train_loss: 30.628023147583008 test_loss:100.5978775024414\n",
            "2079/3000 train_loss: 36.42076873779297 test_loss:97.5589370727539\n",
            "2080/3000 train_loss: 32.6847038269043 test_loss:97.76519012451172\n",
            "2081/3000 train_loss: 36.396484375 test_loss:104.20758819580078\n",
            "2082/3000 train_loss: 36.499855041503906 test_loss:103.36040496826172\n",
            "2083/3000 train_loss: 37.77531433105469 test_loss:95.5280990600586\n",
            "2084/3000 train_loss: 31.07421112060547 test_loss:108.15911865234375\n",
            "2085/3000 train_loss: 37.373191833496094 test_loss:93.4641342163086\n",
            "2086/3000 train_loss: 35.4837532043457 test_loss:99.73783874511719\n",
            "2087/3000 train_loss: 33.69037628173828 test_loss:107.63055419921875\n",
            "2088/3000 train_loss: 35.8056526184082 test_loss:96.348388671875\n",
            "2089/3000 train_loss: 38.93206024169922 test_loss:116.9696273803711\n",
            "2090/3000 train_loss: 34.76716995239258 test_loss:104.27044677734375\n",
            "2091/3000 train_loss: 31.509239196777344 test_loss:92.78963470458984\n",
            "2092/3000 train_loss: 33.10188674926758 test_loss:112.90086364746094\n",
            "2093/3000 train_loss: 32.35763168334961 test_loss:98.95001220703125\n",
            "2094/3000 train_loss: 34.59333801269531 test_loss:105.63462829589844\n",
            "2095/3000 train_loss: 31.181921005249023 test_loss:102.93212890625\n",
            "2096/3000 train_loss: 41.48851776123047 test_loss:103.46253204345703\n",
            "2097/3000 train_loss: 42.21803665161133 test_loss:112.68336486816406\n",
            "2098/3000 train_loss: 33.690399169921875 test_loss:101.98177337646484\n",
            "2099/3000 train_loss: 38.10655975341797 test_loss:101.08726501464844\n",
            "2100/3000 train_loss: 34.18595504760742 test_loss:106.23332214355469\n",
            "2101/3000 train_loss: 39.40840530395508 test_loss:106.72676086425781\n",
            "2102/3000 train_loss: 34.46518325805664 test_loss:96.79219055175781\n",
            "2103/3000 train_loss: 35.31840896606445 test_loss:91.51033782958984\n",
            "2104/3000 train_loss: 43.45443344116211 test_loss:96.70220947265625\n",
            "2105/3000 train_loss: 36.19131088256836 test_loss:94.18989562988281\n",
            "2106/3000 train_loss: 39.716041564941406 test_loss:92.53594207763672\n",
            "2107/3000 train_loss: 35.46507263183594 test_loss:100.17568969726562\n",
            "2108/3000 train_loss: 34.01713180541992 test_loss:96.11136627197266\n",
            "2109/3000 train_loss: 31.473007202148438 test_loss:102.08018493652344\n",
            "2110/3000 train_loss: 31.370853424072266 test_loss:92.4831314086914\n",
            "2111/3000 train_loss: 36.73577117919922 test_loss:99.27420806884766\n",
            "2112/3000 train_loss: 31.18434715270996 test_loss:97.23649597167969\n",
            "2113/3000 train_loss: 31.312633514404297 test_loss:93.93771362304688\n",
            "2114/3000 train_loss: 36.0216064453125 test_loss:105.77584838867188\n",
            "2115/3000 train_loss: 33.25997543334961 test_loss:91.7825698852539\n",
            "2116/3000 train_loss: 29.382761001586914 test_loss:98.93338775634766\n",
            "2117/3000 train_loss: 33.93608474731445 test_loss:96.92613220214844\n",
            "2118/3000 train_loss: 34.22496795654297 test_loss:91.91737365722656\n",
            "2119/3000 train_loss: 32.31074142456055 test_loss:99.19258117675781\n",
            "2120/3000 train_loss: 40.602378845214844 test_loss:103.58685302734375\n",
            "2121/3000 train_loss: 50.309471130371094 test_loss:100.53949737548828\n",
            "2122/3000 train_loss: 39.203697204589844 test_loss:99.1542739868164\n",
            "2123/3000 train_loss: 31.462541580200195 test_loss:95.85397338867188\n",
            "2124/3000 train_loss: 35.79098129272461 test_loss:102.54031372070312\n",
            "2125/3000 train_loss: 32.84149932861328 test_loss:96.5680160522461\n",
            "2126/3000 train_loss: 29.804758071899414 test_loss:97.12403106689453\n",
            "2127/3000 train_loss: 37.872982025146484 test_loss:101.76097106933594\n",
            "2128/3000 train_loss: 27.319599151611328 test_loss:98.31037902832031\n",
            "2129/3000 train_loss: 33.617042541503906 test_loss:113.18170166015625\n",
            "2130/3000 train_loss: 35.21766662597656 test_loss:100.10840606689453\n",
            "2131/3000 train_loss: 34.35467529296875 test_loss:98.16670227050781\n",
            "2132/3000 train_loss: 36.35429763793945 test_loss:93.49655151367188\n",
            "2133/3000 train_loss: 31.059246063232422 test_loss:92.71318054199219\n",
            "2134/3000 train_loss: 37.480224609375 test_loss:99.15013122558594\n",
            "2135/3000 train_loss: 30.83859634399414 test_loss:92.21524047851562\n",
            "2136/3000 train_loss: 33.095184326171875 test_loss:103.29705047607422\n",
            "2137/3000 train_loss: 34.173038482666016 test_loss:92.58233642578125\n",
            "2138/3000 train_loss: 30.266883850097656 test_loss:105.13749694824219\n",
            "2139/3000 train_loss: 34.134864807128906 test_loss:87.43437957763672\n",
            "2140/3000 train_loss: 34.89140701293945 test_loss:89.22742462158203\n",
            "2141/3000 train_loss: 39.83734893798828 test_loss:96.75315856933594\n",
            "2142/3000 train_loss: 33.182090759277344 test_loss:93.56915283203125\n",
            "2143/3000 train_loss: 34.19720458984375 test_loss:96.67839050292969\n",
            "2144/3000 train_loss: 32.66033935546875 test_loss:90.59404754638672\n",
            "2145/3000 train_loss: 33.806541442871094 test_loss:92.05281829833984\n",
            "2146/3000 train_loss: 37.29924774169922 test_loss:112.36817932128906\n",
            "2147/3000 train_loss: 34.93532180786133 test_loss:96.25303649902344\n",
            "2148/3000 train_loss: 30.98944091796875 test_loss:89.111328125\n",
            "2149/3000 train_loss: 33.77659606933594 test_loss:98.1922607421875\n",
            "2150/3000 train_loss: 37.507164001464844 test_loss:99.28915405273438\n",
            "2151/3000 train_loss: 35.84255599975586 test_loss:90.12169647216797\n",
            "2152/3000 train_loss: 39.20852279663086 test_loss:95.6513671875\n",
            "2153/3000 train_loss: 36.324737548828125 test_loss:105.07259368896484\n",
            "2154/3000 train_loss: 30.851701736450195 test_loss:87.7345199584961\n",
            "2155/3000 train_loss: 36.48479080200195 test_loss:91.848876953125\n",
            "2156/3000 train_loss: 31.453998565673828 test_loss:94.31114959716797\n",
            "2157/3000 train_loss: 38.2589111328125 test_loss:110.79222106933594\n",
            "2158/3000 train_loss: 36.30304718017578 test_loss:105.33773803710938\n",
            "2159/3000 train_loss: 32.77447509765625 test_loss:88.48973846435547\n",
            "2160/3000 train_loss: 35.18966293334961 test_loss:97.3806381225586\n",
            "2161/3000 train_loss: 30.93423080444336 test_loss:87.83797454833984\n",
            "2162/3000 train_loss: 31.805450439453125 test_loss:99.13098907470703\n",
            "2163/3000 train_loss: 36.6744270324707 test_loss:96.60099792480469\n",
            "2164/3000 train_loss: 27.464893341064453 test_loss:96.24449157714844\n",
            "2165/3000 train_loss: 32.89201736450195 test_loss:93.6441421508789\n",
            "2166/3000 train_loss: 36.6469841003418 test_loss:103.20792388916016\n",
            "2167/3000 train_loss: 37.34664535522461 test_loss:88.04859161376953\n",
            "2168/3000 train_loss: 33.74028778076172 test_loss:87.65716552734375\n",
            "2169/3000 train_loss: 31.884862899780273 test_loss:90.23124694824219\n",
            "2170/3000 train_loss: 29.952280044555664 test_loss:94.27059936523438\n",
            "2171/3000 train_loss: 30.702251434326172 test_loss:90.98773193359375\n",
            "2172/3000 train_loss: 36.315975189208984 test_loss:91.54722595214844\n",
            "2173/3000 train_loss: 39.590511322021484 test_loss:88.91999816894531\n",
            "2174/3000 train_loss: 41.76398468017578 test_loss:84.70295715332031\n",
            "2175/3000 train_loss: 40.253440856933594 test_loss:95.12322235107422\n",
            "2176/3000 train_loss: 39.63572692871094 test_loss:93.43635559082031\n",
            "2177/3000 train_loss: 31.403106689453125 test_loss:94.57401275634766\n",
            "2178/3000 train_loss: 28.366432189941406 test_loss:99.03089141845703\n",
            "2179/3000 train_loss: 38.76767349243164 test_loss:87.45529174804688\n",
            "2180/3000 train_loss: 31.633651733398438 test_loss:95.80079650878906\n",
            "2181/3000 train_loss: 37.26215362548828 test_loss:99.44788360595703\n",
            "2182/3000 train_loss: 36.4459228515625 test_loss:96.37376403808594\n",
            "2183/3000 train_loss: 31.867719650268555 test_loss:92.86390686035156\n",
            "2184/3000 train_loss: 27.556690216064453 test_loss:87.91309356689453\n",
            "2185/3000 train_loss: 27.709978103637695 test_loss:94.12094116210938\n",
            "2186/3000 train_loss: 30.975000381469727 test_loss:94.55892944335938\n",
            "2187/3000 train_loss: 29.201736450195312 test_loss:90.0398178100586\n",
            "2188/3000 train_loss: 32.106483459472656 test_loss:92.1884765625\n",
            "2189/3000 train_loss: 29.23933219909668 test_loss:97.37004089355469\n",
            "2190/3000 train_loss: 34.46638107299805 test_loss:87.38733673095703\n",
            "2191/3000 train_loss: 32.760433197021484 test_loss:99.09931945800781\n",
            "2192/3000 train_loss: 32.85150146484375 test_loss:90.53443145751953\n",
            "2193/3000 train_loss: 30.0600643157959 test_loss:95.80665588378906\n",
            "2194/3000 train_loss: 28.856847763061523 test_loss:87.13579559326172\n",
            "2195/3000 train_loss: 32.902095794677734 test_loss:97.4543685913086\n",
            "2196/3000 train_loss: 38.10383987426758 test_loss:87.73713684082031\n",
            "2197/3000 train_loss: 34.500179290771484 test_loss:92.65332794189453\n",
            "2198/3000 train_loss: 30.80540657043457 test_loss:101.1993179321289\n",
            "2199/3000 train_loss: 29.026906967163086 test_loss:95.10533905029297\n",
            "2200/3000 train_loss: 33.112205505371094 test_loss:101.79971313476562\n",
            "2201/3000 train_loss: 28.041378021240234 test_loss:90.90166473388672\n",
            "2202/3000 train_loss: 31.277481079101562 test_loss:99.9753646850586\n",
            "2203/3000 train_loss: 30.03857421875 test_loss:87.11021423339844\n",
            "2204/3000 train_loss: 37.389183044433594 test_loss:103.45979309082031\n",
            "2205/3000 train_loss: 33.29270553588867 test_loss:98.37425231933594\n",
            "2206/3000 train_loss: 40.63677215576172 test_loss:90.87889099121094\n",
            "2207/3000 train_loss: 32.733760833740234 test_loss:103.9017562866211\n",
            "2208/3000 train_loss: 32.804264068603516 test_loss:101.52043914794922\n",
            "2209/3000 train_loss: 34.159793853759766 test_loss:94.21194458007812\n",
            "2210/3000 train_loss: 27.828407287597656 test_loss:95.47667694091797\n",
            "2211/3000 train_loss: 27.668481826782227 test_loss:93.45597839355469\n",
            "2212/3000 train_loss: 34.21529006958008 test_loss:99.81175231933594\n",
            "2213/3000 train_loss: 33.82279968261719 test_loss:94.99324798583984\n",
            "2214/3000 train_loss: 28.658117294311523 test_loss:89.84730529785156\n",
            "2215/3000 train_loss: 29.31631851196289 test_loss:97.78862762451172\n",
            "2216/3000 train_loss: 25.00788116455078 test_loss:87.27747344970703\n",
            "2217/3000 train_loss: 33.21333312988281 test_loss:96.16584014892578\n",
            "2218/3000 train_loss: 37.51804733276367 test_loss:90.33081817626953\n",
            "2219/3000 train_loss: 28.78606605529785 test_loss:102.3792953491211\n",
            "2220/3000 train_loss: 27.95896339416504 test_loss:104.31084442138672\n",
            "2221/3000 train_loss: 34.044464111328125 test_loss:85.47465515136719\n",
            "2222/3000 train_loss: 39.15969467163086 test_loss:92.27427673339844\n",
            "2223/3000 train_loss: 40.21442794799805 test_loss:87.57540130615234\n",
            "2224/3000 train_loss: 39.094757080078125 test_loss:91.23397064208984\n",
            "2225/3000 train_loss: 42.76085662841797 test_loss:108.23927307128906\n",
            "2226/3000 train_loss: 34.69001388549805 test_loss:96.49560546875\n",
            "2227/3000 train_loss: 31.390438079833984 test_loss:93.83108520507812\n",
            "2228/3000 train_loss: 41.02531051635742 test_loss:97.22805786132812\n",
            "2229/3000 train_loss: 33.06746292114258 test_loss:86.75685119628906\n",
            "2230/3000 train_loss: 35.07149887084961 test_loss:119.7430191040039\n",
            "2231/3000 train_loss: 33.862735748291016 test_loss:91.69163513183594\n",
            "2232/3000 train_loss: 30.1330623626709 test_loss:89.57007598876953\n",
            "2233/3000 train_loss: 29.693836212158203 test_loss:107.16759490966797\n",
            "2234/3000 train_loss: 28.500728607177734 test_loss:93.45034790039062\n",
            "2235/3000 train_loss: 30.928165435791016 test_loss:93.84416961669922\n",
            "2236/3000 train_loss: 30.236488342285156 test_loss:92.90992736816406\n",
            "2237/3000 train_loss: 32.87534713745117 test_loss:96.31439208984375\n",
            "2238/3000 train_loss: 35.268531799316406 test_loss:81.95138549804688\n",
            "2239/3000 train_loss: 35.39760971069336 test_loss:85.52241516113281\n",
            "2240/3000 train_loss: 39.08291244506836 test_loss:105.91924285888672\n",
            "2241/3000 train_loss: 36.176849365234375 test_loss:86.16630554199219\n",
            "2242/3000 train_loss: 27.1446533203125 test_loss:92.45890045166016\n",
            "2243/3000 train_loss: 27.664892196655273 test_loss:101.383056640625\n",
            "2244/3000 train_loss: 31.4458065032959 test_loss:88.8929443359375\n",
            "2245/3000 train_loss: 29.88150978088379 test_loss:95.76286315917969\n",
            "2246/3000 train_loss: 38.323760986328125 test_loss:101.35661315917969\n",
            "2247/3000 train_loss: 29.5357608795166 test_loss:99.11180114746094\n",
            "2248/3000 train_loss: 32.1349983215332 test_loss:90.7454833984375\n",
            "2249/3000 train_loss: 31.678970336914062 test_loss:88.50968933105469\n",
            "2250/3000 train_loss: 31.776222229003906 test_loss:97.82316589355469\n",
            "2251/3000 train_loss: 30.252164840698242 test_loss:87.8911361694336\n",
            "2252/3000 train_loss: 31.652074813842773 test_loss:90.26429748535156\n",
            "2253/3000 train_loss: 32.504547119140625 test_loss:87.40339660644531\n",
            "2254/3000 train_loss: 30.394319534301758 test_loss:101.79422760009766\n",
            "2255/3000 train_loss: 39.35101318359375 test_loss:91.51773071289062\n",
            "2256/3000 train_loss: 38.348201751708984 test_loss:88.21878051757812\n",
            "2257/3000 train_loss: 35.759891510009766 test_loss:103.22193908691406\n",
            "2258/3000 train_loss: 32.97954559326172 test_loss:93.04025268554688\n",
            "2259/3000 train_loss: 27.80211639404297 test_loss:84.16207885742188\n",
            "2260/3000 train_loss: 32.522586822509766 test_loss:88.94548034667969\n",
            "2261/3000 train_loss: 34.78646469116211 test_loss:95.99118041992188\n",
            "2262/3000 train_loss: 28.733930587768555 test_loss:87.0798568725586\n",
            "2263/3000 train_loss: 32.688236236572266 test_loss:89.21387481689453\n",
            "2264/3000 train_loss: 26.94647216796875 test_loss:93.48161315917969\n",
            "2265/3000 train_loss: 27.221237182617188 test_loss:89.171875\n",
            "2266/3000 train_loss: 36.11068344116211 test_loss:84.50228118896484\n",
            "2267/3000 train_loss: 30.993982315063477 test_loss:97.47183227539062\n",
            "2268/3000 train_loss: 25.932207107543945 test_loss:90.33220672607422\n",
            "2269/3000 train_loss: 31.34543228149414 test_loss:85.73776245117188\n",
            "2270/3000 train_loss: 33.04013442993164 test_loss:96.04974365234375\n",
            "2271/3000 train_loss: 35.934688568115234 test_loss:87.15628051757812\n",
            "2272/3000 train_loss: 27.871139526367188 test_loss:99.93891143798828\n",
            "2273/3000 train_loss: 33.27402114868164 test_loss:97.6063461303711\n",
            "2274/3000 train_loss: 30.087852478027344 test_loss:92.32413482666016\n",
            "2275/3000 train_loss: 32.71940231323242 test_loss:86.37247467041016\n",
            "2276/3000 train_loss: 32.270259857177734 test_loss:88.9351577758789\n",
            "2277/3000 train_loss: 27.674755096435547 test_loss:83.47541046142578\n",
            "2278/3000 train_loss: 33.26148223876953 test_loss:94.20525360107422\n",
            "2279/3000 train_loss: 33.19195556640625 test_loss:86.97692108154297\n",
            "2280/3000 train_loss: 33.38526916503906 test_loss:83.3709716796875\n",
            "2281/3000 train_loss: 34.55912399291992 test_loss:97.78564453125\n",
            "2282/3000 train_loss: 37.202552795410156 test_loss:93.08740234375\n",
            "2283/3000 train_loss: 28.32703399658203 test_loss:80.06522369384766\n",
            "2284/3000 train_loss: 30.408693313598633 test_loss:90.29295349121094\n",
            "2285/3000 train_loss: 36.03248596191406 test_loss:93.22004699707031\n",
            "2286/3000 train_loss: 31.269386291503906 test_loss:93.76046752929688\n",
            "2287/3000 train_loss: 28.775686264038086 test_loss:89.40441131591797\n",
            "2288/3000 train_loss: 28.756771087646484 test_loss:90.38630676269531\n",
            "2289/3000 train_loss: 26.821331024169922 test_loss:107.78423309326172\n",
            "2290/3000 train_loss: 34.234066009521484 test_loss:84.02714538574219\n",
            "2291/3000 train_loss: 29.009429931640625 test_loss:100.14636993408203\n",
            "2292/3000 train_loss: 35.286460876464844 test_loss:100.23799133300781\n",
            "2293/3000 train_loss: 34.4460563659668 test_loss:80.44883728027344\n",
            "2294/3000 train_loss: 32.05604934692383 test_loss:98.3003158569336\n",
            "2295/3000 train_loss: 28.815092086791992 test_loss:85.96282958984375\n",
            "2296/3000 train_loss: 30.561159133911133 test_loss:82.9283676147461\n",
            "2297/3000 train_loss: 36.45834732055664 test_loss:83.48078918457031\n",
            "2298/3000 train_loss: 34.273590087890625 test_loss:92.78130340576172\n",
            "2299/3000 train_loss: 28.204959869384766 test_loss:84.60345458984375\n",
            "2300/3000 train_loss: 33.841129302978516 test_loss:84.13941192626953\n",
            "2301/3000 train_loss: 37.9869384765625 test_loss:92.59819793701172\n",
            "2302/3000 train_loss: 43.39057922363281 test_loss:88.22773742675781\n",
            "2303/3000 train_loss: 32.612579345703125 test_loss:95.71275329589844\n",
            "2304/3000 train_loss: 32.96137619018555 test_loss:95.1084976196289\n",
            "2305/3000 train_loss: 27.9508056640625 test_loss:90.04014587402344\n",
            "2306/3000 train_loss: 28.544157028198242 test_loss:90.3643569946289\n",
            "2307/3000 train_loss: 33.28519821166992 test_loss:95.20658111572266\n",
            "2308/3000 train_loss: 30.958942413330078 test_loss:82.85298919677734\n",
            "2309/3000 train_loss: 30.725940704345703 test_loss:99.57281494140625\n",
            "2310/3000 train_loss: 40.88309097290039 test_loss:84.83424377441406\n",
            "2311/3000 train_loss: 33.12653732299805 test_loss:87.627197265625\n",
            "2312/3000 train_loss: 28.83409309387207 test_loss:95.18170928955078\n",
            "2313/3000 train_loss: 25.1213321685791 test_loss:87.20088958740234\n",
            "2314/3000 train_loss: 40.12345886230469 test_loss:97.05901336669922\n",
            "2315/3000 train_loss: 29.022985458374023 test_loss:82.03471374511719\n",
            "2316/3000 train_loss: 30.847503662109375 test_loss:84.81573486328125\n",
            "2317/3000 train_loss: 26.614768981933594 test_loss:85.86274719238281\n",
            "2318/3000 train_loss: 32.28678512573242 test_loss:91.25248718261719\n",
            "2319/3000 train_loss: 40.49650955200195 test_loss:88.48249816894531\n",
            "2320/3000 train_loss: 27.39898681640625 test_loss:82.15351104736328\n",
            "2321/3000 train_loss: 31.91842269897461 test_loss:92.0879898071289\n",
            "2322/3000 train_loss: 32.45536804199219 test_loss:90.68946838378906\n",
            "2323/3000 train_loss: 30.19619369506836 test_loss:83.73219299316406\n",
            "2324/3000 train_loss: 34.50673294067383 test_loss:91.44701385498047\n",
            "2325/3000 train_loss: 26.802133560180664 test_loss:84.45547485351562\n",
            "2326/3000 train_loss: 34.260623931884766 test_loss:93.27783966064453\n",
            "2327/3000 train_loss: 30.937944412231445 test_loss:94.43965911865234\n",
            "2328/3000 train_loss: 28.863536834716797 test_loss:85.23493194580078\n",
            "2329/3000 train_loss: 28.6051025390625 test_loss:105.0704574584961\n",
            "2330/3000 train_loss: 31.172500610351562 test_loss:91.41551208496094\n",
            "2331/3000 train_loss: 29.482391357421875 test_loss:85.03831481933594\n",
            "2332/3000 train_loss: 29.696617126464844 test_loss:84.42538452148438\n",
            "2333/3000 train_loss: 35.14808654785156 test_loss:86.98446655273438\n",
            "2334/3000 train_loss: 35.87564468383789 test_loss:94.55047607421875\n",
            "2335/3000 train_loss: 33.057647705078125 test_loss:85.74006652832031\n",
            "2336/3000 train_loss: 28.801877975463867 test_loss:92.0787353515625\n",
            "2337/3000 train_loss: 27.99724769592285 test_loss:85.14407348632812\n",
            "2338/3000 train_loss: 31.296249389648438 test_loss:95.10066986083984\n",
            "2339/3000 train_loss: 29.29580307006836 test_loss:88.10062408447266\n",
            "2340/3000 train_loss: 36.61266326904297 test_loss:94.87942504882812\n",
            "2341/3000 train_loss: 30.24750328063965 test_loss:91.5900650024414\n",
            "2342/3000 train_loss: 28.979183197021484 test_loss:95.4777603149414\n",
            "2343/3000 train_loss: 34.40754318237305 test_loss:92.13337707519531\n",
            "2344/3000 train_loss: 31.270963668823242 test_loss:92.68890380859375\n",
            "2345/3000 train_loss: 30.77950096130371 test_loss:84.33458709716797\n",
            "2346/3000 train_loss: 29.95504379272461 test_loss:92.80891418457031\n",
            "2347/3000 train_loss: 34.27030944824219 test_loss:95.2130355834961\n",
            "2348/3000 train_loss: 28.1090145111084 test_loss:89.57701873779297\n",
            "2349/3000 train_loss: 27.202068328857422 test_loss:87.33631896972656\n",
            "2350/3000 train_loss: 28.989398956298828 test_loss:91.0193099975586\n",
            "2351/3000 train_loss: 31.839248657226562 test_loss:85.3493881225586\n",
            "2352/3000 train_loss: 28.356464385986328 test_loss:81.8865966796875\n",
            "2353/3000 train_loss: 34.450923919677734 test_loss:104.27046966552734\n",
            "2354/3000 train_loss: 33.761474609375 test_loss:81.63725280761719\n",
            "2355/3000 train_loss: 25.91986656188965 test_loss:88.72787475585938\n",
            "2356/3000 train_loss: 26.34955596923828 test_loss:86.18785858154297\n",
            "2357/3000 train_loss: 29.104869842529297 test_loss:87.32803344726562\n",
            "2358/3000 train_loss: 31.604598999023438 test_loss:84.94525146484375\n",
            "2359/3000 train_loss: 29.89519691467285 test_loss:90.10212707519531\n",
            "2360/3000 train_loss: 28.462326049804688 test_loss:86.73835754394531\n",
            "2361/3000 train_loss: 31.98865509033203 test_loss:89.4698486328125\n",
            "2362/3000 train_loss: 34.216678619384766 test_loss:93.74832153320312\n",
            "2363/3000 train_loss: 30.456449508666992 test_loss:85.12040710449219\n",
            "2364/3000 train_loss: 27.3319149017334 test_loss:84.76253509521484\n",
            "2365/3000 train_loss: 33.98084259033203 test_loss:82.97905731201172\n",
            "2366/3000 train_loss: 27.78833770751953 test_loss:81.21414184570312\n",
            "2367/3000 train_loss: 30.50339126586914 test_loss:88.89033508300781\n",
            "2368/3000 train_loss: 31.73413848876953 test_loss:94.02949523925781\n",
            "2369/3000 train_loss: 28.97343635559082 test_loss:91.10142517089844\n",
            "2370/3000 train_loss: 28.00951385498047 test_loss:83.94361877441406\n",
            "2371/3000 train_loss: 36.53242111206055 test_loss:87.99514770507812\n",
            "2372/3000 train_loss: 27.085182189941406 test_loss:83.69158935546875\n",
            "2373/3000 train_loss: 28.2163028717041 test_loss:88.85475158691406\n",
            "2374/3000 train_loss: 32.96089172363281 test_loss:84.96751403808594\n",
            "2375/3000 train_loss: 29.28940773010254 test_loss:92.02074432373047\n",
            "2376/3000 train_loss: 27.22392463684082 test_loss:84.2409439086914\n",
            "2377/3000 train_loss: 35.42461013793945 test_loss:93.6445083618164\n",
            "2378/3000 train_loss: 28.218673706054688 test_loss:84.52162170410156\n",
            "2379/3000 train_loss: 29.702219009399414 test_loss:106.1148910522461\n",
            "2380/3000 train_loss: 26.356578826904297 test_loss:90.53744506835938\n",
            "2381/3000 train_loss: 29.570463180541992 test_loss:87.05474090576172\n",
            "2382/3000 train_loss: 33.97271728515625 test_loss:86.72869873046875\n",
            "2383/3000 train_loss: 41.09418869018555 test_loss:97.66160583496094\n",
            "2384/3000 train_loss: 26.909345626831055 test_loss:101.3703384399414\n",
            "2385/3000 train_loss: 31.68532943725586 test_loss:88.10328674316406\n",
            "2386/3000 train_loss: 26.083024978637695 test_loss:91.78268432617188\n",
            "2387/3000 train_loss: 26.210330963134766 test_loss:90.80803680419922\n",
            "2388/3000 train_loss: 26.70752716064453 test_loss:91.42428588867188\n",
            "2389/3000 train_loss: 33.613304138183594 test_loss:83.59319305419922\n",
            "2390/3000 train_loss: 27.787748336791992 test_loss:83.99861145019531\n",
            "2391/3000 train_loss: 32.362979888916016 test_loss:81.27188110351562\n",
            "2392/3000 train_loss: 32.4620246887207 test_loss:90.86184692382812\n",
            "2393/3000 train_loss: 30.572402954101562 test_loss:88.02608489990234\n",
            "2394/3000 train_loss: 31.802339553833008 test_loss:85.06710815429688\n",
            "2395/3000 train_loss: 30.40618133544922 test_loss:81.77609252929688\n",
            "2396/3000 train_loss: 31.72850799560547 test_loss:87.5825424194336\n",
            "2397/3000 train_loss: 26.50330352783203 test_loss:91.00463104248047\n",
            "2398/3000 train_loss: 26.30653953552246 test_loss:81.25875091552734\n",
            "2399/3000 train_loss: 28.153881072998047 test_loss:87.92382049560547\n",
            "2400/3000 train_loss: 31.471126556396484 test_loss:88.34423828125\n",
            "2401/3000 train_loss: 26.42380714416504 test_loss:87.1763916015625\n",
            "2402/3000 train_loss: 25.419363021850586 test_loss:86.58109283447266\n",
            "2403/3000 train_loss: 26.854629516601562 test_loss:87.31346130371094\n",
            "2404/3000 train_loss: 30.951879501342773 test_loss:88.13243865966797\n",
            "2405/3000 train_loss: 27.42042350769043 test_loss:84.77774047851562\n",
            "2406/3000 train_loss: 33.64298629760742 test_loss:100.50677490234375\n",
            "2407/3000 train_loss: 29.641620635986328 test_loss:93.99078369140625\n",
            "2408/3000 train_loss: 29.64056968688965 test_loss:91.14836120605469\n",
            "2409/3000 train_loss: 29.22203826904297 test_loss:84.88378143310547\n",
            "2410/3000 train_loss: 30.152381896972656 test_loss:88.53277587890625\n",
            "2411/3000 train_loss: 26.074901580810547 test_loss:100.02857971191406\n",
            "2412/3000 train_loss: 28.513755798339844 test_loss:96.36643981933594\n",
            "2413/3000 train_loss: 31.68126678466797 test_loss:99.19115447998047\n",
            "2414/3000 train_loss: 30.686195373535156 test_loss:82.78661346435547\n",
            "2415/3000 train_loss: 27.503360748291016 test_loss:82.58997344970703\n",
            "2416/3000 train_loss: 25.698572158813477 test_loss:93.05155181884766\n",
            "2417/3000 train_loss: 28.739551544189453 test_loss:81.94908142089844\n",
            "2418/3000 train_loss: 33.06231689453125 test_loss:82.05097961425781\n",
            "2419/3000 train_loss: 28.847232818603516 test_loss:95.34720611572266\n",
            "2420/3000 train_loss: 30.8145809173584 test_loss:82.9737548828125\n",
            "2421/3000 train_loss: 30.809797286987305 test_loss:84.74382781982422\n",
            "2422/3000 train_loss: 26.67009925842285 test_loss:79.45511627197266\n",
            "2423/3000 train_loss: 30.84066390991211 test_loss:90.33583068847656\n",
            "2424/3000 train_loss: 28.58769989013672 test_loss:85.96102142333984\n",
            "2425/3000 train_loss: 28.19101333618164 test_loss:78.47909545898438\n",
            "2426/3000 train_loss: 31.66922378540039 test_loss:87.0475845336914\n",
            "2427/3000 train_loss: 28.44972801208496 test_loss:90.91329956054688\n",
            "2428/3000 train_loss: 27.343109130859375 test_loss:77.9362564086914\n",
            "2429/3000 train_loss: 27.2093505859375 test_loss:85.15730285644531\n",
            "2430/3000 train_loss: 32.47571563720703 test_loss:78.60575103759766\n",
            "2431/3000 train_loss: 31.025066375732422 test_loss:92.06700134277344\n",
            "2432/3000 train_loss: 30.168729782104492 test_loss:93.17415618896484\n",
            "2433/3000 train_loss: 27.50373077392578 test_loss:87.17723846435547\n",
            "2434/3000 train_loss: 31.432659149169922 test_loss:89.03520202636719\n",
            "2435/3000 train_loss: 28.389270782470703 test_loss:93.72947692871094\n",
            "2436/3000 train_loss: 26.241783142089844 test_loss:88.94612121582031\n",
            "2437/3000 train_loss: 27.689006805419922 test_loss:84.06524658203125\n",
            "2438/3000 train_loss: 26.392366409301758 test_loss:88.71678161621094\n",
            "2439/3000 train_loss: 26.91851806640625 test_loss:101.37784576416016\n",
            "2440/3000 train_loss: 37.29144287109375 test_loss:81.73970794677734\n",
            "2441/3000 train_loss: 30.840255737304688 test_loss:83.948974609375\n",
            "2442/3000 train_loss: 30.261211395263672 test_loss:78.0992431640625\n",
            "2443/3000 train_loss: 31.751115798950195 test_loss:89.41783905029297\n",
            "2444/3000 train_loss: 34.377166748046875 test_loss:89.85565948486328\n",
            "2445/3000 train_loss: 27.213420867919922 test_loss:85.94762420654297\n",
            "2446/3000 train_loss: 29.5098876953125 test_loss:90.11287689208984\n",
            "2447/3000 train_loss: 31.142696380615234 test_loss:93.6742172241211\n",
            "2448/3000 train_loss: 31.398435592651367 test_loss:85.74259185791016\n",
            "2449/3000 train_loss: 30.367334365844727 test_loss:87.43385314941406\n",
            "2450/3000 train_loss: 32.642127990722656 test_loss:98.35292053222656\n",
            "2451/3000 train_loss: 30.815237045288086 test_loss:87.51917266845703\n",
            "2452/3000 train_loss: 25.49056625366211 test_loss:96.04289245605469\n",
            "2453/3000 train_loss: 31.329416275024414 test_loss:87.58292388916016\n",
            "2454/3000 train_loss: 37.73504638671875 test_loss:91.8803482055664\n",
            "2455/3000 train_loss: 26.439823150634766 test_loss:93.82403564453125\n",
            "2456/3000 train_loss: 27.594703674316406 test_loss:91.96851348876953\n",
            "2457/3000 train_loss: 30.511436462402344 test_loss:93.97139739990234\n",
            "2458/3000 train_loss: 26.03032112121582 test_loss:86.3406753540039\n",
            "2459/3000 train_loss: 24.072973251342773 test_loss:94.39553833007812\n",
            "2460/3000 train_loss: 26.674102783203125 test_loss:83.15646362304688\n",
            "2461/3000 train_loss: 32.21794128417969 test_loss:91.7620620727539\n",
            "2462/3000 train_loss: 29.96833038330078 test_loss:105.66453552246094\n",
            "2463/3000 train_loss: 36.88856887817383 test_loss:85.37025451660156\n",
            "2464/3000 train_loss: 27.044496536254883 test_loss:85.93740844726562\n",
            "2465/3000 train_loss: 29.504396438598633 test_loss:99.08282470703125\n",
            "2466/3000 train_loss: 28.111242294311523 test_loss:89.46934509277344\n",
            "2467/3000 train_loss: 27.58844757080078 test_loss:91.1651611328125\n",
            "2468/3000 train_loss: 28.803667068481445 test_loss:89.05477142333984\n",
            "2469/3000 train_loss: 29.44935417175293 test_loss:97.33876037597656\n",
            "2470/3000 train_loss: 30.32655906677246 test_loss:87.17120361328125\n",
            "2471/3000 train_loss: 28.107067108154297 test_loss:85.7792739868164\n",
            "2472/3000 train_loss: 27.82818603515625 test_loss:89.74372100830078\n",
            "2473/3000 train_loss: 28.660011291503906 test_loss:84.27216339111328\n",
            "2474/3000 train_loss: 29.056259155273438 test_loss:94.21260833740234\n",
            "2475/3000 train_loss: 32.50459671020508 test_loss:89.41613006591797\n",
            "2476/3000 train_loss: 26.61210823059082 test_loss:102.7234878540039\n",
            "2477/3000 train_loss: 29.128829956054688 test_loss:101.23554229736328\n",
            "2478/3000 train_loss: 32.737247467041016 test_loss:99.54881286621094\n",
            "2479/3000 train_loss: 30.103912353515625 test_loss:87.87783813476562\n",
            "2480/3000 train_loss: 25.61750602722168 test_loss:89.96420288085938\n",
            "2481/3000 train_loss: 37.17922592163086 test_loss:89.04388427734375\n",
            "2482/3000 train_loss: 27.526927947998047 test_loss:91.52925872802734\n",
            "2483/3000 train_loss: 26.066970825195312 test_loss:88.16499328613281\n",
            "2484/3000 train_loss: 23.886911392211914 test_loss:85.93576049804688\n",
            "2485/3000 train_loss: 33.69083023071289 test_loss:82.52168273925781\n",
            "2486/3000 train_loss: 34.48719787597656 test_loss:84.54386901855469\n",
            "2487/3000 train_loss: 32.39741897583008 test_loss:94.73644256591797\n",
            "2488/3000 train_loss: 32.524330139160156 test_loss:103.69640350341797\n",
            "2489/3000 train_loss: 35.219200134277344 test_loss:90.41421508789062\n",
            "2490/3000 train_loss: 31.902576446533203 test_loss:87.95111083984375\n",
            "2491/3000 train_loss: 26.218963623046875 test_loss:86.5047607421875\n",
            "2492/3000 train_loss: 34.24582290649414 test_loss:94.14068603515625\n",
            "2493/3000 train_loss: 27.218027114868164 test_loss:88.45469665527344\n",
            "2494/3000 train_loss: 28.24125862121582 test_loss:86.39230346679688\n",
            "2495/3000 train_loss: 26.94327735900879 test_loss:88.00398254394531\n",
            "2496/3000 train_loss: 31.025806427001953 test_loss:85.08890533447266\n",
            "2497/3000 train_loss: 28.33614730834961 test_loss:90.20124816894531\n",
            "2498/3000 train_loss: 26.711515426635742 test_loss:88.53485107421875\n",
            "2499/3000 train_loss: 28.434528350830078 test_loss:94.45455932617188\n",
            "2500/3000 train_loss: 30.138046264648438 test_loss:84.59538269042969\n",
            "2501/3000 train_loss: 33.66217041015625 test_loss:80.4317626953125\n",
            "2502/3000 train_loss: 29.09188461303711 test_loss:93.72974395751953\n",
            "2503/3000 train_loss: 28.54606056213379 test_loss:88.5320816040039\n",
            "2504/3000 train_loss: 33.443416595458984 test_loss:83.69976043701172\n",
            "2505/3000 train_loss: 25.9985294342041 test_loss:90.39325714111328\n",
            "2506/3000 train_loss: 35.01860046386719 test_loss:91.79052734375\n",
            "2507/3000 train_loss: 30.653881072998047 test_loss:91.37960815429688\n",
            "2508/3000 train_loss: 29.701725006103516 test_loss:90.21125793457031\n",
            "2509/3000 train_loss: 25.987916946411133 test_loss:86.5440673828125\n",
            "2510/3000 train_loss: 28.58280372619629 test_loss:89.8391342163086\n",
            "2511/3000 train_loss: 42.38679504394531 test_loss:82.46186065673828\n",
            "2512/3000 train_loss: 31.640422821044922 test_loss:80.21430206298828\n",
            "2513/3000 train_loss: 33.4566535949707 test_loss:87.56167602539062\n",
            "2514/3000 train_loss: 21.086261749267578 test_loss:84.3669204711914\n",
            "2515/3000 train_loss: 30.64023780822754 test_loss:85.35769653320312\n",
            "2516/3000 train_loss: 25.428726196289062 test_loss:87.43569946289062\n",
            "2517/3000 train_loss: 29.604969024658203 test_loss:77.33441162109375\n",
            "2518/3000 train_loss: 37.678550720214844 test_loss:97.91187286376953\n",
            "2519/3000 train_loss: 27.665611267089844 test_loss:91.15458679199219\n",
            "2520/3000 train_loss: 29.51589584350586 test_loss:94.36478424072266\n",
            "2521/3000 train_loss: 30.230005264282227 test_loss:85.26614379882812\n",
            "2522/3000 train_loss: 31.057533264160156 test_loss:82.38772583007812\n",
            "2523/3000 train_loss: 22.815006256103516 test_loss:86.93347930908203\n",
            "2524/3000 train_loss: 24.814958572387695 test_loss:84.55469512939453\n",
            "2525/3000 train_loss: 27.80574607849121 test_loss:83.81896209716797\n",
            "2526/3000 train_loss: 22.63036346435547 test_loss:76.52589416503906\n",
            "2527/3000 train_loss: 25.78548812866211 test_loss:81.44245147705078\n",
            "2528/3000 train_loss: 27.18263053894043 test_loss:83.23837280273438\n",
            "2529/3000 train_loss: 24.76226043701172 test_loss:85.25159454345703\n",
            "2530/3000 train_loss: 29.96562385559082 test_loss:87.89078521728516\n",
            "2531/3000 train_loss: 23.072982788085938 test_loss:86.31653594970703\n",
            "2532/3000 train_loss: 30.960975646972656 test_loss:89.34556579589844\n",
            "2533/3000 train_loss: 31.362560272216797 test_loss:90.93327331542969\n",
            "2534/3000 train_loss: 31.639925003051758 test_loss:82.8145523071289\n",
            "2535/3000 train_loss: 29.682756423950195 test_loss:94.5309829711914\n",
            "2536/3000 train_loss: 31.84100914001465 test_loss:82.37884521484375\n",
            "2537/3000 train_loss: 25.974018096923828 test_loss:80.24385070800781\n",
            "2538/3000 train_loss: 26.341482162475586 test_loss:80.63172912597656\n",
            "2539/3000 train_loss: 34.920528411865234 test_loss:95.96260070800781\n",
            "2540/3000 train_loss: 31.210533142089844 test_loss:83.98226928710938\n",
            "2541/3000 train_loss: 28.330665588378906 test_loss:82.9656982421875\n",
            "2542/3000 train_loss: 28.17759132385254 test_loss:77.31074523925781\n",
            "2543/3000 train_loss: 27.860864639282227 test_loss:93.94512176513672\n",
            "2544/3000 train_loss: 30.089014053344727 test_loss:84.93598175048828\n",
            "2545/3000 train_loss: 28.35843849182129 test_loss:81.04658508300781\n",
            "2546/3000 train_loss: 31.09247398376465 test_loss:81.85020446777344\n",
            "2547/3000 train_loss: 25.54532814025879 test_loss:89.33245086669922\n",
            "2548/3000 train_loss: 28.498708724975586 test_loss:85.21694946289062\n",
            "2549/3000 train_loss: 22.634532928466797 test_loss:81.2087173461914\n",
            "2550/3000 train_loss: 30.854381561279297 test_loss:90.5782470703125\n",
            "2551/3000 train_loss: 29.429447174072266 test_loss:89.04113006591797\n",
            "2552/3000 train_loss: 34.43036651611328 test_loss:86.93858337402344\n",
            "2553/3000 train_loss: 28.936674118041992 test_loss:93.33599090576172\n",
            "2554/3000 train_loss: 26.273653030395508 test_loss:82.14551544189453\n",
            "2555/3000 train_loss: 26.817296981811523 test_loss:85.25856018066406\n",
            "2556/3000 train_loss: 25.59089469909668 test_loss:95.03369903564453\n",
            "2557/3000 train_loss: 24.331729888916016 test_loss:83.26994323730469\n",
            "2558/3000 train_loss: 23.477840423583984 test_loss:90.39158630371094\n",
            "2559/3000 train_loss: 28.751501083374023 test_loss:95.85929870605469\n",
            "2560/3000 train_loss: 26.034082412719727 test_loss:80.96468353271484\n",
            "2561/3000 train_loss: 32.82841873168945 test_loss:92.00779724121094\n",
            "2562/3000 train_loss: 30.041690826416016 test_loss:91.37683868408203\n",
            "2563/3000 train_loss: 24.746841430664062 test_loss:85.48504638671875\n",
            "2564/3000 train_loss: 27.634593963623047 test_loss:87.07369232177734\n",
            "2565/3000 train_loss: 31.195823669433594 test_loss:87.08309173583984\n",
            "2566/3000 train_loss: 33.61186599731445 test_loss:90.76504516601562\n",
            "2567/3000 train_loss: 34.69370651245117 test_loss:84.63485717773438\n",
            "2568/3000 train_loss: 28.45669937133789 test_loss:84.87342071533203\n",
            "2569/3000 train_loss: 28.03523063659668 test_loss:85.79745483398438\n",
            "2570/3000 train_loss: 32.915565490722656 test_loss:81.02104187011719\n",
            "2571/3000 train_loss: 30.656658172607422 test_loss:87.64537811279297\n",
            "2572/3000 train_loss: 36.020973205566406 test_loss:92.57642364501953\n",
            "2573/3000 train_loss: 31.68497657775879 test_loss:84.75730895996094\n",
            "2574/3000 train_loss: 27.849172592163086 test_loss:75.24811553955078\n",
            "2575/3000 train_loss: 33.606727600097656 test_loss:83.17987823486328\n",
            "2576/3000 train_loss: 35.60070037841797 test_loss:79.0994873046875\n",
            "2577/3000 train_loss: 27.39183235168457 test_loss:77.47502136230469\n",
            "2578/3000 train_loss: 29.28964614868164 test_loss:79.80577850341797\n",
            "2579/3000 train_loss: 28.407371520996094 test_loss:79.80728149414062\n",
            "2580/3000 train_loss: 25.806102752685547 test_loss:76.20567321777344\n",
            "2581/3000 train_loss: 35.56969451904297 test_loss:84.74430847167969\n",
            "2582/3000 train_loss: 28.10022735595703 test_loss:88.03856658935547\n",
            "2583/3000 train_loss: 35.73395919799805 test_loss:77.98857116699219\n",
            "2584/3000 train_loss: 29.99565315246582 test_loss:79.51757049560547\n",
            "2585/3000 train_loss: 25.562786102294922 test_loss:82.35667419433594\n",
            "2586/3000 train_loss: 26.13484764099121 test_loss:77.81981658935547\n",
            "2587/3000 train_loss: 26.98303985595703 test_loss:78.41436767578125\n",
            "2588/3000 train_loss: 25.475509643554688 test_loss:75.78211975097656\n",
            "2589/3000 train_loss: 24.87334632873535 test_loss:75.25469207763672\n",
            "2590/3000 train_loss: 28.808944702148438 test_loss:79.7416763305664\n",
            "2591/3000 train_loss: 20.85907554626465 test_loss:76.36199951171875\n",
            "2592/3000 train_loss: 25.165016174316406 test_loss:74.11872100830078\n",
            "2593/3000 train_loss: 29.866147994995117 test_loss:76.22488403320312\n",
            "2594/3000 train_loss: 26.515871047973633 test_loss:79.02481079101562\n",
            "2595/3000 train_loss: 24.900524139404297 test_loss:82.11119079589844\n",
            "2596/3000 train_loss: 27.975284576416016 test_loss:76.04312896728516\n",
            "2597/3000 train_loss: 27.517715454101562 test_loss:77.76139068603516\n",
            "2598/3000 train_loss: 28.179725646972656 test_loss:81.16630554199219\n",
            "2599/3000 train_loss: 32.71159744262695 test_loss:88.04295349121094\n",
            "2600/3000 train_loss: 28.367595672607422 test_loss:83.56422424316406\n",
            "2601/3000 train_loss: 27.34853744506836 test_loss:86.61526489257812\n",
            "2602/3000 train_loss: 26.060443878173828 test_loss:79.62285614013672\n",
            "2603/3000 train_loss: 25.081310272216797 test_loss:82.82527160644531\n",
            "2604/3000 train_loss: 29.151885986328125 test_loss:91.4940185546875\n",
            "2605/3000 train_loss: 28.19109344482422 test_loss:77.85213470458984\n",
            "2606/3000 train_loss: 26.039905548095703 test_loss:89.04507446289062\n",
            "2607/3000 train_loss: 28.198579788208008 test_loss:76.8366928100586\n",
            "2608/3000 train_loss: 31.74068260192871 test_loss:93.48722076416016\n",
            "2609/3000 train_loss: 32.965904235839844 test_loss:82.32162475585938\n",
            "2610/3000 train_loss: 27.699087142944336 test_loss:78.18830108642578\n",
            "2611/3000 train_loss: 25.309446334838867 test_loss:75.12931060791016\n",
            "2612/3000 train_loss: 30.154794692993164 test_loss:87.06371307373047\n",
            "2613/3000 train_loss: 27.421144485473633 test_loss:89.58767700195312\n",
            "2614/3000 train_loss: 29.025821685791016 test_loss:88.03976440429688\n",
            "2615/3000 train_loss: 29.59864044189453 test_loss:85.92800903320312\n",
            "2616/3000 train_loss: 26.81066131591797 test_loss:82.10785675048828\n",
            "2617/3000 train_loss: 30.84033203125 test_loss:78.29161071777344\n",
            "2618/3000 train_loss: 29.849605560302734 test_loss:85.23472595214844\n",
            "2619/3000 train_loss: 28.326078414916992 test_loss:81.56562042236328\n",
            "2620/3000 train_loss: 25.83146858215332 test_loss:90.31893157958984\n",
            "2621/3000 train_loss: 27.38031005859375 test_loss:77.04867553710938\n",
            "2622/3000 train_loss: 28.08946990966797 test_loss:90.01276397705078\n",
            "2623/3000 train_loss: 27.366975784301758 test_loss:96.57221984863281\n",
            "2624/3000 train_loss: 30.2670841217041 test_loss:82.2966537475586\n",
            "2625/3000 train_loss: 23.576370239257812 test_loss:77.0\n",
            "2626/3000 train_loss: 28.02157974243164 test_loss:83.11311340332031\n",
            "2627/3000 train_loss: 29.156631469726562 test_loss:84.7762451171875\n",
            "2628/3000 train_loss: 37.22358703613281 test_loss:85.58609771728516\n",
            "2629/3000 train_loss: 29.498430252075195 test_loss:81.5458755493164\n",
            "2630/3000 train_loss: 33.1236457824707 test_loss:86.94270324707031\n",
            "2631/3000 train_loss: 24.595012664794922 test_loss:85.59248352050781\n",
            "2632/3000 train_loss: 27.1971435546875 test_loss:92.42375946044922\n",
            "2633/3000 train_loss: 26.183603286743164 test_loss:80.69690704345703\n",
            "2634/3000 train_loss: 26.857561111450195 test_loss:87.00810241699219\n",
            "2635/3000 train_loss: 27.90544891357422 test_loss:87.34844207763672\n",
            "2636/3000 train_loss: 27.992435455322266 test_loss:83.28177642822266\n",
            "2637/3000 train_loss: 31.053401947021484 test_loss:85.85921478271484\n",
            "2638/3000 train_loss: 24.139074325561523 test_loss:84.36370086669922\n",
            "2639/3000 train_loss: 32.25473403930664 test_loss:75.4769287109375\n",
            "2640/3000 train_loss: 29.571313858032227 test_loss:83.95979309082031\n",
            "2641/3000 train_loss: 22.010906219482422 test_loss:80.52287292480469\n",
            "2642/3000 train_loss: 28.51291275024414 test_loss:86.16813659667969\n",
            "2643/3000 train_loss: 26.817230224609375 test_loss:81.73066711425781\n",
            "2644/3000 train_loss: 21.773395538330078 test_loss:80.38153839111328\n",
            "2645/3000 train_loss: 26.569944381713867 test_loss:79.56124877929688\n",
            "2646/3000 train_loss: 25.281360626220703 test_loss:83.59031677246094\n",
            "2647/3000 train_loss: 31.085657119750977 test_loss:79.53064727783203\n",
            "2648/3000 train_loss: 32.27857208251953 test_loss:82.3089599609375\n",
            "2649/3000 train_loss: 28.062358856201172 test_loss:84.84835815429688\n",
            "2650/3000 train_loss: 26.914342880249023 test_loss:80.3495864868164\n",
            "2651/3000 train_loss: 27.743797302246094 test_loss:82.42703247070312\n",
            "2652/3000 train_loss: 27.074634552001953 test_loss:83.63233184814453\n",
            "2653/3000 train_loss: 23.38463592529297 test_loss:81.74219512939453\n",
            "2654/3000 train_loss: 24.49353790283203 test_loss:96.00688171386719\n",
            "2655/3000 train_loss: 24.151159286499023 test_loss:84.49830627441406\n",
            "2656/3000 train_loss: 30.75652313232422 test_loss:80.11659240722656\n",
            "2657/3000 train_loss: 28.26387596130371 test_loss:81.9100112915039\n",
            "2658/3000 train_loss: 28.117155075073242 test_loss:87.29411315917969\n",
            "2659/3000 train_loss: 26.360977172851562 test_loss:82.87942504882812\n",
            "2660/3000 train_loss: 23.846500396728516 test_loss:77.32040405273438\n",
            "2661/3000 train_loss: 27.34780502319336 test_loss:90.41490173339844\n",
            "2662/3000 train_loss: 24.858509063720703 test_loss:78.45465850830078\n",
            "2663/3000 train_loss: 27.140270233154297 test_loss:84.48701477050781\n",
            "2664/3000 train_loss: 28.439781188964844 test_loss:80.54680633544922\n",
            "2665/3000 train_loss: 25.72240447998047 test_loss:73.18489074707031\n",
            "2666/3000 train_loss: 25.188629150390625 test_loss:81.04601287841797\n",
            "2667/3000 train_loss: 27.585710525512695 test_loss:86.54144287109375\n",
            "2668/3000 train_loss: 24.11946678161621 test_loss:78.7804946899414\n",
            "2669/3000 train_loss: 28.85453224182129 test_loss:84.52581024169922\n",
            "2670/3000 train_loss: 25.524789810180664 test_loss:81.62285614013672\n",
            "2671/3000 train_loss: 25.6138973236084 test_loss:87.56128692626953\n",
            "2672/3000 train_loss: 28.446481704711914 test_loss:93.76023864746094\n",
            "2673/3000 train_loss: 24.63347816467285 test_loss:76.56240844726562\n",
            "2674/3000 train_loss: 36.64402770996094 test_loss:94.9775390625\n",
            "2675/3000 train_loss: 31.622695922851562 test_loss:78.82072448730469\n",
            "2676/3000 train_loss: 30.560874938964844 test_loss:79.10865020751953\n",
            "2677/3000 train_loss: 28.70119857788086 test_loss:88.547119140625\n",
            "2678/3000 train_loss: 24.644119262695312 test_loss:79.60247802734375\n",
            "2679/3000 train_loss: 29.274728775024414 test_loss:79.47409057617188\n",
            "2680/3000 train_loss: 30.504940032958984 test_loss:92.14069366455078\n",
            "2681/3000 train_loss: 28.49520492553711 test_loss:82.98892211914062\n",
            "2682/3000 train_loss: 23.865922927856445 test_loss:84.1816177368164\n",
            "2683/3000 train_loss: 22.689956665039062 test_loss:82.56599426269531\n",
            "2684/3000 train_loss: 24.215084075927734 test_loss:83.41138458251953\n",
            "2685/3000 train_loss: 21.761920928955078 test_loss:89.0312728881836\n",
            "2686/3000 train_loss: 26.507152557373047 test_loss:84.94132995605469\n",
            "2687/3000 train_loss: 25.33283233642578 test_loss:78.54685974121094\n",
            "2688/3000 train_loss: 27.448650360107422 test_loss:94.84856414794922\n",
            "2689/3000 train_loss: 33.59124755859375 test_loss:78.72389221191406\n",
            "2690/3000 train_loss: 35.74705123901367 test_loss:76.44564056396484\n",
            "2691/3000 train_loss: 27.31340217590332 test_loss:86.63713073730469\n",
            "2692/3000 train_loss: 27.67120933532715 test_loss:78.51509857177734\n",
            "2693/3000 train_loss: 25.834144592285156 test_loss:78.12347412109375\n",
            "2694/3000 train_loss: 25.381216049194336 test_loss:83.67285919189453\n",
            "2695/3000 train_loss: 23.915252685546875 test_loss:84.93205261230469\n",
            "2696/3000 train_loss: 25.6922550201416 test_loss:84.38278198242188\n",
            "2697/3000 train_loss: 23.19775390625 test_loss:80.22479248046875\n",
            "2698/3000 train_loss: 23.228059768676758 test_loss:77.97734069824219\n",
            "2699/3000 train_loss: 23.747163772583008 test_loss:87.1644515991211\n",
            "2700/3000 train_loss: 30.503585815429688 test_loss:79.3530044555664\n",
            "2701/3000 train_loss: 27.959346771240234 test_loss:83.49090576171875\n",
            "2702/3000 train_loss: 21.747779846191406 test_loss:82.30076599121094\n",
            "2703/3000 train_loss: 27.11541748046875 test_loss:86.63712310791016\n",
            "2704/3000 train_loss: 22.9487361907959 test_loss:78.86248779296875\n",
            "2705/3000 train_loss: 22.60874366760254 test_loss:79.22924041748047\n",
            "2706/3000 train_loss: 27.550479888916016 test_loss:87.9850082397461\n",
            "2707/3000 train_loss: 34.24861145019531 test_loss:88.96102142333984\n",
            "2708/3000 train_loss: 30.452863693237305 test_loss:88.65003204345703\n",
            "2709/3000 train_loss: 27.18267822265625 test_loss:86.77322387695312\n",
            "2710/3000 train_loss: 28.018274307250977 test_loss:81.5785140991211\n",
            "2711/3000 train_loss: 25.04986572265625 test_loss:81.62580108642578\n",
            "2712/3000 train_loss: 26.239883422851562 test_loss:87.57499694824219\n",
            "2713/3000 train_loss: 27.844600677490234 test_loss:86.55514526367188\n",
            "2714/3000 train_loss: 26.01941680908203 test_loss:80.60818481445312\n",
            "2715/3000 train_loss: 27.960968017578125 test_loss:76.7337875366211\n",
            "2716/3000 train_loss: 27.231199264526367 test_loss:82.79591369628906\n",
            "2717/3000 train_loss: 23.491897583007812 test_loss:84.27696228027344\n",
            "2718/3000 train_loss: 26.744873046875 test_loss:90.35708618164062\n",
            "2719/3000 train_loss: 21.616233825683594 test_loss:82.31071472167969\n",
            "2720/3000 train_loss: 24.049680709838867 test_loss:81.75564575195312\n",
            "2721/3000 train_loss: 22.785598754882812 test_loss:80.85652923583984\n",
            "2722/3000 train_loss: 27.265321731567383 test_loss:83.93142700195312\n",
            "2723/3000 train_loss: 22.41794204711914 test_loss:78.73114013671875\n",
            "2724/3000 train_loss: 26.131547927856445 test_loss:81.53196716308594\n",
            "2725/3000 train_loss: 28.824573516845703 test_loss:76.87380981445312\n",
            "2726/3000 train_loss: 25.098684310913086 test_loss:80.75381469726562\n",
            "2727/3000 train_loss: 27.422447204589844 test_loss:80.77977752685547\n",
            "2728/3000 train_loss: 28.43412971496582 test_loss:77.02696990966797\n",
            "2729/3000 train_loss: 34.165775299072266 test_loss:81.62566375732422\n",
            "2730/3000 train_loss: 26.106231689453125 test_loss:79.87962341308594\n",
            "2731/3000 train_loss: 28.079349517822266 test_loss:87.12384796142578\n",
            "2732/3000 train_loss: 28.91141700744629 test_loss:75.4747314453125\n",
            "2733/3000 train_loss: 24.11762809753418 test_loss:78.22174072265625\n",
            "2734/3000 train_loss: 24.026456832885742 test_loss:84.52938842773438\n",
            "2735/3000 train_loss: 24.977222442626953 test_loss:79.26533508300781\n",
            "2736/3000 train_loss: 24.27361297607422 test_loss:87.27926635742188\n",
            "2737/3000 train_loss: 28.936279296875 test_loss:77.78468322753906\n",
            "2738/3000 train_loss: 33.140419006347656 test_loss:77.33159637451172\n",
            "2739/3000 train_loss: 26.025367736816406 test_loss:82.38998413085938\n",
            "2740/3000 train_loss: 24.600866317749023 test_loss:92.69143676757812\n",
            "2741/3000 train_loss: 30.21124267578125 test_loss:90.67415618896484\n",
            "2742/3000 train_loss: 25.670520782470703 test_loss:84.38157653808594\n",
            "2743/3000 train_loss: 25.466903686523438 test_loss:86.22698211669922\n",
            "2744/3000 train_loss: 26.615921020507812 test_loss:77.35626220703125\n",
            "2745/3000 train_loss: 27.629064559936523 test_loss:77.33626556396484\n",
            "2746/3000 train_loss: 32.96504592895508 test_loss:85.86079406738281\n",
            "2747/3000 train_loss: 26.464733123779297 test_loss:75.3578109741211\n",
            "2748/3000 train_loss: 28.72202491760254 test_loss:79.46944427490234\n",
            "2749/3000 train_loss: 25.038467407226562 test_loss:81.28581237792969\n",
            "2750/3000 train_loss: 28.454092025756836 test_loss:81.41709899902344\n",
            "2751/3000 train_loss: 24.291114807128906 test_loss:78.15487670898438\n",
            "2752/3000 train_loss: 27.08383560180664 test_loss:77.4659652709961\n",
            "2753/3000 train_loss: 26.229549407958984 test_loss:73.89236450195312\n",
            "2754/3000 train_loss: 24.678152084350586 test_loss:74.78946685791016\n",
            "2755/3000 train_loss: 29.015094757080078 test_loss:79.69548797607422\n",
            "2756/3000 train_loss: 29.954336166381836 test_loss:76.04468536376953\n",
            "2757/3000 train_loss: 23.025634765625 test_loss:81.76985931396484\n",
            "2758/3000 train_loss: 24.17888641357422 test_loss:80.17864990234375\n",
            "2759/3000 train_loss: 22.89306640625 test_loss:83.10948944091797\n",
            "2760/3000 train_loss: 29.817340850830078 test_loss:76.57487487792969\n",
            "2761/3000 train_loss: 24.068574905395508 test_loss:76.81884765625\n",
            "2762/3000 train_loss: 23.668041229248047 test_loss:80.0561752319336\n",
            "2763/3000 train_loss: 28.445011138916016 test_loss:77.29045867919922\n",
            "2764/3000 train_loss: 24.282732009887695 test_loss:80.94496154785156\n",
            "2765/3000 train_loss: 21.787899017333984 test_loss:75.88862609863281\n",
            "2766/3000 train_loss: 25.605182647705078 test_loss:82.69749450683594\n",
            "2767/3000 train_loss: 18.127880096435547 test_loss:81.8158950805664\n",
            "2768/3000 train_loss: 20.828222274780273 test_loss:78.76282501220703\n",
            "2769/3000 train_loss: 27.23324203491211 test_loss:76.40031433105469\n",
            "2770/3000 train_loss: 24.5130615234375 test_loss:79.39183807373047\n",
            "2771/3000 train_loss: 32.07863235473633 test_loss:83.58275604248047\n",
            "2772/3000 train_loss: 31.767047882080078 test_loss:92.32423400878906\n",
            "2773/3000 train_loss: 25.96178436279297 test_loss:78.27561950683594\n",
            "2774/3000 train_loss: 22.2864990234375 test_loss:88.04131317138672\n",
            "2775/3000 train_loss: 28.959047317504883 test_loss:84.05435180664062\n",
            "2776/3000 train_loss: 28.402759552001953 test_loss:82.7649917602539\n",
            "2777/3000 train_loss: 35.84807205200195 test_loss:91.93849182128906\n",
            "2778/3000 train_loss: 31.686479568481445 test_loss:85.20114135742188\n",
            "2779/3000 train_loss: 27.982587814331055 test_loss:84.63775634765625\n",
            "2780/3000 train_loss: 26.606243133544922 test_loss:88.6077651977539\n",
            "2781/3000 train_loss: 26.716197967529297 test_loss:85.33361053466797\n",
            "2782/3000 train_loss: 30.36273956298828 test_loss:82.23820495605469\n",
            "2783/3000 train_loss: 26.15813636779785 test_loss:85.55682373046875\n",
            "2784/3000 train_loss: 24.66592788696289 test_loss:86.44453430175781\n",
            "2785/3000 train_loss: 22.70822525024414 test_loss:83.39002227783203\n",
            "2786/3000 train_loss: 29.493581771850586 test_loss:79.73681640625\n",
            "2787/3000 train_loss: 28.45740509033203 test_loss:83.95098876953125\n",
            "2788/3000 train_loss: 25.297649383544922 test_loss:79.36181640625\n",
            "2789/3000 train_loss: 22.831947326660156 test_loss:95.03517150878906\n",
            "2790/3000 train_loss: 25.49039649963379 test_loss:89.75154113769531\n",
            "2791/3000 train_loss: 27.970491409301758 test_loss:83.27091979980469\n",
            "2792/3000 train_loss: 28.739572525024414 test_loss:86.3651123046875\n",
            "2793/3000 train_loss: 33.16246032714844 test_loss:79.74922943115234\n",
            "2794/3000 train_loss: 25.421592712402344 test_loss:80.74317169189453\n",
            "2795/3000 train_loss: 27.14572525024414 test_loss:80.67764282226562\n",
            "2796/3000 train_loss: 23.18767738342285 test_loss:80.24847412109375\n",
            "2797/3000 train_loss: 23.937170028686523 test_loss:81.60816955566406\n",
            "2798/3000 train_loss: 27.14036750793457 test_loss:78.01692199707031\n",
            "2799/3000 train_loss: 24.338449478149414 test_loss:83.96874237060547\n",
            "2800/3000 train_loss: 24.712446212768555 test_loss:81.63961791992188\n",
            "2801/3000 train_loss: 20.96857452392578 test_loss:81.14917755126953\n",
            "2802/3000 train_loss: 23.809873580932617 test_loss:75.45536804199219\n",
            "2803/3000 train_loss: 29.989227294921875 test_loss:76.94316101074219\n",
            "2804/3000 train_loss: 24.598838806152344 test_loss:75.61943817138672\n",
            "2805/3000 train_loss: 28.788631439208984 test_loss:74.17446899414062\n",
            "2806/3000 train_loss: 24.071529388427734 test_loss:78.38180541992188\n",
            "2807/3000 train_loss: 33.60894775390625 test_loss:74.71603393554688\n",
            "2808/3000 train_loss: 24.349884033203125 test_loss:79.7837905883789\n",
            "2809/3000 train_loss: 24.6519775390625 test_loss:76.09068298339844\n",
            "2810/3000 train_loss: 27.164085388183594 test_loss:84.95758819580078\n",
            "2811/3000 train_loss: 31.747013092041016 test_loss:80.7918930053711\n",
            "2812/3000 train_loss: 22.86538314819336 test_loss:74.3722915649414\n",
            "2813/3000 train_loss: 25.92913818359375 test_loss:76.65644836425781\n",
            "2814/3000 train_loss: 30.98515510559082 test_loss:84.33218383789062\n",
            "2815/3000 train_loss: 22.48674201965332 test_loss:78.92691802978516\n",
            "2816/3000 train_loss: 27.042387008666992 test_loss:87.33244323730469\n",
            "2817/3000 train_loss: 26.66094970703125 test_loss:78.42880249023438\n",
            "2818/3000 train_loss: 22.651628494262695 test_loss:84.81055450439453\n",
            "2819/3000 train_loss: 27.454545974731445 test_loss:79.43920135498047\n",
            "2820/3000 train_loss: 25.803050994873047 test_loss:76.99470520019531\n",
            "2821/3000 train_loss: 28.228540420532227 test_loss:81.74310302734375\n",
            "2822/3000 train_loss: 30.213451385498047 test_loss:75.49954223632812\n",
            "2823/3000 train_loss: 26.988828659057617 test_loss:75.62939453125\n",
            "2824/3000 train_loss: 29.370235443115234 test_loss:84.14114379882812\n",
            "2825/3000 train_loss: 25.520925521850586 test_loss:85.84873962402344\n",
            "2826/3000 train_loss: 30.066431045532227 test_loss:77.08211517333984\n",
            "2827/3000 train_loss: 28.566959381103516 test_loss:73.260009765625\n",
            "2828/3000 train_loss: 25.117563247680664 test_loss:79.3200912475586\n",
            "2829/3000 train_loss: 26.991619110107422 test_loss:81.30535125732422\n",
            "2830/3000 train_loss: 24.19925308227539 test_loss:101.43286895751953\n",
            "2831/3000 train_loss: 28.373464584350586 test_loss:80.48948669433594\n",
            "2832/3000 train_loss: 27.992826461791992 test_loss:83.95600128173828\n",
            "2833/3000 train_loss: 22.19124984741211 test_loss:84.46723175048828\n",
            "2834/3000 train_loss: 28.199844360351562 test_loss:91.23944091796875\n",
            "2835/3000 train_loss: 43.174835205078125 test_loss:81.55224609375\n",
            "2836/3000 train_loss: 31.309755325317383 test_loss:86.71228790283203\n",
            "2837/3000 train_loss: 23.65924072265625 test_loss:92.43119812011719\n",
            "2838/3000 train_loss: 25.09432029724121 test_loss:90.49626159667969\n",
            "2839/3000 train_loss: 27.959156036376953 test_loss:98.07807922363281\n",
            "2840/3000 train_loss: 25.365270614624023 test_loss:94.97805786132812\n",
            "2841/3000 train_loss: 24.52838134765625 test_loss:82.2418212890625\n",
            "2842/3000 train_loss: 23.717220306396484 test_loss:92.9355239868164\n",
            "2843/3000 train_loss: 28.713455200195312 test_loss:88.30447387695312\n",
            "2844/3000 train_loss: 28.08055877685547 test_loss:88.01651000976562\n",
            "2845/3000 train_loss: 26.22437858581543 test_loss:83.46465301513672\n",
            "2846/3000 train_loss: 25.74717140197754 test_loss:89.16242218017578\n",
            "2847/3000 train_loss: 25.873743057250977 test_loss:79.2222900390625\n",
            "2848/3000 train_loss: 21.44231605529785 test_loss:81.027587890625\n",
            "2849/3000 train_loss: 26.023948669433594 test_loss:85.6385269165039\n",
            "2850/3000 train_loss: 22.524028778076172 test_loss:83.40623474121094\n",
            "2851/3000 train_loss: 20.201522827148438 test_loss:79.50859069824219\n",
            "2852/3000 train_loss: 27.56505584716797 test_loss:75.52611541748047\n",
            "2853/3000 train_loss: 25.092710494995117 test_loss:84.35064697265625\n",
            "2854/3000 train_loss: 30.350875854492188 test_loss:76.70536041259766\n",
            "2855/3000 train_loss: 30.967470169067383 test_loss:86.4364013671875\n",
            "2856/3000 train_loss: 26.95885467529297 test_loss:92.53247833251953\n",
            "2857/3000 train_loss: 26.603267669677734 test_loss:80.66608428955078\n",
            "2858/3000 train_loss: 26.318374633789062 test_loss:81.63098907470703\n",
            "2859/3000 train_loss: 26.06934928894043 test_loss:75.03599548339844\n",
            "2860/3000 train_loss: 24.511306762695312 test_loss:79.3305892944336\n",
            "2861/3000 train_loss: 26.406675338745117 test_loss:88.30899047851562\n",
            "2862/3000 train_loss: 24.590991973876953 test_loss:80.71907043457031\n",
            "2863/3000 train_loss: 24.319913864135742 test_loss:73.24224853515625\n",
            "2864/3000 train_loss: 23.95879364013672 test_loss:74.22303771972656\n",
            "2865/3000 train_loss: 27.77033233642578 test_loss:85.43302917480469\n",
            "2866/3000 train_loss: 23.64938735961914 test_loss:79.75186920166016\n",
            "2867/3000 train_loss: 29.346506118774414 test_loss:80.45314025878906\n",
            "2868/3000 train_loss: 27.037431716918945 test_loss:76.88606262207031\n",
            "2869/3000 train_loss: 22.813392639160156 test_loss:84.86506652832031\n",
            "2870/3000 train_loss: 30.38710594177246 test_loss:92.19357299804688\n",
            "2871/3000 train_loss: 27.98337173461914 test_loss:83.1728286743164\n",
            "2872/3000 train_loss: 25.111295700073242 test_loss:83.72994232177734\n",
            "2873/3000 train_loss: 23.592174530029297 test_loss:79.58184814453125\n",
            "2874/3000 train_loss: 28.6018009185791 test_loss:81.51927185058594\n",
            "2875/3000 train_loss: 24.614290237426758 test_loss:78.79130554199219\n",
            "2876/3000 train_loss: 29.942211151123047 test_loss:82.4372329711914\n",
            "2877/3000 train_loss: 24.612041473388672 test_loss:73.86149597167969\n",
            "2878/3000 train_loss: 23.988271713256836 test_loss:74.99896240234375\n",
            "2879/3000 train_loss: 28.436601638793945 test_loss:82.08863067626953\n",
            "2880/3000 train_loss: 27.118131637573242 test_loss:78.42518615722656\n",
            "2881/3000 train_loss: 24.506072998046875 test_loss:77.52193450927734\n",
            "2882/3000 train_loss: 29.824514389038086 test_loss:90.37511444091797\n",
            "2883/3000 train_loss: 24.798185348510742 test_loss:75.15337371826172\n",
            "2884/3000 train_loss: 22.55655860900879 test_loss:80.46491241455078\n",
            "2885/3000 train_loss: 25.442073822021484 test_loss:77.73027801513672\n",
            "2886/3000 train_loss: 20.869550704956055 test_loss:72.3392333984375\n",
            "2887/3000 train_loss: 28.429126739501953 test_loss:77.23345947265625\n",
            "2888/3000 train_loss: 24.13279151916504 test_loss:79.44010162353516\n",
            "2889/3000 train_loss: 20.501319885253906 test_loss:70.75657653808594\n",
            "2890/3000 train_loss: 29.422935485839844 test_loss:76.00389099121094\n",
            "2891/3000 train_loss: 25.27046012878418 test_loss:76.29145050048828\n",
            "2892/3000 train_loss: 24.109046936035156 test_loss:74.6906509399414\n",
            "2893/3000 train_loss: 27.402490615844727 test_loss:70.05762481689453\n",
            "2894/3000 train_loss: 25.34044647216797 test_loss:69.92866516113281\n",
            "2895/3000 train_loss: 27.268491744995117 test_loss:74.296630859375\n",
            "2896/3000 train_loss: 21.4155216217041 test_loss:76.11061096191406\n",
            "2897/3000 train_loss: 23.775049209594727 test_loss:82.59528350830078\n",
            "2898/3000 train_loss: 31.629724502563477 test_loss:78.97940826416016\n",
            "2899/3000 train_loss: 27.10776710510254 test_loss:75.58252716064453\n",
            "2900/3000 train_loss: 31.491207122802734 test_loss:73.80834197998047\n",
            "2901/3000 train_loss: 25.9388370513916 test_loss:88.5926284790039\n",
            "2902/3000 train_loss: 22.734281539916992 test_loss:91.3816146850586\n",
            "2903/3000 train_loss: 25.848718643188477 test_loss:87.98145294189453\n",
            "2904/3000 train_loss: 28.70718765258789 test_loss:75.79806518554688\n",
            "2905/3000 train_loss: 24.32042121887207 test_loss:75.49171447753906\n",
            "2906/3000 train_loss: 24.730478286743164 test_loss:92.11276245117188\n",
            "2907/3000 train_loss: 22.82974624633789 test_loss:77.36691284179688\n",
            "2908/3000 train_loss: 23.421154022216797 test_loss:81.95423889160156\n",
            "2909/3000 train_loss: 20.857810974121094 test_loss:74.20230102539062\n",
            "2910/3000 train_loss: 29.292491912841797 test_loss:82.88201904296875\n",
            "2911/3000 train_loss: 23.508708953857422 test_loss:80.71756744384766\n",
            "2912/3000 train_loss: 22.586673736572266 test_loss:71.3626937866211\n",
            "2913/3000 train_loss: 34.66104507446289 test_loss:84.28832244873047\n",
            "2914/3000 train_loss: 23.78020668029785 test_loss:85.02237701416016\n",
            "2915/3000 train_loss: 24.807086944580078 test_loss:86.63520050048828\n",
            "2916/3000 train_loss: 23.760231018066406 test_loss:74.70088195800781\n",
            "2917/3000 train_loss: 20.108217239379883 test_loss:80.5856704711914\n",
            "2918/3000 train_loss: 20.09979248046875 test_loss:77.36573791503906\n",
            "2919/3000 train_loss: 22.10365867614746 test_loss:84.808349609375\n",
            "2920/3000 train_loss: 25.433868408203125 test_loss:81.83306884765625\n",
            "2921/3000 train_loss: 35.006263732910156 test_loss:81.02684020996094\n",
            "2922/3000 train_loss: 35.05453872680664 test_loss:81.25849914550781\n",
            "2923/3000 train_loss: 28.338727951049805 test_loss:77.01915740966797\n",
            "2924/3000 train_loss: 27.793481826782227 test_loss:80.59966278076172\n",
            "2925/3000 train_loss: 22.297849655151367 test_loss:78.07819366455078\n",
            "2926/3000 train_loss: 23.3533992767334 test_loss:70.60320281982422\n",
            "2927/3000 train_loss: 27.605419158935547 test_loss:72.32269287109375\n",
            "2928/3000 train_loss: 30.460105895996094 test_loss:77.87409973144531\n",
            "2929/3000 train_loss: 23.853172302246094 test_loss:71.42237854003906\n",
            "2930/3000 train_loss: 24.991037368774414 test_loss:84.31439208984375\n",
            "2931/3000 train_loss: 22.66691017150879 test_loss:79.02815246582031\n",
            "2932/3000 train_loss: 22.16593360900879 test_loss:73.27672576904297\n",
            "2933/3000 train_loss: 23.103717803955078 test_loss:77.79906463623047\n",
            "2934/3000 train_loss: 21.88495445251465 test_loss:76.73469543457031\n",
            "2935/3000 train_loss: 32.632591247558594 test_loss:78.55570983886719\n",
            "2936/3000 train_loss: 23.36370849609375 test_loss:74.22755432128906\n",
            "2937/3000 train_loss: 27.540740966796875 test_loss:74.36382293701172\n",
            "2938/3000 train_loss: 26.29935073852539 test_loss:76.42022705078125\n",
            "2939/3000 train_loss: 21.163440704345703 test_loss:76.81957244873047\n",
            "2940/3000 train_loss: 21.163673400878906 test_loss:74.55690002441406\n",
            "2941/3000 train_loss: 21.624114990234375 test_loss:74.77142333984375\n",
            "2942/3000 train_loss: 23.536930084228516 test_loss:82.12294006347656\n",
            "2943/3000 train_loss: 20.419538497924805 test_loss:79.27295684814453\n",
            "2944/3000 train_loss: 22.086742401123047 test_loss:73.20806884765625\n",
            "2945/3000 train_loss: 26.567663192749023 test_loss:78.37178039550781\n",
            "2946/3000 train_loss: 24.431982040405273 test_loss:79.87142944335938\n",
            "2947/3000 train_loss: 21.174209594726562 test_loss:75.25747680664062\n",
            "2948/3000 train_loss: 25.74464988708496 test_loss:73.07942199707031\n",
            "2949/3000 train_loss: 25.14535903930664 test_loss:95.66243743896484\n",
            "2950/3000 train_loss: 27.661375045776367 test_loss:76.14625549316406\n",
            "2951/3000 train_loss: 29.9782772064209 test_loss:81.6524429321289\n",
            "2952/3000 train_loss: 28.709157943725586 test_loss:85.03218078613281\n",
            "2953/3000 train_loss: 25.77313995361328 test_loss:84.09357452392578\n",
            "2954/3000 train_loss: 20.764421463012695 test_loss:75.87069702148438\n",
            "2955/3000 train_loss: 27.585908889770508 test_loss:79.69117736816406\n",
            "2956/3000 train_loss: 26.677221298217773 test_loss:80.7594223022461\n",
            "2957/3000 train_loss: 22.57625961303711 test_loss:82.99212646484375\n",
            "2958/3000 train_loss: 28.070636749267578 test_loss:75.34866333007812\n",
            "2959/3000 train_loss: 22.035493850708008 test_loss:79.4453353881836\n",
            "2960/3000 train_loss: 25.91299819946289 test_loss:84.88038635253906\n",
            "2961/3000 train_loss: 24.14021110534668 test_loss:79.43986511230469\n",
            "2962/3000 train_loss: 25.94855308532715 test_loss:83.61151123046875\n",
            "2963/3000 train_loss: 28.074291229248047 test_loss:86.87938690185547\n",
            "2964/3000 train_loss: 21.444278717041016 test_loss:74.98664855957031\n",
            "2965/3000 train_loss: 22.267597198486328 test_loss:72.95632934570312\n",
            "2966/3000 train_loss: 24.021997451782227 test_loss:81.46253967285156\n",
            "2967/3000 train_loss: 21.886110305786133 test_loss:78.27021789550781\n",
            "2968/3000 train_loss: 22.99506187438965 test_loss:73.87828063964844\n",
            "2969/3000 train_loss: 19.846965789794922 test_loss:78.75963592529297\n",
            "2970/3000 train_loss: 23.386716842651367 test_loss:79.31517028808594\n",
            "2971/3000 train_loss: 24.559551239013672 test_loss:86.19419860839844\n",
            "2972/3000 train_loss: 25.262155532836914 test_loss:75.18878173828125\n",
            "2973/3000 train_loss: 24.56573486328125 test_loss:74.28082275390625\n",
            "2974/3000 train_loss: 23.206357955932617 test_loss:79.26551055908203\n",
            "2975/3000 train_loss: 25.237577438354492 test_loss:84.1921157836914\n",
            "2976/3000 train_loss: 22.646703720092773 test_loss:75.91393280029297\n",
            "2977/3000 train_loss: 24.92879867553711 test_loss:75.64295959472656\n",
            "2978/3000 train_loss: 21.293493270874023 test_loss:79.39077758789062\n",
            "2979/3000 train_loss: 23.944026947021484 test_loss:74.61477661132812\n",
            "2980/3000 train_loss: 22.193273544311523 test_loss:78.57209777832031\n",
            "2981/3000 train_loss: 22.71405029296875 test_loss:73.22862243652344\n",
            "2982/3000 train_loss: 22.5076904296875 test_loss:81.60655975341797\n",
            "2983/3000 train_loss: 29.179611206054688 test_loss:77.4437026977539\n",
            "2984/3000 train_loss: 23.46263313293457 test_loss:80.78199768066406\n",
            "2985/3000 train_loss: 25.448135375976562 test_loss:79.27691650390625\n",
            "2986/3000 train_loss: 22.213886260986328 test_loss:78.70272064208984\n",
            "2987/3000 train_loss: 23.86734962463379 test_loss:83.19978332519531\n",
            "2988/3000 train_loss: 23.734085083007812 test_loss:83.11154174804688\n",
            "2989/3000 train_loss: 18.20061492919922 test_loss:86.9522933959961\n",
            "2990/3000 train_loss: 22.350154876708984 test_loss:80.94647979736328\n",
            "2991/3000 train_loss: 24.99056625366211 test_loss:77.58460235595703\n",
            "2992/3000 train_loss: 23.546829223632812 test_loss:83.6340103149414\n",
            "2993/3000 train_loss: 23.216890335083008 test_loss:84.70374298095703\n",
            "2994/3000 train_loss: 21.629276275634766 test_loss:82.12679290771484\n",
            "2995/3000 train_loss: 23.448997497558594 test_loss:72.64374542236328\n",
            "2996/3000 train_loss: 24.417695999145508 test_loss:73.24877166748047\n",
            "2997/3000 train_loss: 22.987071990966797 test_loss:79.70875549316406\n",
            "2998/3000 train_loss: 21.70106315612793 test_loss:85.70878601074219\n",
            "2999/3000 train_loss: 21.51673698425293 test_loss:79.58492279052734\n",
            "3000/3000 train_loss: 19.274885177612305 test_loss:80.7620620727539\n"
          ]
        }
      ],
      "source": [
        "losses = train(model = unet, optimizer = optimizer, criterion=criterion, data_tr=train_data,\n",
        "               data_val = test_data, scheduler = scheduler,device = device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "6Ew7_F0-q7aL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e68becf0-414a-42e9-f970-80502a85e0cd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(80.7621)"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ],
      "source": [
        "avg_loss = 0\n",
        "# unet.eval()\n",
        "# unet.train()\n",
        "preds = []\n",
        "i = 0\n",
        "test_anomaly_losses = []\n",
        "test_normal_losses = []\n",
        "test_losses = []\n",
        "test_real = y_test.tolist()\n",
        "for batch in test_data:\n",
        "  with torch.no_grad():\n",
        "    # unet.train()\n",
        "    predictions = unet(batch.to(device)).cpu()\n",
        "    preds.append(predictions)\n",
        "  loss = criterion(predictions, batch.cpu())\n",
        "  for j in range(len(predictions)):\n",
        "    if int(y_test[i]) == 1:\n",
        "      test_anomaly_losses.append(float(criterion(predictions[j], batch[j])))\n",
        "    else:\n",
        "      test_normal_losses.append(float(criterion(predictions[j], batch[j])))\n",
        "    i += 1\n",
        "    test_losses.append(criterion(predictions[j], batch[j]))\n",
        "  # print(loss)\n",
        "  # print(loss)\n",
        "  avg_loss += loss / len(test_data)\n",
        "# avg_loss\n",
        "\n",
        "avg_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "VpDKorrRso9o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71901749-efa0-450e-bd90-9a00d5f277e3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(165.20969526290892, 38.72874658196061)"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ],
      "source": [
        "sum(test_anomaly_losses)/len(test_anomaly_losses) , sum(test_normal_losses)/len(test_normal_losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "cJE0-57Qts3E"
      },
      "outputs": [],
      "source": [
        "# torch.save(unet, \"unet_fan2_2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LEvbZKYuh7J",
        "outputId": "cd799d42-cadb-4342-951f-d19b519c24f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9213117283950618\n"
          ]
        }
      ],
      "source": [
        "results = []\n",
        "vals = np.arange(40, 900, 0.5).tolist()\n",
        "for threshold in vals:\n",
        "  preds = []\n",
        "  for j in range(len(test_losses)):\n",
        "    if test_losses[j] > threshold:\n",
        "      preds.append(1)\n",
        "    else:\n",
        "      preds.append(0)\n",
        "  \n",
        "  results.append(roc_auc_score(test_real,preds))\n",
        "\n",
        "print(max(results))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "W4H4vpFX35yK"
      },
      "outputs": [],
      "source": [
        "def get_logmelspectrogram(waveform):\n",
        "    melspec = librosa.feature.melspectrogram(y=waveform.numpy(), hop_length=250, n_mels = 304)\n",
        "\n",
        "    logmelspec = librosa.power_to_db(melspec)\n",
        "\n",
        "    return logmelspec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vo1-S_qcuUZR"
      },
      "outputs": [],
      "source": [
        "# train_logmelspecs, test_logmelspecs = mean_logmelspecs(df_train), mean_logmelspecs(df_test)\n",
        "train_data1 = []\n",
        "for wave in df_train:\n",
        "  train_data1.append(get_logmelspectrogram(wave)[0])\n",
        "\n",
        "test_data1 = []\n",
        "for wave in df_test:\n",
        "  test_data1.append(get_logmelspectrogram(wave)[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGd9oI5IEVMx",
        "outputId": "73a79d4e-9e50-4b29-e7ea-55ee60a08c22"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-30-68ec04120629>:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  train_data1 = torch.FloatTensor(train_data1)\n"
          ]
        }
      ],
      "source": [
        "train_data1 = torch.FloatTensor(train_data1)\n",
        "test_data1 = torch.FloatTensor(test_data1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMOi9331OVb4"
      },
      "outputs": [],
      "source": [
        "train_logs = DataLoader(train_data1.reshape(916*304,641),batch_size = 304)\n",
        "test_logs = DataLoader(test_data1.reshape(459*304,641),batch_size = 304)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9rD6tuI1rfe"
      },
      "outputs": [],
      "source": [
        "unet1 = UNet_FC(in_features=641).to(device)\n",
        "optimizer1 = Adam(params = unet1.parameters(), lr = 10e-3)\n",
        "# optimizer = Adam(params = unet.parameters())\n",
        "criterion1 = nn.MSELoss()\n",
        "scheduler1 = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer1, factor=0.5, \n",
        "                                                       min_lr=10e-4, mode = 'min',\n",
        "                                                       patience = 30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qr6k85ma3ftD",
        "outputId": "67174f45-79f9-41f1-8959-a0c75a2b17fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 train_loss: 21.703369140625 test_loss:17.446470260620117\n"
          ]
        }
      ],
      "source": [
        "losses = train(model = unet1, optimizer = optimizer1, criterion=criterion1, data_tr=train_logs,\n",
        "               data_val = test_logs, device = device, epochs = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrPbpKgSPx7v",
        "outputId": "a7bd10e8-e2bd-4579-8211-d6eaaa879711"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(17.4465)"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "avg_loss = 0\n",
        "# unet.eval()\n",
        "# unet.train()\n",
        "preds = []\n",
        "i = 0\n",
        "test_anomaly_losses = []\n",
        "test_normal_losses = []\n",
        "test_losses = []\n",
        "test_real = y_test.tolist()\n",
        "for batch in test_logs:\n",
        "  with torch.no_grad():\n",
        "    # unet.train()\n",
        "    predictions = unet1(batch.to(device)).cpu()\n",
        "    preds.append(predictions)\n",
        "    loss = criterion(predictions, batch.cpu())\n",
        "    test_losses.append(loss)\n",
        "    if int(y_test[i]) == 1:\n",
        "      test_anomaly_losses.append(loss)\n",
        "    else:\n",
        "      test_normal_losses.append(loss)\n",
        "    i += 1\n",
        "  # print(loss)\n",
        "  # print(loss)\n",
        "  avg_loss += loss / len(test_logs)\n",
        "# avg_loss\n",
        "\n",
        "avg_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Z5Z1XYFN_x2",
        "outputId": "9d767817-5525-443f-db72-2588edb4cfbc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor(18.0466), tensor(15.2920))"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sum(test_anomaly_losses)/len(test_anomaly_losses) , sum(test_normal_losses)/len(test_normal_losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "er74WfG7P_B1",
        "outputId": "884ef8f3-8bfe-4d71-bc01-534f5d5a0a03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.5875626740947075\n"
          ]
        }
      ],
      "source": [
        "results = []\n",
        "vals = np.arange(10, 21, 0.1).tolist()\n",
        "for threshold in vals:\n",
        "  preds = []\n",
        "  for j in range(len(test_losses)):\n",
        "    if test_losses[j] > threshold:\n",
        "      preds.append(1)\n",
        "    else:\n",
        "      preds.append(0)\n",
        "  \n",
        "  results.append(roc_auc_score(test_real,preds))\n",
        "\n",
        "print(max(results))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QaSSqG8SbAw2"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}