{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {
        "id": "9SStKf4G0V5H"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim import Adam\n",
        "import torchaudio\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from skimage.util import img_as_ubyte\n",
        "import pandas as pd\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import librosa\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import io\n",
        "from sklearn.metrics import roc_auc_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XtxbKLZq5KX",
        "outputId": "70b3b12a-86d9-4700-b3f3-0d21ffd2c95e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "execution_count": 185,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYxHegIM0Z4i",
        "outputId": "dc65c06f-5ba5-48f3-b132-4a69b3745d73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {
        "id": "h9DATQwS0ivD"
      },
      "outputs": [],
      "source": [
        "class MimiiDataset(Dataset):\n",
        "    def __init__(self,audio_dir, n_fft = 1024, win_length = 1024,\n",
        "                 hop_length = 512,power = 2,n_mels = 128,pad_mode = 'reflect',\n",
        "                 sr = 16000,center = True,norm = None):\n",
        "      \n",
        "        super(MimiiDataset, self).__init__()\n",
        "        self.audio_dir = audio_dir\n",
        "        self.n_mels = n_mels\n",
        "        self.n_fft = n_fft\n",
        "        self.win_length = win_length\n",
        "        self.hop_length = hop_length\n",
        "        self.power = power\n",
        "        self.pad_mode = pad_mode\n",
        "        self.sr = sr\n",
        "        self.center = center\n",
        "        self.norm = norm\n",
        "\n",
        "    def get_files(self):\n",
        "       return self.train_files, self.test_files\n",
        "    \n",
        "    def get_data(self,device, id):\n",
        "        \n",
        "        self.train_files, self.train_labels = self._train_file_list(device, id)\n",
        "        self.test_files, self.test_labels = self._test_file_list(device, id)\n",
        "        \n",
        "        self.train_data = self.get_audios(self.train_files)\n",
        "        self.test_data = self.get_audios(self.test_files)\n",
        "        \n",
        "        return self.train_data, self.test_data, self.train_labels, self.test_labels\n",
        "    \n",
        "    def _train_file_list(self, device, id):\n",
        "        query = os.path.abspath(\n",
        "            f\"{self.audio_dir}/{device}/train/normal_id_0{id}*.wav\"\n",
        "        )\n",
        "        train_normal_files = sorted(glob.glob(query))\n",
        "        train_normal_labels = np.zeros(len(train_normal_files))\n",
        "        \n",
        "        query = os.path.abspath(\n",
        "                f\"{self.audio_dir}/{device}/train/anomaly_id_0{id}*.wav\"\n",
        "            )\n",
        "        train_anomaly_files = sorted(glob.glob(query))\n",
        "        train_anomaly_labels = np.ones(len(train_anomaly_files))\n",
        "        \n",
        "        train_file_list = np.concatenate((train_normal_files, train_anomaly_files), axis=0)\n",
        "        train_labels = np.concatenate((train_normal_labels, train_anomaly_labels), axis=0)\n",
        "        \n",
        "        return train_file_list, train_labels\n",
        "    \n",
        "    def _test_file_list(self, device, id):     \n",
        "        query = os.path.abspath(\n",
        "            f\"{self.audio_dir}/{device}/test/normal_id_0{id}*.wav\"\n",
        "            )\n",
        "        test_normal_files = sorted(glob.glob(query))\n",
        "        test_normal_labels = np.zeros(len(test_normal_files))\n",
        "        \n",
        "        query = os.path.abspath(\n",
        "            f\"{self.audio_dir}/{device}/test/anomaly_id_0{id}*.wav\"\n",
        "            )\n",
        "        test_anomaly_files = sorted(glob.glob(query))\n",
        "        test_anomaly_labels = np.ones(len(test_anomaly_files))\n",
        "        \n",
        "        test_file_list = np.concatenate((test_normal_files, \n",
        "                                          test_anomaly_files), axis=0)\n",
        "        test_labels = np.concatenate((test_normal_labels,\n",
        "                                      test_anomaly_labels), axis=0)\n",
        "          \n",
        "        return test_file_list, test_labels\n",
        "\n",
        "    def normalize(self,tensor):\n",
        "        tensor_minusmean = tensor - tensor.mean()\n",
        "        return tensor_minusmean/np.absolute(tensor_minusmean).max()\n",
        "\n",
        "    def make0min(self,tensornd):\n",
        "        tensor = tensornd.numpy()\n",
        "        res = np.where(tensor == 0, 1E-19 , tensor)\n",
        "        return torch.from_numpy(res)\n",
        "\n",
        "    def spectrogrameToImage(self,specgram):\n",
        "        # specgram = torchaudio.transforms.MelSpectrogram(n_fft=1024, win_length=1024, \n",
        "        #                                                 hop_length=512, power=2, \n",
        "        #                                                 normalized=True, n_mels=128)(waveform )\n",
        "        specgram= self.make0min(specgram)\n",
        "        specgram = specgram.log2()[0,:,:].numpy()\n",
        "        \n",
        "        tr2image = transforms.Compose([transforms.ToPILImage()])\n",
        "\n",
        "        specgram= self.normalize(specgram)\n",
        "        # specgram = img_as_ubyte(specgram)\n",
        "        specgramImage = tr2image(specgram)\n",
        "        return specgramImage\n",
        "\n",
        "    def get_logmelspectrogram(self, waveform):\n",
        "        melspec = librosa.feature.melspectrogram(\n",
        "          n_fft=self.n_fft, win_length=self.win_length, hop_length=self.hop_length,\n",
        "          power=self.power,n_mels=self.n_mels,pad_mode=self.pad_mode,sr=self.sr,\n",
        "          center=self.center,norm=self.norm,htk=True,\n",
        "          y = waveform.numpy()\n",
        "        )\n",
        "\n",
        "        logmelspec = librosa.power_to_db(melspec)\n",
        "\n",
        "        return logmelspec\n",
        "\n",
        "    def get_melspectrogram(self,waveform):\n",
        "        melspec = librosa.feature.melspectrogram(\n",
        "            n_fft=self.n_fft, win_length=self.win_length, hop_length=self.hop_length,\n",
        "            power=self.power,n_mels=self.n_mels,pad_mode=self.pad_mode,sr=self.sr,\n",
        "            center=self.center,norm=self.norm,htk=True,\n",
        "            y = waveform.numpy()\n",
        "        )\n",
        "\n",
        "        return melspec\n",
        "    \n",
        "    def get_mfcc(self,waveform):\n",
        "        mfcc = librosa.feature.mfcc(    \n",
        "            n_fft=self.n_fft, win_length=self.win_length, \n",
        "            hop_length=self.hop_length,pad_mode=self.pad_mode,sr=self.sr,\n",
        "            center=self.center,norm=self.norm,n_mfcc=40,\n",
        "            y = waveform.numpy()\n",
        "        )\n",
        "\n",
        "        return mfcc\n",
        "\n",
        "    def get_chroma_stft(self,waveform):\n",
        "        stft = librosa.feature.chroma_stft(\n",
        "            n_fft=self.n_fft, win_length=self.win_length, \n",
        "            hop_length=self.hop_length,pad_mode=self.pad_mode,sr=self.sr,\n",
        "            center=self.center,norm=self.norm,n_chroma=12,\n",
        "            y=waveform.numpy()\n",
        "        )\n",
        "\n",
        "        return stft\n",
        "\n",
        "    def get_spectral_contrast(self,waveform):\n",
        "        spec_contrast = librosa.feature.spectral_contrast(    \n",
        "            n_fft=self.n_fft, win_length=self.win_length,center=self.center,\n",
        "            hop_length=self.hop_length,pad_mode=self.pad_mode,sr=self.sr,\n",
        "            y = waveform.numpy()\n",
        "        )\n",
        "\n",
        "        return spec_contrast\n",
        "    \n",
        "    def get_tonnetz(self,waveform):\n",
        "        harmonic = librosa.effects.harmonic(waveform.numpy())\n",
        "        tonnetz = librosa.feature.tonnetz(y=harmonic,sr=self.sr)\n",
        "\n",
        "        return tonnetz\n",
        "\n",
        "    def get_audios(self, file_list):\n",
        "        data = []\n",
        "        for i in range(len(file_list)):\n",
        "          y, sr = torchaudio.load(file_list[i])  \n",
        "          data.append(y)\n",
        "\n",
        "        return data\n",
        "    def _derive_data(self, file_list):\n",
        "        train_data = []\n",
        "        test_data = []\n",
        "        train_mode = True\n",
        "        for file_list in [self.train_files, self.test_files]:\n",
        "          tr2tensor = transforms.Compose([transforms.PILToTensor()])\n",
        "          data = []\n",
        "          for j in range(len(file_list)):\n",
        "            y, sr = torchaudio.load(file_list[j])  \n",
        "            spec = self.get_melspectrogram(y)\n",
        "            spec = self.spectrogrameToImage(spec)\n",
        "            spec = spec.convert('RGB')\n",
        "            vectors = tr2tensor(spec)\n",
        "            if train_mode:     \n",
        "              train_data.append(vectors)\n",
        "            else:\n",
        "              test_data.append(vectors)\n",
        "            \n",
        "          train_mode = False\n",
        "                \n",
        "        return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {
        "id": "S96soeIc0o13"
      },
      "outputs": [],
      "source": [
        "dataset = MimiiDataset('/content/drive/MyDrive/mimii')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "Gn2zdn92doi1"
      },
      "outputs": [],
      "source": [
        "# _, _, y_train_fan0, y_test_fan0 = dataset.get_data('ToyCar', 1)\n",
        "_, _, y_train_fan2, y_test_fan2 = dataset.get_data('ToyConveyor', 2)\n",
        "# _, _, y_train_fan4, y_test_fan4 = dataset.get_data('ToyCar', 3)\n",
        "# _, _, y_train_fan6, y_test_fan6 = dataset.get_data('ToyCar', 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "quhoMJUJsqyU"
      },
      "outputs": [],
      "source": [
        "def mean_mfccs(wave_list):\n",
        "  data = []\n",
        "  for wave in wave_list:\n",
        "    mfcc = np.mean(dataset.get_mfcc(wave)[0], axis = 1)\n",
        "    data.append(mfcc)\n",
        "  \n",
        "  return data\n",
        "\n",
        "def mean_stfts(wave_list):\n",
        "  data = []\n",
        "  for wave in wave_list:\n",
        "    stft = np.mean(dataset.get_chroma_stft(wave)[0], axis = 1)\n",
        "    data.append(stft)\n",
        "  \n",
        "  return data\n",
        "\n",
        "def mean_melspecs(wave_list):\n",
        "  data = []\n",
        "  for wave in wave_list:\n",
        "    melspec = np.mean(dataset.get_melspectrogram(wave)[0], axis = 1)\n",
        "    data.append(melspec)\n",
        "  \n",
        "  return data\n",
        "\n",
        "def mean_spec_contrasts(wave_list):\n",
        "  data = []\n",
        "  for wave in wave_list:\n",
        "    spec_contrast = np.mean(dataset.get_spectral_contrast(wave)[0], axis = 1)\n",
        "    data.append(spec_contrast)\n",
        "  \n",
        "  return data\n",
        "  \n",
        "def mean_tonnetzs(wave_list):\n",
        "  data = []\n",
        "  for wave in wave_list:\n",
        "    tonnetz = np.mean(dataset.get_tonnetz(wave)[0], axis = 1)\n",
        "    data.append(tonnetz)\n",
        "  \n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "C70jD80rrQJ_"
      },
      "outputs": [],
      "source": [
        "# df_train1, df_test1, y_train1, y_test1 = dataset.get_data('ToyConveyor', 1)\n",
        "# df_train2, df_test2, y_train2, y_test2 = dataset.get_data('ToyConveyor', 2)\n",
        "# df_train3, df_test3, y_train3, y_test3 = dataset.get_data('ToyConveyor', 3)\n",
        "df_train4, df_test4, y_train4, y_test4 = dataset.get_data('ToyConveyor', 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDPZ9Zua5M5C",
        "outputId": "2779c611-4b7a-49b4-8d4f-0ba8ee8ed730"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(df_test4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "5NAMxzpLrYJm"
      },
      "outputs": [],
      "source": [
        "train_melspecs, test_melspecs = mean_melspecs(df_train1), mean_melspecs(df_test1)\n",
        "train_mfccs, test_mfccs = mean_mfccs(df_train1), mean_mfccs(df_test1)\n",
        "train_stfts, test_stfts = mean_stfts(df_train1), mean_stfts(df_test1)\n",
        "train_spec_contrasts, test_spec_contrasts = mean_spec_contrasts(df_train1), mean_spec_contrasts(df_test1)\n",
        "train_tonnetzs, test_tonnetzs = mean_tonnetzs(df_train1), mean_tonnetzs(df_test1)\n",
        "\n",
        "train_mixed_f = []\n",
        "test_mixed_f = []\n",
        "\n",
        "for i in range(len(train_melspecs)):\n",
        "\n",
        "  train_mf = np.concatenate((train_melspecs[i],train_mfccs[i],train_stfts[i],\n",
        "                             train_spec_contrasts[i],train_tonnetzs[i])).tolist()\n",
        "\n",
        "  train_mixed_f.append(train_mf)\n",
        "\n",
        "for i in range(len(test_melspecs)):\n",
        "\n",
        "  test_mf = np.concatenate((test_melspecs[i],test_mfccs[i],test_stfts[i],\n",
        "                             test_spec_contrasts[i],test_tonnetzs[i])).tolist()\n",
        "\n",
        "  test_mixed_f.append(test_mf)\n",
        "\n",
        "train_mixed_f = torch.FloatTensor(train_mixed_f)\n",
        "test_mixed_f = torch.FloatTensor(test_mixed_f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "BIY-zDvjrIcu"
      },
      "outputs": [],
      "source": [
        "# torch.save(torch.FloatTensor(y_train_fan0.tolist()), '/content/drive/MyDrive/labels/y_tr_toycar1.pt')\n",
        "# torch.save(torch.FloatTensor(y_test_fan0.tolist()), '/content/drive/MyDrive/labels/y_ts_toycar1.pt')\n",
        "torch.save(torch.FloatTensor(y_train_fan2.tolist()), '/content/drive/MyDrive/labels/y_tr_toyconveyor2.pt')\n",
        "torch.save(torch.FloatTensor(y_test_fan2.tolist()), '/content/drive/MyDrive/labels/y_ts_toyconveyor2.pt')\n",
        "# torch.save(torch.FloatTensor(y_train_fan4.tolist()), '/content/drive/MyDrive/labels/y_tr_toycar3.pt')\n",
        "# torch.save(torch.FloatTensor(y_test_fan4.tolist()), '/content/drive/MyDrive/labels/y_ts_toycar3.pt')\n",
        "# torch.save(torch.FloatTensor(y_train_fan6.tolist()), '/content/drive/MyDrive/labels/y_tr_toycar4.pt')\n",
        "# torch.save(torch.FloatTensor(y_test_fan6.tolist()), '/content/drive/MyDrive/labels/y_ts_toycar4.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {
        "id": "siHf9ch8vFoI"
      },
      "outputs": [],
      "source": [
        "y_train, y_test = torch.load('/content/drive/MyDrive/labels/y_tr_toyconveyor2.pt'), torch.load('/content/drive/MyDrive/labels/y_ts_toyconveyor2.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {
        "id": "WiRjmJe_yH6u"
      },
      "outputs": [],
      "source": [
        "train_mf, test_mf = torch.load('/content/drive/MyDrive/mixed_features/train_mf_toyconveyor2.pt'), torch.load('/content/drive/MyDrive/mixed_features/test_mf_toyconveyor2.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {
        "id": "jWMPVGu1qiEq"
      },
      "outputs": [],
      "source": [
        "train_data = DataLoader(train_mf, batch_size=64, shuffle = True)\n",
        "test_data = DataLoader(test_mf, batch_size = 64, shuffle= False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {
        "id": "vNTBTRe6qnBq"
      },
      "outputs": [],
      "source": [
        "class UNet_FC(nn.Module):\n",
        "\n",
        "  def __init__(self, in_features):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.bn = nn.BatchNorm1d(128)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "    self.fc0 = nn.Linear(in_features=in_features,out_features=in_features)\n",
        "\n",
        "    # encoder\n",
        "    self.fc1 = nn.Linear(in_features=in_features, out_features=128)\n",
        "    self.fc2 = nn.Linear(in_features=128,out_features=128)\n",
        "    self.fc3 = nn.Linear(in_features=128, out_features=128)\n",
        "    self.fc4 = nn.Linear(in_features=128, out_features=128)\n",
        "    self.fc5 = nn.Linear(in_features=128, out_features=8)\n",
        "\n",
        "    # decoder\n",
        "    self.fc6 = nn.Linear(in_features=8, out_features=128)\n",
        "    self.fc7 = nn.Linear(in_features=128*2, out_features=128)\n",
        "    self.fc8 = nn.Linear(in_features=128*2, out_features=128)\n",
        "    self.fc9 = nn.Linear(in_features=128*2, out_features=128)\n",
        "\n",
        "    self.out = nn.Linear(in_features=128*2, out_features=in_features)\n",
        "\n",
        "  def encoder(self, x):\n",
        "    input = self.fc0(x)\n",
        "\n",
        "    x1 = self.relu(self.bn(self.fc1(input)))\n",
        "    x2 = self.relu(self.bn(self.fc2(x1)))\n",
        "    x3 = self.relu(self.bn(self.fc3(x2)))\n",
        "    x4 = self.relu(self.bn(self.fc4(x3)))\n",
        "    x5 = self.relu(self.fc5(x4))\n",
        "\n",
        "    return [x5, x4, x3, x2, x1]\n",
        "\n",
        "  def decoder(self, x):\n",
        "    x6 = self.relu(self.fc6(x[0]))\n",
        "    con1 = torch.cat((x6,x[1]), 1) \n",
        "    x7 = self.relu(self.bn(self.fc7(con1)))\n",
        "    con2 = torch.cat((x7,x[2]), 1)\n",
        "    x8 = self.relu(self.bn(self.fc8(con2)))\n",
        "    con3 = torch.cat((x8,x[3]), 1)\n",
        "    x9 = self.relu(self.bn(self.fc9(con3)))\n",
        "    con4 = torch.cat((x9,x[4]), 1)\n",
        "\n",
        "    x10 = self.out(con4)\n",
        "\n",
        "    return x10\n",
        "  \n",
        "  def forward(self, x):\n",
        "    # encoded = self.encoder(x)\n",
        "\n",
        "    # decoded = self.decoder(encoded)\n",
        "    input = self.fc0(x)\n",
        "\n",
        "    x1 = self.relu(self.bn(self.fc1(input)))\n",
        "    x2 = self.relu(self.bn(self.fc2(x1)))\n",
        "    x3 = self.relu(self.bn(self.fc3(x2)))\n",
        "    x4 = self.relu(self.bn(self.fc4(x3)))\n",
        "    x5 = self.relu(self.fc5(x4))\n",
        "\n",
        "    xy = [x5, x4, x3, x2, x1]\n",
        "\n",
        "    x6 = self.relu(self.fc6(xy[0]))\n",
        "    con1 = torch.cat((x6,xy[1]), 1) \n",
        "    x7 = self.relu(self.bn(self.fc7(con1)))\n",
        "    con2 = torch.cat((x7,xy[2]), 1)\n",
        "    x8 = self.relu(self.bn(self.fc8(con2)))\n",
        "    con3 = torch.cat((x8,xy[3]), 1)\n",
        "    x9 = self.relu(self.bn(self.fc9(con3)))\n",
        "    con4 = torch.cat((x9,xy[4]), 1)\n",
        "\n",
        "    x10 = self.out(con4)\n",
        "\n",
        "    # return decoded\n",
        "    return x10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {
        "id": "ZfgcBtQ3qn5l"
      },
      "outputs": [],
      "source": [
        "def train(model, optimizer, criterion, data_tr, data_val, scheduler = None,\n",
        "          epochs = 3000, device = 'cpu'):\n",
        "    # X_val, Y_val = next(iter(data_val))\n",
        "    losses = []\n",
        "    prev_avg_loss = 100000\n",
        "    for epoch in range(epochs):\n",
        "        train_avg_loss = 0\n",
        "        test_avg_loss = 0\n",
        "        # model.train()  # train mode\n",
        "        for batch in data_tr:\n",
        "          # data to device\n",
        "          batch = batch.to(device)\n",
        "          # set parameter gradients to zero\n",
        "          optimizer.zero_grad()\n",
        "          # forward\n",
        "          # print(Y_batch.shape)\n",
        "          predictions = model(batch)\n",
        "          loss = criterion(predictions, batch)\n",
        "          loss.backward() # backward-pass\n",
        "          optimizer.step()  # update weights\n",
        "          # calculate loss to show the user\n",
        "          if scheduler:\n",
        "            scheduler.step(loss)\n",
        "          train_avg_loss += loss / len(data_tr)\n",
        "\n",
        "        # model.eval()\n",
        "        for batch in data_val:\n",
        "          with torch.no_grad():\n",
        "            preds = model(batch.to(device)).cpu()\n",
        "            loss = criterion(preds,batch)\n",
        "            test_avg_loss += loss / len(data_val)\n",
        "                    \n",
        "        losses.append(train_avg_loss.item())\n",
        "        if (epoch+1)%100 == 0:\n",
        "          print(\"{}/{} train_loss: {} test_loss:{}\".format(epoch+1, epochs, train_avg_loss, test_avg_loss))\n",
        "        # if test_avg_loss < 70:\n",
        "        #   break\n",
        "    return losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {
        "id": "ptkVTF55quOL"
      },
      "outputs": [],
      "source": [
        "unet = UNet_FC(in_features=193).to(device)\n",
        "optimizer = Adam(params = unet.parameters(), lr = 10e-3)\n",
        "# optimizer = Adam(params = unet.parameters())\n",
        "criterion = nn.MSELoss()\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, \n",
        "                                                       min_lr=10e-4, mode = 'min',\n",
        "                                                       patience = 30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkfmYl9oXhcB",
        "outputId": "4b84c633-cfe5-486f-fe43-5acdc905d2cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100/3000 train_loss: 315.37255859375 test_loss:438.9318542480469\n",
            "200/3000 train_loss: 180.82762145996094 test_loss:305.8702392578125\n",
            "300/3000 train_loss: 128.97378540039062 test_loss:261.9559326171875\n",
            "400/3000 train_loss: 106.0121078491211 test_loss:227.0610809326172\n",
            "500/3000 train_loss: 97.91429138183594 test_loss:199.74057006835938\n",
            "600/3000 train_loss: 74.28448486328125 test_loss:164.73703002929688\n",
            "700/3000 train_loss: 67.63738250732422 test_loss:146.58656311035156\n",
            "800/3000 train_loss: 50.658592224121094 test_loss:134.86741638183594\n",
            "900/3000 train_loss: 57.494171142578125 test_loss:126.25639343261719\n",
            "1000/3000 train_loss: 44.01288604736328 test_loss:123.82656860351562\n",
            "1100/3000 train_loss: 39.84221267700195 test_loss:109.88436889648438\n",
            "1200/3000 train_loss: 41.39299011230469 test_loss:107.15560913085938\n",
            "1300/3000 train_loss: 38.01734924316406 test_loss:104.9056396484375\n",
            "1400/3000 train_loss: 33.95450973510742 test_loss:97.30458068847656\n",
            "1500/3000 train_loss: 34.34166717529297 test_loss:93.05233764648438\n",
            "1600/3000 train_loss: 26.960872650146484 test_loss:85.88546752929688\n",
            "1700/3000 train_loss: 29.36807632446289 test_loss:88.35247802734375\n",
            "1800/3000 train_loss: 27.139516830444336 test_loss:85.17095947265625\n",
            "1900/3000 train_loss: 24.6021785736084 test_loss:82.43350982666016\n",
            "2000/3000 train_loss: 22.983848571777344 test_loss:80.94144439697266\n",
            "2100/3000 train_loss: 20.480918884277344 test_loss:78.43572998046875\n",
            "2200/3000 train_loss: 24.980976104736328 test_loss:83.29927062988281\n",
            "2300/3000 train_loss: 22.385690689086914 test_loss:79.90972900390625\n",
            "2400/3000 train_loss: 19.534957885742188 test_loss:77.03335571289062\n",
            "2500/3000 train_loss: 18.86412239074707 test_loss:78.76044464111328\n",
            "2600/3000 train_loss: 23.919862747192383 test_loss:70.55313110351562\n",
            "2700/3000 train_loss: 14.502059936523438 test_loss:67.7763442993164\n",
            "2800/3000 train_loss: 16.762250900268555 test_loss:71.73548126220703\n",
            "2900/3000 train_loss: 31.62783432006836 test_loss:77.62551879882812\n",
            "3000/3000 train_loss: 17.680803298950195 test_loss:63.14591598510742\n"
          ]
        }
      ],
      "source": [
        "losses= train(model = unet, optimizer = optimizer, criterion=criterion, data_tr=train_data,\n",
        "               data_val = test_data, scheduler = scheduler, device = device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Ew7_F0-q7aL",
        "outputId": "c8bf0209-bdf3-452a-c742-846996969925"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(63.1459)"
            ]
          },
          "execution_count": 173,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "avg_loss = 0\n",
        "# unet.eval()\n",
        "# unet.train()\n",
        "preds = []\n",
        "i = 0\n",
        "test_anomaly_losses = []\n",
        "test_normal_losses = []\n",
        "test_losses = []\n",
        "# test_real = y_test_fan0.tolist()\n",
        "# y_test_fan0 = y_test_fan0.tolist()\n",
        "for batch in test_data:\n",
        "  with torch.no_grad():\n",
        "    # unet.train()\n",
        "    predictions = unet(batch.to(device)).cpu()\n",
        "    preds.append(predictions)\n",
        "  loss = criterion(predictions, batch.cpu())\n",
        "  for j in range(len(predictions)):\n",
        "    if int(y_test[i]) == 1:\n",
        "      test_anomaly_losses.append(float(criterion(predictions[j], batch[j])))\n",
        "    else:\n",
        "      test_normal_losses.append(float(criterion(predictions[j], batch[j])))\n",
        "    i += 1\n",
        "    test_losses.append(criterion(predictions[j], batch[j]))\n",
        "  # print(loss)\n",
        "  # print(loss)\n",
        "  avg_loss += loss / len(test_data)\n",
        "# avg_loss\n",
        "\n",
        "avg_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VpDKorrRso9o",
        "outputId": "5a5ddf05-9fa2-4773-cd6f-c9494b37fb65"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(51.44308523258693, 21.17981971502304)"
            ]
          },
          "execution_count": 174,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sum(test_anomaly_losses)/len(test_anomaly_losses) , sum(test_normal_losses)/len(test_normal_losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {
        "id": "cJE0-57Qts3E"
      },
      "outputs": [],
      "source": [
        "# torch.save(unet, \"unet_fan2_2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDIzf9t_H3pt",
        "outputId": "f6a24a44-816c-464d-bb26-933e6a95d7d8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "19"
            ]
          },
          "execution_count": 176,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LEvbZKYuh7J",
        "outputId": "c5ac607d-8600-4610-e70d-e8fdc63b47d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.8074471830985916\n"
          ]
        }
      ],
      "source": [
        "results = []\n",
        "vals = np.arange(20, 60, 0.5).tolist()\n",
        "for threshold in vals:\n",
        "  preds = []\n",
        "  for j in range(len(test_losses)):\n",
        "    if test_losses[j] > threshold:\n",
        "      preds.append(1)\n",
        "    else:\n",
        "      preds.append(0)\n",
        "  \n",
        "  results.append(roc_auc_score(y_test,preds))\n",
        "\n",
        "print(max(results))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4H4vpFX35yK"
      },
      "outputs": [],
      "source": [
        "def get_logmelspectrogram(waveform):\n",
        "    melspec = librosa.feature.melspectrogram(y=waveform.numpy(), hop_length=250, n_mels = 304)\n",
        "\n",
        "    logmelspec = librosa.power_to_db(melspec)\n",
        "\n",
        "    return logmelspec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vo1-S_qcuUZR"
      },
      "outputs": [],
      "source": [
        "# train_logmelspecs, test_logmelspecs = mean_logmelspecs(df_train), mean_logmelspecs(df_test)\n",
        "train_data1 = []\n",
        "for wave in df_train:\n",
        "  train_data1.append(get_logmelspectrogram(wave)[0])\n",
        "\n",
        "test_data1 = []\n",
        "for wave in df_test:\n",
        "  test_data1.append(get_logmelspectrogram(wave)[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGd9oI5IEVMx",
        "outputId": "73a79d4e-9e50-4b29-e7ea-55ee60a08c22"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-30-68ec04120629>:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  train_data1 = torch.FloatTensor(train_data1)\n"
          ]
        }
      ],
      "source": [
        "train_data1 = torch.FloatTensor(train_data1)\n",
        "test_data1 = torch.FloatTensor(test_data1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMOi9331OVb4"
      },
      "outputs": [],
      "source": [
        "train_logs = DataLoader(train_data1.reshape(916*304,641),batch_size = 304)\n",
        "test_logs = DataLoader(test_data1.reshape(459*304,641),batch_size = 304)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9rD6tuI1rfe"
      },
      "outputs": [],
      "source": [
        "unet1 = UNet_FC(in_features=641).to(device)\n",
        "optimizer1 = Adam(params = unet1.parameters(), lr = 10e-3)\n",
        "# optimizer = Adam(params = unet.parameters())\n",
        "criterion1 = nn.MSELoss()\n",
        "scheduler1 = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer1, factor=0.5, \n",
        "                                                       min_lr=10e-4, mode = 'min',\n",
        "                                                       patience = 30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qr6k85ma3ftD",
        "outputId": "67174f45-79f9-41f1-8959-a0c75a2b17fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 train_loss: 21.703369140625 test_loss:17.446470260620117\n"
          ]
        }
      ],
      "source": [
        "losses = train(model = unet1, optimizer = optimizer1, criterion=criterion1, data_tr=train_logs,\n",
        "               data_val = test_logs, device = device, epochs = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrPbpKgSPx7v",
        "outputId": "a7bd10e8-e2bd-4579-8211-d6eaaa879711"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(17.4465)"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "avg_loss = 0\n",
        "# unet.eval()\n",
        "# unet.train()\n",
        "preds = []\n",
        "i = 0\n",
        "test_anomaly_losses = []\n",
        "test_normal_losses = []\n",
        "test_losses = []\n",
        "test_real = y_test.tolist()\n",
        "for batch in test_logs:\n",
        "  with torch.no_grad():\n",
        "    # unet.train()\n",
        "    predictions = unet1(batch.to(device)).cpu()\n",
        "    preds.append(predictions)\n",
        "    loss = criterion(predictions, batch.cpu())\n",
        "    test_losses.append(loss)\n",
        "    if int(y_test[i]) == 1:\n",
        "      test_anomaly_losses.append(loss)\n",
        "    else:\n",
        "      test_normal_losses.append(loss)\n",
        "    i += 1\n",
        "  # print(loss)\n",
        "  # print(loss)\n",
        "  avg_loss += loss / len(test_logs)\n",
        "# avg_loss\n",
        "\n",
        "avg_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Z5Z1XYFN_x2",
        "outputId": "9d767817-5525-443f-db72-2588edb4cfbc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor(18.0466), tensor(15.2920))"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sum(test_anomaly_losses)/len(test_anomaly_losses) , sum(test_normal_losses)/len(test_normal_losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "er74WfG7P_B1",
        "outputId": "884ef8f3-8bfe-4d71-bc01-534f5d5a0a03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.5875626740947075\n"
          ]
        }
      ],
      "source": [
        "results = []\n",
        "vals = np.arange(10, 21, 0.1).tolist()\n",
        "for threshold in vals:\n",
        "  preds = []\n",
        "  for j in range(len(test_losses)):\n",
        "    if test_losses[j] > threshold:\n",
        "      preds.append(1)\n",
        "    else:\n",
        "      preds.append(0)\n",
        "  \n",
        "  results.append(roc_auc_score(test_real,preds))\n",
        "\n",
        "print(max(results))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QaSSqG8SbAw2"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
