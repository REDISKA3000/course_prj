{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "9SStKf4G0V5H"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "import torchaudio\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from skimage.util import img_as_ubyte\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import io\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_XtxbKLZq5KX",
    "outputId": "0c6de58c-f736-440b-85ce-a6ad2ae77c29"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CYxHegIM0Z4i",
    "outputId": "8b35e1ba-3d36-4aca-c569-e94fbad47a14"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive/\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "h9DATQwS0ivD"
   },
   "outputs": [],
   "source": [
    "class MimiiDataset(Dataset):\n",
    "    def __init__(self,audio_dir, n_fft = 1024, win_length = 1024,\n",
    "                 hop_length = 512,power = 2,n_mels = 128,pad_mode = 'reflect',\n",
    "                 sr = 16000,center = True,norm = None):\n",
    "      \n",
    "        super(MimiiDataset, self).__init__()\n",
    "        self.audio_dir = audio_dir\n",
    "        self.n_mels = n_mels\n",
    "        self.n_fft = n_fft\n",
    "        self.win_length = win_length\n",
    "        self.hop_length = hop_length\n",
    "        self.power = power\n",
    "        self.pad_mode = pad_mode\n",
    "        self.sr = sr\n",
    "        self.center = center\n",
    "        self.norm = norm\n",
    "\n",
    "    def get_files(self):\n",
    "       return self.train_files, self.test_files\n",
    "    \n",
    "    def get_data(self,device, id):\n",
    "        \n",
    "        self.train_files, self.train_labels = self._train_file_list(device, id)\n",
    "        self.test_files, self.test_labels = self._test_file_list(device, id)\n",
    "        \n",
    "        self.train_data = self.get_audios(self.train_files)\n",
    "        self.test_data = self.get_audios(self.test_files)\n",
    "        \n",
    "        return self.train_data, self.test_data, self.train_labels, self.test_labels\n",
    "    \n",
    "    def _train_file_list(self, device, id):\n",
    "        query = os.path.abspath(\n",
    "            f\"{self.audio_dir}/{device}/train/normal_id_0{id}*.wav\"\n",
    "        )\n",
    "        train_normal_files = sorted(glob.glob(query))\n",
    "        train_normal_labels = np.zeros(len(train_normal_files))\n",
    "        \n",
    "        query = os.path.abspath(\n",
    "                f\"{self.audio_dir}/{device}/train/anomaly_id_0{id}*.wav\"\n",
    "            )\n",
    "        train_anomaly_files = sorted(glob.glob(query))\n",
    "        train_anomaly_labels = np.ones(len(train_anomaly_files))\n",
    "        \n",
    "        train_file_list = np.concatenate((train_normal_files, train_anomaly_files), axis=0)\n",
    "        train_labels = np.concatenate((train_normal_labels, train_anomaly_labels), axis=0)\n",
    "        \n",
    "        return train_file_list, train_labels\n",
    "    \n",
    "    def _test_file_list(self, device, id):     \n",
    "        query = os.path.abspath(\n",
    "            f\"{self.audio_dir}/{device}/test/normal_id_0{id}*.wav\"\n",
    "            )\n",
    "        test_normal_files = sorted(glob.glob(query))\n",
    "        test_normal_labels = np.zeros(len(test_normal_files))\n",
    "        \n",
    "        query = os.path.abspath(\n",
    "            f\"{self.audio_dir}/{device}/test/anomaly_id_0{id}*.wav\"\n",
    "            )\n",
    "        test_anomaly_files = sorted(glob.glob(query))\n",
    "        test_anomaly_labels = np.ones(len(test_anomaly_files))\n",
    "        \n",
    "        test_file_list = np.concatenate((test_normal_files, \n",
    "                                          test_anomaly_files), axis=0)\n",
    "        test_labels = np.concatenate((test_normal_labels,\n",
    "                                      test_anomaly_labels), axis=0)\n",
    "          \n",
    "        return test_file_list, test_labels\n",
    "\n",
    "    def normalize(self,tensor):\n",
    "        tensor_minusmean = tensor - tensor.mean()\n",
    "        return tensor_minusmean/np.absolute(tensor_minusmean).max()\n",
    "\n",
    "    def make0min(self,tensornd):\n",
    "        tensor = tensornd.numpy()\n",
    "        res = np.where(tensor == 0, 1E-19 , tensor)\n",
    "        return torch.from_numpy(res)\n",
    "\n",
    "    def spectrogrameToImage(self,specgram):\n",
    "        # specgram = torchaudio.transforms.MelSpectrogram(n_fft=1024, win_length=1024, \n",
    "        #                                                 hop_length=512, power=2, \n",
    "        #                                                 normalized=True, n_mels=128)(waveform )\n",
    "        specgram= self.make0min(specgram)\n",
    "        specgram = specgram.log2()[0,:,:].numpy()\n",
    "        \n",
    "        tr2image = transforms.Compose([transforms.ToPILImage()])\n",
    "\n",
    "        specgram= self.normalize(specgram)\n",
    "        # specgram = img_as_ubyte(specgram)\n",
    "        specgramImage = tr2image(specgram)\n",
    "        return specgramImage\n",
    "\n",
    "    def get_logmelspectrogram(self, waveform):\n",
    "        melspec = librosa.feature.melspectrogram(\n",
    "          n_fft=self.n_fft, win_length=self.win_length, hop_length=self.hop_length,\n",
    "          power=self.power,n_mels=self.n_mels,pad_mode=self.pad_mode,sr=self.sr,\n",
    "          center=self.center,norm=self.norm,htk=True,\n",
    "          y = waveform.numpy()\n",
    "        )\n",
    "\n",
    "        logmelspec = librosa.power_to_db(melspec)\n",
    "\n",
    "        return logmelspec\n",
    "\n",
    "    def get_melspectrogram(self,waveform):\n",
    "        melspec = librosa.feature.melspectrogram(\n",
    "            n_fft=self.n_fft, win_length=self.win_length, hop_length=self.hop_length,\n",
    "            power=self.power,n_mels=self.n_mels,pad_mode=self.pad_mode,sr=self.sr,\n",
    "            center=self.center,norm=self.norm,htk=True,\n",
    "            y = waveform.numpy()\n",
    "        )\n",
    "\n",
    "        return melspec\n",
    "    \n",
    "    def get_mfcc(self,waveform):\n",
    "        mfcc = librosa.feature.mfcc(    \n",
    "            n_fft=self.n_fft, win_length=self.win_length, \n",
    "            hop_length=self.hop_length,pad_mode=self.pad_mode,sr=self.sr,\n",
    "            center=self.center,norm=self.norm,n_mfcc=40,\n",
    "            y = waveform.numpy()\n",
    "        )\n",
    "\n",
    "        return mfcc\n",
    "\n",
    "    def get_chroma_stft(self,waveform):\n",
    "        stft = librosa.feature.chroma_stft(\n",
    "            n_fft=self.n_fft, win_length=self.win_length, \n",
    "            hop_length=self.hop_length,pad_mode=self.pad_mode,sr=self.sr,\n",
    "            center=self.center,norm=self.norm,n_chroma=12,\n",
    "            y=waveform.numpy()\n",
    "        )\n",
    "\n",
    "        return stft\n",
    "\n",
    "    def get_spectral_contrast(self,waveform):\n",
    "        spec_contrast = librosa.feature.spectral_contrast(    \n",
    "            n_fft=self.n_fft, win_length=self.win_length,center=self.center,\n",
    "            hop_length=self.hop_length,pad_mode=self.pad_mode,sr=self.sr,\n",
    "            y = waveform.numpy()\n",
    "        )\n",
    "\n",
    "        return spec_contrast\n",
    "    \n",
    "    def get_tonnetz(self,waveform):\n",
    "        harmonic = librosa.effects.harmonic(waveform.numpy())\n",
    "        tonnetz = librosa.feature.tonnetz(y=harmonic,sr=self.sr)\n",
    "\n",
    "        return tonnetz\n",
    "\n",
    "    def get_audios(self, file_list):\n",
    "        data = []\n",
    "        for i in range(len(file_list)):\n",
    "          y, sr = torchaudio.load(file_list[i])  \n",
    "          data.append(y)\n",
    "\n",
    "        return data\n",
    "    def _derive_data(self, file_list):\n",
    "        train_data = []\n",
    "        test_data = []\n",
    "        train_mode = True\n",
    "        for file_list in [self.train_files, self.test_files]:\n",
    "          tr2tensor = transforms.Compose([transforms.PILToTensor()])\n",
    "          data = []\n",
    "          for j in range(len(file_list)):\n",
    "            y, sr = torchaudio.load(file_list[j])  \n",
    "            spec = self.get_melspectrogram(y)\n",
    "            spec = self.spectrogrameToImage(spec)\n",
    "            spec = spec.convert('RGB')\n",
    "            vectors = tr2tensor(spec)\n",
    "            if train_mode:     \n",
    "              train_data.append(vectors)\n",
    "            else:\n",
    "              test_data.append(vectors)\n",
    "            \n",
    "          train_mode = False\n",
    "                \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "S96soeIc0o13"
   },
   "outputs": [],
   "source": [
    "dataset = MimiiDataset('/content/drive/MyDrive/mimii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "Gn2zdn92doi1"
   },
   "outputs": [],
   "source": [
    "_, _, y_train, y_test = dataset.get_data('ToyConveyor', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "id": "SgjpeWy_RV1C"
   },
   "outputs": [],
   "source": [
    "train_mixed_f = torch.load('/content/drive/MyDrive/mixed_features/train_mf_toyconveyor3.pt')\n",
    "test_mixed_f = torch.load('/content/drive/MyDrive/mixed_features/test_mf_toyconveyor3.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "id": "jWMPVGu1qiEq"
   },
   "outputs": [],
   "source": [
    "train_data = DataLoader(train_mixed_f, batch_size=64, shuffle = True)\n",
    "test_data = DataLoader(test_mixed_f, batch_size = 64, shuffle= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "id": "vNTBTRe6qnBq"
   },
   "outputs": [],
   "source": [
    "class UNet_FC(nn.Module):\n",
    "\n",
    "  def __init__(self, in_features):\n",
    "    super().__init__()\n",
    "    \n",
    "    self.bn = nn.BatchNorm1d(128)\n",
    "    self.relu = nn.ReLU()\n",
    "\n",
    "    self.fc0 = nn.Linear(in_features=in_features,out_features=in_features)\n",
    "\n",
    "    # encoder\n",
    "    self.fc1 = nn.Linear(in_features=in_features, out_features=128)\n",
    "    self.fc2 = nn.Linear(in_features=128,out_features=128)\n",
    "    self.fc3 = nn.Linear(in_features=128, out_features=128)\n",
    "    self.fc4 = nn.Linear(in_features=128, out_features=128)\n",
    "    self.fc5 = nn.Linear(in_features=128, out_features=8)\n",
    "\n",
    "    # decoder\n",
    "    self.fc6 = nn.Linear(in_features=8, out_features=128)\n",
    "    self.fc7 = nn.Linear(in_features=128*2, out_features=128)\n",
    "    self.fc8 = nn.Linear(in_features=128*2, out_features=128)\n",
    "    self.fc9 = nn.Linear(in_features=128*2, out_features=128)\n",
    "\n",
    "    self.out = nn.Linear(in_features=128*2, out_features=in_features)\n",
    "\n",
    "  def forward(self, x):\n",
    "    input = self.fc0(x)\n",
    "\n",
    "    x1 = self.relu(self.bn(self.fc1(input)))\n",
    "    x2 = self.relu(self.bn(self.fc2(x1)))\n",
    "    x3 = self.relu(self.bn(self.fc3(x2)))\n",
    "    x4 = self.relu(self.bn(self.fc4(x3)))\n",
    "    x5 = self.relu(self.fc5(x4))\n",
    "\n",
    "    xy = [x5, x4, x3, x2, x1]\n",
    "\n",
    "    x6 = self.relu(self.fc6(xy[0]))\n",
    "    con1 = torch.cat((x6,xy[1]), 1) \n",
    "    x7 = self.relu(self.bn(self.fc7(con1)))\n",
    "    con2 = torch.cat((x7,xy[2]), 1)\n",
    "    x8 = self.relu(self.bn(self.fc8(con2)))\n",
    "    con3 = torch.cat((x8,xy[3]), 1)\n",
    "    x9 = self.relu(self.bn(self.fc9(con3)))\n",
    "    con4 = torch.cat((x9,xy[4]), 1)\n",
    "\n",
    "    x10 = self.out(con4)\n",
    "\n",
    "    return x10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "id": "ZfgcBtQ3qn5l"
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, data_tr, data_val, scheduler = None,\n",
    "          epochs = 3000, device = 'cpu'):\n",
    "    # X_val, Y_val = next(iter(data_val))\n",
    "    losses = []\n",
    "    prev_avg_loss = 100000\n",
    "    for epoch in range(epochs):\n",
    "        train_avg_loss = 0\n",
    "        test_avg_loss = 0\n",
    "        # model.train()  # train mode\n",
    "        for batch in data_tr:\n",
    "          # data to device\n",
    "          batch = batch.to(device)\n",
    "          # set parameter gradients to zero\n",
    "          optimizer.zero_grad()\n",
    "          # forward\n",
    "          # print(Y_batch.shape)\n",
    "          predictions = model(batch)\n",
    "          loss = criterion(predictions, batch)\n",
    "          loss.backward() # backward-pass\n",
    "          optimizer.step()  # update weights\n",
    "          # calculate loss to show the user\n",
    "          if scheduler:\n",
    "            scheduler.step(loss)\n",
    "          train_avg_loss += loss / len(data_tr)\n",
    "\n",
    "        # model.eval()\n",
    "        for batch in data_val:\n",
    "          with torch.no_grad():\n",
    "            preds = model(batch.to(device)).cpu()\n",
    "            loss = criterion(preds,batch)\n",
    "            test_avg_loss += loss / len(data_val)\n",
    "                    \n",
    "        losses.append(train_avg_loss.item())\n",
    "        # if (epoch+1)%50 == 0:\n",
    "        print(\"{}/{} train_loss: {} test_loss:{}\".format(epoch+1, epochs, train_avg_loss, test_avg_loss))\n",
    "        # if test_avg_loss < 70:\n",
    "        #   break\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "id": "ptkVTF55quOL"
   },
   "outputs": [],
   "source": [
    "unet = UNet_FC(in_features=193).to(device)\n",
    "optimizer = Adam(params = unet.parameters(), lr = 10e-3)\n",
    "# optimizer = Adam(params = unet.parameters())\n",
    "criterion = nn.MSELoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, \n",
    "                                                       min_lr=10e-4, mode = 'min',\n",
    "                                                       patience = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QkfmYl9oXhcB",
    "outputId": "e1af99f1-f2b1-4a1a-89db-6a08611d2801"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1/3000 train_loss: 174483.75 test_loss:169948.46875\n",
      "2/3000 train_loss: 170523.078125 test_loss:165456.734375\n",
      "3/3000 train_loss: 165423.171875 test_loss:159649.046875\n",
      "4/3000 train_loss: 158895.578125 test_loss:151968.03125\n",
      "5/3000 train_loss: 150305.078125 test_loss:142444.328125\n",
      "6/3000 train_loss: 139603.359375 test_loss:130516.0546875\n",
      "7/3000 train_loss: 126904.578125 test_loss:117401.0078125\n",
      "8/3000 train_loss: 112923.984375 test_loss:103417.3984375\n",
      "9/3000 train_loss: 98690.421875 test_loss:89099.265625\n",
      "10/3000 train_loss: 83973.6328125 test_loss:75354.3203125\n",
      "11/3000 train_loss: 70733.609375 test_loss:63278.4921875\n",
      "12/3000 train_loss: 58519.4609375 test_loss:51784.03515625\n",
      "13/3000 train_loss: 47346.09765625 test_loss:42102.5390625\n",
      "14/3000 train_loss: 38287.81640625 test_loss:33687.203125\n",
      "15/3000 train_loss: 30257.79296875 test_loss:26301.810546875\n",
      "16/3000 train_loss: 23114.57421875 test_loss:20580.390625\n",
      "17/3000 train_loss: 17818.095703125 test_loss:15488.546875\n",
      "18/3000 train_loss: 12935.1181640625 test_loss:11644.0732421875\n",
      "19/3000 train_loss: 9228.5830078125 test_loss:8594.234375\n",
      "20/3000 train_loss: 6664.8974609375 test_loss:6463.982421875\n",
      "21/3000 train_loss: 4780.5703125 test_loss:4908.33984375\n",
      "22/3000 train_loss: 3372.69970703125 test_loss:4006.49609375\n",
      "23/3000 train_loss: 2364.643310546875 test_loss:3182.669189453125\n",
      "24/3000 train_loss: 2568.710205078125 test_loss:3021.517578125\n",
      "25/3000 train_loss: 1730.72705078125 test_loss:2463.63818359375\n",
      "26/3000 train_loss: 1408.203369140625 test_loss:2078.46240234375\n",
      "27/3000 train_loss: 966.4281005859375 test_loss:1993.68408203125\n",
      "28/3000 train_loss: 852.3289794921875 test_loss:1801.60791015625\n",
      "29/3000 train_loss: 780.339111328125 test_loss:1837.65673828125\n",
      "30/3000 train_loss: 773.159423828125 test_loss:1629.4117431640625\n",
      "31/3000 train_loss: 600.2206420898438 test_loss:1666.0447998046875\n",
      "32/3000 train_loss: 753.382080078125 test_loss:1540.5966796875\n",
      "33/3000 train_loss: 622.0230712890625 test_loss:1487.818603515625\n",
      "34/3000 train_loss: 577.2739868164062 test_loss:1440.416015625\n",
      "35/3000 train_loss: 613.3933715820312 test_loss:1450.9085693359375\n",
      "36/3000 train_loss: 602.8079833984375 test_loss:1425.4500732421875\n",
      "37/3000 train_loss: 516.1997680664062 test_loss:1393.61181640625\n",
      "38/3000 train_loss: 542.1517333984375 test_loss:1383.16552734375\n",
      "39/3000 train_loss: 489.8428039550781 test_loss:1334.9072265625\n",
      "40/3000 train_loss: 495.2994689941406 test_loss:1346.628173828125\n",
      "41/3000 train_loss: 535.0987548828125 test_loss:1332.3194580078125\n",
      "42/3000 train_loss: 515.2144165039062 test_loss:1319.135986328125\n",
      "43/3000 train_loss: 609.6013793945312 test_loss:1297.892578125\n",
      "44/3000 train_loss: 449.30364990234375 test_loss:1275.833251953125\n",
      "45/3000 train_loss: 472.3314514160156 test_loss:1235.614990234375\n",
      "46/3000 train_loss: 489.9756774902344 test_loss:1224.9873046875\n",
      "47/3000 train_loss: 458.6208190917969 test_loss:1253.83984375\n",
      "48/3000 train_loss: 490.3837585449219 test_loss:1225.30615234375\n",
      "49/3000 train_loss: 496.613037109375 test_loss:1216.928466796875\n",
      "50/3000 train_loss: 525.8973999023438 test_loss:1216.0576171875\n",
      "51/3000 train_loss: 537.360107421875 test_loss:1210.00830078125\n",
      "52/3000 train_loss: 502.776123046875 test_loss:1206.1724853515625\n",
      "53/3000 train_loss: 437.955322265625 test_loss:1205.63525390625\n",
      "54/3000 train_loss: 568.0687866210938 test_loss:1150.53173828125\n",
      "55/3000 train_loss: 519.9094848632812 test_loss:1124.08447265625\n",
      "56/3000 train_loss: 442.73193359375 test_loss:1148.8372802734375\n",
      "57/3000 train_loss: 427.0265808105469 test_loss:1113.8621826171875\n",
      "58/3000 train_loss: 423.412353515625 test_loss:1116.0810546875\n",
      "59/3000 train_loss: 437.400146484375 test_loss:1116.003173828125\n",
      "60/3000 train_loss: 391.42529296875 test_loss:1115.462158203125\n",
      "61/3000 train_loss: 473.46905517578125 test_loss:1109.9888916015625\n",
      "62/3000 train_loss: 406.5426330566406 test_loss:1107.84423828125\n",
      "63/3000 train_loss: 387.7052307128906 test_loss:1104.8314208984375\n",
      "64/3000 train_loss: 413.8433837890625 test_loss:1102.4644775390625\n",
      "65/3000 train_loss: 439.73358154296875 test_loss:1074.92626953125\n",
      "66/3000 train_loss: 442.02728271484375 test_loss:1084.23583984375\n",
      "67/3000 train_loss: 407.2757263183594 test_loss:1088.727783203125\n",
      "68/3000 train_loss: 391.4261169433594 test_loss:1058.1968994140625\n",
      "69/3000 train_loss: 425.7530517578125 test_loss:1051.208251953125\n",
      "70/3000 train_loss: 457.08294677734375 test_loss:1052.5889892578125\n",
      "71/3000 train_loss: 398.57861328125 test_loss:1049.361083984375\n",
      "72/3000 train_loss: 377.5718688964844 test_loss:1022.26318359375\n",
      "73/3000 train_loss: 409.2069091796875 test_loss:1020.4420166015625\n",
      "74/3000 train_loss: 455.7118225097656 test_loss:1009.6695556640625\n",
      "75/3000 train_loss: 392.0384216308594 test_loss:998.5203857421875\n",
      "76/3000 train_loss: 369.7993469238281 test_loss:994.1966552734375\n",
      "77/3000 train_loss: 367.88922119140625 test_loss:992.6929931640625\n",
      "78/3000 train_loss: 378.1131896972656 test_loss:995.4541015625\n",
      "79/3000 train_loss: 389.830078125 test_loss:1020.2759399414062\n",
      "80/3000 train_loss: 357.5821533203125 test_loss:992.6607666015625\n",
      "81/3000 train_loss: 387.8507385253906 test_loss:1022.7186279296875\n",
      "82/3000 train_loss: 393.8497314453125 test_loss:984.9173583984375\n",
      "83/3000 train_loss: 362.0075378417969 test_loss:985.9976806640625\n",
      "84/3000 train_loss: 359.4227294921875 test_loss:954.1438598632812\n",
      "85/3000 train_loss: 378.0640869140625 test_loss:954.5023803710938\n",
      "86/3000 train_loss: 370.315185546875 test_loss:953.7822265625\n",
      "87/3000 train_loss: 372.0581970214844 test_loss:953.4063720703125\n",
      "88/3000 train_loss: 369.1414794921875 test_loss:955.2555541992188\n",
      "89/3000 train_loss: 368.5514831542969 test_loss:949.6033325195312\n",
      "90/3000 train_loss: 469.6083679199219 test_loss:934.8394775390625\n",
      "91/3000 train_loss: 394.6695861816406 test_loss:948.3067626953125\n",
      "92/3000 train_loss: 404.95025634765625 test_loss:942.900390625\n",
      "93/3000 train_loss: 364.1508483886719 test_loss:910.7874755859375\n",
      "94/3000 train_loss: 433.0400085449219 test_loss:906.375732421875\n",
      "95/3000 train_loss: 378.8965148925781 test_loss:904.4342041015625\n",
      "96/3000 train_loss: 355.58465576171875 test_loss:932.1939086914062\n",
      "97/3000 train_loss: 393.6438903808594 test_loss:942.0306396484375\n",
      "98/3000 train_loss: 347.52069091796875 test_loss:898.2784423828125\n",
      "99/3000 train_loss: 343.6173400878906 test_loss:895.30712890625\n",
      "100/3000 train_loss: 355.5278625488281 test_loss:882.752197265625\n",
      "101/3000 train_loss: 342.971435546875 test_loss:884.9666748046875\n",
      "102/3000 train_loss: 345.6731872558594 test_loss:878.9961547851562\n",
      "103/3000 train_loss: 337.90478515625 test_loss:871.5267333984375\n",
      "104/3000 train_loss: 348.6815185546875 test_loss:879.0014038085938\n",
      "105/3000 train_loss: 386.5630187988281 test_loss:880.37353515625\n",
      "106/3000 train_loss: 362.412353515625 test_loss:865.449462890625\n",
      "107/3000 train_loss: 333.0746154785156 test_loss:865.5637817382812\n",
      "108/3000 train_loss: 341.03253173828125 test_loss:838.3545532226562\n",
      "109/3000 train_loss: 318.6444396972656 test_loss:836.6649780273438\n",
      "110/3000 train_loss: 361.53448486328125 test_loss:794.4658203125\n",
      "111/3000 train_loss: 339.44403076171875 test_loss:794.908447265625\n",
      "112/3000 train_loss: 318.9593811035156 test_loss:792.62451171875\n",
      "113/3000 train_loss: 325.7616882324219 test_loss:790.4885864257812\n",
      "114/3000 train_loss: 360.0761413574219 test_loss:788.4107666015625\n",
      "115/3000 train_loss: 356.16571044921875 test_loss:791.8529052734375\n",
      "116/3000 train_loss: 418.013916015625 test_loss:765.40966796875\n",
      "117/3000 train_loss: 316.77374267578125 test_loss:720.2464599609375\n",
      "118/3000 train_loss: 329.9604187011719 test_loss:732.6358642578125\n",
      "119/3000 train_loss: 300.1819152832031 test_loss:740.4149780273438\n",
      "120/3000 train_loss: 325.7342834472656 test_loss:723.8358154296875\n",
      "121/3000 train_loss: 342.08026123046875 test_loss:731.8120727539062\n",
      "122/3000 train_loss: 315.83575439453125 test_loss:722.9627685546875\n",
      "123/3000 train_loss: 321.8075256347656 test_loss:733.14453125\n",
      "124/3000 train_loss: 317.47467041015625 test_loss:711.47900390625\n",
      "125/3000 train_loss: 326.3553771972656 test_loss:708.7905883789062\n",
      "126/3000 train_loss: 308.0764465332031 test_loss:700.2879028320312\n",
      "127/3000 train_loss: 330.6263427734375 test_loss:707.2033081054688\n",
      "128/3000 train_loss: 287.0151672363281 test_loss:676.7034912109375\n",
      "129/3000 train_loss: 294.4009704589844 test_loss:660.5316162109375\n",
      "130/3000 train_loss: 302.6566162109375 test_loss:654.08056640625\n",
      "131/3000 train_loss: 287.99969482421875 test_loss:653.95361328125\n",
      "132/3000 train_loss: 319.2454528808594 test_loss:659.2919921875\n",
      "133/3000 train_loss: 309.2535400390625 test_loss:654.6298217773438\n",
      "134/3000 train_loss: 267.0060119628906 test_loss:633.332763671875\n",
      "135/3000 train_loss: 274.3094482421875 test_loss:634.36328125\n",
      "136/3000 train_loss: 301.07098388671875 test_loss:655.84912109375\n",
      "137/3000 train_loss: 302.6365661621094 test_loss:648.8477783203125\n",
      "138/3000 train_loss: 296.09808349609375 test_loss:658.5558471679688\n",
      "139/3000 train_loss: 273.84735107421875 test_loss:627.2787475585938\n",
      "140/3000 train_loss: 278.3612365722656 test_loss:623.905517578125\n",
      "141/3000 train_loss: 258.7410888671875 test_loss:647.832763671875\n",
      "142/3000 train_loss: 274.9412841796875 test_loss:641.8829956054688\n",
      "143/3000 train_loss: 272.466796875 test_loss:637.0718994140625\n",
      "144/3000 train_loss: 280.3425598144531 test_loss:638.6822509765625\n",
      "145/3000 train_loss: 269.4730224609375 test_loss:618.6034545898438\n",
      "146/3000 train_loss: 272.3173522949219 test_loss:617.8005981445312\n",
      "147/3000 train_loss: 288.13812255859375 test_loss:612.4468994140625\n",
      "148/3000 train_loss: 299.1164855957031 test_loss:606.5064697265625\n",
      "149/3000 train_loss: 256.5005798339844 test_loss:604.7099609375\n",
      "150/3000 train_loss: 267.13494873046875 test_loss:595.497802734375\n",
      "151/3000 train_loss: 263.0691833496094 test_loss:578.9080810546875\n",
      "152/3000 train_loss: 246.76531982421875 test_loss:576.0428466796875\n",
      "153/3000 train_loss: 255.10256958007812 test_loss:588.2363891601562\n",
      "154/3000 train_loss: 260.5606689453125 test_loss:574.9950561523438\n",
      "155/3000 train_loss: 265.48712158203125 test_loss:574.0545654296875\n",
      "156/3000 train_loss: 258.0768737792969 test_loss:579.34765625\n",
      "157/3000 train_loss: 252.59681701660156 test_loss:572.0557861328125\n",
      "158/3000 train_loss: 278.2203063964844 test_loss:565.2575073242188\n",
      "159/3000 train_loss: 262.2339782714844 test_loss:570.6965942382812\n",
      "160/3000 train_loss: 252.24549865722656 test_loss:569.6452026367188\n",
      "161/3000 train_loss: 266.5237731933594 test_loss:566.9964599609375\n",
      "162/3000 train_loss: 263.5724792480469 test_loss:573.770751953125\n",
      "163/3000 train_loss: 248.7052764892578 test_loss:552.6068115234375\n",
      "164/3000 train_loss: 274.876220703125 test_loss:554.90576171875\n",
      "165/3000 train_loss: 250.51339721679688 test_loss:552.6015014648438\n",
      "166/3000 train_loss: 249.05702209472656 test_loss:549.589599609375\n",
      "167/3000 train_loss: 263.4794616699219 test_loss:547.23779296875\n",
      "168/3000 train_loss: 252.39613342285156 test_loss:545.96533203125\n",
      "169/3000 train_loss: 274.5668640136719 test_loss:545.0580444335938\n",
      "170/3000 train_loss: 251.31317138671875 test_loss:545.2261962890625\n",
      "171/3000 train_loss: 274.3931579589844 test_loss:540.0420532226562\n",
      "172/3000 train_loss: 241.25167846679688 test_loss:537.89013671875\n",
      "173/3000 train_loss: 249.21546936035156 test_loss:532.562255859375\n",
      "174/3000 train_loss: 264.576171875 test_loss:538.65869140625\n",
      "175/3000 train_loss: 241.99778747558594 test_loss:549.3564453125\n",
      "176/3000 train_loss: 323.635498046875 test_loss:531.6644897460938\n",
      "177/3000 train_loss: 253.5386199951172 test_loss:512.3439331054688\n",
      "178/3000 train_loss: 259.7153625488281 test_loss:503.7344970703125\n",
      "179/3000 train_loss: 235.21231079101562 test_loss:496.56353759765625\n",
      "180/3000 train_loss: 257.37396240234375 test_loss:499.7595520019531\n",
      "181/3000 train_loss: 248.21640014648438 test_loss:491.72515869140625\n",
      "182/3000 train_loss: 275.6290283203125 test_loss:469.5684509277344\n",
      "183/3000 train_loss: 245.07025146484375 test_loss:476.16845703125\n",
      "184/3000 train_loss: 252.1811065673828 test_loss:482.3699951171875\n",
      "185/3000 train_loss: 235.86019897460938 test_loss:478.53765869140625\n",
      "186/3000 train_loss: 257.06396484375 test_loss:478.1306457519531\n",
      "187/3000 train_loss: 255.6363983154297 test_loss:476.3897399902344\n",
      "188/3000 train_loss: 244.56967163085938 test_loss:479.58673095703125\n",
      "189/3000 train_loss: 251.1466064453125 test_loss:475.2979736328125\n",
      "190/3000 train_loss: 236.49368286132812 test_loss:477.8590087890625\n",
      "191/3000 train_loss: 234.5824737548828 test_loss:470.83544921875\n",
      "192/3000 train_loss: 235.49835205078125 test_loss:478.0576171875\n",
      "193/3000 train_loss: 228.91819763183594 test_loss:469.55584716796875\n",
      "194/3000 train_loss: 245.3380889892578 test_loss:468.3924560546875\n",
      "195/3000 train_loss: 223.90850830078125 test_loss:472.1446533203125\n",
      "196/3000 train_loss: 219.71072387695312 test_loss:468.70916748046875\n",
      "197/3000 train_loss: 212.7140350341797 test_loss:466.92633056640625\n",
      "198/3000 train_loss: 221.79103088378906 test_loss:467.5101318359375\n",
      "199/3000 train_loss: 225.64552307128906 test_loss:470.5881652832031\n",
      "200/3000 train_loss: 227.97061157226562 test_loss:466.3627014160156\n",
      "201/3000 train_loss: 231.32325744628906 test_loss:464.7667541503906\n",
      "202/3000 train_loss: 200.97352600097656 test_loss:462.441650390625\n",
      "203/3000 train_loss: 249.6383514404297 test_loss:462.2137756347656\n",
      "204/3000 train_loss: 241.15118408203125 test_loss:464.33538818359375\n",
      "205/3000 train_loss: 228.2616729736328 test_loss:461.325439453125\n",
      "206/3000 train_loss: 232.66024780273438 test_loss:463.04736328125\n",
      "207/3000 train_loss: 249.66444396972656 test_loss:460.8962707519531\n",
      "208/3000 train_loss: 248.67636108398438 test_loss:463.9551086425781\n",
      "209/3000 train_loss: 225.50547790527344 test_loss:459.2741394042969\n",
      "210/3000 train_loss: 207.27210998535156 test_loss:458.93719482421875\n",
      "211/3000 train_loss: 212.27967834472656 test_loss:459.847412109375\n",
      "212/3000 train_loss: 236.61441040039062 test_loss:458.3948059082031\n",
      "213/3000 train_loss: 215.89697265625 test_loss:457.84344482421875\n",
      "214/3000 train_loss: 211.98158264160156 test_loss:456.79315185546875\n",
      "215/3000 train_loss: 211.26718139648438 test_loss:455.5471496582031\n",
      "216/3000 train_loss: 246.3961944580078 test_loss:455.01898193359375\n",
      "217/3000 train_loss: 209.3771209716797 test_loss:455.00994873046875\n",
      "218/3000 train_loss: 272.34051513671875 test_loss:452.5256042480469\n",
      "219/3000 train_loss: 202.62570190429688 test_loss:450.0820617675781\n",
      "220/3000 train_loss: 210.49659729003906 test_loss:459.65643310546875\n",
      "221/3000 train_loss: 222.82534790039062 test_loss:454.2359619140625\n",
      "222/3000 train_loss: 207.38426208496094 test_loss:452.42474365234375\n",
      "223/3000 train_loss: 203.0008087158203 test_loss:451.4297180175781\n",
      "224/3000 train_loss: 198.50869750976562 test_loss:447.88507080078125\n",
      "225/3000 train_loss: 214.8074493408203 test_loss:452.8589172363281\n",
      "226/3000 train_loss: 209.4658203125 test_loss:450.884765625\n",
      "227/3000 train_loss: 205.60147094726562 test_loss:444.75634765625\n",
      "228/3000 train_loss: 201.50526428222656 test_loss:445.0820007324219\n",
      "229/3000 train_loss: 226.46253967285156 test_loss:444.9120178222656\n",
      "230/3000 train_loss: 212.12059020996094 test_loss:443.7593994140625\n",
      "231/3000 train_loss: 208.63885498046875 test_loss:443.96868896484375\n",
      "232/3000 train_loss: 206.95126342773438 test_loss:442.06634521484375\n",
      "233/3000 train_loss: 227.37327575683594 test_loss:450.13262939453125\n",
      "234/3000 train_loss: 210.2365264892578 test_loss:446.0079650878906\n",
      "235/3000 train_loss: 222.2026824951172 test_loss:443.7369079589844\n",
      "236/3000 train_loss: 200.977294921875 test_loss:440.5309143066406\n",
      "237/3000 train_loss: 198.16482543945312 test_loss:439.806884765625\n",
      "238/3000 train_loss: 236.83282470703125 test_loss:442.05242919921875\n",
      "239/3000 train_loss: 209.96109008789062 test_loss:452.96441650390625\n",
      "240/3000 train_loss: 217.30615234375 test_loss:447.75830078125\n",
      "241/3000 train_loss: 210.72467041015625 test_loss:450.2350769042969\n",
      "242/3000 train_loss: 212.0628662109375 test_loss:469.2384948730469\n",
      "243/3000 train_loss: 208.28756713867188 test_loss:467.4785461425781\n",
      "244/3000 train_loss: 193.91139221191406 test_loss:468.1434020996094\n",
      "245/3000 train_loss: 238.93850708007812 test_loss:484.0477294921875\n",
      "246/3000 train_loss: 190.47894287109375 test_loss:460.5465393066406\n",
      "247/3000 train_loss: 211.35755920410156 test_loss:455.807373046875\n",
      "248/3000 train_loss: 194.2579345703125 test_loss:452.1665344238281\n",
      "249/3000 train_loss: 202.64041137695312 test_loss:453.0058288574219\n",
      "250/3000 train_loss: 190.70750427246094 test_loss:451.3812255859375\n",
      "251/3000 train_loss: 193.16859436035156 test_loss:448.42242431640625\n",
      "252/3000 train_loss: 177.30853271484375 test_loss:442.76025390625\n",
      "253/3000 train_loss: 182.03160095214844 test_loss:453.9742431640625\n",
      "254/3000 train_loss: 184.255126953125 test_loss:463.8548583984375\n",
      "255/3000 train_loss: 193.52279663085938 test_loss:468.21319580078125\n",
      "256/3000 train_loss: 191.01393127441406 test_loss:466.29937744140625\n",
      "257/3000 train_loss: 196.16558837890625 test_loss:461.86419677734375\n",
      "258/3000 train_loss: 190.85543823242188 test_loss:460.61090087890625\n",
      "259/3000 train_loss: 190.224365234375 test_loss:461.090576171875\n",
      "260/3000 train_loss: 177.7545166015625 test_loss:465.237548828125\n",
      "261/3000 train_loss: 199.3088836669922 test_loss:472.401611328125\n",
      "262/3000 train_loss: 194.0749969482422 test_loss:474.6338195800781\n",
      "263/3000 train_loss: 235.98846435546875 test_loss:446.45751953125\n",
      "264/3000 train_loss: 215.41729736328125 test_loss:459.78485107421875\n",
      "265/3000 train_loss: 180.8865509033203 test_loss:447.95489501953125\n",
      "266/3000 train_loss: 197.16201782226562 test_loss:447.57696533203125\n",
      "267/3000 train_loss: 199.7361602783203 test_loss:447.61920166015625\n",
      "268/3000 train_loss: 170.3334197998047 test_loss:444.56573486328125\n",
      "269/3000 train_loss: 179.45779418945312 test_loss:443.33380126953125\n",
      "270/3000 train_loss: 189.75758361816406 test_loss:443.1766662597656\n",
      "271/3000 train_loss: 172.46682739257812 test_loss:454.3878173828125\n",
      "272/3000 train_loss: 197.61781311035156 test_loss:454.29827880859375\n",
      "273/3000 train_loss: 163.78358459472656 test_loss:456.81732177734375\n",
      "274/3000 train_loss: 176.31210327148438 test_loss:444.02984619140625\n",
      "275/3000 train_loss: 186.29747009277344 test_loss:438.0072326660156\n",
      "276/3000 train_loss: 171.97328186035156 test_loss:458.519775390625\n",
      "277/3000 train_loss: 183.20697021484375 test_loss:454.095947265625\n",
      "278/3000 train_loss: 184.68869018554688 test_loss:452.4027404785156\n",
      "279/3000 train_loss: 197.58505249023438 test_loss:455.03204345703125\n",
      "280/3000 train_loss: 175.28697204589844 test_loss:450.152099609375\n",
      "281/3000 train_loss: 187.6358642578125 test_loss:436.7442626953125\n",
      "282/3000 train_loss: 168.79177856445312 test_loss:450.7468566894531\n",
      "283/3000 train_loss: 185.22073364257812 test_loss:455.2615966796875\n",
      "284/3000 train_loss: 164.3392333984375 test_loss:448.3919372558594\n",
      "285/3000 train_loss: 172.50685119628906 test_loss:447.8824768066406\n",
      "286/3000 train_loss: 172.6132049560547 test_loss:437.6148376464844\n",
      "287/3000 train_loss: 165.16458129882812 test_loss:447.58929443359375\n",
      "288/3000 train_loss: 196.166259765625 test_loss:458.49468994140625\n",
      "289/3000 train_loss: 187.743408203125 test_loss:451.0281982421875\n",
      "290/3000 train_loss: 193.1803741455078 test_loss:433.0347595214844\n",
      "291/3000 train_loss: 186.9715576171875 test_loss:475.24920654296875\n",
      "292/3000 train_loss: 197.05406188964844 test_loss:428.43060302734375\n",
      "293/3000 train_loss: 170.66041564941406 test_loss:424.35479736328125\n",
      "294/3000 train_loss: 182.35736083984375 test_loss:424.440673828125\n",
      "295/3000 train_loss: 163.4561767578125 test_loss:423.54302978515625\n",
      "296/3000 train_loss: 194.80882263183594 test_loss:426.4233093261719\n",
      "297/3000 train_loss: 179.4467315673828 test_loss:429.00653076171875\n",
      "298/3000 train_loss: 167.5963897705078 test_loss:417.05108642578125\n",
      "299/3000 train_loss: 165.7595977783203 test_loss:419.92144775390625\n",
      "300/3000 train_loss: 160.31246948242188 test_loss:415.3607177734375\n",
      "301/3000 train_loss: 173.5890655517578 test_loss:425.38238525390625\n",
      "302/3000 train_loss: 181.21437072753906 test_loss:430.8455505371094\n",
      "303/3000 train_loss: 166.15650939941406 test_loss:420.30059814453125\n",
      "304/3000 train_loss: 178.21945190429688 test_loss:420.78564453125\n",
      "305/3000 train_loss: 191.07875061035156 test_loss:419.45416259765625\n",
      "306/3000 train_loss: 181.1221466064453 test_loss:414.769775390625\n",
      "307/3000 train_loss: 176.1468048095703 test_loss:431.8153076171875\n",
      "308/3000 train_loss: 158.7089385986328 test_loss:422.59619140625\n",
      "309/3000 train_loss: 186.08694458007812 test_loss:413.80889892578125\n",
      "310/3000 train_loss: 170.51608276367188 test_loss:422.80194091796875\n",
      "311/3000 train_loss: 157.919189453125 test_loss:409.5677185058594\n",
      "312/3000 train_loss: 162.3181915283203 test_loss:414.071044921875\n",
      "313/3000 train_loss: 178.13720703125 test_loss:422.75286865234375\n",
      "314/3000 train_loss: 145.24700927734375 test_loss:429.31585693359375\n",
      "315/3000 train_loss: 170.1923065185547 test_loss:407.8663330078125\n",
      "316/3000 train_loss: 174.2349395751953 test_loss:410.5271301269531\n",
      "317/3000 train_loss: 160.52725219726562 test_loss:410.96307373046875\n",
      "318/3000 train_loss: 164.93551635742188 test_loss:398.7994384765625\n",
      "319/3000 train_loss: 170.68899536132812 test_loss:421.8486633300781\n",
      "320/3000 train_loss: 151.2353973388672 test_loss:420.64080810546875\n",
      "321/3000 train_loss: 147.9200439453125 test_loss:407.31732177734375\n",
      "322/3000 train_loss: 170.59385681152344 test_loss:416.30267333984375\n",
      "323/3000 train_loss: 174.37673950195312 test_loss:431.783447265625\n",
      "324/3000 train_loss: 147.03750610351562 test_loss:396.3994445800781\n",
      "325/3000 train_loss: 142.70904541015625 test_loss:403.6083068847656\n",
      "326/3000 train_loss: 166.50389099121094 test_loss:404.50640869140625\n",
      "327/3000 train_loss: 155.8512725830078 test_loss:414.615478515625\n",
      "328/3000 train_loss: 164.79861450195312 test_loss:406.0433349609375\n",
      "329/3000 train_loss: 144.21282958984375 test_loss:404.093505859375\n",
      "330/3000 train_loss: 171.88641357421875 test_loss:417.2575378417969\n",
      "331/3000 train_loss: 170.03286743164062 test_loss:434.59735107421875\n",
      "332/3000 train_loss: 138.9374237060547 test_loss:419.36785888671875\n",
      "333/3000 train_loss: 166.3974151611328 test_loss:416.83990478515625\n",
      "334/3000 train_loss: 163.84786987304688 test_loss:437.23284912109375\n",
      "335/3000 train_loss: 148.51864624023438 test_loss:431.1905517578125\n",
      "336/3000 train_loss: 151.2728729248047 test_loss:419.671630859375\n",
      "337/3000 train_loss: 167.5560302734375 test_loss:424.44921875\n",
      "338/3000 train_loss: 152.78271484375 test_loss:414.97723388671875\n",
      "339/3000 train_loss: 149.7548065185547 test_loss:417.1203308105469\n",
      "340/3000 train_loss: 153.35720825195312 test_loss:415.3839111328125\n",
      "341/3000 train_loss: 182.30337524414062 test_loss:413.5318603515625\n",
      "342/3000 train_loss: 180.05648803710938 test_loss:415.4781494140625\n",
      "343/3000 train_loss: 173.22581481933594 test_loss:401.33062744140625\n",
      "344/3000 train_loss: 146.67881774902344 test_loss:408.1104736328125\n",
      "345/3000 train_loss: 175.58367919921875 test_loss:415.88623046875\n",
      "346/3000 train_loss: 190.29379272460938 test_loss:398.217529296875\n",
      "347/3000 train_loss: 157.11338806152344 test_loss:414.8814392089844\n",
      "348/3000 train_loss: 164.3540496826172 test_loss:412.4630432128906\n",
      "349/3000 train_loss: 153.4484100341797 test_loss:422.8604736328125\n",
      "350/3000 train_loss: 169.7946319580078 test_loss:405.38189697265625\n",
      "351/3000 train_loss: 150.04718017578125 test_loss:411.66851806640625\n",
      "352/3000 train_loss: 176.11190795898438 test_loss:400.26385498046875\n",
      "353/3000 train_loss: 154.0157470703125 test_loss:415.02880859375\n",
      "354/3000 train_loss: 147.60208129882812 test_loss:416.1986083984375\n",
      "355/3000 train_loss: 172.00233459472656 test_loss:419.7086181640625\n",
      "356/3000 train_loss: 148.35516357421875 test_loss:399.24652099609375\n",
      "357/3000 train_loss: 152.82403564453125 test_loss:399.43280029296875\n",
      "358/3000 train_loss: 144.85044860839844 test_loss:404.3335876464844\n",
      "359/3000 train_loss: 155.23019409179688 test_loss:424.52008056640625\n",
      "360/3000 train_loss: 158.3343048095703 test_loss:416.02862548828125\n",
      "361/3000 train_loss: 143.9351043701172 test_loss:441.0350341796875\n",
      "362/3000 train_loss: 185.08734130859375 test_loss:409.1957092285156\n",
      "363/3000 train_loss: 149.28976440429688 test_loss:417.5633544921875\n",
      "364/3000 train_loss: 134.46652221679688 test_loss:394.60479736328125\n",
      "365/3000 train_loss: 152.73268127441406 test_loss:393.5728454589844\n",
      "366/3000 train_loss: 141.8977508544922 test_loss:430.8077087402344\n",
      "367/3000 train_loss: 133.70440673828125 test_loss:418.75604248046875\n",
      "368/3000 train_loss: 135.72506713867188 test_loss:386.1356506347656\n",
      "369/3000 train_loss: 150.40374755859375 test_loss:397.71600341796875\n",
      "370/3000 train_loss: 139.76858520507812 test_loss:403.6017761230469\n",
      "371/3000 train_loss: 153.91542053222656 test_loss:421.575927734375\n",
      "372/3000 train_loss: 150.0147247314453 test_loss:419.8278503417969\n",
      "373/3000 train_loss: 174.7811279296875 test_loss:411.31103515625\n",
      "374/3000 train_loss: 181.3528594970703 test_loss:388.83477783203125\n",
      "375/3000 train_loss: 149.4530487060547 test_loss:415.27398681640625\n",
      "376/3000 train_loss: 136.40261840820312 test_loss:417.6794738769531\n",
      "377/3000 train_loss: 135.63340759277344 test_loss:401.55963134765625\n",
      "378/3000 train_loss: 133.86611938476562 test_loss:414.70489501953125\n",
      "379/3000 train_loss: 136.72235107421875 test_loss:414.9057312011719\n",
      "380/3000 train_loss: 133.33743286132812 test_loss:390.98260498046875\n",
      "381/3000 train_loss: 138.4320526123047 test_loss:411.71240234375\n",
      "382/3000 train_loss: 142.32919311523438 test_loss:404.6422119140625\n",
      "383/3000 train_loss: 156.507568359375 test_loss:412.3310241699219\n",
      "384/3000 train_loss: 156.59396362304688 test_loss:414.925048828125\n",
      "385/3000 train_loss: 138.3392791748047 test_loss:407.6129455566406\n",
      "386/3000 train_loss: 154.87130737304688 test_loss:412.42279052734375\n",
      "387/3000 train_loss: 142.7882080078125 test_loss:394.1215515136719\n",
      "388/3000 train_loss: 152.4382781982422 test_loss:421.90228271484375\n",
      "389/3000 train_loss: 142.76365661621094 test_loss:416.6536865234375\n",
      "390/3000 train_loss: 144.3397674560547 test_loss:442.48040771484375\n",
      "391/3000 train_loss: 149.5897216796875 test_loss:445.33807373046875\n",
      "392/3000 train_loss: 133.59144592285156 test_loss:398.2410583496094\n",
      "393/3000 train_loss: 133.71878051757812 test_loss:430.0689697265625\n",
      "394/3000 train_loss: 143.932861328125 test_loss:414.62945556640625\n",
      "395/3000 train_loss: 156.80690002441406 test_loss:551.4888305664062\n",
      "396/3000 train_loss: 140.90744018554688 test_loss:540.752685546875\n",
      "397/3000 train_loss: 139.3759002685547 test_loss:536.647216796875\n",
      "398/3000 train_loss: 144.0379180908203 test_loss:515.0281982421875\n",
      "399/3000 train_loss: 140.58462524414062 test_loss:541.24365234375\n",
      "400/3000 train_loss: 132.69772338867188 test_loss:498.6537170410156\n",
      "401/3000 train_loss: 128.3495635986328 test_loss:496.9443664550781\n",
      "402/3000 train_loss: 131.2042236328125 test_loss:461.9637451171875\n",
      "403/3000 train_loss: 145.37657165527344 test_loss:481.3167724609375\n",
      "404/3000 train_loss: 136.0544891357422 test_loss:460.2326965332031\n",
      "405/3000 train_loss: 143.31546020507812 test_loss:461.45977783203125\n",
      "406/3000 train_loss: 125.07115936279297 test_loss:460.992431640625\n",
      "407/3000 train_loss: 132.48419189453125 test_loss:461.80682373046875\n",
      "408/3000 train_loss: 136.18544006347656 test_loss:459.6824035644531\n",
      "409/3000 train_loss: 151.8013916015625 test_loss:475.1855773925781\n",
      "410/3000 train_loss: 152.24559020996094 test_loss:432.948974609375\n",
      "411/3000 train_loss: 137.587890625 test_loss:447.9439392089844\n",
      "412/3000 train_loss: 141.49673461914062 test_loss:430.909912109375\n",
      "413/3000 train_loss: 135.59384155273438 test_loss:444.1019287109375\n",
      "414/3000 train_loss: 133.40701293945312 test_loss:430.42401123046875\n",
      "415/3000 train_loss: 146.0352020263672 test_loss:428.82452392578125\n",
      "416/3000 train_loss: 129.0368194580078 test_loss:429.0357666015625\n",
      "417/3000 train_loss: 130.58631896972656 test_loss:414.5742492675781\n",
      "418/3000 train_loss: 148.6916046142578 test_loss:418.82421875\n",
      "419/3000 train_loss: 125.2972412109375 test_loss:422.27374267578125\n",
      "420/3000 train_loss: 137.20286560058594 test_loss:434.03753662109375\n",
      "421/3000 train_loss: 150.56552124023438 test_loss:432.65966796875\n",
      "422/3000 train_loss: 138.0078125 test_loss:446.38348388671875\n",
      "423/3000 train_loss: 137.20899963378906 test_loss:443.1397399902344\n",
      "424/3000 train_loss: 129.3155517578125 test_loss:442.2017517089844\n",
      "425/3000 train_loss: 134.64944458007812 test_loss:474.529541015625\n",
      "426/3000 train_loss: 126.20061492919922 test_loss:453.5116271972656\n",
      "427/3000 train_loss: 126.47151947021484 test_loss:456.49432373046875\n",
      "428/3000 train_loss: 126.70083618164062 test_loss:454.2069091796875\n",
      "429/3000 train_loss: 162.0043182373047 test_loss:430.74822998046875\n",
      "430/3000 train_loss: 121.67021942138672 test_loss:404.98699951171875\n",
      "431/3000 train_loss: 127.66876983642578 test_loss:395.01580810546875\n",
      "432/3000 train_loss: 120.09568786621094 test_loss:407.09954833984375\n",
      "433/3000 train_loss: 153.28094482421875 test_loss:383.39599609375\n",
      "434/3000 train_loss: 123.03145599365234 test_loss:418.20391845703125\n",
      "435/3000 train_loss: 128.67831420898438 test_loss:417.045166015625\n",
      "436/3000 train_loss: 128.793212890625 test_loss:418.1109619140625\n",
      "437/3000 train_loss: 134.97532653808594 test_loss:418.9342956542969\n",
      "438/3000 train_loss: 120.33168029785156 test_loss:408.42340087890625\n",
      "439/3000 train_loss: 167.16905212402344 test_loss:415.83819580078125\n",
      "440/3000 train_loss: 119.40485382080078 test_loss:397.824462890625\n",
      "441/3000 train_loss: 140.3247528076172 test_loss:389.2607421875\n",
      "442/3000 train_loss: 122.11936950683594 test_loss:395.0303955078125\n",
      "443/3000 train_loss: 119.89744567871094 test_loss:396.4637145996094\n",
      "444/3000 train_loss: 113.64375305175781 test_loss:400.9512023925781\n",
      "445/3000 train_loss: 144.88902282714844 test_loss:399.3431396484375\n",
      "446/3000 train_loss: 140.94772338867188 test_loss:396.607666015625\n",
      "447/3000 train_loss: 117.17513275146484 test_loss:393.83831787109375\n",
      "448/3000 train_loss: 121.98143005371094 test_loss:393.2368469238281\n",
      "449/3000 train_loss: 143.9794158935547 test_loss:394.0548095703125\n",
      "450/3000 train_loss: 121.91605377197266 test_loss:392.70269775390625\n",
      "451/3000 train_loss: 117.69233703613281 test_loss:389.8929443359375\n",
      "452/3000 train_loss: 122.08316040039062 test_loss:400.464111328125\n",
      "453/3000 train_loss: 117.593017578125 test_loss:402.79534912109375\n",
      "454/3000 train_loss: 130.5907745361328 test_loss:399.989990234375\n",
      "455/3000 train_loss: 131.68821716308594 test_loss:436.974365234375\n",
      "456/3000 train_loss: 114.89556884765625 test_loss:389.1478576660156\n",
      "457/3000 train_loss: 124.71375274658203 test_loss:389.922119140625\n",
      "458/3000 train_loss: 134.0081329345703 test_loss:388.44989013671875\n",
      "459/3000 train_loss: 125.0523910522461 test_loss:401.77044677734375\n",
      "460/3000 train_loss: 117.47946166992188 test_loss:397.03070068359375\n",
      "461/3000 train_loss: 127.75060272216797 test_loss:395.2239990234375\n",
      "462/3000 train_loss: 127.98026275634766 test_loss:408.4067077636719\n",
      "463/3000 train_loss: 131.7693634033203 test_loss:440.0281982421875\n",
      "464/3000 train_loss: 132.44357299804688 test_loss:471.10675048828125\n",
      "465/3000 train_loss: 127.25846862792969 test_loss:386.5417785644531\n",
      "466/3000 train_loss: 134.98983764648438 test_loss:408.8541259765625\n",
      "467/3000 train_loss: 123.39384460449219 test_loss:413.138916015625\n",
      "468/3000 train_loss: 128.093994140625 test_loss:409.96649169921875\n",
      "469/3000 train_loss: 131.6188201904297 test_loss:392.05194091796875\n",
      "470/3000 train_loss: 118.09719848632812 test_loss:421.03936767578125\n",
      "471/3000 train_loss: 129.33346557617188 test_loss:401.7417297363281\n",
      "472/3000 train_loss: 132.8509521484375 test_loss:380.559326171875\n",
      "473/3000 train_loss: 118.93997955322266 test_loss:371.2130432128906\n",
      "474/3000 train_loss: 125.81684875488281 test_loss:382.12786865234375\n",
      "475/3000 train_loss: 127.62787628173828 test_loss:367.1292724609375\n",
      "476/3000 train_loss: 113.736572265625 test_loss:364.59881591796875\n",
      "477/3000 train_loss: 125.11766052246094 test_loss:363.67230224609375\n",
      "478/3000 train_loss: 113.73009490966797 test_loss:376.20635986328125\n",
      "479/3000 train_loss: 119.0699462890625 test_loss:366.87664794921875\n",
      "480/3000 train_loss: 128.07278442382812 test_loss:378.3747863769531\n",
      "481/3000 train_loss: 123.09296417236328 test_loss:371.8406982421875\n",
      "482/3000 train_loss: 119.48662567138672 test_loss:379.84088134765625\n",
      "483/3000 train_loss: 107.35197448730469 test_loss:378.94134521484375\n",
      "484/3000 train_loss: 121.43207550048828 test_loss:369.39849853515625\n",
      "485/3000 train_loss: 128.1116485595703 test_loss:376.31072998046875\n",
      "486/3000 train_loss: 121.2569580078125 test_loss:373.97833251953125\n",
      "487/3000 train_loss: 126.39781951904297 test_loss:368.948974609375\n",
      "488/3000 train_loss: 119.37319946289062 test_loss:378.5724792480469\n",
      "489/3000 train_loss: 101.67831420898438 test_loss:368.924072265625\n",
      "490/3000 train_loss: 101.46710205078125 test_loss:368.8911437988281\n",
      "491/3000 train_loss: 108.40202331542969 test_loss:372.4836730957031\n",
      "492/3000 train_loss: 114.5565185546875 test_loss:364.8411865234375\n",
      "493/3000 train_loss: 112.54232025146484 test_loss:354.87835693359375\n",
      "494/3000 train_loss: 103.25155639648438 test_loss:356.89947509765625\n",
      "495/3000 train_loss: 114.43305206298828 test_loss:357.1041564941406\n",
      "496/3000 train_loss: 111.67960357666016 test_loss:357.6838684082031\n",
      "497/3000 train_loss: 125.6363525390625 test_loss:346.7325439453125\n",
      "498/3000 train_loss: 117.54424285888672 test_loss:369.48046875\n",
      "499/3000 train_loss: 103.38461303710938 test_loss:346.86798095703125\n",
      "500/3000 train_loss: 100.01469421386719 test_loss:347.65985107421875\n",
      "501/3000 train_loss: 114.40740203857422 test_loss:370.44744873046875\n",
      "502/3000 train_loss: 117.56735229492188 test_loss:359.19439697265625\n",
      "503/3000 train_loss: 108.46527862548828 test_loss:347.4906005859375\n",
      "504/3000 train_loss: 98.32617950439453 test_loss:355.211181640625\n",
      "505/3000 train_loss: 108.81056213378906 test_loss:356.10137939453125\n",
      "506/3000 train_loss: 130.9649200439453 test_loss:358.055908203125\n",
      "507/3000 train_loss: 112.04740142822266 test_loss:354.20257568359375\n",
      "508/3000 train_loss: 102.83863830566406 test_loss:354.0381164550781\n",
      "509/3000 train_loss: 110.28726196289062 test_loss:358.72772216796875\n",
      "510/3000 train_loss: 110.63421630859375 test_loss:352.145751953125\n",
      "511/3000 train_loss: 108.10867309570312 test_loss:352.25634765625\n",
      "512/3000 train_loss: 105.00090026855469 test_loss:351.6412353515625\n",
      "513/3000 train_loss: 110.72130584716797 test_loss:354.3230895996094\n",
      "514/3000 train_loss: 111.39912414550781 test_loss:342.990234375\n",
      "515/3000 train_loss: 106.067626953125 test_loss:341.8719177246094\n",
      "516/3000 train_loss: 108.98119354248047 test_loss:386.665283203125\n",
      "517/3000 train_loss: 109.00223541259766 test_loss:379.11260986328125\n",
      "518/3000 train_loss: 137.00538635253906 test_loss:373.40234375\n",
      "519/3000 train_loss: 108.2440185546875 test_loss:342.10308837890625\n",
      "520/3000 train_loss: 117.19578552246094 test_loss:359.24139404296875\n",
      "521/3000 train_loss: 105.19070434570312 test_loss:359.97454833984375\n",
      "522/3000 train_loss: 101.84252166748047 test_loss:354.79052734375\n",
      "523/3000 train_loss: 98.74983215332031 test_loss:356.19342041015625\n",
      "524/3000 train_loss: 103.71148681640625 test_loss:355.671142578125\n",
      "525/3000 train_loss: 108.77272033691406 test_loss:352.88360595703125\n",
      "526/3000 train_loss: 108.84342956542969 test_loss:377.6657409667969\n",
      "527/3000 train_loss: 108.15454864501953 test_loss:351.68463134765625\n",
      "528/3000 train_loss: 113.7514419555664 test_loss:355.23876953125\n",
      "529/3000 train_loss: 109.9623794555664 test_loss:355.49835205078125\n",
      "530/3000 train_loss: 102.38428497314453 test_loss:353.81341552734375\n",
      "531/3000 train_loss: 100.51371765136719 test_loss:345.78741455078125\n",
      "532/3000 train_loss: 102.08101654052734 test_loss:364.1755676269531\n",
      "533/3000 train_loss: 93.09587097167969 test_loss:367.2008056640625\n",
      "534/3000 train_loss: 122.51911926269531 test_loss:342.63189697265625\n",
      "535/3000 train_loss: 98.98741912841797 test_loss:343.50244140625\n",
      "536/3000 train_loss: 108.88140869140625 test_loss:351.2568359375\n",
      "537/3000 train_loss: 107.60871124267578 test_loss:353.28021240234375\n",
      "538/3000 train_loss: 109.75672912597656 test_loss:356.9521484375\n",
      "539/3000 train_loss: 112.35588073730469 test_loss:350.69482421875\n",
      "540/3000 train_loss: 119.48219299316406 test_loss:323.4920654296875\n",
      "541/3000 train_loss: 96.47273254394531 test_loss:317.163818359375\n",
      "542/3000 train_loss: 103.37677001953125 test_loss:335.8221435546875\n",
      "543/3000 train_loss: 92.36924743652344 test_loss:347.59454345703125\n",
      "544/3000 train_loss: 113.16616821289062 test_loss:315.2174377441406\n",
      "545/3000 train_loss: 103.3592758178711 test_loss:331.1174011230469\n",
      "546/3000 train_loss: 106.03423309326172 test_loss:318.38665771484375\n",
      "547/3000 train_loss: 107.55799102783203 test_loss:325.7554626464844\n",
      "548/3000 train_loss: 123.28666687011719 test_loss:338.2330627441406\n",
      "549/3000 train_loss: 101.56608581542969 test_loss:313.14373779296875\n",
      "550/3000 train_loss: 91.0031967163086 test_loss:298.9112548828125\n",
      "551/3000 train_loss: 103.4238510131836 test_loss:307.444091796875\n",
      "552/3000 train_loss: 100.79600524902344 test_loss:325.13726806640625\n",
      "553/3000 train_loss: 93.8248519897461 test_loss:307.1650390625\n",
      "554/3000 train_loss: 95.11168670654297 test_loss:308.3463134765625\n",
      "555/3000 train_loss: 103.23391723632812 test_loss:311.7247619628906\n",
      "556/3000 train_loss: 102.9105453491211 test_loss:301.494384765625\n",
      "557/3000 train_loss: 112.86190032958984 test_loss:307.5158386230469\n",
      "558/3000 train_loss: 92.22105407714844 test_loss:306.5116882324219\n",
      "559/3000 train_loss: 100.73076629638672 test_loss:302.37261962890625\n",
      "560/3000 train_loss: 106.20718383789062 test_loss:320.1245422363281\n",
      "561/3000 train_loss: 106.81050872802734 test_loss:313.12786865234375\n",
      "562/3000 train_loss: 97.99110412597656 test_loss:325.58074951171875\n",
      "563/3000 train_loss: 112.65849304199219 test_loss:336.93255615234375\n",
      "564/3000 train_loss: 102.6757583618164 test_loss:324.7052001953125\n",
      "565/3000 train_loss: 102.74866485595703 test_loss:301.338623046875\n",
      "566/3000 train_loss: 97.15830993652344 test_loss:300.89453125\n",
      "567/3000 train_loss: 101.83804321289062 test_loss:331.46588134765625\n",
      "568/3000 train_loss: 97.2869873046875 test_loss:300.67510986328125\n",
      "569/3000 train_loss: 99.14945220947266 test_loss:301.098388671875\n",
      "570/3000 train_loss: 108.4385757446289 test_loss:331.34619140625\n",
      "571/3000 train_loss: 99.54953002929688 test_loss:312.8692626953125\n",
      "572/3000 train_loss: 98.79869079589844 test_loss:324.9888916015625\n",
      "573/3000 train_loss: 97.20867156982422 test_loss:305.8158874511719\n",
      "574/3000 train_loss: 100.71149444580078 test_loss:325.8236083984375\n",
      "575/3000 train_loss: 94.36370849609375 test_loss:305.23931884765625\n",
      "576/3000 train_loss: 89.106201171875 test_loss:306.10504150390625\n",
      "577/3000 train_loss: 92.50959014892578 test_loss:312.22100830078125\n",
      "578/3000 train_loss: 96.96656036376953 test_loss:308.86383056640625\n",
      "579/3000 train_loss: 93.23397827148438 test_loss:303.34979248046875\n",
      "580/3000 train_loss: 97.88311767578125 test_loss:306.106201171875\n",
      "581/3000 train_loss: 104.01483154296875 test_loss:297.10595703125\n",
      "582/3000 train_loss: 97.68919372558594 test_loss:319.3185729980469\n",
      "583/3000 train_loss: 103.15098571777344 test_loss:300.4619140625\n",
      "584/3000 train_loss: 87.99805450439453 test_loss:292.6846923828125\n",
      "585/3000 train_loss: 102.86953735351562 test_loss:295.0706481933594\n",
      "586/3000 train_loss: 97.06120300292969 test_loss:298.6286315917969\n",
      "587/3000 train_loss: 98.12925720214844 test_loss:293.234375\n",
      "588/3000 train_loss: 98.75209045410156 test_loss:296.3631591796875\n",
      "589/3000 train_loss: 118.1775894165039 test_loss:296.388916015625\n",
      "590/3000 train_loss: 105.58811950683594 test_loss:315.95819091796875\n",
      "591/3000 train_loss: 91.0915298461914 test_loss:316.85107421875\n",
      "592/3000 train_loss: 90.70994567871094 test_loss:316.6604919433594\n",
      "593/3000 train_loss: 95.84085845947266 test_loss:314.91632080078125\n",
      "594/3000 train_loss: 90.48848724365234 test_loss:318.7243957519531\n",
      "595/3000 train_loss: 97.14524841308594 test_loss:326.1346740722656\n",
      "596/3000 train_loss: 97.83795928955078 test_loss:298.06695556640625\n",
      "597/3000 train_loss: 94.19720458984375 test_loss:293.7303161621094\n",
      "598/3000 train_loss: 121.17945098876953 test_loss:308.15789794921875\n",
      "599/3000 train_loss: 102.35185241699219 test_loss:330.39727783203125\n",
      "600/3000 train_loss: 105.87652587890625 test_loss:308.278076171875\n",
      "601/3000 train_loss: 94.10237121582031 test_loss:291.9769287109375\n",
      "602/3000 train_loss: 96.33621978759766 test_loss:295.09027099609375\n",
      "603/3000 train_loss: 82.65664672851562 test_loss:296.1138916015625\n",
      "604/3000 train_loss: 91.66780853271484 test_loss:302.49462890625\n",
      "605/3000 train_loss: 83.68075561523438 test_loss:303.8939208984375\n",
      "606/3000 train_loss: 95.15557098388672 test_loss:313.99639892578125\n",
      "607/3000 train_loss: 95.34248352050781 test_loss:306.7392578125\n",
      "608/3000 train_loss: 96.84976959228516 test_loss:291.86590576171875\n",
      "609/3000 train_loss: 88.81803131103516 test_loss:289.2145690917969\n",
      "610/3000 train_loss: 88.3200454711914 test_loss:288.58740234375\n",
      "611/3000 train_loss: 83.70851135253906 test_loss:313.0140075683594\n",
      "612/3000 train_loss: 87.11968994140625 test_loss:281.885009765625\n",
      "613/3000 train_loss: 97.28713989257812 test_loss:312.63372802734375\n",
      "614/3000 train_loss: 93.22818756103516 test_loss:290.41748046875\n",
      "615/3000 train_loss: 84.1325912475586 test_loss:310.6441650390625\n",
      "616/3000 train_loss: 96.98678588867188 test_loss:314.87213134765625\n",
      "617/3000 train_loss: 95.90342712402344 test_loss:307.8003234863281\n",
      "618/3000 train_loss: 93.21581268310547 test_loss:281.98846435546875\n",
      "619/3000 train_loss: 90.85243225097656 test_loss:286.84088134765625\n",
      "620/3000 train_loss: 106.74176025390625 test_loss:285.81158447265625\n",
      "621/3000 train_loss: 88.85871887207031 test_loss:310.7418212890625\n",
      "622/3000 train_loss: 93.8691635131836 test_loss:309.54705810546875\n",
      "623/3000 train_loss: 89.53311920166016 test_loss:283.490478515625\n",
      "624/3000 train_loss: 91.13795471191406 test_loss:288.9323425292969\n",
      "625/3000 train_loss: 80.66932678222656 test_loss:286.05389404296875\n",
      "626/3000 train_loss: 95.68893432617188 test_loss:285.41644287109375\n",
      "627/3000 train_loss: 89.77791595458984 test_loss:285.6523742675781\n",
      "628/3000 train_loss: 90.41194152832031 test_loss:287.05859375\n",
      "629/3000 train_loss: 80.69647979736328 test_loss:288.0546875\n",
      "630/3000 train_loss: 92.35493469238281 test_loss:288.819091796875\n",
      "631/3000 train_loss: 95.44403839111328 test_loss:282.17681884765625\n",
      "632/3000 train_loss: 100.20526885986328 test_loss:285.33636474609375\n",
      "633/3000 train_loss: 94.2920150756836 test_loss:312.9878845214844\n",
      "634/3000 train_loss: 86.09698486328125 test_loss:287.384033203125\n",
      "635/3000 train_loss: 94.06575775146484 test_loss:288.0099182128906\n",
      "636/3000 train_loss: 92.90985107421875 test_loss:293.1314697265625\n",
      "637/3000 train_loss: 94.4421615600586 test_loss:282.9170227050781\n",
      "638/3000 train_loss: 86.06595611572266 test_loss:307.0113525390625\n",
      "639/3000 train_loss: 103.53074645996094 test_loss:315.52716064453125\n",
      "640/3000 train_loss: 89.46642303466797 test_loss:291.4771423339844\n",
      "641/3000 train_loss: 92.61971282958984 test_loss:326.0394287109375\n",
      "642/3000 train_loss: 83.1782455444336 test_loss:304.918701171875\n",
      "643/3000 train_loss: 91.47737121582031 test_loss:367.3585510253906\n",
      "644/3000 train_loss: 78.92433166503906 test_loss:335.5691833496094\n",
      "645/3000 train_loss: 81.5663833618164 test_loss:279.812744140625\n",
      "646/3000 train_loss: 93.8204116821289 test_loss:296.83624267578125\n",
      "647/3000 train_loss: 91.60075378417969 test_loss:337.65093994140625\n",
      "648/3000 train_loss: 91.567138671875 test_loss:280.7254638671875\n",
      "649/3000 train_loss: 81.43712615966797 test_loss:285.975341796875\n",
      "650/3000 train_loss: 86.73492431640625 test_loss:282.497802734375\n",
      "651/3000 train_loss: 103.78396606445312 test_loss:282.45599365234375\n",
      "652/3000 train_loss: 134.80450439453125 test_loss:310.68048095703125\n",
      "653/3000 train_loss: 109.92740631103516 test_loss:296.95343017578125\n",
      "654/3000 train_loss: 123.25666809082031 test_loss:306.3585205078125\n",
      "655/3000 train_loss: 99.73128509521484 test_loss:281.90533447265625\n",
      "656/3000 train_loss: 106.28485107421875 test_loss:284.677490234375\n",
      "657/3000 train_loss: 110.41629791259766 test_loss:287.048095703125\n",
      "658/3000 train_loss: 96.92009735107422 test_loss:278.5155029296875\n",
      "659/3000 train_loss: 127.03733825683594 test_loss:285.6591796875\n",
      "660/3000 train_loss: 106.6885757446289 test_loss:291.377685546875\n",
      "661/3000 train_loss: 113.81844329833984 test_loss:304.19989013671875\n",
      "662/3000 train_loss: 93.77037811279297 test_loss:277.4685363769531\n",
      "663/3000 train_loss: 97.79701232910156 test_loss:284.84283447265625\n",
      "664/3000 train_loss: 113.59315490722656 test_loss:278.5408935546875\n",
      "665/3000 train_loss: 91.00267028808594 test_loss:278.04766845703125\n",
      "666/3000 train_loss: 91.78790283203125 test_loss:273.6982727050781\n",
      "667/3000 train_loss: 92.67901611328125 test_loss:279.6624755859375\n",
      "668/3000 train_loss: 94.1157455444336 test_loss:274.8941650390625\n",
      "669/3000 train_loss: 99.83213806152344 test_loss:271.981689453125\n",
      "670/3000 train_loss: 91.80042266845703 test_loss:271.3910217285156\n",
      "671/3000 train_loss: 85.41006469726562 test_loss:267.6007385253906\n",
      "672/3000 train_loss: 92.70684051513672 test_loss:270.76898193359375\n",
      "673/3000 train_loss: 92.86357116699219 test_loss:266.95672607421875\n",
      "674/3000 train_loss: 90.40205383300781 test_loss:269.4873962402344\n",
      "675/3000 train_loss: 103.37855529785156 test_loss:274.2546081542969\n",
      "676/3000 train_loss: 90.8215560913086 test_loss:271.9210510253906\n",
      "677/3000 train_loss: 78.07830047607422 test_loss:265.2688903808594\n",
      "678/3000 train_loss: 95.2093505859375 test_loss:265.3630065917969\n",
      "679/3000 train_loss: 96.72776794433594 test_loss:277.0986633300781\n",
      "680/3000 train_loss: 92.00740051269531 test_loss:280.9383544921875\n",
      "681/3000 train_loss: 115.41963195800781 test_loss:268.36798095703125\n",
      "682/3000 train_loss: 99.56463623046875 test_loss:262.98773193359375\n",
      "683/3000 train_loss: 94.4631118774414 test_loss:264.9178466796875\n",
      "684/3000 train_loss: 90.77494049072266 test_loss:263.059814453125\n",
      "685/3000 train_loss: 87.0690689086914 test_loss:262.3272399902344\n",
      "686/3000 train_loss: 90.06378173828125 test_loss:272.23944091796875\n",
      "687/3000 train_loss: 86.79380798339844 test_loss:270.3385009765625\n",
      "688/3000 train_loss: 91.77335357666016 test_loss:267.90228271484375\n",
      "689/3000 train_loss: 82.68209838867188 test_loss:267.85540771484375\n",
      "690/3000 train_loss: 94.09099578857422 test_loss:263.7232666015625\n",
      "691/3000 train_loss: 86.67816925048828 test_loss:262.983642578125\n",
      "692/3000 train_loss: 92.64018249511719 test_loss:267.3694763183594\n",
      "693/3000 train_loss: 94.2145767211914 test_loss:270.611572265625\n",
      "694/3000 train_loss: 86.96162414550781 test_loss:273.8448181152344\n",
      "695/3000 train_loss: 100.59562683105469 test_loss:262.46063232421875\n",
      "696/3000 train_loss: 83.70011901855469 test_loss:269.28192138671875\n",
      "697/3000 train_loss: 83.92478942871094 test_loss:268.8007507324219\n",
      "698/3000 train_loss: 94.40660095214844 test_loss:273.70343017578125\n",
      "699/3000 train_loss: 92.06206512451172 test_loss:272.12908935546875\n",
      "700/3000 train_loss: 79.4189224243164 test_loss:264.4864501953125\n",
      "701/3000 train_loss: 97.82745361328125 test_loss:265.4170227050781\n",
      "702/3000 train_loss: 91.17984008789062 test_loss:263.5615234375\n",
      "703/3000 train_loss: 75.83468627929688 test_loss:265.87774658203125\n",
      "704/3000 train_loss: 94.79171752929688 test_loss:267.96533203125\n",
      "705/3000 train_loss: 81.23481750488281 test_loss:265.3312683105469\n",
      "706/3000 train_loss: 98.8931655883789 test_loss:261.1228942871094\n",
      "707/3000 train_loss: 85.12957000732422 test_loss:260.87066650390625\n",
      "708/3000 train_loss: 77.93517303466797 test_loss:263.676513671875\n",
      "709/3000 train_loss: 86.2629165649414 test_loss:260.50408935546875\n",
      "710/3000 train_loss: 92.09833526611328 test_loss:266.861083984375\n",
      "711/3000 train_loss: 80.28057098388672 test_loss:268.8534240722656\n",
      "712/3000 train_loss: 89.02091217041016 test_loss:273.83978271484375\n",
      "713/3000 train_loss: 79.7359619140625 test_loss:264.5911865234375\n",
      "714/3000 train_loss: 75.34059143066406 test_loss:263.2956848144531\n",
      "715/3000 train_loss: 90.80428314208984 test_loss:267.5118103027344\n",
      "716/3000 train_loss: 79.73193359375 test_loss:261.4267578125\n",
      "717/3000 train_loss: 71.85179138183594 test_loss:264.3878173828125\n",
      "718/3000 train_loss: 77.6246337890625 test_loss:264.361328125\n",
      "719/3000 train_loss: 82.50264739990234 test_loss:270.6351013183594\n",
      "720/3000 train_loss: 90.83051300048828 test_loss:283.7598876953125\n",
      "721/3000 train_loss: 87.46332550048828 test_loss:268.5084533691406\n",
      "722/3000 train_loss: 82.22866821289062 test_loss:259.79754638671875\n",
      "723/3000 train_loss: 93.97218322753906 test_loss:261.3288269042969\n",
      "724/3000 train_loss: 94.71561431884766 test_loss:262.0228576660156\n",
      "725/3000 train_loss: 103.43931579589844 test_loss:259.35382080078125\n",
      "726/3000 train_loss: 85.52318572998047 test_loss:261.50885009765625\n",
      "727/3000 train_loss: 72.5046615600586 test_loss:260.71038818359375\n",
      "728/3000 train_loss: 89.37101745605469 test_loss:264.8392028808594\n",
      "729/3000 train_loss: 88.19351959228516 test_loss:262.41387939453125\n",
      "730/3000 train_loss: 80.83887481689453 test_loss:259.17523193359375\n",
      "731/3000 train_loss: 85.07232666015625 test_loss:271.607421875\n",
      "732/3000 train_loss: 76.35685729980469 test_loss:275.7742004394531\n",
      "733/3000 train_loss: 79.10429382324219 test_loss:259.65020751953125\n",
      "734/3000 train_loss: 90.97566223144531 test_loss:258.32879638671875\n",
      "735/3000 train_loss: 81.65016174316406 test_loss:258.2770080566406\n",
      "736/3000 train_loss: 109.00066375732422 test_loss:261.927734375\n",
      "737/3000 train_loss: 80.19571685791016 test_loss:257.8048400878906\n",
      "738/3000 train_loss: 85.4408187866211 test_loss:255.66006469726562\n",
      "739/3000 train_loss: 84.48870849609375 test_loss:259.0165100097656\n",
      "740/3000 train_loss: 79.99732971191406 test_loss:307.4017333984375\n",
      "741/3000 train_loss: 78.76061248779297 test_loss:295.25579833984375\n",
      "742/3000 train_loss: 77.73780059814453 test_loss:307.8376770019531\n",
      "743/3000 train_loss: 78.62818145751953 test_loss:256.5689697265625\n",
      "744/3000 train_loss: 82.29156494140625 test_loss:266.8658752441406\n",
      "745/3000 train_loss: 80.21363830566406 test_loss:256.4984130859375\n",
      "746/3000 train_loss: 90.81468200683594 test_loss:260.6712341308594\n",
      "747/3000 train_loss: 86.70242309570312 test_loss:257.14794921875\n",
      "748/3000 train_loss: 101.7865982055664 test_loss:254.7929229736328\n",
      "749/3000 train_loss: 75.68000793457031 test_loss:256.1780090332031\n",
      "750/3000 train_loss: 77.7730712890625 test_loss:253.2265625\n",
      "751/3000 train_loss: 78.34223175048828 test_loss:256.3388671875\n",
      "752/3000 train_loss: 95.41830444335938 test_loss:271.92315673828125\n",
      "753/3000 train_loss: 88.15745544433594 test_loss:264.12921142578125\n",
      "754/3000 train_loss: 84.35333251953125 test_loss:259.6093444824219\n",
      "755/3000 train_loss: 81.45458221435547 test_loss:256.231689453125\n",
      "756/3000 train_loss: 84.74794006347656 test_loss:254.7841796875\n",
      "757/3000 train_loss: 76.27310180664062 test_loss:254.41036987304688\n",
      "758/3000 train_loss: 89.90802764892578 test_loss:261.17791748046875\n",
      "759/3000 train_loss: 81.9974365234375 test_loss:255.903564453125\n",
      "760/3000 train_loss: 75.23789978027344 test_loss:260.05792236328125\n",
      "761/3000 train_loss: 73.29666900634766 test_loss:255.4705810546875\n",
      "762/3000 train_loss: 79.6211166381836 test_loss:255.12391662597656\n",
      "763/3000 train_loss: 85.1749496459961 test_loss:260.24090576171875\n",
      "764/3000 train_loss: 78.69731903076172 test_loss:251.13308715820312\n",
      "765/3000 train_loss: 71.2219009399414 test_loss:249.58120727539062\n",
      "766/3000 train_loss: 80.57819366455078 test_loss:250.10037231445312\n",
      "767/3000 train_loss: 86.08235168457031 test_loss:253.08065795898438\n",
      "768/3000 train_loss: 79.785888671875 test_loss:257.068359375\n",
      "769/3000 train_loss: 76.4721450805664 test_loss:253.33822631835938\n",
      "770/3000 train_loss: 82.6302490234375 test_loss:251.8424530029297\n",
      "771/3000 train_loss: 80.8780746459961 test_loss:249.42062377929688\n",
      "772/3000 train_loss: 75.4607925415039 test_loss:247.67495727539062\n",
      "773/3000 train_loss: 72.47882843017578 test_loss:247.98605346679688\n",
      "774/3000 train_loss: 77.91280364990234 test_loss:248.7352294921875\n",
      "775/3000 train_loss: 80.35535430908203 test_loss:252.75643920898438\n",
      "776/3000 train_loss: 78.76065063476562 test_loss:257.8460693359375\n",
      "777/3000 train_loss: 75.47505187988281 test_loss:285.18634033203125\n",
      "778/3000 train_loss: 76.43180084228516 test_loss:269.15875244140625\n",
      "779/3000 train_loss: 77.15560913085938 test_loss:264.3624267578125\n",
      "780/3000 train_loss: 78.50074005126953 test_loss:260.98406982421875\n",
      "781/3000 train_loss: 68.64933776855469 test_loss:267.5503234863281\n",
      "782/3000 train_loss: 76.2293472290039 test_loss:265.1134948730469\n",
      "783/3000 train_loss: 72.702880859375 test_loss:264.539794921875\n",
      "784/3000 train_loss: 74.05223846435547 test_loss:251.26177978515625\n",
      "785/3000 train_loss: 91.87727355957031 test_loss:259.5265197753906\n",
      "786/3000 train_loss: 88.88270568847656 test_loss:260.415283203125\n",
      "787/3000 train_loss: 96.62702941894531 test_loss:259.26007080078125\n",
      "788/3000 train_loss: 83.32633209228516 test_loss:274.0055236816406\n",
      "789/3000 train_loss: 93.39241027832031 test_loss:264.45220947265625\n",
      "790/3000 train_loss: 92.89112854003906 test_loss:267.058349609375\n",
      "791/3000 train_loss: 70.0715103149414 test_loss:272.15106201171875\n",
      "792/3000 train_loss: 80.36087036132812 test_loss:262.15985107421875\n",
      "793/3000 train_loss: 71.80229187011719 test_loss:256.2342529296875\n",
      "794/3000 train_loss: 75.65887451171875 test_loss:260.22332763671875\n",
      "795/3000 train_loss: 78.45311737060547 test_loss:262.96575927734375\n",
      "796/3000 train_loss: 77.75764465332031 test_loss:254.3663330078125\n",
      "797/3000 train_loss: 82.01177978515625 test_loss:253.33514404296875\n",
      "798/3000 train_loss: 86.85813903808594 test_loss:251.41781616210938\n",
      "799/3000 train_loss: 81.78040313720703 test_loss:253.6585693359375\n",
      "800/3000 train_loss: 76.44243621826172 test_loss:248.61611938476562\n",
      "801/3000 train_loss: 75.11116027832031 test_loss:248.20285034179688\n",
      "802/3000 train_loss: 68.91315460205078 test_loss:254.90423583984375\n",
      "803/3000 train_loss: 73.80021667480469 test_loss:253.48837280273438\n",
      "804/3000 train_loss: 82.2534408569336 test_loss:276.00390625\n",
      "805/3000 train_loss: 73.3388900756836 test_loss:248.15353393554688\n",
      "806/3000 train_loss: 74.04624938964844 test_loss:262.00335693359375\n",
      "807/3000 train_loss: 72.72577667236328 test_loss:245.6310577392578\n",
      "808/3000 train_loss: 78.59178161621094 test_loss:280.8365173339844\n",
      "809/3000 train_loss: 80.96257781982422 test_loss:256.8028869628906\n",
      "810/3000 train_loss: 73.31591796875 test_loss:270.2942199707031\n",
      "811/3000 train_loss: 74.13188934326172 test_loss:254.91854858398438\n",
      "812/3000 train_loss: 81.6733627319336 test_loss:256.2569580078125\n",
      "813/3000 train_loss: 72.90718841552734 test_loss:260.02130126953125\n",
      "814/3000 train_loss: 83.05511474609375 test_loss:271.47222900390625\n",
      "815/3000 train_loss: 77.5089340209961 test_loss:258.4708251953125\n",
      "816/3000 train_loss: 76.18595886230469 test_loss:264.6634216308594\n",
      "817/3000 train_loss: 76.48661041259766 test_loss:269.8460693359375\n",
      "818/3000 train_loss: 74.97892761230469 test_loss:261.6240234375\n",
      "819/3000 train_loss: 82.29811096191406 test_loss:263.97735595703125\n",
      "820/3000 train_loss: 73.70475769042969 test_loss:267.6688232421875\n",
      "821/3000 train_loss: 75.52804565429688 test_loss:275.2554626464844\n",
      "822/3000 train_loss: 76.89637756347656 test_loss:259.85577392578125\n",
      "823/3000 train_loss: 77.81058502197266 test_loss:252.41305541992188\n",
      "824/3000 train_loss: 72.94547271728516 test_loss:249.250244140625\n",
      "825/3000 train_loss: 73.577880859375 test_loss:277.1349792480469\n",
      "826/3000 train_loss: 74.9752197265625 test_loss:274.3241271972656\n",
      "827/3000 train_loss: 67.7810287475586 test_loss:260.189453125\n",
      "828/3000 train_loss: 79.08246612548828 test_loss:266.00433349609375\n",
      "829/3000 train_loss: 76.93651580810547 test_loss:253.0236053466797\n",
      "830/3000 train_loss: 72.31826782226562 test_loss:248.29501342773438\n",
      "831/3000 train_loss: 67.10112762451172 test_loss:268.3489990234375\n",
      "832/3000 train_loss: 75.9907455444336 test_loss:269.4689636230469\n",
      "833/3000 train_loss: 63.280391693115234 test_loss:267.1112976074219\n",
      "834/3000 train_loss: 81.19537353515625 test_loss:262.09259033203125\n",
      "835/3000 train_loss: 69.26627349853516 test_loss:266.51202392578125\n",
      "836/3000 train_loss: 75.55270385742188 test_loss:274.66253662109375\n",
      "837/3000 train_loss: 79.84964752197266 test_loss:265.7451171875\n",
      "838/3000 train_loss: 83.10802459716797 test_loss:266.2503967285156\n",
      "839/3000 train_loss: 73.58611297607422 test_loss:261.5135192871094\n",
      "840/3000 train_loss: 74.69264221191406 test_loss:265.54571533203125\n",
      "841/3000 train_loss: 82.6495590209961 test_loss:253.14682006835938\n",
      "842/3000 train_loss: 66.53548431396484 test_loss:276.96875\n",
      "843/3000 train_loss: 70.92851257324219 test_loss:267.4584045410156\n",
      "844/3000 train_loss: 64.59593200683594 test_loss:262.576904296875\n",
      "845/3000 train_loss: 75.61138153076172 test_loss:263.23724365234375\n",
      "846/3000 train_loss: 65.50812530517578 test_loss:261.5900573730469\n",
      "847/3000 train_loss: 68.02545166015625 test_loss:263.6266174316406\n",
      "848/3000 train_loss: 63.500484466552734 test_loss:268.3583984375\n",
      "849/3000 train_loss: 78.36736297607422 test_loss:268.96978759765625\n",
      "850/3000 train_loss: 74.56474304199219 test_loss:247.60888671875\n",
      "851/3000 train_loss: 80.95182037353516 test_loss:240.900390625\n",
      "852/3000 train_loss: 78.30707550048828 test_loss:242.14352416992188\n",
      "853/3000 train_loss: 80.57984924316406 test_loss:241.044677734375\n",
      "854/3000 train_loss: 72.10923767089844 test_loss:244.4711151123047\n",
      "855/3000 train_loss: 89.65042877197266 test_loss:244.64529418945312\n",
      "856/3000 train_loss: 93.98050689697266 test_loss:256.7484130859375\n",
      "857/3000 train_loss: 69.81806945800781 test_loss:262.941650390625\n",
      "858/3000 train_loss: 77.11009216308594 test_loss:272.0347900390625\n",
      "859/3000 train_loss: 76.49890899658203 test_loss:264.1322326660156\n",
      "860/3000 train_loss: 64.93228149414062 test_loss:237.4020538330078\n",
      "861/3000 train_loss: 62.19577407836914 test_loss:261.86602783203125\n",
      "862/3000 train_loss: 72.28091430664062 test_loss:253.14967346191406\n",
      "863/3000 train_loss: 66.79890441894531 test_loss:266.85400390625\n",
      "864/3000 train_loss: 73.71047973632812 test_loss:258.1620788574219\n",
      "865/3000 train_loss: 74.82964324951172 test_loss:248.23507690429688\n",
      "866/3000 train_loss: 65.7745361328125 test_loss:261.8825988769531\n",
      "867/3000 train_loss: 70.7174301147461 test_loss:270.4166564941406\n",
      "868/3000 train_loss: 71.89187622070312 test_loss:278.804443359375\n",
      "869/3000 train_loss: 72.0886459350586 test_loss:274.65362548828125\n",
      "870/3000 train_loss: 66.6952133178711 test_loss:277.0454406738281\n",
      "871/3000 train_loss: 75.97461700439453 test_loss:242.08099365234375\n",
      "872/3000 train_loss: 88.3121109008789 test_loss:273.2255859375\n",
      "873/3000 train_loss: 86.96453857421875 test_loss:287.07269287109375\n",
      "874/3000 train_loss: 71.6401596069336 test_loss:259.9508056640625\n",
      "875/3000 train_loss: 78.39974975585938 test_loss:276.10107421875\n",
      "876/3000 train_loss: 66.93098449707031 test_loss:242.14967346191406\n",
      "877/3000 train_loss: 78.87206268310547 test_loss:239.62420654296875\n",
      "878/3000 train_loss: 70.61346435546875 test_loss:235.75750732421875\n",
      "879/3000 train_loss: 62.455711364746094 test_loss:241.73019409179688\n",
      "880/3000 train_loss: 68.42337036132812 test_loss:235.47515869140625\n",
      "881/3000 train_loss: 68.25606536865234 test_loss:244.46646118164062\n",
      "882/3000 train_loss: 72.22074127197266 test_loss:243.58721923828125\n",
      "883/3000 train_loss: 63.2846794128418 test_loss:253.37637329101562\n",
      "884/3000 train_loss: 66.68863677978516 test_loss:232.85476684570312\n",
      "885/3000 train_loss: 71.7415542602539 test_loss:239.55038452148438\n",
      "886/3000 train_loss: 71.40420532226562 test_loss:251.12498474121094\n",
      "887/3000 train_loss: 83.03353118896484 test_loss:273.64886474609375\n",
      "888/3000 train_loss: 68.83948516845703 test_loss:256.14508056640625\n",
      "889/3000 train_loss: 66.81671905517578 test_loss:245.19700622558594\n",
      "890/3000 train_loss: 68.52699279785156 test_loss:247.05804443359375\n",
      "891/3000 train_loss: 67.0863265991211 test_loss:255.48013305664062\n",
      "892/3000 train_loss: 63.73396682739258 test_loss:239.45654296875\n",
      "893/3000 train_loss: 87.8865966796875 test_loss:238.21929931640625\n",
      "894/3000 train_loss: 70.04751586914062 test_loss:237.5513916015625\n",
      "895/3000 train_loss: 69.83930969238281 test_loss:248.3955078125\n",
      "896/3000 train_loss: 71.74420166015625 test_loss:262.91827392578125\n",
      "897/3000 train_loss: 76.85176849365234 test_loss:248.02342224121094\n",
      "898/3000 train_loss: 74.82334899902344 test_loss:250.95025634765625\n",
      "899/3000 train_loss: 77.32061767578125 test_loss:235.00836181640625\n",
      "900/3000 train_loss: 66.560302734375 test_loss:231.106689453125\n",
      "901/3000 train_loss: 72.55995178222656 test_loss:234.08465576171875\n",
      "902/3000 train_loss: 70.72066497802734 test_loss:231.17971801757812\n",
      "903/3000 train_loss: 75.625244140625 test_loss:233.8109893798828\n",
      "904/3000 train_loss: 68.71868896484375 test_loss:270.968017578125\n",
      "905/3000 train_loss: 71.46324920654297 test_loss:244.29971313476562\n",
      "906/3000 train_loss: 90.8003921508789 test_loss:259.33843994140625\n",
      "907/3000 train_loss: 70.3568344116211 test_loss:252.14511108398438\n",
      "908/3000 train_loss: 63.86357879638672 test_loss:252.55113220214844\n",
      "909/3000 train_loss: 71.87931060791016 test_loss:258.9254150390625\n",
      "910/3000 train_loss: 65.74685668945312 test_loss:254.82687377929688\n",
      "911/3000 train_loss: 80.67908477783203 test_loss:260.0828552246094\n",
      "912/3000 train_loss: 70.5131607055664 test_loss:257.0622253417969\n",
      "913/3000 train_loss: 72.0275650024414 test_loss:254.13003540039062\n",
      "914/3000 train_loss: 68.64482116699219 test_loss:261.24688720703125\n",
      "915/3000 train_loss: 76.3260726928711 test_loss:274.6952209472656\n",
      "916/3000 train_loss: 73.6554183959961 test_loss:242.14456176757812\n",
      "917/3000 train_loss: 73.7521743774414 test_loss:258.9791564941406\n",
      "918/3000 train_loss: 73.09353637695312 test_loss:264.37005615234375\n",
      "919/3000 train_loss: 65.08877563476562 test_loss:260.9871520996094\n",
      "920/3000 train_loss: 73.35356903076172 test_loss:267.38494873046875\n",
      "921/3000 train_loss: 71.77262115478516 test_loss:257.9659729003906\n",
      "922/3000 train_loss: 64.38445281982422 test_loss:259.41192626953125\n",
      "923/3000 train_loss: 68.15361785888672 test_loss:269.81109619140625\n",
      "924/3000 train_loss: 69.69608306884766 test_loss:231.93797302246094\n",
      "925/3000 train_loss: 64.94097900390625 test_loss:236.78707885742188\n",
      "926/3000 train_loss: 70.4443130493164 test_loss:249.22894287109375\n",
      "927/3000 train_loss: 63.5571174621582 test_loss:232.94174194335938\n",
      "928/3000 train_loss: 67.14734649658203 test_loss:241.59165954589844\n",
      "929/3000 train_loss: 64.02367401123047 test_loss:233.98480224609375\n",
      "930/3000 train_loss: 64.1978759765625 test_loss:230.78216552734375\n",
      "931/3000 train_loss: 73.24459838867188 test_loss:244.606201171875\n",
      "932/3000 train_loss: 70.69855499267578 test_loss:243.74362182617188\n",
      "933/3000 train_loss: 73.07317352294922 test_loss:234.39190673828125\n",
      "934/3000 train_loss: 60.562931060791016 test_loss:227.51364135742188\n",
      "935/3000 train_loss: 67.77275085449219 test_loss:229.11598205566406\n",
      "936/3000 train_loss: 66.1091537475586 test_loss:225.49002075195312\n",
      "937/3000 train_loss: 63.76133728027344 test_loss:230.52484130859375\n",
      "938/3000 train_loss: 63.216304779052734 test_loss:233.95823669433594\n",
      "939/3000 train_loss: 67.3876953125 test_loss:241.16920471191406\n",
      "940/3000 train_loss: 65.92097473144531 test_loss:228.4464111328125\n",
      "941/3000 train_loss: 73.7125015258789 test_loss:232.1969451904297\n",
      "942/3000 train_loss: 67.52902221679688 test_loss:233.80517578125\n",
      "943/3000 train_loss: 59.72059631347656 test_loss:229.1049041748047\n",
      "944/3000 train_loss: 67.72228240966797 test_loss:229.33840942382812\n",
      "945/3000 train_loss: 63.35991287231445 test_loss:228.67080688476562\n",
      "946/3000 train_loss: 64.66900634765625 test_loss:228.70431518554688\n",
      "947/3000 train_loss: 68.0835952758789 test_loss:227.72125244140625\n",
      "948/3000 train_loss: 67.70478057861328 test_loss:233.74168395996094\n",
      "949/3000 train_loss: 63.58238983154297 test_loss:231.3223419189453\n",
      "950/3000 train_loss: 68.91449737548828 test_loss:236.74209594726562\n",
      "951/3000 train_loss: 64.11409759521484 test_loss:242.52194213867188\n",
      "952/3000 train_loss: 70.20207977294922 test_loss:242.703125\n",
      "953/3000 train_loss: 61.89328384399414 test_loss:230.9151611328125\n",
      "954/3000 train_loss: 69.73239135742188 test_loss:229.3739776611328\n",
      "955/3000 train_loss: 62.25046157836914 test_loss:230.1187744140625\n",
      "956/3000 train_loss: 65.76319885253906 test_loss:226.18455505371094\n",
      "957/3000 train_loss: 66.25858306884766 test_loss:227.09478759765625\n",
      "958/3000 train_loss: 66.37085723876953 test_loss:224.6063690185547\n",
      "959/3000 train_loss: 58.445762634277344 test_loss:227.18701171875\n",
      "960/3000 train_loss: 63.37071990966797 test_loss:225.68820190429688\n",
      "961/3000 train_loss: 59.2526969909668 test_loss:225.95831298828125\n",
      "962/3000 train_loss: 74.83440399169922 test_loss:230.98019409179688\n",
      "963/3000 train_loss: 67.48526763916016 test_loss:236.59494018554688\n",
      "964/3000 train_loss: 66.34840393066406 test_loss:231.54457092285156\n",
      "965/3000 train_loss: 62.87431335449219 test_loss:228.37957763671875\n",
      "966/3000 train_loss: 66.1580581665039 test_loss:226.395263671875\n",
      "967/3000 train_loss: 58.36931610107422 test_loss:228.08639526367188\n",
      "968/3000 train_loss: 64.17198181152344 test_loss:229.5875244140625\n",
      "969/3000 train_loss: 70.72405242919922 test_loss:240.0057830810547\n",
      "970/3000 train_loss: 63.57884216308594 test_loss:231.0688934326172\n",
      "971/3000 train_loss: 71.71297454833984 test_loss:232.29571533203125\n",
      "972/3000 train_loss: 65.94844055175781 test_loss:232.39181518554688\n",
      "973/3000 train_loss: 69.06819915771484 test_loss:238.7575225830078\n",
      "974/3000 train_loss: 58.791847229003906 test_loss:230.65573120117188\n",
      "975/3000 train_loss: 60.79631805419922 test_loss:232.83474731445312\n",
      "976/3000 train_loss: 64.07173156738281 test_loss:241.59539794921875\n",
      "977/3000 train_loss: 67.296630859375 test_loss:244.5950164794922\n",
      "978/3000 train_loss: 63.20616912841797 test_loss:235.72195434570312\n",
      "979/3000 train_loss: 59.19429397583008 test_loss:242.15545654296875\n",
      "980/3000 train_loss: 75.67594909667969 test_loss:240.24478149414062\n",
      "981/3000 train_loss: 65.84882354736328 test_loss:235.05047607421875\n",
      "982/3000 train_loss: 63.82286071777344 test_loss:233.15554809570312\n",
      "983/3000 train_loss: 64.42899322509766 test_loss:245.15399169921875\n",
      "984/3000 train_loss: 60.62666320800781 test_loss:241.6732940673828\n",
      "985/3000 train_loss: 60.636627197265625 test_loss:244.81021118164062\n",
      "986/3000 train_loss: 63.2978515625 test_loss:246.97518920898438\n",
      "987/3000 train_loss: 60.493900299072266 test_loss:243.76068115234375\n",
      "988/3000 train_loss: 64.47344207763672 test_loss:249.96615600585938\n",
      "989/3000 train_loss: 64.7377700805664 test_loss:245.74688720703125\n",
      "990/3000 train_loss: 65.10621643066406 test_loss:241.135009765625\n",
      "991/3000 train_loss: 61.412166595458984 test_loss:243.92098999023438\n",
      "992/3000 train_loss: 58.6854133605957 test_loss:228.1207733154297\n",
      "993/3000 train_loss: 53.744571685791016 test_loss:231.68185424804688\n",
      "994/3000 train_loss: 58.846256256103516 test_loss:223.26779174804688\n",
      "995/3000 train_loss: 72.5062484741211 test_loss:220.81407165527344\n",
      "996/3000 train_loss: 59.33460235595703 test_loss:222.21432495117188\n",
      "997/3000 train_loss: 65.279296875 test_loss:220.9986572265625\n",
      "998/3000 train_loss: 59.32069396972656 test_loss:224.67791748046875\n",
      "999/3000 train_loss: 61.733299255371094 test_loss:227.07327270507812\n",
      "1000/3000 train_loss: 59.92473220825195 test_loss:242.53192138671875\n",
      "1001/3000 train_loss: 60.51463317871094 test_loss:234.28028869628906\n",
      "1002/3000 train_loss: 53.38307571411133 test_loss:223.7765350341797\n",
      "1003/3000 train_loss: 67.607177734375 test_loss:225.05235290527344\n",
      "1004/3000 train_loss: 77.59666442871094 test_loss:238.10952758789062\n",
      "1005/3000 train_loss: 64.47027587890625 test_loss:225.46246337890625\n",
      "1006/3000 train_loss: 57.355926513671875 test_loss:228.54725646972656\n",
      "1007/3000 train_loss: 58.42646789550781 test_loss:222.94183349609375\n",
      "1008/3000 train_loss: 59.6081657409668 test_loss:228.13946533203125\n",
      "1009/3000 train_loss: 59.50651168823242 test_loss:241.55862426757812\n",
      "1010/3000 train_loss: 70.68611145019531 test_loss:225.32144165039062\n",
      "1011/3000 train_loss: 64.73978424072266 test_loss:220.752197265625\n",
      "1012/3000 train_loss: 59.23969650268555 test_loss:222.4053955078125\n",
      "1013/3000 train_loss: 55.46179962158203 test_loss:221.02215576171875\n",
      "1014/3000 train_loss: 65.24310302734375 test_loss:247.3393096923828\n",
      "1015/3000 train_loss: 56.43399429321289 test_loss:242.8653564453125\n",
      "1016/3000 train_loss: 80.27816772460938 test_loss:223.256103515625\n",
      "1017/3000 train_loss: 65.49901580810547 test_loss:221.29324340820312\n",
      "1018/3000 train_loss: 68.09794616699219 test_loss:223.2825927734375\n",
      "1019/3000 train_loss: 58.52385330200195 test_loss:221.93377685546875\n",
      "1020/3000 train_loss: 62.00426483154297 test_loss:228.25802612304688\n",
      "1021/3000 train_loss: 59.90987777709961 test_loss:225.20913696289062\n",
      "1022/3000 train_loss: 64.93171691894531 test_loss:222.0375518798828\n",
      "1023/3000 train_loss: 58.12864685058594 test_loss:221.40646362304688\n",
      "1024/3000 train_loss: 59.401302337646484 test_loss:238.01785278320312\n",
      "1025/3000 train_loss: 64.06399536132812 test_loss:220.47708129882812\n",
      "1026/3000 train_loss: 66.41525268554688 test_loss:219.62307739257812\n",
      "1027/3000 train_loss: 55.97165298461914 test_loss:229.98251342773438\n",
      "1028/3000 train_loss: 71.2010498046875 test_loss:240.90438842773438\n",
      "1029/3000 train_loss: 59.86774444580078 test_loss:227.34254455566406\n",
      "1030/3000 train_loss: 64.31132507324219 test_loss:224.3258819580078\n",
      "1031/3000 train_loss: 66.87841796875 test_loss:225.45309448242188\n",
      "1032/3000 train_loss: 65.75118255615234 test_loss:223.23257446289062\n",
      "1033/3000 train_loss: 72.88287353515625 test_loss:224.88906860351562\n",
      "1034/3000 train_loss: 67.00853729248047 test_loss:218.109375\n",
      "1035/3000 train_loss: 58.59523010253906 test_loss:216.71804809570312\n",
      "1036/3000 train_loss: 74.96501159667969 test_loss:220.6726531982422\n",
      "1037/3000 train_loss: 64.11912536621094 test_loss:219.5428466796875\n",
      "1038/3000 train_loss: 52.226951599121094 test_loss:220.57928466796875\n",
      "1039/3000 train_loss: 54.83363342285156 test_loss:222.81346130371094\n",
      "1040/3000 train_loss: 59.244972229003906 test_loss:227.4317626953125\n",
      "1041/3000 train_loss: 58.85883331298828 test_loss:230.92190551757812\n",
      "1042/3000 train_loss: 73.04077911376953 test_loss:220.32037353515625\n",
      "1043/3000 train_loss: 61.6190185546875 test_loss:230.20046997070312\n",
      "1044/3000 train_loss: 58.83555603027344 test_loss:224.85159301757812\n",
      "1045/3000 train_loss: 64.29644012451172 test_loss:219.12734985351562\n",
      "1046/3000 train_loss: 56.82509994506836 test_loss:219.97543334960938\n",
      "1047/3000 train_loss: 55.01217269897461 test_loss:220.02752685546875\n",
      "1048/3000 train_loss: 57.8941650390625 test_loss:217.1785888671875\n",
      "1049/3000 train_loss: 57.54127502441406 test_loss:217.46279907226562\n",
      "1050/3000 train_loss: 51.15397262573242 test_loss:222.51707458496094\n",
      "1051/3000 train_loss: 54.288352966308594 test_loss:217.85723876953125\n",
      "1052/3000 train_loss: 56.42823791503906 test_loss:223.33448791503906\n",
      "1053/3000 train_loss: 57.5159912109375 test_loss:226.3173828125\n",
      "1054/3000 train_loss: 52.74860382080078 test_loss:225.61647033691406\n",
      "1055/3000 train_loss: 59.95661926269531 test_loss:225.69790649414062\n",
      "1056/3000 train_loss: 63.645477294921875 test_loss:224.06724548339844\n",
      "1057/3000 train_loss: 57.00444412231445 test_loss:221.2716064453125\n",
      "1058/3000 train_loss: 60.58180236816406 test_loss:230.20468139648438\n",
      "1059/3000 train_loss: 57.69819641113281 test_loss:233.37808227539062\n",
      "1060/3000 train_loss: 60.04747772216797 test_loss:228.69769287109375\n",
      "1061/3000 train_loss: 55.1886100769043 test_loss:231.0701904296875\n",
      "1062/3000 train_loss: 76.45917510986328 test_loss:235.82989501953125\n",
      "1063/3000 train_loss: 64.65930938720703 test_loss:228.53245544433594\n",
      "1064/3000 train_loss: 57.887413024902344 test_loss:226.78106689453125\n",
      "1065/3000 train_loss: 63.56645965576172 test_loss:227.5196533203125\n",
      "1066/3000 train_loss: 52.74858474731445 test_loss:228.68173217773438\n",
      "1067/3000 train_loss: 53.4177360534668 test_loss:235.2428741455078\n",
      "1068/3000 train_loss: 55.28535461425781 test_loss:229.40280151367188\n",
      "1069/3000 train_loss: 67.70013427734375 test_loss:234.072021484375\n",
      "1070/3000 train_loss: 56.82329559326172 test_loss:238.1878204345703\n",
      "1071/3000 train_loss: 57.020782470703125 test_loss:224.0303955078125\n",
      "1072/3000 train_loss: 49.737876892089844 test_loss:220.6764373779297\n",
      "1073/3000 train_loss: 64.3440933227539 test_loss:223.71629333496094\n",
      "1074/3000 train_loss: 55.274688720703125 test_loss:223.16302490234375\n",
      "1075/3000 train_loss: 61.16193771362305 test_loss:222.3096160888672\n",
      "1076/3000 train_loss: 69.11515808105469 test_loss:216.44203186035156\n",
      "1077/3000 train_loss: 58.241371154785156 test_loss:218.83297729492188\n",
      "1078/3000 train_loss: 58.34264373779297 test_loss:216.0929412841797\n",
      "1079/3000 train_loss: 57.146995544433594 test_loss:219.96145629882812\n",
      "1080/3000 train_loss: 57.01726150512695 test_loss:215.96633911132812\n",
      "1081/3000 train_loss: 56.985755920410156 test_loss:217.48004150390625\n",
      "1082/3000 train_loss: 55.262393951416016 test_loss:223.66140747070312\n",
      "1083/3000 train_loss: 55.08280944824219 test_loss:225.36032104492188\n",
      "1084/3000 train_loss: 57.64584732055664 test_loss:233.57443237304688\n",
      "1085/3000 train_loss: 55.65167999267578 test_loss:231.3345947265625\n",
      "1086/3000 train_loss: 58.75369644165039 test_loss:225.06253051757812\n",
      "1087/3000 train_loss: 55.10296630859375 test_loss:231.17526245117188\n",
      "1088/3000 train_loss: 54.76714324951172 test_loss:231.0658721923828\n",
      "1089/3000 train_loss: 53.159793853759766 test_loss:231.0147705078125\n",
      "1090/3000 train_loss: 89.71247100830078 test_loss:220.83615112304688\n",
      "1091/3000 train_loss: 57.9559326171875 test_loss:220.0144805908203\n",
      "1092/3000 train_loss: 49.999488830566406 test_loss:213.58306884765625\n",
      "1093/3000 train_loss: 54.02655029296875 test_loss:214.61505126953125\n",
      "1094/3000 train_loss: 58.085201263427734 test_loss:214.3761444091797\n",
      "1095/3000 train_loss: 59.3153076171875 test_loss:212.74380493164062\n",
      "1096/3000 train_loss: 61.548831939697266 test_loss:222.5701904296875\n",
      "1097/3000 train_loss: 51.256629943847656 test_loss:226.63372802734375\n",
      "1098/3000 train_loss: 64.3215103149414 test_loss:216.25546264648438\n",
      "1099/3000 train_loss: 63.28717803955078 test_loss:214.43634033203125\n",
      "1100/3000 train_loss: 53.11758804321289 test_loss:213.76385498046875\n",
      "1101/3000 train_loss: 56.96096420288086 test_loss:213.44097900390625\n",
      "1102/3000 train_loss: 57.590946197509766 test_loss:217.17324829101562\n",
      "1103/3000 train_loss: 61.01390075683594 test_loss:214.02256774902344\n",
      "1104/3000 train_loss: 101.01712799072266 test_loss:244.75436401367188\n",
      "1105/3000 train_loss: 90.41084289550781 test_loss:243.12550354003906\n",
      "1106/3000 train_loss: 65.85038757324219 test_loss:221.75894165039062\n",
      "1107/3000 train_loss: 60.216392517089844 test_loss:220.38510131835938\n",
      "1108/3000 train_loss: 57.09049606323242 test_loss:219.2578125\n",
      "1109/3000 train_loss: 55.64593505859375 test_loss:222.81405639648438\n",
      "1110/3000 train_loss: 56.48276138305664 test_loss:223.5311279296875\n",
      "1111/3000 train_loss: 56.75850296020508 test_loss:217.8916778564453\n",
      "1112/3000 train_loss: 86.06632995605469 test_loss:222.3793182373047\n",
      "1113/3000 train_loss: 63.32695007324219 test_loss:219.38356018066406\n",
      "1114/3000 train_loss: 63.28255844116211 test_loss:215.4242401123047\n",
      "1115/3000 train_loss: 55.22216796875 test_loss:210.9823455810547\n",
      "1116/3000 train_loss: 61.57460021972656 test_loss:214.19268798828125\n",
      "1117/3000 train_loss: 58.222023010253906 test_loss:217.90550231933594\n",
      "1118/3000 train_loss: 56.76881790161133 test_loss:222.41671752929688\n",
      "1119/3000 train_loss: 53.79258346557617 test_loss:217.6453399658203\n",
      "1120/3000 train_loss: 54.20304489135742 test_loss:227.81134033203125\n",
      "1121/3000 train_loss: 53.685035705566406 test_loss:215.8570556640625\n",
      "1122/3000 train_loss: 61.7218017578125 test_loss:220.4595947265625\n",
      "1123/3000 train_loss: 54.022762298583984 test_loss:218.17694091796875\n",
      "1124/3000 train_loss: 53.01853561401367 test_loss:225.49012756347656\n",
      "1125/3000 train_loss: 52.02927017211914 test_loss:228.45700073242188\n",
      "1126/3000 train_loss: 53.24424743652344 test_loss:225.0260009765625\n",
      "1127/3000 train_loss: 51.77167892456055 test_loss:225.49407958984375\n",
      "1128/3000 train_loss: 69.89906311035156 test_loss:222.77682495117188\n",
      "1129/3000 train_loss: 59.30937957763672 test_loss:213.09164428710938\n",
      "1130/3000 train_loss: 52.04505920410156 test_loss:210.72683715820312\n",
      "1131/3000 train_loss: 54.23439407348633 test_loss:211.20648193359375\n",
      "1132/3000 train_loss: 59.39327621459961 test_loss:211.3987579345703\n",
      "1133/3000 train_loss: 52.87298583984375 test_loss:210.73252868652344\n",
      "1134/3000 train_loss: 51.96461868286133 test_loss:212.07200622558594\n",
      "1135/3000 train_loss: 52.625694274902344 test_loss:212.976806640625\n",
      "1136/3000 train_loss: 55.1124153137207 test_loss:222.2363739013672\n",
      "1137/3000 train_loss: 55.526485443115234 test_loss:217.60134887695312\n",
      "1138/3000 train_loss: 56.13646697998047 test_loss:216.76197814941406\n",
      "1139/3000 train_loss: 57.32411193847656 test_loss:214.54701232910156\n",
      "1140/3000 train_loss: 54.245391845703125 test_loss:214.29928588867188\n",
      "1141/3000 train_loss: 51.83005142211914 test_loss:209.85922241210938\n",
      "1142/3000 train_loss: 53.49505615234375 test_loss:225.385986328125\n",
      "1143/3000 train_loss: 53.51852035522461 test_loss:226.44515991210938\n",
      "1144/3000 train_loss: 55.933311462402344 test_loss:214.13475036621094\n",
      "1145/3000 train_loss: 56.808509826660156 test_loss:210.41046142578125\n",
      "1146/3000 train_loss: 51.64402389526367 test_loss:210.17904663085938\n",
      "1147/3000 train_loss: 54.45278549194336 test_loss:212.37387084960938\n",
      "1148/3000 train_loss: 51.845741271972656 test_loss:222.90139770507812\n",
      "1149/3000 train_loss: 59.566131591796875 test_loss:211.85931396484375\n",
      "1150/3000 train_loss: 54.95118713378906 test_loss:212.9910125732422\n",
      "1151/3000 train_loss: 55.10036849975586 test_loss:216.4105224609375\n",
      "1152/3000 train_loss: 49.36460876464844 test_loss:211.77029418945312\n",
      "1153/3000 train_loss: 53.17131805419922 test_loss:222.38253784179688\n",
      "1154/3000 train_loss: 51.450984954833984 test_loss:218.27072143554688\n",
      "1155/3000 train_loss: 55.25981903076172 test_loss:216.8720703125\n",
      "1156/3000 train_loss: 61.4301643371582 test_loss:213.24459838867188\n",
      "1157/3000 train_loss: 53.52960205078125 test_loss:229.86001586914062\n",
      "1158/3000 train_loss: 55.82511520385742 test_loss:210.75091552734375\n",
      "1159/3000 train_loss: 49.67693328857422 test_loss:217.65289306640625\n",
      "1160/3000 train_loss: 54.93439483642578 test_loss:220.6061553955078\n",
      "1161/3000 train_loss: 72.17927551269531 test_loss:215.834228515625\n",
      "1162/3000 train_loss: 55.57871627807617 test_loss:227.49395751953125\n",
      "1163/3000 train_loss: 49.35358428955078 test_loss:214.81417846679688\n",
      "1164/3000 train_loss: 55.320369720458984 test_loss:215.5250244140625\n",
      "1165/3000 train_loss: 50.861175537109375 test_loss:220.63467407226562\n",
      "1166/3000 train_loss: 51.2115592956543 test_loss:214.96157836914062\n",
      "1167/3000 train_loss: 55.07440948486328 test_loss:217.192138671875\n",
      "1168/3000 train_loss: 52.74818420410156 test_loss:215.5640106201172\n",
      "1169/3000 train_loss: 64.48495483398438 test_loss:214.99880981445312\n",
      "1170/3000 train_loss: 55.88072204589844 test_loss:218.31619262695312\n",
      "1171/3000 train_loss: 54.53333282470703 test_loss:209.67430114746094\n",
      "1172/3000 train_loss: 58.646278381347656 test_loss:210.98756408691406\n",
      "1173/3000 train_loss: 48.688209533691406 test_loss:214.84451293945312\n",
      "1174/3000 train_loss: 57.91332244873047 test_loss:212.72869873046875\n",
      "1175/3000 train_loss: 62.661476135253906 test_loss:207.43960571289062\n",
      "1176/3000 train_loss: 54.73481750488281 test_loss:209.04522705078125\n",
      "1177/3000 train_loss: 50.78825378417969 test_loss:209.28099060058594\n",
      "1178/3000 train_loss: 51.515586853027344 test_loss:209.92947387695312\n",
      "1179/3000 train_loss: 48.74689865112305 test_loss:212.50054931640625\n",
      "1180/3000 train_loss: 50.89714050292969 test_loss:211.58297729492188\n",
      "1181/3000 train_loss: 52.56182098388672 test_loss:208.57589721679688\n",
      "1182/3000 train_loss: 50.574520111083984 test_loss:207.4716796875\n",
      "1183/3000 train_loss: 52.0211067199707 test_loss:213.05868530273438\n",
      "1184/3000 train_loss: 55.80103302001953 test_loss:216.58270263671875\n",
      "1185/3000 train_loss: 58.00839614868164 test_loss:207.75634765625\n",
      "1186/3000 train_loss: 50.09254837036133 test_loss:212.980224609375\n",
      "1187/3000 train_loss: 53.247703552246094 test_loss:209.27882385253906\n",
      "1188/3000 train_loss: 45.93654251098633 test_loss:208.369140625\n",
      "1189/3000 train_loss: 53.607940673828125 test_loss:212.52439880371094\n",
      "1190/3000 train_loss: 47.337284088134766 test_loss:224.12803649902344\n",
      "1191/3000 train_loss: 56.37019348144531 test_loss:213.80007934570312\n",
      "1192/3000 train_loss: 54.402061462402344 test_loss:213.0497589111328\n",
      "1193/3000 train_loss: 54.830623626708984 test_loss:213.665771484375\n",
      "1194/3000 train_loss: 49.88774490356445 test_loss:209.60012817382812\n",
      "1195/3000 train_loss: 50.213436126708984 test_loss:207.04330444335938\n",
      "1196/3000 train_loss: 50.54488754272461 test_loss:210.38034057617188\n",
      "1197/3000 train_loss: 58.8457145690918 test_loss:216.72586059570312\n",
      "1198/3000 train_loss: 66.84635162353516 test_loss:223.22198486328125\n",
      "1199/3000 train_loss: 57.71818923950195 test_loss:222.57882690429688\n",
      "1200/3000 train_loss: 48.6497688293457 test_loss:208.15908813476562\n",
      "1201/3000 train_loss: 44.23666763305664 test_loss:211.62664794921875\n",
      "1202/3000 train_loss: 50.27109146118164 test_loss:221.2053680419922\n",
      "1203/3000 train_loss: 53.91073226928711 test_loss:221.5821533203125\n",
      "1204/3000 train_loss: 52.28532791137695 test_loss:226.3594207763672\n",
      "1205/3000 train_loss: 50.25090789794922 test_loss:218.7554931640625\n",
      "1206/3000 train_loss: 51.307151794433594 test_loss:222.57839965820312\n",
      "1207/3000 train_loss: 56.78268814086914 test_loss:233.046142578125\n",
      "1208/3000 train_loss: 54.38618087768555 test_loss:215.25112915039062\n",
      "1209/3000 train_loss: 51.99386215209961 test_loss:223.15301513671875\n",
      "1210/3000 train_loss: 50.76543426513672 test_loss:218.45492553710938\n",
      "1211/3000 train_loss: 46.28278732299805 test_loss:221.49172973632812\n",
      "1212/3000 train_loss: 53.82229995727539 test_loss:210.30422973632812\n",
      "1213/3000 train_loss: 54.18882751464844 test_loss:216.79721069335938\n",
      "1214/3000 train_loss: 53.171566009521484 test_loss:215.6459503173828\n",
      "1215/3000 train_loss: 50.346736907958984 test_loss:212.3668975830078\n",
      "1216/3000 train_loss: 50.5451774597168 test_loss:209.85736083984375\n",
      "1217/3000 train_loss: 50.058326721191406 test_loss:223.2064666748047\n",
      "1218/3000 train_loss: 58.30715560913086 test_loss:218.22947692871094\n",
      "1219/3000 train_loss: 51.69232940673828 test_loss:215.73583984375\n",
      "1220/3000 train_loss: 65.58787536621094 test_loss:212.5242919921875\n",
      "1221/3000 train_loss: 54.06566619873047 test_loss:226.0904541015625\n",
      "1222/3000 train_loss: 53.98835372924805 test_loss:225.7850799560547\n",
      "1223/3000 train_loss: 52.123451232910156 test_loss:218.8749542236328\n",
      "1224/3000 train_loss: 43.38251876831055 test_loss:224.57090759277344\n",
      "1225/3000 train_loss: 48.207820892333984 test_loss:221.60003662109375\n",
      "1226/3000 train_loss: 48.12940216064453 test_loss:225.66839599609375\n",
      "1227/3000 train_loss: 57.8742561340332 test_loss:226.99752807617188\n",
      "1228/3000 train_loss: 48.44267272949219 test_loss:230.37750244140625\n",
      "1229/3000 train_loss: 52.34163284301758 test_loss:227.26040649414062\n",
      "1230/3000 train_loss: 49.81272888183594 test_loss:223.30181884765625\n",
      "1231/3000 train_loss: 46.868465423583984 test_loss:231.57122802734375\n",
      "1232/3000 train_loss: 48.89333724975586 test_loss:217.76283264160156\n",
      "1233/3000 train_loss: 59.55073928833008 test_loss:217.83326721191406\n",
      "1234/3000 train_loss: 55.590972900390625 test_loss:212.67852783203125\n",
      "1235/3000 train_loss: 59.46014404296875 test_loss:208.38430786132812\n",
      "1236/3000 train_loss: 52.893951416015625 test_loss:210.17645263671875\n",
      "1237/3000 train_loss: 51.50908279418945 test_loss:204.86569213867188\n",
      "1238/3000 train_loss: 44.71115493774414 test_loss:206.9679718017578\n",
      "1239/3000 train_loss: 53.06277084350586 test_loss:228.66818237304688\n",
      "1240/3000 train_loss: 50.092567443847656 test_loss:218.29226684570312\n",
      "1241/3000 train_loss: 48.242794036865234 test_loss:216.91024780273438\n",
      "1242/3000 train_loss: 47.802978515625 test_loss:207.08189392089844\n",
      "1243/3000 train_loss: 54.46076965332031 test_loss:212.3172149658203\n",
      "1244/3000 train_loss: 46.88136291503906 test_loss:216.913818359375\n",
      "1245/3000 train_loss: 55.34784698486328 test_loss:220.77825927734375\n",
      "1246/3000 train_loss: 48.49006652832031 test_loss:214.95703125\n",
      "1247/3000 train_loss: 55.20314025878906 test_loss:213.55096435546875\n",
      "1248/3000 train_loss: 50.79195785522461 test_loss:216.8133544921875\n",
      "1249/3000 train_loss: 51.18431091308594 test_loss:206.4503173828125\n",
      "1250/3000 train_loss: 46.695945739746094 test_loss:210.77157592773438\n",
      "1251/3000 train_loss: 51.18323516845703 test_loss:212.76046752929688\n",
      "1252/3000 train_loss: 52.669090270996094 test_loss:209.20626831054688\n",
      "1253/3000 train_loss: 48.32703399658203 test_loss:213.49710083007812\n",
      "1254/3000 train_loss: 61.60179901123047 test_loss:217.25680541992188\n",
      "1255/3000 train_loss: 46.29142379760742 test_loss:211.2139434814453\n",
      "1256/3000 train_loss: 48.222843170166016 test_loss:218.27774047851562\n",
      "1257/3000 train_loss: 58.601356506347656 test_loss:210.49526977539062\n",
      "1258/3000 train_loss: 52.5493278503418 test_loss:207.80987548828125\n",
      "1259/3000 train_loss: 52.079185485839844 test_loss:210.7012939453125\n",
      "1260/3000 train_loss: 48.8525505065918 test_loss:208.85597229003906\n",
      "1261/3000 train_loss: 48.99069595336914 test_loss:208.15914916992188\n",
      "1262/3000 train_loss: 52.472171783447266 test_loss:211.85427856445312\n",
      "1263/3000 train_loss: 49.19697952270508 test_loss:209.10556030273438\n",
      "1264/3000 train_loss: 66.53273010253906 test_loss:219.90936279296875\n",
      "1265/3000 train_loss: 54.120704650878906 test_loss:208.6182098388672\n",
      "1266/3000 train_loss: 48.78437423706055 test_loss:210.32113647460938\n",
      "1267/3000 train_loss: 52.36064910888672 test_loss:213.59130859375\n",
      "1268/3000 train_loss: 47.48632049560547 test_loss:217.4209747314453\n",
      "1269/3000 train_loss: 60.53233337402344 test_loss:208.00686645507812\n",
      "1270/3000 train_loss: 48.05690002441406 test_loss:213.49888610839844\n",
      "1271/3000 train_loss: 55.15495300292969 test_loss:218.80136108398438\n",
      "1272/3000 train_loss: 50.85106658935547 test_loss:205.21673583984375\n",
      "1273/3000 train_loss: 47.638404846191406 test_loss:203.16983032226562\n",
      "1274/3000 train_loss: 51.81700134277344 test_loss:211.56182861328125\n",
      "1275/3000 train_loss: 53.27454376220703 test_loss:208.4222412109375\n",
      "1276/3000 train_loss: 48.27098846435547 test_loss:209.50143432617188\n",
      "1277/3000 train_loss: 49.755428314208984 test_loss:212.01162719726562\n",
      "1278/3000 train_loss: 47.195640563964844 test_loss:213.3743896484375\n",
      "1279/3000 train_loss: 51.51695251464844 test_loss:212.40499877929688\n",
      "1280/3000 train_loss: 60.70051574707031 test_loss:203.29782104492188\n",
      "1281/3000 train_loss: 49.6880989074707 test_loss:209.26806640625\n",
      "1282/3000 train_loss: 46.70098114013672 test_loss:209.72314453125\n",
      "1283/3000 train_loss: 45.2266845703125 test_loss:211.3088836669922\n",
      "1284/3000 train_loss: 52.16410446166992 test_loss:219.2567138671875\n",
      "1285/3000 train_loss: 47.47621154785156 test_loss:213.8855438232422\n",
      "1286/3000 train_loss: 46.93612289428711 test_loss:209.0061798095703\n",
      "1287/3000 train_loss: 48.68979263305664 test_loss:210.14544677734375\n",
      "1288/3000 train_loss: 48.20582580566406 test_loss:206.18316650390625\n",
      "1289/3000 train_loss: 51.33584213256836 test_loss:204.35113525390625\n",
      "1290/3000 train_loss: 46.3614501953125 test_loss:209.23663330078125\n",
      "1291/3000 train_loss: 42.953128814697266 test_loss:206.09207153320312\n",
      "1292/3000 train_loss: 50.602020263671875 test_loss:209.943359375\n",
      "1293/3000 train_loss: 62.99840545654297 test_loss:216.288330078125\n",
      "1294/3000 train_loss: 61.7514533996582 test_loss:221.36026000976562\n",
      "1295/3000 train_loss: 48.774105072021484 test_loss:206.7650604248047\n",
      "1296/3000 train_loss: 43.877830505371094 test_loss:206.34014892578125\n",
      "1297/3000 train_loss: 51.71205520629883 test_loss:212.61244201660156\n",
      "1298/3000 train_loss: 48.80504608154297 test_loss:213.0865478515625\n",
      "1299/3000 train_loss: 46.35420608520508 test_loss:207.70176696777344\n",
      "1300/3000 train_loss: 48.64168167114258 test_loss:208.94473266601562\n",
      "1301/3000 train_loss: 44.78013229370117 test_loss:216.7910919189453\n",
      "1302/3000 train_loss: 48.91078186035156 test_loss:214.00384521484375\n",
      "1303/3000 train_loss: 47.50843048095703 test_loss:215.78709411621094\n",
      "1304/3000 train_loss: 47.57957077026367 test_loss:217.38595581054688\n",
      "1305/3000 train_loss: 49.94783401489258 test_loss:221.86317443847656\n",
      "1306/3000 train_loss: 55.810943603515625 test_loss:220.59254455566406\n",
      "1307/3000 train_loss: 49.317909240722656 test_loss:220.28929138183594\n",
      "1308/3000 train_loss: 49.50394821166992 test_loss:203.36000061035156\n",
      "1309/3000 train_loss: 49.94892120361328 test_loss:209.45790100097656\n",
      "1310/3000 train_loss: 52.709190368652344 test_loss:205.50892639160156\n",
      "1311/3000 train_loss: 46.162723541259766 test_loss:211.7336883544922\n",
      "1312/3000 train_loss: 49.79530715942383 test_loss:210.62802124023438\n",
      "1313/3000 train_loss: 47.53535079956055 test_loss:205.8150634765625\n",
      "1314/3000 train_loss: 42.61593246459961 test_loss:203.18251037597656\n",
      "1315/3000 train_loss: 43.23389434814453 test_loss:213.8238525390625\n",
      "1316/3000 train_loss: 43.20557403564453 test_loss:217.8740997314453\n",
      "1317/3000 train_loss: 49.28057861328125 test_loss:218.41493225097656\n",
      "1318/3000 train_loss: 50.91951370239258 test_loss:209.04434204101562\n",
      "1319/3000 train_loss: 42.573368072509766 test_loss:212.74496459960938\n",
      "1320/3000 train_loss: 43.8973503112793 test_loss:213.97027587890625\n",
      "1321/3000 train_loss: 44.15516662597656 test_loss:208.02804565429688\n",
      "1322/3000 train_loss: 47.19612121582031 test_loss:208.76583862304688\n",
      "1323/3000 train_loss: 62.72664260864258 test_loss:221.45706176757812\n",
      "1324/3000 train_loss: 43.06595993041992 test_loss:224.83877563476562\n",
      "1325/3000 train_loss: 63.38361358642578 test_loss:215.36558532714844\n",
      "1326/3000 train_loss: 51.205467224121094 test_loss:218.9093017578125\n",
      "1327/3000 train_loss: 58.17570114135742 test_loss:219.3599853515625\n",
      "1328/3000 train_loss: 47.470703125 test_loss:205.3731689453125\n",
      "1329/3000 train_loss: 47.449058532714844 test_loss:209.47731018066406\n",
      "1330/3000 train_loss: 47.262325286865234 test_loss:207.71807861328125\n",
      "1331/3000 train_loss: 44.23963928222656 test_loss:210.80551147460938\n",
      "1332/3000 train_loss: 48.99563217163086 test_loss:205.69895935058594\n",
      "1333/3000 train_loss: 43.20221710205078 test_loss:211.42807006835938\n",
      "1334/3000 train_loss: 54.615840911865234 test_loss:214.10220336914062\n",
      "1335/3000 train_loss: 46.92566680908203 test_loss:213.4493408203125\n",
      "1336/3000 train_loss: 46.248023986816406 test_loss:209.28948974609375\n",
      "1337/3000 train_loss: 44.72568893432617 test_loss:213.70462036132812\n",
      "1338/3000 train_loss: 48.766990661621094 test_loss:228.67507934570312\n",
      "1339/3000 train_loss: 42.55770492553711 test_loss:206.5253143310547\n",
      "1340/3000 train_loss: 46.92665481567383 test_loss:227.29119873046875\n",
      "1341/3000 train_loss: 47.6427116394043 test_loss:227.5441131591797\n",
      "1342/3000 train_loss: 48.69361877441406 test_loss:220.14694213867188\n",
      "1343/3000 train_loss: 43.91559982299805 test_loss:225.92022705078125\n",
      "1344/3000 train_loss: 45.340904235839844 test_loss:214.51681518554688\n",
      "1345/3000 train_loss: 50.590213775634766 test_loss:215.0192413330078\n",
      "1346/3000 train_loss: 50.69137191772461 test_loss:228.0448455810547\n",
      "1347/3000 train_loss: 45.914554595947266 test_loss:222.86459350585938\n",
      "1348/3000 train_loss: 48.83746337890625 test_loss:213.34454345703125\n",
      "1349/3000 train_loss: 53.26348114013672 test_loss:205.07781982421875\n",
      "1350/3000 train_loss: 52.66696548461914 test_loss:215.46759033203125\n",
      "1351/3000 train_loss: 51.63683319091797 test_loss:209.88534545898438\n",
      "1352/3000 train_loss: 43.446712493896484 test_loss:210.4202117919922\n",
      "1353/3000 train_loss: 44.461299896240234 test_loss:200.91836547851562\n",
      "1354/3000 train_loss: 45.22964859008789 test_loss:211.4997100830078\n",
      "1355/3000 train_loss: 42.14923095703125 test_loss:213.5609130859375\n",
      "1356/3000 train_loss: 42.14808654785156 test_loss:209.56655883789062\n",
      "1357/3000 train_loss: 45.651641845703125 test_loss:213.59320068359375\n",
      "1358/3000 train_loss: 54.527259826660156 test_loss:207.65591430664062\n",
      "1359/3000 train_loss: 61.324405670166016 test_loss:213.8585205078125\n",
      "1360/3000 train_loss: 51.93901062011719 test_loss:214.47824096679688\n",
      "1361/3000 train_loss: 55.13097381591797 test_loss:215.61370849609375\n",
      "1362/3000 train_loss: 49.472259521484375 test_loss:212.70867919921875\n",
      "1363/3000 train_loss: 57.32683181762695 test_loss:212.828857421875\n",
      "1364/3000 train_loss: 41.90034103393555 test_loss:204.84188842773438\n",
      "1365/3000 train_loss: 42.224735260009766 test_loss:201.12103271484375\n",
      "1366/3000 train_loss: 43.86530303955078 test_loss:203.9979705810547\n",
      "1367/3000 train_loss: 46.73127746582031 test_loss:200.6790771484375\n",
      "1368/3000 train_loss: 48.10005187988281 test_loss:200.8954315185547\n",
      "1369/3000 train_loss: 49.7433967590332 test_loss:211.83592224121094\n",
      "1370/3000 train_loss: 48.82134246826172 test_loss:212.23361206054688\n",
      "1371/3000 train_loss: 50.08551025390625 test_loss:204.03521728515625\n",
      "1372/3000 train_loss: 54.22399139404297 test_loss:200.6187744140625\n",
      "1373/3000 train_loss: 44.24579620361328 test_loss:206.10549926757812\n",
      "1374/3000 train_loss: 46.78546142578125 test_loss:203.31805419921875\n",
      "1375/3000 train_loss: 45.56593322753906 test_loss:203.23916625976562\n",
      "1376/3000 train_loss: 50.6069221496582 test_loss:203.32525634765625\n",
      "1377/3000 train_loss: 44.77924346923828 test_loss:220.52169799804688\n",
      "1378/3000 train_loss: 52.64099884033203 test_loss:215.2482452392578\n",
      "1379/3000 train_loss: 55.59479904174805 test_loss:212.8402862548828\n",
      "1380/3000 train_loss: 46.75806427001953 test_loss:201.93032836914062\n",
      "1381/3000 train_loss: 43.1907844543457 test_loss:212.00112915039062\n",
      "1382/3000 train_loss: 50.455291748046875 test_loss:209.14053344726562\n",
      "1383/3000 train_loss: 43.55300521850586 test_loss:210.96449279785156\n",
      "1384/3000 train_loss: 50.17219543457031 test_loss:213.95419311523438\n",
      "1385/3000 train_loss: 42.37901306152344 test_loss:221.57481384277344\n",
      "1386/3000 train_loss: 50.29280090332031 test_loss:214.27676391601562\n",
      "1387/3000 train_loss: 43.74374008178711 test_loss:214.88430786132812\n",
      "1388/3000 train_loss: 47.294822692871094 test_loss:200.18719482421875\n",
      "1389/3000 train_loss: 45.09003448486328 test_loss:206.54287719726562\n",
      "1390/3000 train_loss: 42.71378707885742 test_loss:209.3488311767578\n",
      "1391/3000 train_loss: 45.817325592041016 test_loss:227.87533569335938\n",
      "1392/3000 train_loss: 44.694122314453125 test_loss:217.74281311035156\n",
      "1393/3000 train_loss: 41.403438568115234 test_loss:226.32763671875\n",
      "1394/3000 train_loss: 41.11341094970703 test_loss:224.48770141601562\n",
      "1395/3000 train_loss: 46.288822174072266 test_loss:217.9625701904297\n",
      "1396/3000 train_loss: 44.73688888549805 test_loss:227.8858642578125\n",
      "1397/3000 train_loss: 43.44744110107422 test_loss:219.884765625\n",
      "1398/3000 train_loss: 46.44139099121094 test_loss:226.75692749023438\n",
      "1399/3000 train_loss: 40.031005859375 test_loss:234.73654174804688\n",
      "1400/3000 train_loss: 44.33494186401367 test_loss:216.86378479003906\n",
      "1401/3000 train_loss: 45.28509521484375 test_loss:216.4788818359375\n",
      "1402/3000 train_loss: 45.15398025512695 test_loss:218.70782470703125\n",
      "1403/3000 train_loss: 53.09963607788086 test_loss:220.8624267578125\n",
      "1404/3000 train_loss: 48.5014762878418 test_loss:218.77688598632812\n",
      "1405/3000 train_loss: 41.1750373840332 test_loss:221.75289916992188\n",
      "1406/3000 train_loss: 40.34907913208008 test_loss:216.485595703125\n",
      "1407/3000 train_loss: 52.46110916137695 test_loss:223.71646118164062\n",
      "1408/3000 train_loss: 48.81753921508789 test_loss:203.55172729492188\n",
      "1409/3000 train_loss: 46.21202850341797 test_loss:206.68556213378906\n",
      "1410/3000 train_loss: 61.8506965637207 test_loss:201.32818603515625\n",
      "1411/3000 train_loss: 46.749996185302734 test_loss:202.01016235351562\n",
      "1412/3000 train_loss: 45.39692687988281 test_loss:203.30319213867188\n",
      "1413/3000 train_loss: 50.373077392578125 test_loss:200.34658813476562\n",
      "1414/3000 train_loss: 40.396263122558594 test_loss:197.92947387695312\n",
      "1415/3000 train_loss: 41.576927185058594 test_loss:200.30755615234375\n",
      "1416/3000 train_loss: 40.984535217285156 test_loss:199.7467498779297\n",
      "1417/3000 train_loss: 43.588600158691406 test_loss:199.898193359375\n",
      "1418/3000 train_loss: 44.61259460449219 test_loss:199.3677978515625\n",
      "1419/3000 train_loss: 42.54255676269531 test_loss:204.01272583007812\n",
      "1420/3000 train_loss: 38.68963623046875 test_loss:201.89663696289062\n",
      "1421/3000 train_loss: 54.67526626586914 test_loss:201.62518310546875\n",
      "1422/3000 train_loss: 39.34606170654297 test_loss:208.37353515625\n",
      "1423/3000 train_loss: 43.73841857910156 test_loss:206.98626708984375\n",
      "1424/3000 train_loss: 38.903255462646484 test_loss:206.53912353515625\n",
      "1425/3000 train_loss: 40.36225128173828 test_loss:205.47598266601562\n",
      "1426/3000 train_loss: 42.372772216796875 test_loss:206.75033569335938\n",
      "1427/3000 train_loss: 46.30141067504883 test_loss:216.222412109375\n",
      "1428/3000 train_loss: 52.084800720214844 test_loss:218.8248291015625\n",
      "1429/3000 train_loss: 41.091094970703125 test_loss:224.13340759277344\n",
      "1430/3000 train_loss: 41.01116180419922 test_loss:223.24122619628906\n",
      "1431/3000 train_loss: 44.16289520263672 test_loss:214.4186553955078\n",
      "1432/3000 train_loss: 41.41783905029297 test_loss:213.01177978515625\n",
      "1433/3000 train_loss: 43.537330627441406 test_loss:227.12100219726562\n",
      "1434/3000 train_loss: 50.46357727050781 test_loss:218.35031127929688\n",
      "1435/3000 train_loss: 44.09156799316406 test_loss:208.77972412109375\n",
      "1436/3000 train_loss: 44.601131439208984 test_loss:203.874267578125\n",
      "1437/3000 train_loss: 45.43107986450195 test_loss:202.10655212402344\n",
      "1438/3000 train_loss: 37.6026725769043 test_loss:205.11663818359375\n",
      "1439/3000 train_loss: 40.72119903564453 test_loss:205.4580078125\n",
      "1440/3000 train_loss: 47.54702377319336 test_loss:207.20965576171875\n",
      "1441/3000 train_loss: 38.60658264160156 test_loss:208.61904907226562\n",
      "1442/3000 train_loss: 41.16560745239258 test_loss:220.6178436279297\n",
      "1443/3000 train_loss: 47.74996566772461 test_loss:216.68832397460938\n",
      "1444/3000 train_loss: 45.64529037475586 test_loss:213.64259338378906\n",
      "1445/3000 train_loss: 43.56880569458008 test_loss:215.06546020507812\n",
      "1446/3000 train_loss: 45.38413619995117 test_loss:204.82888793945312\n",
      "1447/3000 train_loss: 49.725982666015625 test_loss:206.12405395507812\n",
      "1448/3000 train_loss: 44.17997741699219 test_loss:213.81674194335938\n",
      "1449/3000 train_loss: 50.21086120605469 test_loss:216.7451171875\n",
      "1450/3000 train_loss: 47.344058990478516 test_loss:211.4077911376953\n",
      "1451/3000 train_loss: 50.29839324951172 test_loss:211.53024291992188\n",
      "1452/3000 train_loss: 42.741966247558594 test_loss:219.16098022460938\n",
      "1453/3000 train_loss: 45.460853576660156 test_loss:200.564453125\n",
      "1454/3000 train_loss: 51.21678924560547 test_loss:217.47100830078125\n",
      "1455/3000 train_loss: 40.39732360839844 test_loss:210.77578735351562\n",
      "1456/3000 train_loss: 43.37042999267578 test_loss:222.6121826171875\n",
      "1457/3000 train_loss: 44.833744049072266 test_loss:214.83782958984375\n",
      "1458/3000 train_loss: 45.48688888549805 test_loss:206.72174072265625\n",
      "1459/3000 train_loss: 43.17853546142578 test_loss:204.07876586914062\n",
      "1460/3000 train_loss: 45.40510559082031 test_loss:196.80673217773438\n",
      "1461/3000 train_loss: 47.30975341796875 test_loss:200.0293426513672\n",
      "1462/3000 train_loss: 39.77099609375 test_loss:198.96958923339844\n",
      "1463/3000 train_loss: 38.93077087402344 test_loss:196.64227294921875\n",
      "1464/3000 train_loss: 44.1830940246582 test_loss:196.59776306152344\n",
      "1465/3000 train_loss: 42.53096389770508 test_loss:200.08950805664062\n",
      "1466/3000 train_loss: 40.125205993652344 test_loss:203.6120147705078\n",
      "1467/3000 train_loss: 40.64304733276367 test_loss:200.94940185546875\n",
      "1468/3000 train_loss: 42.85552215576172 test_loss:201.3347930908203\n",
      "1469/3000 train_loss: 38.43537521362305 test_loss:203.64605712890625\n",
      "1470/3000 train_loss: 42.83228302001953 test_loss:204.4609375\n",
      "1471/3000 train_loss: 40.34360885620117 test_loss:197.67312622070312\n",
      "1472/3000 train_loss: 41.2698860168457 test_loss:193.2982177734375\n",
      "1473/3000 train_loss: 39.78760528564453 test_loss:195.0992431640625\n",
      "1474/3000 train_loss: 42.21991729736328 test_loss:196.6828155517578\n",
      "1475/3000 train_loss: 51.87278747558594 test_loss:198.46087646484375\n",
      "1476/3000 train_loss: 45.43802261352539 test_loss:205.225341796875\n",
      "1477/3000 train_loss: 41.969207763671875 test_loss:200.38134765625\n",
      "1478/3000 train_loss: 40.159889221191406 test_loss:200.03219604492188\n",
      "1479/3000 train_loss: 39.98788070678711 test_loss:202.94503784179688\n",
      "1480/3000 train_loss: 47.68766403198242 test_loss:202.79324340820312\n",
      "1481/3000 train_loss: 43.361358642578125 test_loss:208.7714080810547\n",
      "1482/3000 train_loss: 41.89057922363281 test_loss:207.99925231933594\n",
      "1483/3000 train_loss: 46.34833908081055 test_loss:204.27110290527344\n",
      "1484/3000 train_loss: 46.33429718017578 test_loss:202.1368408203125\n",
      "1485/3000 train_loss: 38.25339126586914 test_loss:206.80857849121094\n",
      "1486/3000 train_loss: 43.74768829345703 test_loss:199.28668212890625\n",
      "1487/3000 train_loss: 41.69432067871094 test_loss:208.79989624023438\n",
      "1488/3000 train_loss: 42.464378356933594 test_loss:206.16627502441406\n",
      "1489/3000 train_loss: 39.34495544433594 test_loss:199.15467834472656\n",
      "1490/3000 train_loss: 36.53179168701172 test_loss:197.58883666992188\n",
      "1491/3000 train_loss: 41.80900955200195 test_loss:222.53469848632812\n",
      "1492/3000 train_loss: 52.93403625488281 test_loss:203.84124755859375\n",
      "1493/3000 train_loss: 41.99285888671875 test_loss:202.72927856445312\n",
      "1494/3000 train_loss: 46.709197998046875 test_loss:208.55670166015625\n",
      "1495/3000 train_loss: 53.508575439453125 test_loss:200.77410888671875\n",
      "1496/3000 train_loss: 56.52488327026367 test_loss:194.12750244140625\n",
      "1497/3000 train_loss: 38.44202423095703 test_loss:196.54998779296875\n",
      "1498/3000 train_loss: 51.80023193359375 test_loss:203.86318969726562\n",
      "1499/3000 train_loss: 46.418277740478516 test_loss:194.7259521484375\n",
      "1500/3000 train_loss: 39.665428161621094 test_loss:193.36753845214844\n",
      "1501/3000 train_loss: 49.005950927734375 test_loss:209.24423217773438\n",
      "1502/3000 train_loss: 50.51130294799805 test_loss:198.6422119140625\n",
      "1503/3000 train_loss: 41.89996337890625 test_loss:200.8062744140625\n",
      "1504/3000 train_loss: 39.819557189941406 test_loss:197.4759521484375\n",
      "1505/3000 train_loss: 39.43849182128906 test_loss:199.71481323242188\n",
      "1506/3000 train_loss: 45.161476135253906 test_loss:198.19570922851562\n",
      "1507/3000 train_loss: 39.07543182373047 test_loss:201.7830810546875\n",
      "1508/3000 train_loss: 40.48032760620117 test_loss:202.1534423828125\n",
      "1509/3000 train_loss: 44.21196746826172 test_loss:207.8387451171875\n",
      "1510/3000 train_loss: 47.033447265625 test_loss:215.3243408203125\n",
      "1511/3000 train_loss: 40.0592155456543 test_loss:197.18719482421875\n",
      "1512/3000 train_loss: 41.30022048950195 test_loss:197.1995086669922\n",
      "1513/3000 train_loss: 41.846595764160156 test_loss:199.60984802246094\n",
      "1514/3000 train_loss: 37.99851608276367 test_loss:198.87725830078125\n",
      "1515/3000 train_loss: 43.55377197265625 test_loss:200.28941345214844\n",
      "1516/3000 train_loss: 40.8121223449707 test_loss:203.35110473632812\n",
      "1517/3000 train_loss: 39.80326461791992 test_loss:209.82510375976562\n",
      "1518/3000 train_loss: 38.54054260253906 test_loss:199.27410888671875\n",
      "1519/3000 train_loss: 39.42837905883789 test_loss:199.6114959716797\n",
      "1520/3000 train_loss: 38.21595001220703 test_loss:198.0720977783203\n",
      "1521/3000 train_loss: 35.79301834106445 test_loss:208.2435302734375\n",
      "1522/3000 train_loss: 39.271732330322266 test_loss:196.84127807617188\n",
      "1523/3000 train_loss: 45.26802062988281 test_loss:203.96383666992188\n",
      "1524/3000 train_loss: 40.7613410949707 test_loss:200.51443481445312\n",
      "1525/3000 train_loss: 44.155738830566406 test_loss:199.65391540527344\n",
      "1526/3000 train_loss: 41.40961837768555 test_loss:200.119384765625\n",
      "1527/3000 train_loss: 39.99650192260742 test_loss:211.78729248046875\n",
      "1528/3000 train_loss: 45.01154708862305 test_loss:212.11993408203125\n",
      "1529/3000 train_loss: 44.113372802734375 test_loss:209.9238739013672\n",
      "1530/3000 train_loss: 37.33534240722656 test_loss:216.6846160888672\n",
      "1531/3000 train_loss: 44.144508361816406 test_loss:214.5083465576172\n",
      "1532/3000 train_loss: 45.573265075683594 test_loss:204.19607543945312\n",
      "1533/3000 train_loss: 44.93113327026367 test_loss:205.81893920898438\n",
      "1534/3000 train_loss: 40.51409912109375 test_loss:201.53469848632812\n",
      "1535/3000 train_loss: 52.47840118408203 test_loss:225.95062255859375\n",
      "1536/3000 train_loss: 42.79555892944336 test_loss:203.24285888671875\n",
      "1537/3000 train_loss: 57.03809356689453 test_loss:195.46261596679688\n",
      "1538/3000 train_loss: 42.191280364990234 test_loss:201.9879608154297\n",
      "1539/3000 train_loss: 40.85382080078125 test_loss:203.76199340820312\n",
      "1540/3000 train_loss: 41.40459060668945 test_loss:194.78982543945312\n",
      "1541/3000 train_loss: 42.3635368347168 test_loss:193.92648315429688\n",
      "1542/3000 train_loss: 37.196224212646484 test_loss:196.02896118164062\n",
      "1543/3000 train_loss: 38.660743713378906 test_loss:197.807861328125\n",
      "1544/3000 train_loss: 37.10440444946289 test_loss:194.98406982421875\n",
      "1545/3000 train_loss: 40.389190673828125 test_loss:193.325927734375\n",
      "1546/3000 train_loss: 43.637718200683594 test_loss:199.04669189453125\n",
      "1547/3000 train_loss: 38.996524810791016 test_loss:194.38478088378906\n",
      "1548/3000 train_loss: 40.9710693359375 test_loss:196.51443481445312\n",
      "1549/3000 train_loss: 45.68907928466797 test_loss:206.07611083984375\n",
      "1550/3000 train_loss: 36.799537658691406 test_loss:193.45639038085938\n",
      "1551/3000 train_loss: 35.7368278503418 test_loss:203.51626586914062\n",
      "1552/3000 train_loss: 40.91950225830078 test_loss:205.99346923828125\n",
      "1553/3000 train_loss: 40.48280334472656 test_loss:204.3697509765625\n",
      "1554/3000 train_loss: 40.93766784667969 test_loss:210.86959838867188\n",
      "1555/3000 train_loss: 41.355918884277344 test_loss:201.05349731445312\n",
      "1556/3000 train_loss: 44.69990539550781 test_loss:194.59461975097656\n",
      "1557/3000 train_loss: 45.68061447143555 test_loss:197.34652709960938\n",
      "1558/3000 train_loss: 35.437801361083984 test_loss:190.33163452148438\n",
      "1559/3000 train_loss: 36.47996139526367 test_loss:193.43182373046875\n",
      "1560/3000 train_loss: 35.499481201171875 test_loss:192.26600646972656\n",
      "1561/3000 train_loss: 41.291934967041016 test_loss:200.02874755859375\n",
      "1562/3000 train_loss: 36.11103057861328 test_loss:194.52163696289062\n",
      "1563/3000 train_loss: 38.50782775878906 test_loss:191.63677978515625\n",
      "1564/3000 train_loss: 36.582820892333984 test_loss:206.41839599609375\n",
      "1565/3000 train_loss: 38.59103775024414 test_loss:210.9255828857422\n",
      "1566/3000 train_loss: 40.704811096191406 test_loss:195.0921630859375\n",
      "1567/3000 train_loss: 37.54036331176758 test_loss:200.244140625\n",
      "1568/3000 train_loss: 41.97785949707031 test_loss:196.46441650390625\n",
      "1569/3000 train_loss: 48.14529800415039 test_loss:199.9864501953125\n",
      "1570/3000 train_loss: 41.57201385498047 test_loss:195.42739868164062\n",
      "1571/3000 train_loss: 42.16593551635742 test_loss:210.07232666015625\n",
      "1572/3000 train_loss: 35.420169830322266 test_loss:209.17068481445312\n",
      "1573/3000 train_loss: 39.63167190551758 test_loss:204.09713745117188\n",
      "1574/3000 train_loss: 38.37434005737305 test_loss:197.78274536132812\n",
      "1575/3000 train_loss: 37.763675689697266 test_loss:195.59701538085938\n",
      "1576/3000 train_loss: 49.728328704833984 test_loss:192.15277099609375\n",
      "1577/3000 train_loss: 38.422019958496094 test_loss:202.78895568847656\n",
      "1578/3000 train_loss: 37.37847137451172 test_loss:207.8330078125\n",
      "1579/3000 train_loss: 39.8201789855957 test_loss:196.08123779296875\n",
      "1580/3000 train_loss: 42.200435638427734 test_loss:195.9747314453125\n",
      "1581/3000 train_loss: 52.02797317504883 test_loss:194.62649536132812\n",
      "1582/3000 train_loss: 38.419490814208984 test_loss:196.27658081054688\n",
      "1583/3000 train_loss: 41.978336334228516 test_loss:199.08868408203125\n",
      "1584/3000 train_loss: 40.8126335144043 test_loss:202.77316284179688\n",
      "1585/3000 train_loss: 43.491920471191406 test_loss:200.3126983642578\n",
      "1586/3000 train_loss: 38.627174377441406 test_loss:202.2672119140625\n",
      "1587/3000 train_loss: 43.73398208618164 test_loss:199.7296600341797\n",
      "1588/3000 train_loss: 38.016597747802734 test_loss:194.45101928710938\n",
      "1589/3000 train_loss: 39.442138671875 test_loss:200.87545776367188\n",
      "1590/3000 train_loss: 43.740806579589844 test_loss:212.889404296875\n",
      "1591/3000 train_loss: 41.949283599853516 test_loss:209.23922729492188\n",
      "1592/3000 train_loss: 36.43893051147461 test_loss:188.20254516601562\n",
      "1593/3000 train_loss: 33.22578048706055 test_loss:189.3114776611328\n",
      "1594/3000 train_loss: 33.59093475341797 test_loss:190.75155639648438\n",
      "1595/3000 train_loss: 37.89177322387695 test_loss:205.35452270507812\n",
      "1596/3000 train_loss: 35.89057159423828 test_loss:201.67544555664062\n",
      "1597/3000 train_loss: 36.64199447631836 test_loss:208.93301391601562\n",
      "1598/3000 train_loss: 34.99123001098633 test_loss:206.70394897460938\n",
      "1599/3000 train_loss: 34.378787994384766 test_loss:203.41659545898438\n",
      "1600/3000 train_loss: 44.843589782714844 test_loss:216.29278564453125\n",
      "1601/3000 train_loss: 51.82297134399414 test_loss:203.4024658203125\n",
      "1602/3000 train_loss: 43.90265655517578 test_loss:209.4318389892578\n",
      "1603/3000 train_loss: 35.51930236816406 test_loss:199.7083740234375\n",
      "1604/3000 train_loss: 48.758583068847656 test_loss:204.6220703125\n",
      "1605/3000 train_loss: 39.386932373046875 test_loss:201.37985229492188\n",
      "1606/3000 train_loss: 40.935142517089844 test_loss:207.10235595703125\n",
      "1607/3000 train_loss: 38.83326721191406 test_loss:199.50543212890625\n",
      "1608/3000 train_loss: 35.96862030029297 test_loss:197.90884399414062\n",
      "1609/3000 train_loss: 36.749412536621094 test_loss:198.64822387695312\n",
      "1610/3000 train_loss: 40.725826263427734 test_loss:197.13937377929688\n",
      "1611/3000 train_loss: 38.752506256103516 test_loss:211.00611877441406\n",
      "1612/3000 train_loss: 44.987850189208984 test_loss:206.74456787109375\n",
      "1613/3000 train_loss: 38.525482177734375 test_loss:205.91102600097656\n",
      "1614/3000 train_loss: 37.32475280761719 test_loss:201.54885864257812\n",
      "1615/3000 train_loss: 41.68312454223633 test_loss:197.4008331298828\n",
      "1616/3000 train_loss: 62.621788024902344 test_loss:203.19729614257812\n",
      "1617/3000 train_loss: 44.001197814941406 test_loss:209.4473876953125\n",
      "1618/3000 train_loss: 45.24104309082031 test_loss:201.8607177734375\n",
      "1619/3000 train_loss: 41.84727478027344 test_loss:209.0487060546875\n",
      "1620/3000 train_loss: 36.96662521362305 test_loss:197.55296325683594\n",
      "1621/3000 train_loss: 37.742061614990234 test_loss:199.171630859375\n",
      "1622/3000 train_loss: 38.47725296020508 test_loss:191.17669677734375\n",
      "1623/3000 train_loss: 41.96864318847656 test_loss:206.39047241210938\n",
      "1624/3000 train_loss: 35.18815231323242 test_loss:198.10702514648438\n",
      "1625/3000 train_loss: 41.60504150390625 test_loss:205.12567138671875\n",
      "1626/3000 train_loss: 44.23961639404297 test_loss:194.75205993652344\n",
      "1627/3000 train_loss: 37.919124603271484 test_loss:185.92996215820312\n",
      "1628/3000 train_loss: 40.416358947753906 test_loss:182.1519775390625\n",
      "1629/3000 train_loss: 35.56367492675781 test_loss:192.4573974609375\n",
      "1630/3000 train_loss: 34.75917434692383 test_loss:183.101318359375\n",
      "1631/3000 train_loss: 38.7828483581543 test_loss:187.1562042236328\n",
      "1632/3000 train_loss: 36.65544891357422 test_loss:182.77871704101562\n",
      "1633/3000 train_loss: 37.11656188964844 test_loss:201.99432373046875\n",
      "1634/3000 train_loss: 40.41023254394531 test_loss:183.10488891601562\n",
      "1635/3000 train_loss: 45.70716857910156 test_loss:203.75296020507812\n",
      "1636/3000 train_loss: 33.733612060546875 test_loss:186.00035095214844\n",
      "1637/3000 train_loss: 35.76166534423828 test_loss:186.91357421875\n",
      "1638/3000 train_loss: 35.027713775634766 test_loss:186.64370727539062\n",
      "1639/3000 train_loss: 35.54700469970703 test_loss:184.6025848388672\n",
      "1640/3000 train_loss: 36.279605865478516 test_loss:208.50888061523438\n",
      "1641/3000 train_loss: 31.081762313842773 test_loss:197.149169921875\n",
      "1642/3000 train_loss: 38.64435577392578 test_loss:188.44937133789062\n",
      "1643/3000 train_loss: 42.03351593017578 test_loss:184.58949279785156\n",
      "1644/3000 train_loss: 39.456687927246094 test_loss:187.27674865722656\n",
      "1645/3000 train_loss: 37.3537483215332 test_loss:195.3557586669922\n",
      "1646/3000 train_loss: 42.149940490722656 test_loss:191.82736206054688\n",
      "1647/3000 train_loss: 37.75959777832031 test_loss:191.65924072265625\n",
      "1648/3000 train_loss: 42.778385162353516 test_loss:190.9267578125\n",
      "1649/3000 train_loss: 47.62156295776367 test_loss:187.39022827148438\n",
      "1650/3000 train_loss: 34.193416595458984 test_loss:208.363525390625\n",
      "1651/3000 train_loss: 31.366235733032227 test_loss:197.83468627929688\n",
      "1652/3000 train_loss: 41.34618377685547 test_loss:193.39031982421875\n",
      "1653/3000 train_loss: 36.07799530029297 test_loss:192.71804809570312\n",
      "1654/3000 train_loss: 38.16485595703125 test_loss:188.24530029296875\n",
      "1655/3000 train_loss: 41.778053283691406 test_loss:184.36264038085938\n",
      "1656/3000 train_loss: 35.29255676269531 test_loss:188.32974243164062\n",
      "1657/3000 train_loss: 36.72187423706055 test_loss:184.40084838867188\n",
      "1658/3000 train_loss: 54.360206604003906 test_loss:187.3009033203125\n",
      "1659/3000 train_loss: 47.78779220581055 test_loss:202.56448364257812\n",
      "1660/3000 train_loss: 41.405879974365234 test_loss:190.25979614257812\n",
      "1661/3000 train_loss: 36.499053955078125 test_loss:185.25442504882812\n",
      "1662/3000 train_loss: 37.25399398803711 test_loss:179.2303466796875\n",
      "1663/3000 train_loss: 35.47572326660156 test_loss:191.14205932617188\n",
      "1664/3000 train_loss: 36.53965377807617 test_loss:187.04913330078125\n",
      "1665/3000 train_loss: 43.12906265258789 test_loss:196.38990783691406\n",
      "1666/3000 train_loss: 39.64762496948242 test_loss:199.57017517089844\n",
      "1667/3000 train_loss: 32.63107681274414 test_loss:203.15701293945312\n",
      "1668/3000 train_loss: 34.652687072753906 test_loss:183.0825958251953\n",
      "1669/3000 train_loss: 35.298038482666016 test_loss:185.7504119873047\n",
      "1670/3000 train_loss: 31.486114501953125 test_loss:200.3619384765625\n",
      "1671/3000 train_loss: 34.04991912841797 test_loss:214.71458435058594\n",
      "1672/3000 train_loss: 35.65678787231445 test_loss:178.55819702148438\n",
      "1673/3000 train_loss: 33.38662338256836 test_loss:184.91802978515625\n",
      "1674/3000 train_loss: 36.62067794799805 test_loss:189.10894775390625\n",
      "1675/3000 train_loss: 39.355125427246094 test_loss:185.37547302246094\n",
      "1676/3000 train_loss: 35.47806930541992 test_loss:183.55197143554688\n",
      "1677/3000 train_loss: 35.435909271240234 test_loss:185.77105712890625\n",
      "1678/3000 train_loss: 30.619770050048828 test_loss:186.05520629882812\n",
      "1679/3000 train_loss: 37.533294677734375 test_loss:196.25282287597656\n",
      "1680/3000 train_loss: 38.67660140991211 test_loss:182.61956787109375\n",
      "1681/3000 train_loss: 40.41260528564453 test_loss:186.31121826171875\n",
      "1682/3000 train_loss: 37.63946533203125 test_loss:184.5399932861328\n",
      "1683/3000 train_loss: 37.67814636230469 test_loss:196.78851318359375\n",
      "1684/3000 train_loss: 38.32398223876953 test_loss:185.58651733398438\n",
      "1685/3000 train_loss: 39.00876998901367 test_loss:188.23348999023438\n",
      "1686/3000 train_loss: 36.70642852783203 test_loss:183.84658813476562\n",
      "1687/3000 train_loss: 34.69976043701172 test_loss:186.80552673339844\n",
      "1688/3000 train_loss: 36.50526428222656 test_loss:183.2315673828125\n",
      "1689/3000 train_loss: 36.299652099609375 test_loss:185.18014526367188\n",
      "1690/3000 train_loss: 32.44089126586914 test_loss:188.924072265625\n",
      "1691/3000 train_loss: 40.690982818603516 test_loss:183.64877319335938\n",
      "1692/3000 train_loss: 31.356739044189453 test_loss:187.63157653808594\n",
      "1693/3000 train_loss: 34.301918029785156 test_loss:198.8379669189453\n",
      "1694/3000 train_loss: 34.435089111328125 test_loss:188.86993408203125\n",
      "1695/3000 train_loss: 44.33540725708008 test_loss:183.9917755126953\n",
      "1696/3000 train_loss: 38.13709259033203 test_loss:184.70831298828125\n",
      "1697/3000 train_loss: 35.3525276184082 test_loss:181.54486083984375\n",
      "1698/3000 train_loss: 34.930938720703125 test_loss:184.08145141601562\n",
      "1699/3000 train_loss: 32.33987808227539 test_loss:184.20266723632812\n",
      "1700/3000 train_loss: 32.54927444458008 test_loss:210.99560546875\n",
      "1701/3000 train_loss: 36.7728271484375 test_loss:209.95382690429688\n",
      "1702/3000 train_loss: 34.39356231689453 test_loss:203.68788146972656\n",
      "1703/3000 train_loss: 37.096473693847656 test_loss:199.57586669921875\n",
      "1704/3000 train_loss: 32.532596588134766 test_loss:205.8845672607422\n",
      "1705/3000 train_loss: 38.28250503540039 test_loss:207.114501953125\n",
      "1706/3000 train_loss: 40.998497009277344 test_loss:202.48638916015625\n",
      "1707/3000 train_loss: 38.28824234008789 test_loss:214.7047119140625\n",
      "1708/3000 train_loss: 44.247581481933594 test_loss:207.54824829101562\n",
      "1709/3000 train_loss: 35.20764923095703 test_loss:219.762939453125\n",
      "1710/3000 train_loss: 48.21733474731445 test_loss:206.5223388671875\n",
      "1711/3000 train_loss: 36.23836898803711 test_loss:191.97662353515625\n",
      "1712/3000 train_loss: 34.77910614013672 test_loss:189.53233337402344\n",
      "1713/3000 train_loss: 31.99317741394043 test_loss:185.3693389892578\n",
      "1714/3000 train_loss: 38.010318756103516 test_loss:191.71090698242188\n",
      "1715/3000 train_loss: 35.57899856567383 test_loss:192.59939575195312\n",
      "1716/3000 train_loss: 35.75609588623047 test_loss:182.3914794921875\n",
      "1717/3000 train_loss: 33.533714294433594 test_loss:189.78506469726562\n",
      "1718/3000 train_loss: 37.350162506103516 test_loss:212.0236358642578\n",
      "1719/3000 train_loss: 43.89925003051758 test_loss:203.38876342773438\n",
      "1720/3000 train_loss: 33.132240295410156 test_loss:206.12423706054688\n",
      "1721/3000 train_loss: 36.579586029052734 test_loss:202.29693603515625\n",
      "1722/3000 train_loss: 34.915958404541016 test_loss:207.28793334960938\n",
      "1723/3000 train_loss: 32.83956527709961 test_loss:200.93751525878906\n",
      "1724/3000 train_loss: 32.30514907836914 test_loss:194.33758544921875\n",
      "1725/3000 train_loss: 40.70222091674805 test_loss:238.6576385498047\n",
      "1726/3000 train_loss: 35.783966064453125 test_loss:203.37228393554688\n",
      "1727/3000 train_loss: 33.63105010986328 test_loss:208.503173828125\n",
      "1728/3000 train_loss: 32.996971130371094 test_loss:211.849365234375\n",
      "1729/3000 train_loss: 33.02507400512695 test_loss:226.8970947265625\n",
      "1730/3000 train_loss: 32.58899688720703 test_loss:209.58395385742188\n",
      "1731/3000 train_loss: 33.74256896972656 test_loss:209.94943237304688\n",
      "1732/3000 train_loss: 31.32725715637207 test_loss:217.95880126953125\n",
      "1733/3000 train_loss: 38.181453704833984 test_loss:220.05731201171875\n",
      "1734/3000 train_loss: 39.61945343017578 test_loss:190.24560546875\n",
      "1735/3000 train_loss: 44.594600677490234 test_loss:192.743896484375\n",
      "1736/3000 train_loss: 34.219993591308594 test_loss:191.3306884765625\n",
      "1737/3000 train_loss: 33.21177673339844 test_loss:217.3597412109375\n",
      "1738/3000 train_loss: 49.417667388916016 test_loss:207.19998168945312\n",
      "1739/3000 train_loss: 33.74134826660156 test_loss:200.78025817871094\n",
      "1740/3000 train_loss: 38.71214294433594 test_loss:201.24215698242188\n",
      "1741/3000 train_loss: 37.330780029296875 test_loss:195.735595703125\n",
      "1742/3000 train_loss: 30.99229621887207 test_loss:179.24588012695312\n",
      "1743/3000 train_loss: 31.176971435546875 test_loss:202.7420654296875\n",
      "1744/3000 train_loss: 33.50712203979492 test_loss:189.05357360839844\n",
      "1745/3000 train_loss: 32.75850296020508 test_loss:187.15084838867188\n",
      "1746/3000 train_loss: 33.414695739746094 test_loss:187.94493103027344\n",
      "1747/3000 train_loss: 35.506858825683594 test_loss:188.74990844726562\n",
      "1748/3000 train_loss: 40.430694580078125 test_loss:191.55882263183594\n",
      "1749/3000 train_loss: 31.212692260742188 test_loss:180.91128540039062\n",
      "1750/3000 train_loss: 34.961875915527344 test_loss:184.5274658203125\n",
      "1751/3000 train_loss: 40.0388069152832 test_loss:180.29989624023438\n",
      "1752/3000 train_loss: 33.684181213378906 test_loss:188.49380493164062\n",
      "1753/3000 train_loss: 36.936431884765625 test_loss:182.11724853515625\n",
      "1754/3000 train_loss: 32.884952545166016 test_loss:183.33558654785156\n",
      "1755/3000 train_loss: 35.98436737060547 test_loss:194.97862243652344\n",
      "1756/3000 train_loss: 35.816646575927734 test_loss:192.42117309570312\n",
      "1757/3000 train_loss: 42.58526611328125 test_loss:192.424072265625\n",
      "1758/3000 train_loss: 32.541664123535156 test_loss:196.28793334960938\n",
      "1759/3000 train_loss: 35.21586608886719 test_loss:193.1690216064453\n",
      "1760/3000 train_loss: 34.33861541748047 test_loss:187.69776916503906\n",
      "1761/3000 train_loss: 34.794029235839844 test_loss:186.56124877929688\n",
      "1762/3000 train_loss: 36.117645263671875 test_loss:188.39529418945312\n",
      "1763/3000 train_loss: 32.279903411865234 test_loss:192.81068420410156\n",
      "1764/3000 train_loss: 31.329345703125 test_loss:185.19886779785156\n",
      "1765/3000 train_loss: 39.06732177734375 test_loss:191.15762329101562\n",
      "1766/3000 train_loss: 37.245243072509766 test_loss:194.5146484375\n",
      "1767/3000 train_loss: 33.78929901123047 test_loss:192.96084594726562\n",
      "1768/3000 train_loss: 36.03900146484375 test_loss:187.52066040039062\n",
      "1769/3000 train_loss: 30.170181274414062 test_loss:192.65115356445312\n",
      "1770/3000 train_loss: 37.05134582519531 test_loss:189.62814331054688\n",
      "1771/3000 train_loss: 34.92976760864258 test_loss:187.18606567382812\n",
      "1772/3000 train_loss: 33.13837814331055 test_loss:189.88009643554688\n",
      "1773/3000 train_loss: 34.60078430175781 test_loss:192.24493408203125\n",
      "1774/3000 train_loss: 34.10362243652344 test_loss:196.23336791992188\n",
      "1775/3000 train_loss: 36.154762268066406 test_loss:190.01870727539062\n",
      "1776/3000 train_loss: 36.62165069580078 test_loss:192.974609375\n",
      "1777/3000 train_loss: 30.181682586669922 test_loss:192.14459228515625\n",
      "1778/3000 train_loss: 34.430145263671875 test_loss:183.52157592773438\n",
      "1779/3000 train_loss: 36.29524612426758 test_loss:185.37445068359375\n",
      "1780/3000 train_loss: 38.96088409423828 test_loss:200.41297912597656\n",
      "1781/3000 train_loss: 40.047203063964844 test_loss:188.43746948242188\n",
      "1782/3000 train_loss: 37.35365676879883 test_loss:186.78738403320312\n",
      "1783/3000 train_loss: 36.461280822753906 test_loss:193.3105010986328\n",
      "1784/3000 train_loss: 37.44621658325195 test_loss:183.28321838378906\n",
      "1785/3000 train_loss: 38.5074577331543 test_loss:181.58302307128906\n",
      "1786/3000 train_loss: 33.91343688964844 test_loss:182.43008422851562\n",
      "1787/3000 train_loss: 41.11000061035156 test_loss:192.0087127685547\n",
      "1788/3000 train_loss: 36.30596160888672 test_loss:200.40762329101562\n",
      "1789/3000 train_loss: 38.00762939453125 test_loss:202.24020385742188\n",
      "1790/3000 train_loss: 46.94380187988281 test_loss:191.4749298095703\n",
      "1791/3000 train_loss: 33.043575286865234 test_loss:182.93177795410156\n",
      "1792/3000 train_loss: 36.704036712646484 test_loss:183.6898956298828\n",
      "1793/3000 train_loss: 30.548233032226562 test_loss:182.2266845703125\n",
      "1794/3000 train_loss: 33.18282699584961 test_loss:181.24574279785156\n",
      "1795/3000 train_loss: 29.85315704345703 test_loss:185.04705810546875\n",
      "1796/3000 train_loss: 33.381046295166016 test_loss:183.46652221679688\n",
      "1797/3000 train_loss: 38.24513626098633 test_loss:188.70953369140625\n",
      "1798/3000 train_loss: 31.040626525878906 test_loss:185.65289306640625\n",
      "1799/3000 train_loss: 27.43422508239746 test_loss:184.86663818359375\n",
      "1800/3000 train_loss: 33.19527816772461 test_loss:183.02679443359375\n",
      "1801/3000 train_loss: 33.30088424682617 test_loss:187.36203002929688\n",
      "1802/3000 train_loss: 32.956207275390625 test_loss:183.63558959960938\n",
      "1803/3000 train_loss: 35.331398010253906 test_loss:187.6497802734375\n",
      "1804/3000 train_loss: 36.84193420410156 test_loss:185.41790771484375\n",
      "1805/3000 train_loss: 35.367427825927734 test_loss:178.27468872070312\n",
      "1806/3000 train_loss: 37.65892028808594 test_loss:185.69003295898438\n",
      "1807/3000 train_loss: 36.073177337646484 test_loss:182.73037719726562\n",
      "1808/3000 train_loss: 29.21255111694336 test_loss:180.29165649414062\n",
      "1809/3000 train_loss: 32.05888366699219 test_loss:180.052978515625\n",
      "1810/3000 train_loss: 33.73335266113281 test_loss:185.94189453125\n",
      "1811/3000 train_loss: 31.948028564453125 test_loss:186.3909149169922\n",
      "1812/3000 train_loss: 36.52540588378906 test_loss:185.16326904296875\n",
      "1813/3000 train_loss: 32.811641693115234 test_loss:184.3242950439453\n",
      "1814/3000 train_loss: 32.33039093017578 test_loss:178.01278686523438\n",
      "1815/3000 train_loss: 33.136131286621094 test_loss:185.40052795410156\n",
      "1816/3000 train_loss: 36.648162841796875 test_loss:183.4781036376953\n",
      "1817/3000 train_loss: 35.212493896484375 test_loss:179.3185577392578\n",
      "1818/3000 train_loss: 30.121715545654297 test_loss:185.84239196777344\n",
      "1819/3000 train_loss: 30.421947479248047 test_loss:184.060791015625\n",
      "1820/3000 train_loss: 46.51622009277344 test_loss:186.64259338378906\n",
      "1821/3000 train_loss: 30.64963722229004 test_loss:186.7330322265625\n",
      "1822/3000 train_loss: 36.061256408691406 test_loss:177.97372436523438\n",
      "1823/3000 train_loss: 32.23912048339844 test_loss:175.70803833007812\n",
      "1824/3000 train_loss: 35.079551696777344 test_loss:181.4468994140625\n",
      "1825/3000 train_loss: 30.548070907592773 test_loss:181.4171142578125\n",
      "1826/3000 train_loss: 32.75482940673828 test_loss:185.84490966796875\n",
      "1827/3000 train_loss: 32.459224700927734 test_loss:179.10780334472656\n",
      "1828/3000 train_loss: 31.2471981048584 test_loss:180.76190185546875\n",
      "1829/3000 train_loss: 34.715126037597656 test_loss:183.46502685546875\n",
      "1830/3000 train_loss: 37.75657653808594 test_loss:199.33851623535156\n",
      "1831/3000 train_loss: 33.207698822021484 test_loss:192.32376098632812\n",
      "1832/3000 train_loss: 37.494869232177734 test_loss:183.07757568359375\n",
      "1833/3000 train_loss: 31.33529281616211 test_loss:180.9278564453125\n",
      "1834/3000 train_loss: 33.79269027709961 test_loss:184.31265258789062\n",
      "1835/3000 train_loss: 37.9013671875 test_loss:186.46636962890625\n",
      "1836/3000 train_loss: 32.16720199584961 test_loss:183.85891723632812\n",
      "1837/3000 train_loss: 35.187225341796875 test_loss:188.11764526367188\n",
      "1838/3000 train_loss: 32.47010040283203 test_loss:178.78717041015625\n",
      "1839/3000 train_loss: 33.24015808105469 test_loss:185.83795166015625\n",
      "1840/3000 train_loss: 28.98818588256836 test_loss:185.585693359375\n",
      "1841/3000 train_loss: 36.439735412597656 test_loss:179.85000610351562\n",
      "1842/3000 train_loss: 34.6180419921875 test_loss:180.2828369140625\n",
      "1843/3000 train_loss: 31.437814712524414 test_loss:176.406982421875\n",
      "1844/3000 train_loss: 31.965158462524414 test_loss:179.59010314941406\n",
      "1845/3000 train_loss: 31.705745697021484 test_loss:180.38345336914062\n",
      "1846/3000 train_loss: 35.051048278808594 test_loss:181.4273223876953\n",
      "1847/3000 train_loss: 34.74514389038086 test_loss:176.03829956054688\n",
      "1848/3000 train_loss: 28.91848373413086 test_loss:177.88619995117188\n",
      "1849/3000 train_loss: 29.811857223510742 test_loss:175.552001953125\n",
      "1850/3000 train_loss: 28.334022521972656 test_loss:174.59567260742188\n",
      "1851/3000 train_loss: 32.04481887817383 test_loss:180.29080200195312\n",
      "1852/3000 train_loss: 37.99966812133789 test_loss:179.45326232910156\n",
      "1853/3000 train_loss: 33.01313400268555 test_loss:175.52932739257812\n",
      "1854/3000 train_loss: 33.822872161865234 test_loss:176.942626953125\n",
      "1855/3000 train_loss: 32.38090896606445 test_loss:189.1165313720703\n",
      "1856/3000 train_loss: 31.512939453125 test_loss:176.95053100585938\n",
      "1857/3000 train_loss: 33.234527587890625 test_loss:177.63304138183594\n",
      "1858/3000 train_loss: 29.218128204345703 test_loss:178.74154663085938\n",
      "1859/3000 train_loss: 30.552444458007812 test_loss:177.7150421142578\n",
      "1860/3000 train_loss: 30.21290397644043 test_loss:181.4728546142578\n",
      "1861/3000 train_loss: 32.918148040771484 test_loss:177.95855712890625\n",
      "1862/3000 train_loss: 35.253074645996094 test_loss:175.72647094726562\n",
      "1863/3000 train_loss: 33.809024810791016 test_loss:175.98590087890625\n",
      "1864/3000 train_loss: 30.91823959350586 test_loss:177.98739624023438\n",
      "1865/3000 train_loss: 33.06352233886719 test_loss:180.46774291992188\n",
      "1866/3000 train_loss: 39.65105438232422 test_loss:193.0864715576172\n",
      "1867/3000 train_loss: 38.71398162841797 test_loss:174.4632568359375\n",
      "1868/3000 train_loss: 33.69391632080078 test_loss:177.65188598632812\n",
      "1869/3000 train_loss: 32.873233795166016 test_loss:178.58763122558594\n",
      "1870/3000 train_loss: 31.928552627563477 test_loss:181.57847595214844\n",
      "1871/3000 train_loss: 31.122465133666992 test_loss:191.58978271484375\n",
      "1872/3000 train_loss: 31.48442840576172 test_loss:176.90756225585938\n",
      "1873/3000 train_loss: 32.14474105834961 test_loss:174.64971923828125\n",
      "1874/3000 train_loss: 39.13407897949219 test_loss:181.35745239257812\n",
      "1875/3000 train_loss: 29.9692325592041 test_loss:171.71112060546875\n",
      "1876/3000 train_loss: 33.520355224609375 test_loss:173.23275756835938\n",
      "1877/3000 train_loss: 29.373172760009766 test_loss:179.0884552001953\n",
      "1878/3000 train_loss: 29.33394432067871 test_loss:177.459228515625\n",
      "1879/3000 train_loss: 29.463943481445312 test_loss:173.09320068359375\n",
      "1880/3000 train_loss: 28.330913543701172 test_loss:173.66270446777344\n",
      "1881/3000 train_loss: 33.323875427246094 test_loss:175.92453002929688\n",
      "1882/3000 train_loss: 36.07468795776367 test_loss:180.44688415527344\n",
      "1883/3000 train_loss: 33.46284866333008 test_loss:177.57516479492188\n",
      "1884/3000 train_loss: 37.145469665527344 test_loss:180.69769287109375\n",
      "1885/3000 train_loss: 34.43644714355469 test_loss:181.21322631835938\n",
      "1886/3000 train_loss: 30.512054443359375 test_loss:175.06875610351562\n",
      "1887/3000 train_loss: 32.32621765136719 test_loss:183.33494567871094\n",
      "1888/3000 train_loss: 30.77655601501465 test_loss:176.16110229492188\n",
      "1889/3000 train_loss: 32.395626068115234 test_loss:180.01071166992188\n",
      "1890/3000 train_loss: 35.55656051635742 test_loss:175.6363067626953\n",
      "1891/3000 train_loss: 33.86539077758789 test_loss:175.23931884765625\n",
      "1892/3000 train_loss: 45.20294952392578 test_loss:177.11471557617188\n",
      "1893/3000 train_loss: 31.046321868896484 test_loss:172.8603515625\n",
      "1894/3000 train_loss: 34.8537712097168 test_loss:172.497802734375\n",
      "1895/3000 train_loss: 30.41501235961914 test_loss:177.55624389648438\n",
      "1896/3000 train_loss: 35.64242172241211 test_loss:177.12698364257812\n",
      "1897/3000 train_loss: 31.641130447387695 test_loss:177.33985900878906\n",
      "1898/3000 train_loss: 38.34428405761719 test_loss:181.11712646484375\n",
      "1899/3000 train_loss: 31.46475601196289 test_loss:181.50515747070312\n",
      "1900/3000 train_loss: 34.757110595703125 test_loss:179.30136108398438\n",
      "1901/3000 train_loss: 33.66926574707031 test_loss:175.888916015625\n",
      "1902/3000 train_loss: 29.18132972717285 test_loss:172.0006103515625\n",
      "1903/3000 train_loss: 35.064449310302734 test_loss:172.5175018310547\n",
      "1904/3000 train_loss: 33.00157165527344 test_loss:176.65786743164062\n",
      "1905/3000 train_loss: 33.95107650756836 test_loss:177.58775329589844\n",
      "1906/3000 train_loss: 28.573955535888672 test_loss:172.11891174316406\n",
      "1907/3000 train_loss: 30.476417541503906 test_loss:174.3551483154297\n",
      "1908/3000 train_loss: 33.47503662109375 test_loss:173.8394775390625\n",
      "1909/3000 train_loss: 29.6068058013916 test_loss:175.97781372070312\n",
      "1910/3000 train_loss: 30.84748077392578 test_loss:182.5299072265625\n",
      "1911/3000 train_loss: 32.66788101196289 test_loss:174.3289794921875\n",
      "1912/3000 train_loss: 30.167579650878906 test_loss:174.7017822265625\n",
      "1913/3000 train_loss: 34.9904670715332 test_loss:176.7333984375\n",
      "1914/3000 train_loss: 29.26875114440918 test_loss:181.0032958984375\n",
      "1915/3000 train_loss: 32.63279342651367 test_loss:174.27484130859375\n",
      "1916/3000 train_loss: 39.10226058959961 test_loss:183.77545166015625\n",
      "1917/3000 train_loss: 30.570098876953125 test_loss:173.65313720703125\n",
      "1918/3000 train_loss: 33.80537796020508 test_loss:191.47032165527344\n",
      "1919/3000 train_loss: 29.604890823364258 test_loss:182.4871368408203\n",
      "1920/3000 train_loss: 33.080257415771484 test_loss:176.48477172851562\n",
      "1921/3000 train_loss: 35.748016357421875 test_loss:180.00340270996094\n",
      "1922/3000 train_loss: 28.875551223754883 test_loss:192.57919311523438\n",
      "1923/3000 train_loss: 26.99578857421875 test_loss:192.03530883789062\n",
      "1924/3000 train_loss: 29.51241683959961 test_loss:195.6607666015625\n",
      "1925/3000 train_loss: 29.796287536621094 test_loss:203.30044555664062\n",
      "1926/3000 train_loss: 28.434226989746094 test_loss:195.30123901367188\n",
      "1927/3000 train_loss: 28.033748626708984 test_loss:188.38674926757812\n",
      "1928/3000 train_loss: 38.7016716003418 test_loss:180.0946807861328\n",
      "1929/3000 train_loss: 27.8476505279541 test_loss:184.18191528320312\n",
      "1930/3000 train_loss: 29.855953216552734 test_loss:182.33135986328125\n",
      "1931/3000 train_loss: 32.17975616455078 test_loss:172.30648803710938\n",
      "1932/3000 train_loss: 32.378360748291016 test_loss:182.338623046875\n",
      "1933/3000 train_loss: 31.54949188232422 test_loss:177.54214477539062\n",
      "1934/3000 train_loss: 37.389122009277344 test_loss:169.6119384765625\n",
      "1935/3000 train_loss: 30.86763572692871 test_loss:177.77975463867188\n",
      "1936/3000 train_loss: 30.878915786743164 test_loss:173.58706665039062\n",
      "1937/3000 train_loss: 30.31636619567871 test_loss:174.50674438476562\n",
      "1938/3000 train_loss: 29.827850341796875 test_loss:173.42234802246094\n",
      "1939/3000 train_loss: 31.121007919311523 test_loss:171.73268127441406\n",
      "1940/3000 train_loss: 28.860599517822266 test_loss:189.03924560546875\n",
      "1941/3000 train_loss: 30.45594596862793 test_loss:169.5738525390625\n",
      "1942/3000 train_loss: 33.29794692993164 test_loss:187.53208923339844\n",
      "1943/3000 train_loss: 31.096954345703125 test_loss:173.81008911132812\n",
      "1944/3000 train_loss: 32.93539047241211 test_loss:172.96139526367188\n",
      "1945/3000 train_loss: 35.556243896484375 test_loss:175.16355895996094\n",
      "1946/3000 train_loss: 33.94135665893555 test_loss:174.56893920898438\n",
      "1947/3000 train_loss: 34.35646438598633 test_loss:193.25173950195312\n",
      "1948/3000 train_loss: 29.289398193359375 test_loss:194.06100463867188\n",
      "1949/3000 train_loss: 29.651002883911133 test_loss:180.23141479492188\n",
      "1950/3000 train_loss: 35.70998764038086 test_loss:187.12826538085938\n",
      "1951/3000 train_loss: 29.043794631958008 test_loss:185.36920166015625\n",
      "1952/3000 train_loss: 31.51559829711914 test_loss:192.44313049316406\n",
      "1953/3000 train_loss: 31.920459747314453 test_loss:193.49436950683594\n",
      "1954/3000 train_loss: 29.612468719482422 test_loss:178.7534942626953\n",
      "1955/3000 train_loss: 28.153505325317383 test_loss:183.863525390625\n",
      "1956/3000 train_loss: 28.72517204284668 test_loss:174.35963439941406\n",
      "1957/3000 train_loss: 33.91582107543945 test_loss:171.4310760498047\n",
      "1958/3000 train_loss: 29.338504791259766 test_loss:171.51959228515625\n",
      "1959/3000 train_loss: 30.225324630737305 test_loss:172.6053466796875\n",
      "1960/3000 train_loss: 28.55339813232422 test_loss:176.42489624023438\n",
      "1961/3000 train_loss: 27.592370986938477 test_loss:169.47706604003906\n",
      "1962/3000 train_loss: 28.427242279052734 test_loss:171.51742553710938\n",
      "1963/3000 train_loss: 30.124774932861328 test_loss:174.952880859375\n",
      "1964/3000 train_loss: 32.70879364013672 test_loss:174.53094482421875\n",
      "1965/3000 train_loss: 29.1754093170166 test_loss:172.95169067382812\n",
      "1966/3000 train_loss: 29.55510902404785 test_loss:176.6893310546875\n",
      "1967/3000 train_loss: 39.90227508544922 test_loss:176.21937561035156\n",
      "1968/3000 train_loss: 30.16912841796875 test_loss:179.577880859375\n",
      "1969/3000 train_loss: 33.046016693115234 test_loss:172.8662109375\n",
      "1970/3000 train_loss: 28.30936050415039 test_loss:172.5596923828125\n",
      "1971/3000 train_loss: 33.03818130493164 test_loss:170.62789916992188\n",
      "1972/3000 train_loss: 30.639739990234375 test_loss:176.80413818359375\n",
      "1973/3000 train_loss: 37.82933807373047 test_loss:175.29083251953125\n",
      "1974/3000 train_loss: 31.631494522094727 test_loss:176.45315551757812\n",
      "1975/3000 train_loss: 28.263063430786133 test_loss:173.85829162597656\n",
      "1976/3000 train_loss: 29.164323806762695 test_loss:175.00393676757812\n",
      "1977/3000 train_loss: 26.55034065246582 test_loss:174.02841186523438\n",
      "1978/3000 train_loss: 31.356704711914062 test_loss:180.1021728515625\n",
      "1979/3000 train_loss: 31.34513282775879 test_loss:178.54049682617188\n",
      "1980/3000 train_loss: 34.506752014160156 test_loss:174.6147003173828\n",
      "1981/3000 train_loss: 27.43529510498047 test_loss:178.2584991455078\n",
      "1982/3000 train_loss: 28.35944938659668 test_loss:175.81439208984375\n",
      "1983/3000 train_loss: 26.866172790527344 test_loss:175.0773468017578\n",
      "1984/3000 train_loss: 29.587074279785156 test_loss:177.98013305664062\n",
      "1985/3000 train_loss: 35.683082580566406 test_loss:174.34866333007812\n",
      "1986/3000 train_loss: 30.434911727905273 test_loss:179.591552734375\n",
      "1987/3000 train_loss: 32.001670837402344 test_loss:179.40658569335938\n",
      "1988/3000 train_loss: 32.02375030517578 test_loss:174.02655029296875\n",
      "1989/3000 train_loss: 31.488256454467773 test_loss:174.06478881835938\n",
      "1990/3000 train_loss: 29.03957748413086 test_loss:176.5424346923828\n",
      "1991/3000 train_loss: 30.017419815063477 test_loss:173.10247802734375\n",
      "1992/3000 train_loss: 28.182228088378906 test_loss:176.07122802734375\n",
      "1993/3000 train_loss: 32.253753662109375 test_loss:171.667724609375\n",
      "1994/3000 train_loss: 38.48830795288086 test_loss:175.47296142578125\n",
      "1995/3000 train_loss: 31.562511444091797 test_loss:174.29776000976562\n",
      "1996/3000 train_loss: 30.6342830657959 test_loss:170.5872802734375\n",
      "1997/3000 train_loss: 28.818500518798828 test_loss:170.82217407226562\n",
      "1998/3000 train_loss: 29.49087142944336 test_loss:170.7849578857422\n",
      "1999/3000 train_loss: 28.86191749572754 test_loss:174.47010803222656\n",
      "2000/3000 train_loss: 26.553668975830078 test_loss:171.13450622558594\n",
      "2001/3000 train_loss: 26.82277488708496 test_loss:173.9116973876953\n",
      "2002/3000 train_loss: 31.147380828857422 test_loss:174.14248657226562\n",
      "2003/3000 train_loss: 29.891427993774414 test_loss:171.87655639648438\n",
      "2004/3000 train_loss: 29.01259994506836 test_loss:168.86215209960938\n",
      "2005/3000 train_loss: 28.430692672729492 test_loss:174.5286865234375\n",
      "2006/3000 train_loss: 33.83327865600586 test_loss:172.53599548339844\n",
      "2007/3000 train_loss: 28.114225387573242 test_loss:174.35044860839844\n",
      "2008/3000 train_loss: 26.37201499938965 test_loss:174.04478454589844\n",
      "2009/3000 train_loss: 29.661205291748047 test_loss:179.150390625\n",
      "2010/3000 train_loss: 37.74913024902344 test_loss:173.44876098632812\n",
      "2011/3000 train_loss: 31.2489070892334 test_loss:173.4250030517578\n",
      "2012/3000 train_loss: 34.259578704833984 test_loss:171.65322875976562\n",
      "2013/3000 train_loss: 35.28916549682617 test_loss:172.32559204101562\n",
      "2014/3000 train_loss: 27.35915756225586 test_loss:173.14654541015625\n",
      "2015/3000 train_loss: 26.84610939025879 test_loss:169.41159057617188\n",
      "2016/3000 train_loss: 25.737401962280273 test_loss:170.56202697753906\n",
      "2017/3000 train_loss: 28.578109741210938 test_loss:171.09503173828125\n",
      "2018/3000 train_loss: 29.028568267822266 test_loss:175.32215881347656\n",
      "2019/3000 train_loss: 28.40627098083496 test_loss:177.54058837890625\n",
      "2020/3000 train_loss: 27.113155364990234 test_loss:173.2237091064453\n",
      "2021/3000 train_loss: 29.93510627746582 test_loss:170.77969360351562\n",
      "2022/3000 train_loss: 32.075077056884766 test_loss:170.9717559814453\n",
      "2023/3000 train_loss: 30.773727416992188 test_loss:186.87591552734375\n",
      "2024/3000 train_loss: 30.494083404541016 test_loss:183.45205688476562\n",
      "2025/3000 train_loss: 32.764461517333984 test_loss:189.9894256591797\n",
      "2026/3000 train_loss: 34.152015686035156 test_loss:197.97744750976562\n",
      "2027/3000 train_loss: 34.11613845825195 test_loss:197.7685089111328\n",
      "2028/3000 train_loss: 33.31045913696289 test_loss:176.6494903564453\n",
      "2029/3000 train_loss: 33.08349609375 test_loss:170.52816772460938\n",
      "2030/3000 train_loss: 29.58812141418457 test_loss:169.8945770263672\n",
      "2031/3000 train_loss: 29.57487678527832 test_loss:168.81028747558594\n",
      "2032/3000 train_loss: 28.935749053955078 test_loss:172.89303588867188\n",
      "2033/3000 train_loss: 30.979793548583984 test_loss:171.76092529296875\n",
      "2034/3000 train_loss: 28.632837295532227 test_loss:173.24044799804688\n",
      "2035/3000 train_loss: 30.365930557250977 test_loss:168.67202758789062\n",
      "2036/3000 train_loss: 30.680387496948242 test_loss:172.72000122070312\n",
      "2037/3000 train_loss: 35.50519561767578 test_loss:170.30621337890625\n",
      "2038/3000 train_loss: 25.293399810791016 test_loss:169.4424285888672\n",
      "2039/3000 train_loss: 29.15268325805664 test_loss:167.4272918701172\n",
      "2040/3000 train_loss: 25.464540481567383 test_loss:168.72146606445312\n",
      "2041/3000 train_loss: 31.84086799621582 test_loss:173.6678924560547\n",
      "2042/3000 train_loss: 27.349994659423828 test_loss:171.70822143554688\n",
      "2043/3000 train_loss: 29.75712013244629 test_loss:169.6829071044922\n",
      "2044/3000 train_loss: 31.01618003845215 test_loss:169.8137664794922\n",
      "2045/3000 train_loss: 30.439756393432617 test_loss:175.2345733642578\n",
      "2046/3000 train_loss: 28.771793365478516 test_loss:168.54193115234375\n",
      "2047/3000 train_loss: 29.958097457885742 test_loss:169.64315795898438\n",
      "2048/3000 train_loss: 25.539979934692383 test_loss:175.97845458984375\n",
      "2049/3000 train_loss: 32.729736328125 test_loss:180.11141967773438\n",
      "2050/3000 train_loss: 32.46589660644531 test_loss:173.99891662597656\n",
      "2051/3000 train_loss: 26.77773094177246 test_loss:179.57427978515625\n",
      "2052/3000 train_loss: 33.12628173828125 test_loss:173.7398681640625\n",
      "2053/3000 train_loss: 34.165958404541016 test_loss:171.10760498046875\n",
      "2054/3000 train_loss: 29.827301025390625 test_loss:170.1460723876953\n",
      "2055/3000 train_loss: 30.690933227539062 test_loss:175.298828125\n",
      "2056/3000 train_loss: 27.42270278930664 test_loss:174.3205108642578\n",
      "2057/3000 train_loss: 30.978601455688477 test_loss:171.60711669921875\n",
      "2058/3000 train_loss: 29.21541404724121 test_loss:170.04017639160156\n",
      "2059/3000 train_loss: 30.590885162353516 test_loss:171.69244384765625\n",
      "2060/3000 train_loss: 26.800418853759766 test_loss:170.28353881835938\n",
      "2061/3000 train_loss: 32.33491516113281 test_loss:171.8522491455078\n",
      "2062/3000 train_loss: 27.394306182861328 test_loss:168.45521545410156\n",
      "2063/3000 train_loss: 30.735260009765625 test_loss:170.9696502685547\n",
      "2064/3000 train_loss: 32.43550491333008 test_loss:170.68380737304688\n",
      "2065/3000 train_loss: 29.046607971191406 test_loss:172.56121826171875\n",
      "2066/3000 train_loss: 25.853422164916992 test_loss:167.28187561035156\n",
      "2067/3000 train_loss: 32.813785552978516 test_loss:168.33865356445312\n",
      "2068/3000 train_loss: 27.11737060546875 test_loss:173.03634643554688\n",
      "2069/3000 train_loss: 29.346281051635742 test_loss:171.4439697265625\n",
      "2070/3000 train_loss: 26.288288116455078 test_loss:171.74789428710938\n",
      "2071/3000 train_loss: 29.48323631286621 test_loss:170.70779418945312\n",
      "2072/3000 train_loss: 25.956626892089844 test_loss:174.3911895751953\n",
      "2073/3000 train_loss: 35.626346588134766 test_loss:168.71401977539062\n",
      "2074/3000 train_loss: 27.893775939941406 test_loss:176.76516723632812\n",
      "2075/3000 train_loss: 28.624526977539062 test_loss:172.34437561035156\n",
      "2076/3000 train_loss: 27.9820556640625 test_loss:168.10276794433594\n",
      "2077/3000 train_loss: 28.913288116455078 test_loss:171.78948974609375\n",
      "2078/3000 train_loss: 26.080982208251953 test_loss:170.666259765625\n",
      "2079/3000 train_loss: 24.622865676879883 test_loss:173.9994354248047\n",
      "2080/3000 train_loss: 23.874210357666016 test_loss:170.51190185546875\n",
      "2081/3000 train_loss: 28.269533157348633 test_loss:175.77516174316406\n",
      "2082/3000 train_loss: 32.52610778808594 test_loss:171.27769470214844\n",
      "2083/3000 train_loss: 29.550004959106445 test_loss:175.4124755859375\n",
      "2084/3000 train_loss: 27.093170166015625 test_loss:168.85733032226562\n",
      "2085/3000 train_loss: 26.815174102783203 test_loss:173.315185546875\n",
      "2086/3000 train_loss: 33.26677703857422 test_loss:170.63047790527344\n",
      "2087/3000 train_loss: 24.80533790588379 test_loss:173.138671875\n",
      "2088/3000 train_loss: 28.53973388671875 test_loss:171.98699951171875\n",
      "2089/3000 train_loss: 30.27766227722168 test_loss:174.25869750976562\n",
      "2090/3000 train_loss: 32.22565460205078 test_loss:173.4237518310547\n",
      "2091/3000 train_loss: 29.79058837890625 test_loss:171.4944305419922\n",
      "2092/3000 train_loss: 27.494293212890625 test_loss:169.49325561523438\n",
      "2093/3000 train_loss: 29.302553176879883 test_loss:169.33187866210938\n",
      "2094/3000 train_loss: 28.49921989440918 test_loss:167.76284790039062\n",
      "2095/3000 train_loss: 27.080795288085938 test_loss:170.95034790039062\n",
      "2096/3000 train_loss: 29.173927307128906 test_loss:169.0623321533203\n",
      "2097/3000 train_loss: 24.857057571411133 test_loss:170.97621154785156\n",
      "2098/3000 train_loss: 24.438554763793945 test_loss:170.07058715820312\n",
      "2099/3000 train_loss: 24.90008544921875 test_loss:172.9207763671875\n",
      "2100/3000 train_loss: 31.123769760131836 test_loss:171.9273681640625\n",
      "2101/3000 train_loss: 29.295867919921875 test_loss:169.88104248046875\n",
      "2102/3000 train_loss: 30.657684326171875 test_loss:167.5386199951172\n",
      "2103/3000 train_loss: 30.67991065979004 test_loss:169.73272705078125\n",
      "2104/3000 train_loss: 27.85003662109375 test_loss:169.7644500732422\n",
      "2105/3000 train_loss: 25.114870071411133 test_loss:170.30589294433594\n",
      "2106/3000 train_loss: 26.18062400817871 test_loss:171.2483367919922\n",
      "2107/3000 train_loss: 30.00308609008789 test_loss:168.707763671875\n",
      "2108/3000 train_loss: 28.401927947998047 test_loss:172.52850341796875\n",
      "2109/3000 train_loss: 25.450258255004883 test_loss:169.8325653076172\n",
      "2110/3000 train_loss: 28.485319137573242 test_loss:171.09884643554688\n",
      "2111/3000 train_loss: 27.2507266998291 test_loss:169.22372436523438\n",
      "2112/3000 train_loss: 29.66640281677246 test_loss:169.77301025390625\n",
      "2113/3000 train_loss: 27.255128860473633 test_loss:169.05795288085938\n",
      "2114/3000 train_loss: 24.830448150634766 test_loss:170.05096435546875\n",
      "2115/3000 train_loss: 26.144081115722656 test_loss:172.6682891845703\n",
      "2116/3000 train_loss: 28.82482147216797 test_loss:168.15289306640625\n",
      "2117/3000 train_loss: 28.49648094177246 test_loss:174.3231201171875\n",
      "2118/3000 train_loss: 27.704143524169922 test_loss:167.20687866210938\n",
      "2119/3000 train_loss: 26.94980812072754 test_loss:169.97605895996094\n",
      "2120/3000 train_loss: 30.354625701904297 test_loss:169.24276733398438\n",
      "2121/3000 train_loss: 24.645261764526367 test_loss:165.96922302246094\n",
      "2122/3000 train_loss: 27.615816116333008 test_loss:172.1875\n",
      "2123/3000 train_loss: 27.40613555908203 test_loss:168.3421630859375\n",
      "2124/3000 train_loss: 26.02311134338379 test_loss:169.96115112304688\n",
      "2125/3000 train_loss: 26.199092864990234 test_loss:168.58181762695312\n",
      "2126/3000 train_loss: 39.85762023925781 test_loss:173.52915954589844\n",
      "2127/3000 train_loss: 26.6268253326416 test_loss:169.84439086914062\n",
      "2128/3000 train_loss: 24.565689086914062 test_loss:168.73464965820312\n",
      "2129/3000 train_loss: 28.678373336791992 test_loss:169.46688842773438\n",
      "2130/3000 train_loss: 27.282590866088867 test_loss:166.43881225585938\n",
      "2131/3000 train_loss: 24.361909866333008 test_loss:169.691162109375\n",
      "2132/3000 train_loss: 26.99127960205078 test_loss:167.81607055664062\n",
      "2133/3000 train_loss: 29.03060531616211 test_loss:170.9517822265625\n",
      "2134/3000 train_loss: 29.63421630859375 test_loss:176.38943481445312\n",
      "2135/3000 train_loss: 24.636369705200195 test_loss:169.90733337402344\n",
      "2136/3000 train_loss: 27.974254608154297 test_loss:168.3893585205078\n",
      "2137/3000 train_loss: 32.62030029296875 test_loss:171.39443969726562\n",
      "2138/3000 train_loss: 25.305679321289062 test_loss:170.8636474609375\n",
      "2139/3000 train_loss: 27.957551956176758 test_loss:171.6549072265625\n",
      "2140/3000 train_loss: 27.46773910522461 test_loss:172.0615234375\n",
      "2141/3000 train_loss: 27.527679443359375 test_loss:174.64544677734375\n",
      "2142/3000 train_loss: 24.065927505493164 test_loss:171.43771362304688\n",
      "2143/3000 train_loss: 23.63125991821289 test_loss:173.1163787841797\n",
      "2144/3000 train_loss: 27.428020477294922 test_loss:168.63168334960938\n",
      "2145/3000 train_loss: 31.609041213989258 test_loss:169.87960815429688\n",
      "2146/3000 train_loss: 33.53647994995117 test_loss:167.53323364257812\n",
      "2147/3000 train_loss: 25.64530372619629 test_loss:169.56399536132812\n",
      "2148/3000 train_loss: 26.347736358642578 test_loss:165.49436950683594\n",
      "2149/3000 train_loss: 32.94230270385742 test_loss:167.24574279785156\n",
      "2150/3000 train_loss: 26.05937957763672 test_loss:171.12655639648438\n",
      "2151/3000 train_loss: 24.143844604492188 test_loss:167.93377685546875\n",
      "2152/3000 train_loss: 24.024921417236328 test_loss:168.35610961914062\n",
      "2153/3000 train_loss: 30.470722198486328 test_loss:171.44973754882812\n",
      "2154/3000 train_loss: 30.032943725585938 test_loss:169.82034301757812\n",
      "2155/3000 train_loss: 43.44318389892578 test_loss:166.63076782226562\n",
      "2156/3000 train_loss: 31.943492889404297 test_loss:167.3256378173828\n",
      "2157/3000 train_loss: 29.65083122253418 test_loss:167.31478881835938\n",
      "2158/3000 train_loss: 25.809917449951172 test_loss:166.25277709960938\n",
      "2159/3000 train_loss: 27.23059844970703 test_loss:167.64242553710938\n",
      "2160/3000 train_loss: 30.476642608642578 test_loss:167.64523315429688\n",
      "2161/3000 train_loss: 30.378129959106445 test_loss:168.75906372070312\n",
      "2162/3000 train_loss: 40.73507308959961 test_loss:165.94020080566406\n",
      "2163/3000 train_loss: 26.562978744506836 test_loss:165.44784545898438\n",
      "2164/3000 train_loss: 26.74666976928711 test_loss:168.1593780517578\n",
      "2165/3000 train_loss: 25.697229385375977 test_loss:164.370849609375\n",
      "2166/3000 train_loss: 27.06321907043457 test_loss:165.25587463378906\n",
      "2167/3000 train_loss: 26.03160285949707 test_loss:163.62213134765625\n",
      "2168/3000 train_loss: 31.40127182006836 test_loss:169.78262329101562\n",
      "2169/3000 train_loss: 29.843677520751953 test_loss:166.2417755126953\n",
      "2170/3000 train_loss: 23.890228271484375 test_loss:165.7187957763672\n",
      "2171/3000 train_loss: 24.010723114013672 test_loss:166.4387969970703\n",
      "2172/3000 train_loss: 28.46752166748047 test_loss:168.0807342529297\n",
      "2173/3000 train_loss: 30.621925354003906 test_loss:170.01025390625\n",
      "2174/3000 train_loss: 28.296627044677734 test_loss:166.14913940429688\n",
      "2175/3000 train_loss: 27.422710418701172 test_loss:166.31634521484375\n",
      "2176/3000 train_loss: 28.569971084594727 test_loss:168.83172607421875\n",
      "2177/3000 train_loss: 25.413488388061523 test_loss:165.981201171875\n",
      "2178/3000 train_loss: 29.996288299560547 test_loss:166.63156127929688\n",
      "2179/3000 train_loss: 26.774932861328125 test_loss:164.05996704101562\n",
      "2180/3000 train_loss: 28.12447738647461 test_loss:165.64114379882812\n",
      "2181/3000 train_loss: 27.045238494873047 test_loss:164.4077911376953\n",
      "2182/3000 train_loss: 23.968196868896484 test_loss:167.24380493164062\n",
      "2183/3000 train_loss: 24.71973419189453 test_loss:164.3106231689453\n",
      "2184/3000 train_loss: 27.140033721923828 test_loss:175.11679077148438\n",
      "2185/3000 train_loss: 33.343505859375 test_loss:167.2219696044922\n",
      "2186/3000 train_loss: 32.8250617980957 test_loss:174.96365356445312\n",
      "2187/3000 train_loss: 25.372394561767578 test_loss:172.00540161132812\n",
      "2188/3000 train_loss: 30.057079315185547 test_loss:175.87057495117188\n",
      "2189/3000 train_loss: 29.830575942993164 test_loss:167.0531005859375\n",
      "2190/3000 train_loss: 30.205522537231445 test_loss:168.9752197265625\n",
      "2191/3000 train_loss: 25.41946029663086 test_loss:173.37911987304688\n",
      "2192/3000 train_loss: 26.78014373779297 test_loss:171.93954467773438\n",
      "2193/3000 train_loss: 27.411195755004883 test_loss:169.11419677734375\n",
      "2194/3000 train_loss: 24.607269287109375 test_loss:173.82919311523438\n",
      "2195/3000 train_loss: 25.880653381347656 test_loss:168.84129333496094\n",
      "2196/3000 train_loss: 25.584383010864258 test_loss:172.26828002929688\n",
      "2197/3000 train_loss: 24.602909088134766 test_loss:170.5915069580078\n",
      "2198/3000 train_loss: 24.84493637084961 test_loss:167.1533203125\n",
      "2199/3000 train_loss: 24.233566284179688 test_loss:169.1223602294922\n",
      "2200/3000 train_loss: 26.438432693481445 test_loss:172.3777313232422\n",
      "2201/3000 train_loss: 24.85063934326172 test_loss:170.629638671875\n",
      "2202/3000 train_loss: 27.26021957397461 test_loss:170.18130493164062\n",
      "2203/3000 train_loss: 26.46751594543457 test_loss:177.28289794921875\n",
      "2204/3000 train_loss: 30.284048080444336 test_loss:172.84246826171875\n",
      "2205/3000 train_loss: 26.94536590576172 test_loss:172.70758056640625\n",
      "2206/3000 train_loss: 24.428125381469727 test_loss:167.52078247070312\n",
      "2207/3000 train_loss: 25.87517547607422 test_loss:168.31396484375\n",
      "2208/3000 train_loss: 28.27621841430664 test_loss:169.31619262695312\n",
      "2209/3000 train_loss: 30.679195404052734 test_loss:170.56884765625\n",
      "2210/3000 train_loss: 26.11595344543457 test_loss:165.46292114257812\n",
      "2211/3000 train_loss: 22.17037582397461 test_loss:168.75729370117188\n",
      "2212/3000 train_loss: 22.890350341796875 test_loss:168.560546875\n",
      "2213/3000 train_loss: 35.6110954284668 test_loss:168.41807556152344\n",
      "2214/3000 train_loss: 30.309982299804688 test_loss:171.3339080810547\n",
      "2215/3000 train_loss: 25.192100524902344 test_loss:168.06689453125\n",
      "2216/3000 train_loss: 28.120119094848633 test_loss:170.17401123046875\n",
      "2217/3000 train_loss: 29.884624481201172 test_loss:168.64849853515625\n",
      "2218/3000 train_loss: 34.66514205932617 test_loss:172.89157104492188\n",
      "2219/3000 train_loss: 27.237651824951172 test_loss:169.80111694335938\n",
      "2220/3000 train_loss: 26.437332153320312 test_loss:172.67727661132812\n",
      "2221/3000 train_loss: 30.127971649169922 test_loss:166.20867919921875\n",
      "2222/3000 train_loss: 23.731460571289062 test_loss:169.2394256591797\n",
      "2223/3000 train_loss: 23.826946258544922 test_loss:168.7764434814453\n",
      "2224/3000 train_loss: 25.040874481201172 test_loss:167.01124572753906\n",
      "2225/3000 train_loss: 30.847436904907227 test_loss:169.41860961914062\n",
      "2226/3000 train_loss: 26.19481086730957 test_loss:169.42709350585938\n",
      "2227/3000 train_loss: 25.114978790283203 test_loss:167.64955139160156\n",
      "2228/3000 train_loss: 22.98574447631836 test_loss:166.9545135498047\n",
      "2229/3000 train_loss: 29.0057315826416 test_loss:173.61163330078125\n",
      "2230/3000 train_loss: 28.281688690185547 test_loss:175.536376953125\n",
      "2231/3000 train_loss: 31.081727981567383 test_loss:168.34657287597656\n",
      "2232/3000 train_loss: 28.347171783447266 test_loss:166.41888427734375\n",
      "2233/3000 train_loss: 23.789356231689453 test_loss:168.15496826171875\n",
      "2234/3000 train_loss: 29.42037010192871 test_loss:169.56370544433594\n",
      "2235/3000 train_loss: 36.8871955871582 test_loss:168.84893798828125\n",
      "2236/3000 train_loss: 28.742454528808594 test_loss:167.14537048339844\n",
      "2237/3000 train_loss: 26.555438995361328 test_loss:166.77210998535156\n",
      "2238/3000 train_loss: 29.209882736206055 test_loss:171.01461791992188\n",
      "2239/3000 train_loss: 24.4245662689209 test_loss:165.06997680664062\n",
      "2240/3000 train_loss: 31.660888671875 test_loss:168.6842041015625\n",
      "2241/3000 train_loss: 26.516387939453125 test_loss:169.34481811523438\n",
      "2242/3000 train_loss: 26.787309646606445 test_loss:165.2644805908203\n",
      "2243/3000 train_loss: 24.3057804107666 test_loss:164.8682861328125\n",
      "2244/3000 train_loss: 26.900575637817383 test_loss:165.27679443359375\n",
      "2245/3000 train_loss: 25.756311416625977 test_loss:168.65306091308594\n",
      "2246/3000 train_loss: 25.282411575317383 test_loss:168.27532958984375\n",
      "2247/3000 train_loss: 31.54990005493164 test_loss:165.45123291015625\n",
      "2248/3000 train_loss: 28.867055892944336 test_loss:167.7256622314453\n",
      "2249/3000 train_loss: 28.40567398071289 test_loss:166.1719207763672\n",
      "2250/3000 train_loss: 29.054595947265625 test_loss:164.68414306640625\n",
      "2251/3000 train_loss: 25.86595344543457 test_loss:166.30860900878906\n",
      "2252/3000 train_loss: 26.343547821044922 test_loss:167.32968139648438\n",
      "2253/3000 train_loss: 23.09077262878418 test_loss:167.1316680908203\n",
      "2254/3000 train_loss: 22.582162857055664 test_loss:166.21343994140625\n",
      "2255/3000 train_loss: 23.172626495361328 test_loss:166.68576049804688\n",
      "2256/3000 train_loss: 23.969907760620117 test_loss:163.73272705078125\n",
      "2257/3000 train_loss: 32.75141525268555 test_loss:167.48223876953125\n",
      "2258/3000 train_loss: 24.922107696533203 test_loss:167.34677124023438\n",
      "2259/3000 train_loss: 23.42624855041504 test_loss:168.94412231445312\n",
      "2260/3000 train_loss: 26.320884704589844 test_loss:167.00497436523438\n",
      "2261/3000 train_loss: 25.18053436279297 test_loss:167.41940307617188\n",
      "2262/3000 train_loss: 24.727333068847656 test_loss:167.5210418701172\n",
      "2263/3000 train_loss: 25.9302921295166 test_loss:170.19927978515625\n",
      "2264/3000 train_loss: 25.345659255981445 test_loss:166.9865264892578\n",
      "2265/3000 train_loss: 24.225217819213867 test_loss:170.1569366455078\n",
      "2266/3000 train_loss: 23.416215896606445 test_loss:169.32568359375\n",
      "2267/3000 train_loss: 27.176767349243164 test_loss:166.5315704345703\n",
      "2268/3000 train_loss: 34.270572662353516 test_loss:175.32916259765625\n",
      "2269/3000 train_loss: 28.297609329223633 test_loss:166.087646484375\n",
      "2270/3000 train_loss: 25.616413116455078 test_loss:165.96258544921875\n",
      "2271/3000 train_loss: 22.701244354248047 test_loss:168.6083984375\n",
      "2272/3000 train_loss: 30.19802474975586 test_loss:166.12655639648438\n",
      "2273/3000 train_loss: 25.300491333007812 test_loss:168.99864196777344\n",
      "2274/3000 train_loss: 25.079360961914062 test_loss:166.3616943359375\n",
      "2275/3000 train_loss: 26.55170249938965 test_loss:166.69210815429688\n",
      "2276/3000 train_loss: 40.370506286621094 test_loss:168.9730224609375\n",
      "2277/3000 train_loss: 26.41559410095215 test_loss:167.793212890625\n",
      "2278/3000 train_loss: 25.85842514038086 test_loss:173.2659912109375\n",
      "2279/3000 train_loss: 27.981800079345703 test_loss:168.80865478515625\n",
      "2280/3000 train_loss: 29.9316349029541 test_loss:168.27943420410156\n",
      "2281/3000 train_loss: 26.511396408081055 test_loss:163.77174377441406\n",
      "2282/3000 train_loss: 26.665393829345703 test_loss:166.21803283691406\n",
      "2283/3000 train_loss: 26.483469009399414 test_loss:167.04013061523438\n",
      "2284/3000 train_loss: 24.370258331298828 test_loss:170.11648559570312\n",
      "2285/3000 train_loss: 24.742286682128906 test_loss:162.66876220703125\n",
      "2286/3000 train_loss: 26.308422088623047 test_loss:164.6102752685547\n",
      "2287/3000 train_loss: 25.16855239868164 test_loss:166.19154357910156\n",
      "2288/3000 train_loss: 21.852527618408203 test_loss:164.37063598632812\n",
      "2289/3000 train_loss: 20.60089683532715 test_loss:164.86936950683594\n",
      "2290/3000 train_loss: 24.82135581970215 test_loss:165.57528686523438\n",
      "2291/3000 train_loss: 27.58631134033203 test_loss:170.62359619140625\n",
      "2292/3000 train_loss: 29.120521545410156 test_loss:162.0228729248047\n",
      "2293/3000 train_loss: 27.411643981933594 test_loss:170.06698608398438\n",
      "2294/3000 train_loss: 24.601205825805664 test_loss:163.76808166503906\n",
      "2295/3000 train_loss: 33.94994354248047 test_loss:162.5145263671875\n",
      "2296/3000 train_loss: 25.972640991210938 test_loss:167.21444702148438\n",
      "2297/3000 train_loss: 24.802431106567383 test_loss:171.93499755859375\n",
      "2298/3000 train_loss: 27.774219512939453 test_loss:164.52969360351562\n",
      "2299/3000 train_loss: 27.452777862548828 test_loss:165.79730224609375\n",
      "2300/3000 train_loss: 26.14303207397461 test_loss:176.22158813476562\n",
      "2301/3000 train_loss: 26.049612045288086 test_loss:167.71852111816406\n",
      "2302/3000 train_loss: 34.520626068115234 test_loss:164.56642150878906\n",
      "2303/3000 train_loss: 24.345943450927734 test_loss:165.44126892089844\n",
      "2304/3000 train_loss: 25.448341369628906 test_loss:166.31735229492188\n",
      "2305/3000 train_loss: 22.188199996948242 test_loss:163.66293334960938\n",
      "2306/3000 train_loss: 21.701675415039062 test_loss:164.63645935058594\n",
      "2307/3000 train_loss: 24.9075984954834 test_loss:166.48165893554688\n",
      "2308/3000 train_loss: 24.54278564453125 test_loss:168.1782684326172\n",
      "2309/3000 train_loss: 24.309762954711914 test_loss:170.50296020507812\n",
      "2310/3000 train_loss: 33.440650939941406 test_loss:171.07025146484375\n",
      "2311/3000 train_loss: 25.238216400146484 test_loss:171.79052734375\n",
      "2312/3000 train_loss: 25.112285614013672 test_loss:168.76344299316406\n",
      "2313/3000 train_loss: 26.81899642944336 test_loss:161.70401000976562\n",
      "2314/3000 train_loss: 28.278030395507812 test_loss:163.93504333496094\n",
      "2315/3000 train_loss: 21.907588958740234 test_loss:164.54840087890625\n",
      "2316/3000 train_loss: 25.051687240600586 test_loss:163.26942443847656\n",
      "2317/3000 train_loss: 29.591083526611328 test_loss:167.47613525390625\n",
      "2318/3000 train_loss: 26.858211517333984 test_loss:166.75347900390625\n",
      "2319/3000 train_loss: 23.711864471435547 test_loss:168.77947998046875\n",
      "2320/3000 train_loss: 24.305694580078125 test_loss:167.62242126464844\n",
      "2321/3000 train_loss: 22.247543334960938 test_loss:169.2898406982422\n",
      "2322/3000 train_loss: 23.267520904541016 test_loss:168.62942504882812\n",
      "2323/3000 train_loss: 26.791339874267578 test_loss:165.04849243164062\n",
      "2324/3000 train_loss: 22.948871612548828 test_loss:176.04190063476562\n",
      "2325/3000 train_loss: 33.45745086669922 test_loss:167.95846557617188\n",
      "2326/3000 train_loss: 30.03536033630371 test_loss:164.77565002441406\n",
      "2327/3000 train_loss: 27.956600189208984 test_loss:170.79417419433594\n",
      "2328/3000 train_loss: 27.251367568969727 test_loss:167.18832397460938\n",
      "2329/3000 train_loss: 22.252687454223633 test_loss:168.275634765625\n",
      "2330/3000 train_loss: 23.90845489501953 test_loss:167.86013793945312\n",
      "2331/3000 train_loss: 21.60309600830078 test_loss:168.1582489013672\n",
      "2332/3000 train_loss: 23.9573974609375 test_loss:165.60446166992188\n",
      "2333/3000 train_loss: 25.67913818359375 test_loss:167.9493408203125\n",
      "2334/3000 train_loss: 26.24679946899414 test_loss:164.716796875\n",
      "2335/3000 train_loss: 33.065887451171875 test_loss:162.4605712890625\n",
      "2336/3000 train_loss: 25.865741729736328 test_loss:165.46295166015625\n",
      "2337/3000 train_loss: 24.98912239074707 test_loss:165.8701934814453\n",
      "2338/3000 train_loss: 25.601503372192383 test_loss:165.2357635498047\n",
      "2339/3000 train_loss: 23.770307540893555 test_loss:162.41287231445312\n",
      "2340/3000 train_loss: 21.087421417236328 test_loss:161.6890869140625\n",
      "2341/3000 train_loss: 21.21481704711914 test_loss:166.80960083007812\n",
      "2342/3000 train_loss: 31.92891502380371 test_loss:167.68255615234375\n",
      "2343/3000 train_loss: 26.293243408203125 test_loss:166.1837158203125\n",
      "2344/3000 train_loss: 24.054500579833984 test_loss:163.2176055908203\n",
      "2345/3000 train_loss: 27.194507598876953 test_loss:165.21163940429688\n",
      "2346/3000 train_loss: 23.256982803344727 test_loss:166.52243041992188\n",
      "2347/3000 train_loss: 25.179841995239258 test_loss:168.22857666015625\n",
      "2348/3000 train_loss: 34.398765563964844 test_loss:167.02247619628906\n",
      "2349/3000 train_loss: 23.082935333251953 test_loss:166.94381713867188\n",
      "2350/3000 train_loss: 25.468656539916992 test_loss:170.1253204345703\n",
      "2351/3000 train_loss: 25.597421646118164 test_loss:165.8481903076172\n",
      "2352/3000 train_loss: 23.646263122558594 test_loss:163.90911865234375\n",
      "2353/3000 train_loss: 26.53909683227539 test_loss:162.706787109375\n",
      "2354/3000 train_loss: 34.34654235839844 test_loss:160.83863830566406\n",
      "2355/3000 train_loss: 23.205984115600586 test_loss:163.1478271484375\n",
      "2356/3000 train_loss: 25.408708572387695 test_loss:160.11447143554688\n",
      "2357/3000 train_loss: 24.834632873535156 test_loss:162.53665161132812\n",
      "2358/3000 train_loss: 29.19650650024414 test_loss:164.83010864257812\n",
      "2359/3000 train_loss: 26.928606033325195 test_loss:163.88900756835938\n",
      "2360/3000 train_loss: 26.98223304748535 test_loss:165.25021362304688\n",
      "2361/3000 train_loss: 24.70633888244629 test_loss:164.49044799804688\n",
      "2362/3000 train_loss: 26.51875877380371 test_loss:169.36785888671875\n",
      "2363/3000 train_loss: 26.443084716796875 test_loss:166.06536865234375\n",
      "2364/3000 train_loss: 25.17681884765625 test_loss:163.931396484375\n",
      "2365/3000 train_loss: 24.661975860595703 test_loss:164.24923706054688\n",
      "2366/3000 train_loss: 27.740310668945312 test_loss:166.02481079101562\n",
      "2367/3000 train_loss: 23.564041137695312 test_loss:161.35787963867188\n",
      "2368/3000 train_loss: 33.67327880859375 test_loss:169.96133422851562\n",
      "2369/3000 train_loss: 26.516321182250977 test_loss:167.9046630859375\n",
      "2370/3000 train_loss: 25.638843536376953 test_loss:167.76113891601562\n",
      "2371/3000 train_loss: 29.02023696899414 test_loss:166.2752685546875\n",
      "2372/3000 train_loss: 34.57117462158203 test_loss:169.1017608642578\n",
      "2373/3000 train_loss: 27.698997497558594 test_loss:168.70974731445312\n",
      "2374/3000 train_loss: 23.85560417175293 test_loss:167.79379272460938\n",
      "2375/3000 train_loss: 25.524627685546875 test_loss:167.0489501953125\n",
      "2376/3000 train_loss: 31.345653533935547 test_loss:173.99964904785156\n",
      "2377/3000 train_loss: 27.934062957763672 test_loss:167.26881408691406\n",
      "2378/3000 train_loss: 30.627744674682617 test_loss:163.96385192871094\n",
      "2379/3000 train_loss: 21.01819610595703 test_loss:165.5111846923828\n",
      "2380/3000 train_loss: 24.645732879638672 test_loss:166.95204162597656\n",
      "2381/3000 train_loss: 25.84779930114746 test_loss:165.16310119628906\n",
      "2382/3000 train_loss: 24.38381576538086 test_loss:171.66561889648438\n",
      "2383/3000 train_loss: 29.79322624206543 test_loss:167.95664978027344\n",
      "2384/3000 train_loss: 22.420289993286133 test_loss:163.31741333007812\n",
      "2385/3000 train_loss: 24.841434478759766 test_loss:164.92626953125\n",
      "2386/3000 train_loss: 24.512449264526367 test_loss:163.11138916015625\n",
      "2387/3000 train_loss: 21.585527420043945 test_loss:164.10421752929688\n",
      "2388/3000 train_loss: 23.56592559814453 test_loss:164.732421875\n",
      "2389/3000 train_loss: 22.379615783691406 test_loss:162.74981689453125\n",
      "2390/3000 train_loss: 23.42788314819336 test_loss:166.15548706054688\n",
      "2391/3000 train_loss: 24.7513370513916 test_loss:162.43289184570312\n",
      "2392/3000 train_loss: 25.841907501220703 test_loss:165.71560668945312\n",
      "2393/3000 train_loss: 23.141231536865234 test_loss:165.63107299804688\n",
      "2394/3000 train_loss: 22.087108612060547 test_loss:163.22764587402344\n",
      "2395/3000 train_loss: 23.938081741333008 test_loss:167.3609619140625\n",
      "2396/3000 train_loss: 22.011507034301758 test_loss:168.19647216796875\n",
      "2397/3000 train_loss: 25.672666549682617 test_loss:166.35720825195312\n",
      "2398/3000 train_loss: 24.50465965270996 test_loss:168.12173461914062\n",
      "2399/3000 train_loss: 23.047401428222656 test_loss:162.9962158203125\n",
      "2400/3000 train_loss: 24.397872924804688 test_loss:163.8096923828125\n",
      "2401/3000 train_loss: 27.38318634033203 test_loss:165.39471435546875\n",
      "2402/3000 train_loss: 23.212528228759766 test_loss:163.67431640625\n",
      "2403/3000 train_loss: 25.182435989379883 test_loss:161.06219482421875\n",
      "2404/3000 train_loss: 28.43175506591797 test_loss:170.66390991210938\n",
      "2405/3000 train_loss: 23.780832290649414 test_loss:165.57615661621094\n",
      "2406/3000 train_loss: 26.089527130126953 test_loss:166.41256713867188\n",
      "2407/3000 train_loss: 30.00255584716797 test_loss:165.6182861328125\n",
      "2408/3000 train_loss: 22.589948654174805 test_loss:165.8456268310547\n",
      "2409/3000 train_loss: 21.91729164123535 test_loss:162.76315307617188\n",
      "2410/3000 train_loss: 25.120899200439453 test_loss:159.52589416503906\n",
      "2411/3000 train_loss: 26.385942459106445 test_loss:165.18939208984375\n",
      "2412/3000 train_loss: 31.683273315429688 test_loss:165.1630401611328\n",
      "2413/3000 train_loss: 22.098581314086914 test_loss:161.48434448242188\n",
      "2414/3000 train_loss: 25.1746883392334 test_loss:164.90199279785156\n",
      "2415/3000 train_loss: 29.072647094726562 test_loss:166.1083984375\n",
      "2416/3000 train_loss: 23.509601593017578 test_loss:165.0697021484375\n",
      "2417/3000 train_loss: 21.746517181396484 test_loss:165.75881958007812\n",
      "2418/3000 train_loss: 26.907726287841797 test_loss:165.09112548828125\n",
      "2419/3000 train_loss: 23.429916381835938 test_loss:166.04910278320312\n",
      "2420/3000 train_loss: 24.751638412475586 test_loss:167.7381591796875\n",
      "2421/3000 train_loss: 24.98527717590332 test_loss:169.20433044433594\n",
      "2422/3000 train_loss: 23.90203857421875 test_loss:166.97442626953125\n",
      "2423/3000 train_loss: 22.254390716552734 test_loss:164.1973114013672\n",
      "2424/3000 train_loss: 23.57762908935547 test_loss:165.18067932128906\n",
      "2425/3000 train_loss: 20.826526641845703 test_loss:168.21087646484375\n",
      "2426/3000 train_loss: 24.274919509887695 test_loss:163.83970642089844\n",
      "2427/3000 train_loss: 26.37342071533203 test_loss:163.83416748046875\n",
      "2428/3000 train_loss: 26.580669403076172 test_loss:167.13267517089844\n",
      "2429/3000 train_loss: 25.741893768310547 test_loss:166.2195587158203\n",
      "2430/3000 train_loss: 23.4387149810791 test_loss:163.19479370117188\n",
      "2431/3000 train_loss: 24.11065673828125 test_loss:173.90280151367188\n",
      "2432/3000 train_loss: 21.592697143554688 test_loss:167.50595092773438\n",
      "2433/3000 train_loss: 24.580223083496094 test_loss:163.62509155273438\n",
      "2434/3000 train_loss: 25.950618743896484 test_loss:164.2407684326172\n",
      "2435/3000 train_loss: 26.583805084228516 test_loss:164.73622131347656\n",
      "2436/3000 train_loss: 31.21649169921875 test_loss:167.02854919433594\n",
      "2437/3000 train_loss: 25.770620346069336 test_loss:165.70936584472656\n",
      "2438/3000 train_loss: 27.227039337158203 test_loss:167.05506896972656\n",
      "2439/3000 train_loss: 24.590896606445312 test_loss:163.94808959960938\n",
      "2440/3000 train_loss: 25.65381622314453 test_loss:168.34271240234375\n",
      "2441/3000 train_loss: 24.870025634765625 test_loss:171.3898468017578\n",
      "2442/3000 train_loss: 25.483400344848633 test_loss:161.66307067871094\n",
      "2443/3000 train_loss: 28.231891632080078 test_loss:172.16213989257812\n",
      "2444/3000 train_loss: 23.77682876586914 test_loss:165.26548767089844\n",
      "2445/3000 train_loss: 23.112773895263672 test_loss:161.76124572753906\n",
      "2446/3000 train_loss: 26.166778564453125 test_loss:166.50030517578125\n",
      "2447/3000 train_loss: 22.597307205200195 test_loss:163.7349853515625\n",
      "2448/3000 train_loss: 20.570812225341797 test_loss:164.03359985351562\n",
      "2449/3000 train_loss: 25.576013565063477 test_loss:167.24087524414062\n",
      "2450/3000 train_loss: 25.83856964111328 test_loss:166.26739501953125\n",
      "2451/3000 train_loss: 22.734968185424805 test_loss:161.8602294921875\n",
      "2452/3000 train_loss: 22.963937759399414 test_loss:163.43528747558594\n",
      "2453/3000 train_loss: 26.365638732910156 test_loss:175.32431030273438\n",
      "2454/3000 train_loss: 21.463420867919922 test_loss:182.74832153320312\n",
      "2455/3000 train_loss: 22.765871047973633 test_loss:165.1929168701172\n",
      "2456/3000 train_loss: 20.67012596130371 test_loss:161.57647705078125\n",
      "2457/3000 train_loss: 21.601726531982422 test_loss:166.79562377929688\n",
      "2458/3000 train_loss: 24.862686157226562 test_loss:161.75299072265625\n",
      "2459/3000 train_loss: 28.071552276611328 test_loss:161.9037322998047\n",
      "2460/3000 train_loss: 24.953842163085938 test_loss:170.4524383544922\n",
      "2461/3000 train_loss: 22.755577087402344 test_loss:160.49012756347656\n",
      "2462/3000 train_loss: 20.0197811126709 test_loss:160.80661010742188\n",
      "2463/3000 train_loss: 26.917028427124023 test_loss:165.419921875\n",
      "2464/3000 train_loss: 23.630138397216797 test_loss:163.79965209960938\n",
      "2465/3000 train_loss: 21.481382369995117 test_loss:159.4009552001953\n",
      "2466/3000 train_loss: 22.322256088256836 test_loss:162.3607177734375\n",
      "2467/3000 train_loss: 22.82921028137207 test_loss:161.22848510742188\n",
      "2468/3000 train_loss: 21.485727310180664 test_loss:163.19369506835938\n",
      "2469/3000 train_loss: 20.954214096069336 test_loss:164.0740966796875\n",
      "2470/3000 train_loss: 21.726961135864258 test_loss:169.0330352783203\n",
      "2471/3000 train_loss: 23.681520462036133 test_loss:167.99188232421875\n",
      "2472/3000 train_loss: 25.157045364379883 test_loss:166.1607666015625\n",
      "2473/3000 train_loss: 21.39016342163086 test_loss:164.1040496826172\n",
      "2474/3000 train_loss: 19.085451126098633 test_loss:161.674072265625\n",
      "2475/3000 train_loss: 21.77334976196289 test_loss:164.69024658203125\n",
      "2476/3000 train_loss: 22.33529281616211 test_loss:160.5965118408203\n",
      "2477/3000 train_loss: 20.845191955566406 test_loss:161.9321746826172\n",
      "2478/3000 train_loss: 24.388105392456055 test_loss:160.04244995117188\n",
      "2479/3000 train_loss: 25.104169845581055 test_loss:163.33709716796875\n",
      "2480/3000 train_loss: 23.309951782226562 test_loss:163.3965301513672\n",
      "2481/3000 train_loss: 22.836488723754883 test_loss:163.77243041992188\n",
      "2482/3000 train_loss: 25.045833587646484 test_loss:160.87420654296875\n",
      "2483/3000 train_loss: 27.326019287109375 test_loss:164.439697265625\n",
      "2484/3000 train_loss: 21.266273498535156 test_loss:163.009033203125\n",
      "2485/3000 train_loss: 22.609315872192383 test_loss:163.91981506347656\n",
      "2486/3000 train_loss: 21.924379348754883 test_loss:164.5461883544922\n",
      "2487/3000 train_loss: 23.411279678344727 test_loss:164.35968017578125\n",
      "2488/3000 train_loss: 27.651288986206055 test_loss:166.3935089111328\n",
      "2489/3000 train_loss: 21.53275489807129 test_loss:165.51449584960938\n",
      "2490/3000 train_loss: 20.183868408203125 test_loss:165.89956665039062\n",
      "2491/3000 train_loss: 24.667953491210938 test_loss:161.5998992919922\n",
      "2492/3000 train_loss: 21.297527313232422 test_loss:168.46917724609375\n",
      "2493/3000 train_loss: 27.106279373168945 test_loss:173.13754272460938\n",
      "2494/3000 train_loss: 22.485118865966797 test_loss:165.1927490234375\n",
      "2495/3000 train_loss: 24.555273056030273 test_loss:166.75205993652344\n",
      "2496/3000 train_loss: 22.931921005249023 test_loss:165.222412109375\n",
      "2497/3000 train_loss: 20.864107131958008 test_loss:164.3733673095703\n",
      "2498/3000 train_loss: 24.76765251159668 test_loss:159.9635467529297\n",
      "2499/3000 train_loss: 22.831092834472656 test_loss:161.40316772460938\n",
      "2500/3000 train_loss: 21.29704475402832 test_loss:163.62774658203125\n",
      "2501/3000 train_loss: 23.5361328125 test_loss:163.61219787597656\n",
      "2502/3000 train_loss: 24.656702041625977 test_loss:159.03395080566406\n",
      "2503/3000 train_loss: 21.856164932250977 test_loss:159.27066040039062\n",
      "2504/3000 train_loss: 23.97058868408203 test_loss:161.2940216064453\n",
      "2505/3000 train_loss: 38.38798522949219 test_loss:163.04147338867188\n",
      "2506/3000 train_loss: 22.55013084411621 test_loss:167.90501403808594\n",
      "2507/3000 train_loss: 28.3680477142334 test_loss:167.14439392089844\n",
      "2508/3000 train_loss: 22.854333877563477 test_loss:164.75994873046875\n",
      "2509/3000 train_loss: 34.12506866455078 test_loss:163.3488311767578\n",
      "2510/3000 train_loss: 26.685932159423828 test_loss:164.1952667236328\n",
      "2511/3000 train_loss: 24.77707290649414 test_loss:161.89761352539062\n",
      "2512/3000 train_loss: 22.452442169189453 test_loss:166.61412048339844\n",
      "2513/3000 train_loss: 24.967052459716797 test_loss:163.3999481201172\n",
      "2514/3000 train_loss: 21.1299991607666 test_loss:159.98458862304688\n",
      "2515/3000 train_loss: 23.147602081298828 test_loss:165.6392822265625\n",
      "2516/3000 train_loss: 27.171483993530273 test_loss:163.27468872070312\n",
      "2517/3000 train_loss: 21.560787200927734 test_loss:166.99337768554688\n",
      "2518/3000 train_loss: 26.172283172607422 test_loss:164.7286834716797\n",
      "2519/3000 train_loss: 22.62738609313965 test_loss:160.48147583007812\n",
      "2520/3000 train_loss: 21.043357849121094 test_loss:162.04782104492188\n",
      "2521/3000 train_loss: 26.44296646118164 test_loss:159.86398315429688\n",
      "2522/3000 train_loss: 21.980194091796875 test_loss:161.87176513671875\n",
      "2523/3000 train_loss: 21.32682228088379 test_loss:164.2374267578125\n",
      "2524/3000 train_loss: 22.2717227935791 test_loss:162.15859985351562\n",
      "2525/3000 train_loss: 25.05277442932129 test_loss:164.4311981201172\n",
      "2526/3000 train_loss: 24.878902435302734 test_loss:169.91419982910156\n",
      "2527/3000 train_loss: 20.218154907226562 test_loss:162.74176025390625\n",
      "2528/3000 train_loss: 20.374380111694336 test_loss:165.43177795410156\n",
      "2529/3000 train_loss: 24.303407669067383 test_loss:162.06344604492188\n",
      "2530/3000 train_loss: 23.187255859375 test_loss:162.83004760742188\n",
      "2531/3000 train_loss: 22.54374885559082 test_loss:163.55982971191406\n",
      "2532/3000 train_loss: 25.561935424804688 test_loss:162.81597900390625\n",
      "2533/3000 train_loss: 40.2939338684082 test_loss:161.7730712890625\n",
      "2534/3000 train_loss: 24.360864639282227 test_loss:159.969970703125\n",
      "2535/3000 train_loss: 22.894786834716797 test_loss:160.91326904296875\n",
      "2536/3000 train_loss: 21.979724884033203 test_loss:157.1636962890625\n",
      "2537/3000 train_loss: 23.16388702392578 test_loss:160.3341064453125\n",
      "2538/3000 train_loss: 22.705984115600586 test_loss:158.53981018066406\n",
      "2539/3000 train_loss: 23.851055145263672 test_loss:157.27069091796875\n",
      "2540/3000 train_loss: 31.470611572265625 test_loss:160.9335174560547\n",
      "2541/3000 train_loss: 20.2004451751709 test_loss:156.92396545410156\n",
      "2542/3000 train_loss: 24.353515625 test_loss:169.40771484375\n",
      "2543/3000 train_loss: 26.989421844482422 test_loss:165.70718383789062\n",
      "2544/3000 train_loss: 19.776981353759766 test_loss:154.78330993652344\n",
      "2545/3000 train_loss: 22.737062454223633 test_loss:157.0669708251953\n",
      "2546/3000 train_loss: 24.60769271850586 test_loss:157.9513397216797\n",
      "2547/3000 train_loss: 20.99968910217285 test_loss:160.24131774902344\n",
      "2548/3000 train_loss: 23.769947052001953 test_loss:162.2449951171875\n",
      "2549/3000 train_loss: 22.138843536376953 test_loss:157.3131103515625\n",
      "2550/3000 train_loss: 31.025802612304688 test_loss:156.96115112304688\n",
      "2551/3000 train_loss: 23.358154296875 test_loss:160.355712890625\n",
      "2552/3000 train_loss: 27.216548919677734 test_loss:157.39361572265625\n",
      "2553/3000 train_loss: 25.314908981323242 test_loss:161.6422119140625\n",
      "2554/3000 train_loss: 22.763626098632812 test_loss:158.67758178710938\n",
      "2555/3000 train_loss: 22.89064598083496 test_loss:159.61746215820312\n",
      "2556/3000 train_loss: 19.809110641479492 test_loss:159.68093872070312\n",
      "2557/3000 train_loss: 19.91712188720703 test_loss:161.62498474121094\n",
      "2558/3000 train_loss: 22.260690689086914 test_loss:159.2646942138672\n",
      "2559/3000 train_loss: 20.523569107055664 test_loss:157.54287719726562\n",
      "2560/3000 train_loss: 20.743505477905273 test_loss:160.62149047851562\n",
      "2561/3000 train_loss: 25.217247009277344 test_loss:159.3173828125\n",
      "2562/3000 train_loss: 21.991527557373047 test_loss:154.66270446777344\n",
      "2563/3000 train_loss: 28.23118019104004 test_loss:158.49676513671875\n",
      "2564/3000 train_loss: 20.99862289428711 test_loss:157.3041229248047\n",
      "2565/3000 train_loss: 23.06011962890625 test_loss:155.86703491210938\n",
      "2566/3000 train_loss: 21.624704360961914 test_loss:157.01922607421875\n",
      "2567/3000 train_loss: 18.855844497680664 test_loss:159.43637084960938\n",
      "2568/3000 train_loss: 22.701005935668945 test_loss:160.77606201171875\n",
      "2569/3000 train_loss: 26.674699783325195 test_loss:160.8843994140625\n",
      "2570/3000 train_loss: 24.547710418701172 test_loss:156.7934112548828\n",
      "2571/3000 train_loss: 23.611942291259766 test_loss:159.08583068847656\n",
      "2572/3000 train_loss: 20.902034759521484 test_loss:161.34103393554688\n",
      "2573/3000 train_loss: 22.25217628479004 test_loss:158.1263427734375\n",
      "2574/3000 train_loss: 19.970245361328125 test_loss:158.0487823486328\n",
      "2575/3000 train_loss: 21.074108123779297 test_loss:160.17276000976562\n",
      "2576/3000 train_loss: 27.525819778442383 test_loss:164.93670654296875\n",
      "2577/3000 train_loss: 21.967374801635742 test_loss:158.729736328125\n",
      "2578/3000 train_loss: 22.375137329101562 test_loss:162.27410888671875\n",
      "2579/3000 train_loss: 22.36014747619629 test_loss:163.1553497314453\n",
      "2580/3000 train_loss: 19.834003448486328 test_loss:165.7692108154297\n",
      "2581/3000 train_loss: 23.757030487060547 test_loss:163.24203491210938\n",
      "2582/3000 train_loss: 23.29329490661621 test_loss:158.75767517089844\n",
      "2583/3000 train_loss: 24.940052032470703 test_loss:161.15652465820312\n",
      "2584/3000 train_loss: 23.242685317993164 test_loss:158.32244873046875\n",
      "2585/3000 train_loss: 23.03843879699707 test_loss:160.29055786132812\n",
      "2586/3000 train_loss: 22.31354522705078 test_loss:162.552978515625\n",
      "2587/3000 train_loss: 25.13949966430664 test_loss:163.24139404296875\n",
      "2588/3000 train_loss: 22.573850631713867 test_loss:166.72010803222656\n",
      "2589/3000 train_loss: 21.384061813354492 test_loss:164.05691528320312\n",
      "2590/3000 train_loss: 19.949480056762695 test_loss:162.2916259765625\n",
      "2591/3000 train_loss: 21.96358871459961 test_loss:159.11322021484375\n",
      "2592/3000 train_loss: 24.25846290588379 test_loss:161.1073760986328\n",
      "2593/3000 train_loss: 24.76517677307129 test_loss:167.26747131347656\n",
      "2594/3000 train_loss: 26.67193603515625 test_loss:162.1088409423828\n",
      "2595/3000 train_loss: 33.52921676635742 test_loss:165.28810119628906\n",
      "2596/3000 train_loss: 26.008867263793945 test_loss:159.30853271484375\n",
      "2597/3000 train_loss: 22.331756591796875 test_loss:157.068115234375\n",
      "2598/3000 train_loss: 25.170068740844727 test_loss:162.80157470703125\n",
      "2599/3000 train_loss: 23.959566116333008 test_loss:161.3751220703125\n",
      "2600/3000 train_loss: 23.175878524780273 test_loss:159.79217529296875\n",
      "2601/3000 train_loss: 23.672027587890625 test_loss:160.52426147460938\n",
      "2602/3000 train_loss: 22.13103485107422 test_loss:159.16357421875\n",
      "2603/3000 train_loss: 19.662490844726562 test_loss:158.2901611328125\n",
      "2604/3000 train_loss: 20.532459259033203 test_loss:160.28973388671875\n",
      "2605/3000 train_loss: 20.537132263183594 test_loss:160.76486206054688\n",
      "2606/3000 train_loss: 25.620220184326172 test_loss:162.79417419433594\n",
      "2607/3000 train_loss: 23.88251495361328 test_loss:163.371337890625\n",
      "2608/3000 train_loss: 21.675670623779297 test_loss:165.91514587402344\n",
      "2609/3000 train_loss: 20.637123107910156 test_loss:163.0772705078125\n",
      "2610/3000 train_loss: 24.77438735961914 test_loss:167.60552978515625\n",
      "2611/3000 train_loss: 25.018259048461914 test_loss:162.91650390625\n",
      "2612/3000 train_loss: 20.675756454467773 test_loss:162.97137451171875\n",
      "2613/3000 train_loss: 23.939769744873047 test_loss:163.77694702148438\n",
      "2614/3000 train_loss: 24.30945587158203 test_loss:165.28369140625\n",
      "2615/3000 train_loss: 22.502737045288086 test_loss:170.9344024658203\n",
      "2616/3000 train_loss: 34.258609771728516 test_loss:170.79910278320312\n",
      "2617/3000 train_loss: 22.870304107666016 test_loss:170.860107421875\n",
      "2618/3000 train_loss: 23.986434936523438 test_loss:174.20477294921875\n",
      "2619/3000 train_loss: 23.239017486572266 test_loss:160.43136596679688\n",
      "2620/3000 train_loss: 20.650957107543945 test_loss:162.1861572265625\n",
      "2621/3000 train_loss: 21.75395393371582 test_loss:160.9109649658203\n",
      "2622/3000 train_loss: 22.715845108032227 test_loss:173.05918884277344\n",
      "2623/3000 train_loss: 21.27083969116211 test_loss:160.8969268798828\n",
      "2624/3000 train_loss: 22.197412490844727 test_loss:165.08914184570312\n",
      "2625/3000 train_loss: 21.620492935180664 test_loss:162.20632934570312\n",
      "2626/3000 train_loss: 22.449323654174805 test_loss:166.1645050048828\n",
      "2627/3000 train_loss: 21.363889694213867 test_loss:165.1158447265625\n",
      "2628/3000 train_loss: 25.516273498535156 test_loss:160.55447387695312\n",
      "2629/3000 train_loss: 21.57305908203125 test_loss:159.09910583496094\n",
      "2630/3000 train_loss: 20.274572372436523 test_loss:160.8600311279297\n",
      "2631/3000 train_loss: 22.1270751953125 test_loss:156.8678741455078\n",
      "2632/3000 train_loss: 19.489242553710938 test_loss:168.65145874023438\n",
      "2633/3000 train_loss: 21.429349899291992 test_loss:161.275634765625\n",
      "2634/3000 train_loss: 19.57103729248047 test_loss:158.3228302001953\n",
      "2635/3000 train_loss: 19.020416259765625 test_loss:160.80364990234375\n",
      "2636/3000 train_loss: 21.417476654052734 test_loss:159.46621704101562\n",
      "2637/3000 train_loss: 20.519392013549805 test_loss:158.6613311767578\n",
      "2638/3000 train_loss: 21.91839027404785 test_loss:159.03880310058594\n",
      "2639/3000 train_loss: 18.984783172607422 test_loss:159.70797729492188\n",
      "2640/3000 train_loss: 19.010587692260742 test_loss:164.61544799804688\n",
      "2641/3000 train_loss: 23.530561447143555 test_loss:158.3024444580078\n",
      "2642/3000 train_loss: 21.4515380859375 test_loss:159.24392700195312\n",
      "2643/3000 train_loss: 18.945114135742188 test_loss:156.4624481201172\n",
      "2644/3000 train_loss: 20.394380569458008 test_loss:160.10165405273438\n",
      "2645/3000 train_loss: 24.067893981933594 test_loss:159.02996826171875\n",
      "2646/3000 train_loss: 24.87714195251465 test_loss:164.58230590820312\n",
      "2647/3000 train_loss: 21.652193069458008 test_loss:158.5389404296875\n",
      "2648/3000 train_loss: 23.624835968017578 test_loss:161.3480224609375\n",
      "2649/3000 train_loss: 22.251541137695312 test_loss:159.69912719726562\n",
      "2650/3000 train_loss: 22.036985397338867 test_loss:159.9911651611328\n",
      "2651/3000 train_loss: 25.53524398803711 test_loss:161.2267608642578\n",
      "2652/3000 train_loss: 21.723670959472656 test_loss:158.09799194335938\n",
      "2653/3000 train_loss: 21.1986141204834 test_loss:162.1002960205078\n",
      "2654/3000 train_loss: 22.363521575927734 test_loss:157.47618103027344\n",
      "2655/3000 train_loss: 22.580514907836914 test_loss:157.97601318359375\n",
      "2656/3000 train_loss: 23.18749237060547 test_loss:154.71405029296875\n",
      "2657/3000 train_loss: 27.39693260192871 test_loss:165.63064575195312\n",
      "2658/3000 train_loss: 19.255008697509766 test_loss:163.44674682617188\n",
      "2659/3000 train_loss: 23.060447692871094 test_loss:162.88539123535156\n",
      "2660/3000 train_loss: 19.566457748413086 test_loss:158.38055419921875\n",
      "2661/3000 train_loss: 22.643966674804688 test_loss:161.08570861816406\n",
      "2662/3000 train_loss: 24.725292205810547 test_loss:170.95481872558594\n",
      "2663/3000 train_loss: 18.930831909179688 test_loss:157.84780883789062\n",
      "2664/3000 train_loss: 21.222549438476562 test_loss:163.0721435546875\n",
      "2665/3000 train_loss: 20.315460205078125 test_loss:158.99661254882812\n",
      "2666/3000 train_loss: 22.694719314575195 test_loss:157.5849151611328\n",
      "2667/3000 train_loss: 20.342191696166992 test_loss:156.66311645507812\n",
      "2668/3000 train_loss: 20.279285430908203 test_loss:156.56736755371094\n",
      "2669/3000 train_loss: 19.87265968322754 test_loss:159.99700927734375\n",
      "2670/3000 train_loss: 23.402040481567383 test_loss:163.57064819335938\n",
      "2671/3000 train_loss: 20.26376724243164 test_loss:159.88955688476562\n",
      "2672/3000 train_loss: 20.93279266357422 test_loss:159.73297119140625\n",
      "2673/3000 train_loss: 19.372880935668945 test_loss:158.64212036132812\n",
      "2674/3000 train_loss: 19.12009048461914 test_loss:161.2496337890625\n",
      "2675/3000 train_loss: 23.167667388916016 test_loss:158.10772705078125\n",
      "2676/3000 train_loss: 20.161888122558594 test_loss:161.89907836914062\n",
      "2677/3000 train_loss: 21.674890518188477 test_loss:158.02105712890625\n",
      "2678/3000 train_loss: 23.09604263305664 test_loss:157.58428955078125\n",
      "2679/3000 train_loss: 20.74997329711914 test_loss:160.35263061523438\n",
      "2680/3000 train_loss: 19.77766227722168 test_loss:158.57762145996094\n",
      "2681/3000 train_loss: 26.701101303100586 test_loss:162.46481323242188\n",
      "2682/3000 train_loss: 23.013702392578125 test_loss:159.04946899414062\n",
      "2683/3000 train_loss: 23.058368682861328 test_loss:165.75741577148438\n",
      "2684/3000 train_loss: 22.501968383789062 test_loss:172.71310424804688\n",
      "2685/3000 train_loss: 20.709413528442383 test_loss:158.99607849121094\n",
      "2686/3000 train_loss: 18.31927490234375 test_loss:158.7355194091797\n",
      "2687/3000 train_loss: 24.90479278564453 test_loss:181.6796875\n",
      "2688/3000 train_loss: 24.402986526489258 test_loss:157.9093780517578\n",
      "2689/3000 train_loss: 23.625680923461914 test_loss:172.89627075195312\n",
      "2690/3000 train_loss: 18.348388671875 test_loss:158.84994506835938\n",
      "2691/3000 train_loss: 24.956436157226562 test_loss:155.3480224609375\n",
      "2692/3000 train_loss: 25.535457611083984 test_loss:166.87356567382812\n",
      "2693/3000 train_loss: 25.255203247070312 test_loss:156.75650024414062\n",
      "2694/3000 train_loss: 23.218873977661133 test_loss:160.4291229248047\n",
      "2695/3000 train_loss: 22.055814743041992 test_loss:158.1068115234375\n",
      "2696/3000 train_loss: 22.46540641784668 test_loss:157.38758850097656\n",
      "2697/3000 train_loss: 22.145214080810547 test_loss:158.01693725585938\n",
      "2698/3000 train_loss: 21.275634765625 test_loss:162.41004943847656\n",
      "2699/3000 train_loss: 19.686304092407227 test_loss:157.04637145996094\n",
      "2700/3000 train_loss: 20.782489776611328 test_loss:155.44503784179688\n",
      "2701/3000 train_loss: 22.329631805419922 test_loss:158.35537719726562\n",
      "2702/3000 train_loss: 20.129642486572266 test_loss:164.997314453125\n",
      "2703/3000 train_loss: 19.81523323059082 test_loss:160.87261962890625\n",
      "2704/3000 train_loss: 26.26112937927246 test_loss:163.14218139648438\n",
      "2705/3000 train_loss: 18.020503997802734 test_loss:155.42752075195312\n",
      "2706/3000 train_loss: 21.54033088684082 test_loss:154.44314575195312\n",
      "2707/3000 train_loss: 21.063642501831055 test_loss:153.811767578125\n",
      "2708/3000 train_loss: 23.166229248046875 test_loss:161.71963500976562\n",
      "2709/3000 train_loss: 16.97716522216797 test_loss:158.41957092285156\n",
      "2710/3000 train_loss: 22.099660873413086 test_loss:159.75851440429688\n",
      "2711/3000 train_loss: 20.605709075927734 test_loss:158.81216430664062\n",
      "2712/3000 train_loss: 20.52060317993164 test_loss:157.62689208984375\n",
      "2713/3000 train_loss: 18.82275390625 test_loss:159.16238403320312\n",
      "2714/3000 train_loss: 20.53547477722168 test_loss:154.84561157226562\n",
      "2715/3000 train_loss: 20.092044830322266 test_loss:157.63357543945312\n",
      "2716/3000 train_loss: 23.110488891601562 test_loss:157.4063262939453\n",
      "2717/3000 train_loss: 17.762704849243164 test_loss:156.03089904785156\n",
      "2718/3000 train_loss: 20.629425048828125 test_loss:156.9698486328125\n",
      "2719/3000 train_loss: 27.379514694213867 test_loss:155.2483367919922\n",
      "2720/3000 train_loss: 22.684452056884766 test_loss:158.85113525390625\n",
      "2721/3000 train_loss: 19.09457778930664 test_loss:155.76133728027344\n",
      "2722/3000 train_loss: 19.789016723632812 test_loss:157.19554138183594\n",
      "2723/3000 train_loss: 19.176145553588867 test_loss:154.81394958496094\n",
      "2724/3000 train_loss: 22.146015167236328 test_loss:162.44488525390625\n",
      "2725/3000 train_loss: 21.41887855529785 test_loss:159.81744384765625\n",
      "2726/3000 train_loss: 22.54152488708496 test_loss:157.82168579101562\n",
      "2727/3000 train_loss: 20.119853973388672 test_loss:156.39154052734375\n",
      "2728/3000 train_loss: 20.088844299316406 test_loss:157.3031463623047\n",
      "2729/3000 train_loss: 27.103431701660156 test_loss:168.71221923828125\n",
      "2730/3000 train_loss: 23.78509521484375 test_loss:161.53143310546875\n",
      "2731/3000 train_loss: 27.99687385559082 test_loss:164.4648895263672\n",
      "2732/3000 train_loss: 19.89203643798828 test_loss:165.0677490234375\n",
      "2733/3000 train_loss: 22.68901824951172 test_loss:163.48193359375\n",
      "2734/3000 train_loss: 32.99412536621094 test_loss:159.28785705566406\n",
      "2735/3000 train_loss: 25.774959564208984 test_loss:160.84056091308594\n",
      "2736/3000 train_loss: 28.14859390258789 test_loss:163.29815673828125\n",
      "2737/3000 train_loss: 21.973649978637695 test_loss:160.21865844726562\n",
      "2738/3000 train_loss: 23.528459548950195 test_loss:157.90557861328125\n",
      "2739/3000 train_loss: 33.4227180480957 test_loss:158.86630249023438\n",
      "2740/3000 train_loss: 27.03559112548828 test_loss:162.33216857910156\n",
      "2741/3000 train_loss: 20.321609497070312 test_loss:159.9373321533203\n",
      "2742/3000 train_loss: 24.34849739074707 test_loss:158.76551818847656\n",
      "2743/3000 train_loss: 22.2821102142334 test_loss:158.9878692626953\n",
      "2744/3000 train_loss: 24.260942459106445 test_loss:165.241455078125\n",
      "2745/3000 train_loss: 26.808273315429688 test_loss:166.12741088867188\n",
      "2746/3000 train_loss: 21.396467208862305 test_loss:156.93350219726562\n",
      "2747/3000 train_loss: 21.154176712036133 test_loss:161.3561248779297\n",
      "2748/3000 train_loss: 19.183517456054688 test_loss:157.81243896484375\n",
      "2749/3000 train_loss: 21.172119140625 test_loss:160.48387145996094\n",
      "2750/3000 train_loss: 23.470788955688477 test_loss:158.7675323486328\n",
      "2751/3000 train_loss: 18.61711311340332 test_loss:167.60638427734375\n",
      "2752/3000 train_loss: 20.588239669799805 test_loss:160.1090087890625\n",
      "2753/3000 train_loss: 19.315637588500977 test_loss:156.7372589111328\n",
      "2754/3000 train_loss: 28.35260772705078 test_loss:161.41653442382812\n",
      "2755/3000 train_loss: 19.034509658813477 test_loss:159.8961181640625\n",
      "2756/3000 train_loss: 18.867326736450195 test_loss:161.70025634765625\n",
      "2757/3000 train_loss: 20.437238693237305 test_loss:158.4895782470703\n",
      "2758/3000 train_loss: 25.389747619628906 test_loss:161.5255126953125\n",
      "2759/3000 train_loss: 22.132909774780273 test_loss:162.28515625\n",
      "2760/3000 train_loss: 21.374103546142578 test_loss:162.06451416015625\n",
      "2761/3000 train_loss: 21.262039184570312 test_loss:161.3556365966797\n",
      "2762/3000 train_loss: 21.4719181060791 test_loss:160.12350463867188\n",
      "2763/3000 train_loss: 20.206571578979492 test_loss:156.8770751953125\n",
      "2764/3000 train_loss: 19.021394729614258 test_loss:161.02064514160156\n",
      "2765/3000 train_loss: 19.367856979370117 test_loss:157.91282653808594\n",
      "2766/3000 train_loss: 22.10814666748047 test_loss:157.6842498779297\n",
      "2767/3000 train_loss: 20.718996047973633 test_loss:161.77114868164062\n",
      "2768/3000 train_loss: 20.201255798339844 test_loss:155.41355895996094\n",
      "2769/3000 train_loss: 21.617107391357422 test_loss:161.54074096679688\n",
      "2770/3000 train_loss: 18.786006927490234 test_loss:158.93045043945312\n",
      "2771/3000 train_loss: 20.862791061401367 test_loss:156.1734161376953\n",
      "2772/3000 train_loss: 21.697771072387695 test_loss:156.6915740966797\n",
      "2773/3000 train_loss: 19.623329162597656 test_loss:157.03564453125\n",
      "2774/3000 train_loss: 21.54549217224121 test_loss:157.62893676757812\n",
      "2775/3000 train_loss: 21.736494064331055 test_loss:159.3802032470703\n",
      "2776/3000 train_loss: 19.294113159179688 test_loss:163.72494506835938\n",
      "2777/3000 train_loss: 38.301605224609375 test_loss:179.0787811279297\n",
      "2778/3000 train_loss: 21.70108985900879 test_loss:168.35336303710938\n",
      "2779/3000 train_loss: 19.431413650512695 test_loss:163.02545166015625\n",
      "2780/3000 train_loss: 22.05354881286621 test_loss:162.1290283203125\n",
      "2781/3000 train_loss: 25.992652893066406 test_loss:163.4684295654297\n",
      "2782/3000 train_loss: 23.927589416503906 test_loss:166.1425323486328\n",
      "2783/3000 train_loss: 22.214963912963867 test_loss:170.32913208007812\n",
      "2784/3000 train_loss: 28.09227752685547 test_loss:167.81231689453125\n",
      "2785/3000 train_loss: 20.867124557495117 test_loss:162.8551025390625\n",
      "2786/3000 train_loss: 20.801687240600586 test_loss:158.03060913085938\n",
      "2787/3000 train_loss: 22.310409545898438 test_loss:160.46365356445312\n",
      "2788/3000 train_loss: 20.300867080688477 test_loss:155.62046813964844\n",
      "2789/3000 train_loss: 18.133155822753906 test_loss:163.26882934570312\n",
      "2790/3000 train_loss: 18.845415115356445 test_loss:167.12779235839844\n",
      "2791/3000 train_loss: 18.34607696533203 test_loss:163.86325073242188\n",
      "2792/3000 train_loss: 22.20717430114746 test_loss:171.40606689453125\n",
      "2793/3000 train_loss: 18.38936424255371 test_loss:158.35604858398438\n",
      "2794/3000 train_loss: 29.120281219482422 test_loss:161.0541229248047\n",
      "2795/3000 train_loss: 22.681650161743164 test_loss:159.38816833496094\n",
      "2796/3000 train_loss: 20.790050506591797 test_loss:157.78146362304688\n",
      "2797/3000 train_loss: 21.46502685546875 test_loss:157.77749633789062\n",
      "2798/3000 train_loss: 20.571340560913086 test_loss:159.2139129638672\n",
      "2799/3000 train_loss: 19.754676818847656 test_loss:169.17312622070312\n",
      "2800/3000 train_loss: 22.768678665161133 test_loss:167.619384765625\n",
      "2801/3000 train_loss: 20.186656951904297 test_loss:157.74342346191406\n",
      "2802/3000 train_loss: 23.583847045898438 test_loss:156.63064575195312\n",
      "2803/3000 train_loss: 23.569580078125 test_loss:156.74354553222656\n",
      "2804/3000 train_loss: 21.11437225341797 test_loss:158.7096405029297\n",
      "2805/3000 train_loss: 18.646080017089844 test_loss:156.3738555908203\n",
      "2806/3000 train_loss: 19.57292938232422 test_loss:157.25076293945312\n",
      "2807/3000 train_loss: 20.709569931030273 test_loss:155.8977508544922\n",
      "2808/3000 train_loss: 19.79641342163086 test_loss:158.19483947753906\n",
      "2809/3000 train_loss: 20.736127853393555 test_loss:158.77371215820312\n",
      "2810/3000 train_loss: 20.026212692260742 test_loss:161.3931121826172\n",
      "2811/3000 train_loss: 21.95763397216797 test_loss:157.45872497558594\n",
      "2812/3000 train_loss: 22.960386276245117 test_loss:156.62294006347656\n",
      "2813/3000 train_loss: 20.529678344726562 test_loss:158.79275512695312\n",
      "2814/3000 train_loss: 21.461599349975586 test_loss:154.9337158203125\n",
      "2815/3000 train_loss: 25.526172637939453 test_loss:165.9606170654297\n",
      "2816/3000 train_loss: 19.35426902770996 test_loss:164.83795166015625\n",
      "2817/3000 train_loss: 19.251876831054688 test_loss:163.50814819335938\n",
      "2818/3000 train_loss: 21.6494083404541 test_loss:160.40545654296875\n",
      "2819/3000 train_loss: 22.70566749572754 test_loss:163.8071746826172\n",
      "2820/3000 train_loss: 21.56819725036621 test_loss:159.81626892089844\n",
      "2821/3000 train_loss: 22.907470703125 test_loss:171.8369140625\n",
      "2822/3000 train_loss: 17.417850494384766 test_loss:161.56747436523438\n",
      "2823/3000 train_loss: 22.443958282470703 test_loss:163.0449981689453\n",
      "2824/3000 train_loss: 24.820064544677734 test_loss:162.09906005859375\n",
      "2825/3000 train_loss: 22.74124526977539 test_loss:156.20718383789062\n",
      "2826/3000 train_loss: 18.196279525756836 test_loss:157.22230529785156\n",
      "2827/3000 train_loss: 19.70963478088379 test_loss:158.8870849609375\n",
      "2828/3000 train_loss: 24.152090072631836 test_loss:160.66070556640625\n",
      "2829/3000 train_loss: 18.053226470947266 test_loss:158.40966796875\n",
      "2830/3000 train_loss: 25.660259246826172 test_loss:155.85035705566406\n",
      "2831/3000 train_loss: 22.54899024963379 test_loss:155.31167602539062\n",
      "2832/3000 train_loss: 18.108745574951172 test_loss:163.79574584960938\n",
      "2833/3000 train_loss: 22.143768310546875 test_loss:169.23190307617188\n",
      "2834/3000 train_loss: 20.132057189941406 test_loss:158.08651733398438\n",
      "2835/3000 train_loss: 27.699663162231445 test_loss:166.8478546142578\n",
      "2836/3000 train_loss: 22.811386108398438 test_loss:155.554931640625\n",
      "2837/3000 train_loss: 21.871273040771484 test_loss:151.77256774902344\n",
      "2838/3000 train_loss: 20.79191780090332 test_loss:153.00653076171875\n",
      "2839/3000 train_loss: 22.34377098083496 test_loss:165.49502563476562\n",
      "2840/3000 train_loss: 17.199060440063477 test_loss:165.71676635742188\n",
      "2841/3000 train_loss: 18.714237213134766 test_loss:155.75271606445312\n",
      "2842/3000 train_loss: 23.98499870300293 test_loss:164.34451293945312\n",
      "2843/3000 train_loss: 18.45990562438965 test_loss:158.7178192138672\n",
      "2844/3000 train_loss: 19.580982208251953 test_loss:165.4583282470703\n",
      "2845/3000 train_loss: 19.75998306274414 test_loss:173.3031005859375\n",
      "2846/3000 train_loss: 19.873106002807617 test_loss:157.37335205078125\n",
      "2847/3000 train_loss: 17.674041748046875 test_loss:158.25747680664062\n",
      "2848/3000 train_loss: 17.18259620666504 test_loss:158.32852172851562\n",
      "2849/3000 train_loss: 19.92005157470703 test_loss:159.41238403320312\n",
      "2850/3000 train_loss: 23.573049545288086 test_loss:157.55838012695312\n",
      "2851/3000 train_loss: 20.446792602539062 test_loss:154.95077514648438\n",
      "2852/3000 train_loss: 18.605072021484375 test_loss:156.75262451171875\n",
      "2853/3000 train_loss: 21.412670135498047 test_loss:154.92385864257812\n",
      "2854/3000 train_loss: 19.497323989868164 test_loss:154.75216674804688\n",
      "2855/3000 train_loss: 21.848480224609375 test_loss:156.36651611328125\n",
      "2856/3000 train_loss: 20.413406372070312 test_loss:154.73739624023438\n",
      "2857/3000 train_loss: 20.689241409301758 test_loss:153.5825958251953\n",
      "2858/3000 train_loss: 18.91092872619629 test_loss:154.90402221679688\n",
      "2859/3000 train_loss: 23.259313583374023 test_loss:155.76173400878906\n",
      "2860/3000 train_loss: 19.522876739501953 test_loss:155.3356475830078\n",
      "2861/3000 train_loss: 17.76560401916504 test_loss:151.97811889648438\n",
      "2862/3000 train_loss: 18.444015502929688 test_loss:153.083740234375\n",
      "2863/3000 train_loss: 18.993040084838867 test_loss:156.61236572265625\n",
      "2864/3000 train_loss: 18.431259155273438 test_loss:151.83343505859375\n",
      "2865/3000 train_loss: 20.308055877685547 test_loss:154.05276489257812\n",
      "2866/3000 train_loss: 21.714427947998047 test_loss:153.8495635986328\n",
      "2867/3000 train_loss: 19.861305236816406 test_loss:156.09622192382812\n",
      "2868/3000 train_loss: 18.743595123291016 test_loss:153.9800567626953\n",
      "2869/3000 train_loss: 16.403215408325195 test_loss:155.0965576171875\n",
      "2870/3000 train_loss: 25.26041603088379 test_loss:153.98941040039062\n",
      "2871/3000 train_loss: 18.66013526916504 test_loss:152.61973571777344\n",
      "2872/3000 train_loss: 19.917293548583984 test_loss:154.6887969970703\n",
      "2873/3000 train_loss: 19.247770309448242 test_loss:152.0923309326172\n",
      "2874/3000 train_loss: 17.957733154296875 test_loss:151.44757080078125\n",
      "2875/3000 train_loss: 22.023380279541016 test_loss:151.80276489257812\n",
      "2876/3000 train_loss: 22.219696044921875 test_loss:152.90560913085938\n",
      "2877/3000 train_loss: 19.192867279052734 test_loss:150.43341064453125\n",
      "2878/3000 train_loss: 17.934316635131836 test_loss:152.86996459960938\n",
      "2879/3000 train_loss: 17.850317001342773 test_loss:151.4851837158203\n",
      "2880/3000 train_loss: 19.087312698364258 test_loss:150.11898803710938\n",
      "2881/3000 train_loss: 18.98484230041504 test_loss:151.2225341796875\n",
      "2882/3000 train_loss: 24.306135177612305 test_loss:152.49005126953125\n",
      "2883/3000 train_loss: 20.9072265625 test_loss:155.21946716308594\n",
      "2884/3000 train_loss: 18.29705047607422 test_loss:155.6063232421875\n",
      "2885/3000 train_loss: 19.142364501953125 test_loss:153.24462890625\n",
      "2886/3000 train_loss: 16.766937255859375 test_loss:156.32704162597656\n",
      "2887/3000 train_loss: 20.284488677978516 test_loss:155.60360717773438\n",
      "2888/3000 train_loss: 18.714599609375 test_loss:153.34512329101562\n",
      "2889/3000 train_loss: 17.651052474975586 test_loss:154.9988555908203\n",
      "2890/3000 train_loss: 20.235294342041016 test_loss:155.5023193359375\n",
      "2891/3000 train_loss: 20.16915512084961 test_loss:156.96263122558594\n",
      "2892/3000 train_loss: 23.630935668945312 test_loss:157.47802734375\n",
      "2893/3000 train_loss: 23.867385864257812 test_loss:156.44186401367188\n",
      "2894/3000 train_loss: 20.999374389648438 test_loss:161.24937438964844\n",
      "2895/3000 train_loss: 22.361520767211914 test_loss:156.00930786132812\n",
      "2896/3000 train_loss: 24.624582290649414 test_loss:153.64291381835938\n",
      "2897/3000 train_loss: 22.33138084411621 test_loss:156.76779174804688\n",
      "2898/3000 train_loss: 18.981670379638672 test_loss:164.00686645507812\n",
      "2899/3000 train_loss: 23.037525177001953 test_loss:156.95108032226562\n",
      "2900/3000 train_loss: 19.21748924255371 test_loss:155.74560546875\n",
      "2901/3000 train_loss: 20.18145179748535 test_loss:159.16851806640625\n",
      "2902/3000 train_loss: 18.31134033203125 test_loss:155.22604370117188\n",
      "2903/3000 train_loss: 18.127086639404297 test_loss:157.5779571533203\n",
      "2904/3000 train_loss: 21.17213249206543 test_loss:159.57711791992188\n",
      "2905/3000 train_loss: 25.406770706176758 test_loss:153.4317626953125\n",
      "2906/3000 train_loss: 20.89328384399414 test_loss:153.21165466308594\n",
      "2907/3000 train_loss: 26.5500431060791 test_loss:157.55435180664062\n",
      "2908/3000 train_loss: 21.48550033569336 test_loss:151.46151733398438\n",
      "2909/3000 train_loss: 19.608600616455078 test_loss:155.463623046875\n",
      "2910/3000 train_loss: 15.662145614624023 test_loss:155.21923828125\n",
      "2911/3000 train_loss: 16.522750854492188 test_loss:154.16769409179688\n",
      "2912/3000 train_loss: 17.0906925201416 test_loss:151.94482421875\n",
      "2913/3000 train_loss: 19.67483901977539 test_loss:154.88548278808594\n",
      "2914/3000 train_loss: 19.873586654663086 test_loss:154.7734375\n",
      "2915/3000 train_loss: 21.172080993652344 test_loss:165.65423583984375\n",
      "2916/3000 train_loss: 22.096303939819336 test_loss:154.6326904296875\n",
      "2917/3000 train_loss: 20.94814109802246 test_loss:156.36874389648438\n",
      "2918/3000 train_loss: 21.16123390197754 test_loss:154.3785858154297\n",
      "2919/3000 train_loss: 21.208858489990234 test_loss:155.23223876953125\n",
      "2920/3000 train_loss: 21.944808959960938 test_loss:155.02337646484375\n",
      "2921/3000 train_loss: 18.440122604370117 test_loss:158.7916259765625\n",
      "2922/3000 train_loss: 23.001283645629883 test_loss:152.40521240234375\n",
      "2923/3000 train_loss: 19.351871490478516 test_loss:155.6214599609375\n",
      "2924/3000 train_loss: 18.216175079345703 test_loss:152.78065490722656\n",
      "2925/3000 train_loss: 16.844207763671875 test_loss:157.1065216064453\n",
      "2926/3000 train_loss: 19.487802505493164 test_loss:155.99273681640625\n",
      "2927/3000 train_loss: 24.721332550048828 test_loss:169.23687744140625\n",
      "2928/3000 train_loss: 20.362577438354492 test_loss:154.75103759765625\n",
      "2929/3000 train_loss: 18.826669692993164 test_loss:151.7074432373047\n",
      "2930/3000 train_loss: 19.991077423095703 test_loss:152.49020385742188\n",
      "2931/3000 train_loss: 16.605091094970703 test_loss:154.87937927246094\n",
      "2932/3000 train_loss: 21.31663703918457 test_loss:155.37359619140625\n",
      "2933/3000 train_loss: 18.051456451416016 test_loss:151.8316650390625\n",
      "2934/3000 train_loss: 20.239904403686523 test_loss:154.07957458496094\n",
      "2935/3000 train_loss: 20.955598831176758 test_loss:156.444091796875\n",
      "2936/3000 train_loss: 21.357101440429688 test_loss:152.74156188964844\n",
      "2937/3000 train_loss: 22.33033561706543 test_loss:153.89535522460938\n",
      "2938/3000 train_loss: 18.54825782775879 test_loss:153.71505737304688\n",
      "2939/3000 train_loss: 25.53870964050293 test_loss:153.39334106445312\n",
      "2940/3000 train_loss: 22.75859260559082 test_loss:157.28555297851562\n",
      "2941/3000 train_loss: 22.15434455871582 test_loss:159.868896484375\n",
      "2942/3000 train_loss: 21.294851303100586 test_loss:152.64663696289062\n",
      "2943/3000 train_loss: 23.052146911621094 test_loss:161.9996795654297\n",
      "2944/3000 train_loss: 21.845687866210938 test_loss:156.4786834716797\n",
      "2945/3000 train_loss: 17.688716888427734 test_loss:152.94741821289062\n",
      "2946/3000 train_loss: 20.827926635742188 test_loss:156.43264770507812\n",
      "2947/3000 train_loss: 21.604894638061523 test_loss:153.09255981445312\n",
      "2948/3000 train_loss: 18.067218780517578 test_loss:155.72647094726562\n",
      "2949/3000 train_loss: 26.793289184570312 test_loss:164.12631225585938\n",
      "2950/3000 train_loss: 20.53500747680664 test_loss:152.95083618164062\n",
      "2951/3000 train_loss: 21.389184951782227 test_loss:162.240966796875\n",
      "2952/3000 train_loss: 19.02933692932129 test_loss:155.250244140625\n",
      "2953/3000 train_loss: 25.216222763061523 test_loss:156.3851776123047\n",
      "2954/3000 train_loss: 23.304933547973633 test_loss:153.10731506347656\n",
      "2955/3000 train_loss: 28.497333526611328 test_loss:150.1459197998047\n",
      "2956/3000 train_loss: 20.892261505126953 test_loss:157.44541931152344\n",
      "2957/3000 train_loss: 22.6895809173584 test_loss:153.28712463378906\n",
      "2958/3000 train_loss: 18.610841751098633 test_loss:148.0849151611328\n",
      "2959/3000 train_loss: 17.930068969726562 test_loss:146.82281494140625\n",
      "2960/3000 train_loss: 19.045433044433594 test_loss:147.87660217285156\n",
      "2961/3000 train_loss: 20.201221466064453 test_loss:150.020263671875\n",
      "2962/3000 train_loss: 21.924226760864258 test_loss:149.30142211914062\n",
      "2963/3000 train_loss: 20.459693908691406 test_loss:150.80380249023438\n",
      "2964/3000 train_loss: 20.742551803588867 test_loss:153.04547119140625\n",
      "2965/3000 train_loss: 19.445354461669922 test_loss:155.29888916015625\n",
      "2966/3000 train_loss: 17.982851028442383 test_loss:154.53074645996094\n",
      "2967/3000 train_loss: 20.891855239868164 test_loss:161.6116180419922\n",
      "2968/3000 train_loss: 20.525693893432617 test_loss:158.0463409423828\n",
      "2969/3000 train_loss: 19.831506729125977 test_loss:155.6254119873047\n",
      "2970/3000 train_loss: 18.82068634033203 test_loss:166.2845458984375\n",
      "2971/3000 train_loss: 19.604116439819336 test_loss:153.32347106933594\n",
      "2972/3000 train_loss: 21.400196075439453 test_loss:153.11398315429688\n",
      "2973/3000 train_loss: 17.407272338867188 test_loss:174.5772705078125\n",
      "2974/3000 train_loss: 20.060775756835938 test_loss:150.68267822265625\n",
      "2975/3000 train_loss: 17.91071128845215 test_loss:151.90597534179688\n",
      "2976/3000 train_loss: 17.609750747680664 test_loss:151.3694610595703\n",
      "2977/3000 train_loss: 19.203227996826172 test_loss:149.47723388671875\n",
      "2978/3000 train_loss: 21.503173828125 test_loss:153.564208984375\n",
      "2979/3000 train_loss: 22.449979782104492 test_loss:151.850830078125\n",
      "2980/3000 train_loss: 16.942039489746094 test_loss:150.72999572753906\n",
      "2981/3000 train_loss: 17.78373908996582 test_loss:153.75674438476562\n",
      "2982/3000 train_loss: 17.600147247314453 test_loss:154.81040954589844\n",
      "2983/3000 train_loss: 23.33197593688965 test_loss:150.00338745117188\n",
      "2984/3000 train_loss: 20.109933853149414 test_loss:152.11314392089844\n",
      "2985/3000 train_loss: 18.49869155883789 test_loss:150.72865295410156\n",
      "2986/3000 train_loss: 19.514753341674805 test_loss:153.45758056640625\n",
      "2987/3000 train_loss: 18.065641403198242 test_loss:151.7040252685547\n",
      "2988/3000 train_loss: 18.876909255981445 test_loss:150.6593017578125\n",
      "2989/3000 train_loss: 29.4124755859375 test_loss:151.1278839111328\n",
      "2990/3000 train_loss: 17.939210891723633 test_loss:151.61102294921875\n",
      "2991/3000 train_loss: 16.700119018554688 test_loss:153.04742431640625\n",
      "2992/3000 train_loss: 18.526079177856445 test_loss:147.9835205078125\n",
      "2993/3000 train_loss: 16.963871002197266 test_loss:150.8922882080078\n",
      "2994/3000 train_loss: 21.51264762878418 test_loss:154.53433227539062\n",
      "2995/3000 train_loss: 17.708829879760742 test_loss:152.08126831054688\n",
      "2996/3000 train_loss: 19.96634292602539 test_loss:152.07533264160156\n",
      "2997/3000 train_loss: 20.627904891967773 test_loss:151.32199096679688\n",
      "2998/3000 train_loss: 18.557567596435547 test_loss:152.11175537109375\n",
      "2999/3000 train_loss: 18.395185470581055 test_loss:152.16262817382812\n",
      "3000/3000 train_loss: 18.24256134033203 test_loss:152.19891357421875\n"
     ]
    }
   ],
   "source": [
    "losses = train(model = unet, optimizer = optimizer, criterion=criterion, data_tr=train_data,\n",
    "               data_val = test_data, scheduler = scheduler,device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "id": "6Ew7_F0-q7aL",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "747f7b0e-038c-4164-ab38-ffc798185d68"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(152.1989)"
      ]
     },
     "metadata": {},
     "execution_count": 123
    }
   ],
   "source": [
    "avg_loss = 0\n",
    "# unet.eval()\n",
    "# unet.train()\n",
    "preds = []\n",
    "i = 0\n",
    "test_anomaly_losses = []\n",
    "test_normal_losses = []\n",
    "test_losses = []\n",
    "test_real = y_test.tolist()\n",
    "for batch in test_data:\n",
    "  with torch.no_grad():\n",
    "    # unet.train()\n",
    "    predictions = unet(batch.to(device)).cpu()\n",
    "    preds.append(predictions)\n",
    "  loss = criterion(predictions, batch.cpu())\n",
    "  for j in range(len(predictions)):\n",
    "    if int(y_test[i]) == 1:\n",
    "      test_anomaly_losses.append(float(criterion(predictions[j], batch[j])))\n",
    "    else:\n",
    "      test_normal_losses.append(float(criterion(predictions[j], batch[j])))\n",
    "    i += 1\n",
    "    test_losses.append(criterion(predictions[j], batch[j]))\n",
    "  # print(loss)\n",
    "  # print(loss)\n",
    "  avg_loss += loss / len(test_data)\n",
    "# avg_loss\n",
    "\n",
    "avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "id": "VpDKorrRso9o",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "040d7fe4-5178-42f4-c843-589cce8915d9"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(132.06896661033093, 22.785639918044453)"
      ]
     },
     "metadata": {},
     "execution_count": 124
    }
   ],
   "source": [
    "sum(test_anomaly_losses)/len(test_anomaly_losses) , sum(test_normal_losses)/len(test_normal_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5LEvbZKYuh7J",
    "outputId": "293479a9-0fa1-45d2-c091-961395c93479"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.9679264573674841\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "vals = np.arange(40, 900, 0.5).tolist()\n",
    "for threshold in vals:\n",
    "  preds = []\n",
    "  for j in range(len(test_losses)):\n",
    "    if test_losses[j] > threshold:\n",
    "      preds.append(1)\n",
    "    else:\n",
    "      preds.append(0)\n",
    "  \n",
    "  results.append(roc_auc_score(test_real,preds))\n",
    "\n",
    "print(max(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QaSSqG8SbAw2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
