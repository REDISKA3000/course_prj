{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9SStKf4G0V5H"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim import Adam\n",
        "import torchaudio\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from skimage.util import img_as_ubyte\n",
        "import pandas as pd\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import librosa\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import io\n",
        "from sklearn.metrics import roc_auc_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XtxbKLZq5KX",
        "outputId": "37023990-2b40-4e1f-e63e-1dba18c3acdb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYxHegIM0Z4i",
        "outputId": "f68f6fab-6d0f-4c1f-9d0a-527e166123f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "h9DATQwS0ivD"
      },
      "outputs": [],
      "source": [
        "class MimiiDataset(Dataset):\n",
        "    def __init__(self,audio_dir, n_fft = 1024, win_length = 1024,\n",
        "                 hop_length = 512,power = 2,n_mels = 128,pad_mode = 'reflect',\n",
        "                 sr = 16000,center = True,norm = None):\n",
        "      \n",
        "        super(MimiiDataset, self).__init__()\n",
        "        self.audio_dir = audio_dir\n",
        "        self.n_mels = n_mels\n",
        "        self.n_fft = n_fft\n",
        "        self.win_length = win_length\n",
        "        self.hop_length = hop_length\n",
        "        self.power = power\n",
        "        self.pad_mode = pad_mode\n",
        "        self.sr = sr\n",
        "        self.center = center\n",
        "        self.norm = norm\n",
        "\n",
        "    def get_files(self):\n",
        "       return self.train_files, self.test_files\n",
        "    \n",
        "    def get_data(self,device, id):\n",
        "        \n",
        "        self.train_files, self.train_labels = self._train_file_list(device, id)\n",
        "        self.test_files, self.test_labels = self._test_file_list(device, id)\n",
        "        \n",
        "        self.train_data = self.get_audios(self.train_files)\n",
        "        self.test_data = self.get_audios(self.test_files)\n",
        "        \n",
        "        return self.train_data, self.test_data, self.train_labels, self.test_labels\n",
        "    \n",
        "    def _train_file_list(self, device, id):\n",
        "        query = os.path.abspath(\n",
        "            f\"{self.audio_dir}/{device}/train/normal_id_0{id}*.wav\"\n",
        "        )\n",
        "        train_normal_files = sorted(glob.glob(query))\n",
        "        train_normal_labels = np.zeros(len(train_normal_files))\n",
        "        \n",
        "        query = os.path.abspath(\n",
        "                f\"{self.audio_dir}/{device}/train/anomaly_id_0{id}*.wav\"\n",
        "            )\n",
        "        train_anomaly_files = sorted(glob.glob(query))\n",
        "        train_anomaly_labels = np.ones(len(train_anomaly_files))\n",
        "        \n",
        "        train_file_list = np.concatenate((train_normal_files, train_anomaly_files), axis=0)\n",
        "        train_labels = np.concatenate((train_normal_labels, train_anomaly_labels), axis=0)\n",
        "        \n",
        "        return train_file_list, train_labels\n",
        "    \n",
        "    def _test_file_list(self, device, id):     \n",
        "        query = os.path.abspath(\n",
        "            f\"{self.audio_dir}/{device}/test/normal_id_0{id}*.wav\"\n",
        "            )\n",
        "        test_normal_files = sorted(glob.glob(query))\n",
        "        test_normal_labels = np.zeros(len(test_normal_files))\n",
        "        \n",
        "        query = os.path.abspath(\n",
        "            f\"{self.audio_dir}/{device}/test/anomaly_id_0{id}*.wav\"\n",
        "            )\n",
        "        test_anomaly_files = sorted(glob.glob(query))\n",
        "        test_anomaly_labels = np.ones(len(test_anomaly_files))\n",
        "        \n",
        "        test_file_list = np.concatenate((test_normal_files, \n",
        "                                          test_anomaly_files), axis=0)\n",
        "        test_labels = np.concatenate((test_normal_labels,\n",
        "                                      test_anomaly_labels), axis=0)\n",
        "          \n",
        "        return test_file_list, test_labels\n",
        "\n",
        "    def normalize(self,tensor):\n",
        "        tensor_minusmean = tensor - tensor.mean()\n",
        "        return tensor_minusmean/np.absolute(tensor_minusmean).max()\n",
        "\n",
        "    def make0min(self,tensornd):\n",
        "        tensor = tensornd.numpy()\n",
        "        res = np.where(tensor == 0, 1E-19 , tensor)\n",
        "        return torch.from_numpy(res)\n",
        "\n",
        "    def spectrogrameToImage(self,specgram):\n",
        "        # specgram = torchaudio.transforms.MelSpectrogram(n_fft=1024, win_length=1024, \n",
        "        #                                                 hop_length=512, power=2, \n",
        "        #                                                 normalized=True, n_mels=128)(waveform )\n",
        "        specgram= self.make0min(specgram)\n",
        "        specgram = specgram.log2()[0,:,:].numpy()\n",
        "        \n",
        "        tr2image = transforms.Compose([transforms.ToPILImage()])\n",
        "\n",
        "        specgram= self.normalize(specgram)\n",
        "        # specgram = img_as_ubyte(specgram)\n",
        "        specgramImage = tr2image(specgram)\n",
        "        return specgramImage\n",
        "\n",
        "    def get_logmelspectrogram(self, waveform):\n",
        "        melspec = librosa.feature.melspectrogram(\n",
        "          n_fft=self.n_fft, win_length=self.win_length, hop_length=self.hop_length,\n",
        "          power=self.power,n_mels=self.n_mels,pad_mode=self.pad_mode,sr=self.sr,\n",
        "          center=self.center,norm=self.norm,htk=True,\n",
        "          y = waveform.numpy()\n",
        "        )\n",
        "\n",
        "        logmelspec = librosa.power_to_db(melspec)\n",
        "\n",
        "        return logmelspec\n",
        "\n",
        "    def get_melspectrogram(self,waveform):\n",
        "        melspec = librosa.feature.melspectrogram(\n",
        "            n_fft=self.n_fft, win_length=self.win_length, hop_length=self.hop_length,\n",
        "            power=self.power,n_mels=self.n_mels,pad_mode=self.pad_mode,sr=self.sr,\n",
        "            center=self.center,norm=self.norm,htk=True,\n",
        "            y = waveform.numpy()\n",
        "        )\n",
        "\n",
        "        return melspec\n",
        "    \n",
        "    def get_mfcc(self,waveform):\n",
        "        mfcc = librosa.feature.mfcc(    \n",
        "            n_fft=self.n_fft, win_length=self.win_length, \n",
        "            hop_length=self.hop_length,pad_mode=self.pad_mode,sr=self.sr,\n",
        "            center=self.center,norm=self.norm,n_mfcc=40,\n",
        "            y = waveform.numpy()\n",
        "        )\n",
        "\n",
        "        return mfcc\n",
        "\n",
        "    def get_chroma_stft(self,waveform):\n",
        "        stft = librosa.feature.chroma_stft(\n",
        "            n_fft=self.n_fft, win_length=self.win_length, \n",
        "            hop_length=self.hop_length,pad_mode=self.pad_mode,sr=self.sr,\n",
        "            center=self.center,norm=self.norm,n_chroma=12,\n",
        "            y=waveform.numpy()\n",
        "        )\n",
        "\n",
        "        return stft\n",
        "\n",
        "    def get_spectral_contrast(self,waveform):\n",
        "        spec_contrast = librosa.feature.spectral_contrast(    \n",
        "            n_fft=self.n_fft, win_length=self.win_length,center=self.center,\n",
        "            hop_length=self.hop_length,pad_mode=self.pad_mode,sr=self.sr,\n",
        "            y = waveform.numpy()\n",
        "        )\n",
        "\n",
        "        return spec_contrast\n",
        "    \n",
        "    def get_tonnetz(self,waveform):\n",
        "        harmonic = librosa.effects.harmonic(waveform.numpy())\n",
        "        tonnetz = librosa.feature.tonnetz(y=harmonic,sr=self.sr)\n",
        "\n",
        "        return tonnetz\n",
        "\n",
        "    def get_audios(self, file_list):\n",
        "        data = []\n",
        "        for i in range(len(file_list)):\n",
        "          y, sr = torchaudio.load(file_list[i])  \n",
        "          data.append(y)\n",
        "\n",
        "        return data\n",
        "    def _derive_data(self, file_list):\n",
        "        train_data = []\n",
        "        test_data = []\n",
        "        train_mode = True\n",
        "        for file_list in [self.train_files, self.test_files]:\n",
        "          tr2tensor = transforms.Compose([transforms.PILToTensor()])\n",
        "          data = []\n",
        "          for j in range(len(file_list)):\n",
        "            y, sr = torchaudio.load(file_list[j])  \n",
        "            spec = self.get_melspectrogram(y)\n",
        "            spec = self.spectrogrameToImage(spec)\n",
        "            spec = spec.convert('RGB')\n",
        "            vectors = tr2tensor(spec)\n",
        "            if train_mode:     \n",
        "              train_data.append(vectors)\n",
        "            else:\n",
        "              test_data.append(vectors)\n",
        "            \n",
        "          train_mode = False\n",
        "                \n",
        "        return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "S96soeIc0o13"
      },
      "outputs": [],
      "source": [
        "dataset = MimiiDataset('/content/drive/MyDrive/mimii')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "Gn2zdn92doi1"
      },
      "outputs": [],
      "source": [
        "_, _, y_train, y_test = dataset.get_data('valve', 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "2bi9uuBmiVBu"
      },
      "outputs": [],
      "source": [
        "def mean_mfccs(wave_list):\n",
        "  data = []\n",
        "  for wave in wave_list:\n",
        "    mfcc = np.mean(dataset.get_mfcc(wave)[0], axis = 1)\n",
        "    data.append(mfcc)\n",
        "  \n",
        "  return data\n",
        "\n",
        "def mean_stfts(wave_list):\n",
        "  data = []\n",
        "  for wave in wave_list:\n",
        "    stft = np.mean(dataset.get_chroma_stft(wave)[0], axis = 1)\n",
        "    data.append(stft)\n",
        "  \n",
        "  return data\n",
        "\n",
        "def mean_melspecs(wave_list):\n",
        "  data = []\n",
        "  for wave in wave_list:\n",
        "    melspec = np.mean(dataset.get_melspectrogram(wave)[0], axis = 1)\n",
        "    data.append(melspec)\n",
        "  \n",
        "  return data\n",
        "\n",
        "def mean_spec_contrasts(wave_list):\n",
        "  data = []\n",
        "  for wave in wave_list:\n",
        "    spec_contrast = np.mean(dataset.get_spectral_contrast(wave)[0], axis = 1)\n",
        "    data.append(spec_contrast)\n",
        "  \n",
        "  return data\n",
        "  \n",
        "def mean_tonnetzs(wave_list):\n",
        "  data = []\n",
        "  for wave in wave_list:\n",
        "    tonnetz = np.mean(dataset.get_tonnetz(wave)[0], axis = 1)\n",
        "    data.append(tonnetz)\n",
        "  \n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_i9ezYUFiU_b"
      },
      "outputs": [],
      "source": [
        "train_melspecs, test_melspecs = mean_melspecs(df_train), mean_melspecs(df_test)\n",
        "train_mfccs, test_mfccs = mean_mfccs(df_train), mean_mfccs(df_test)\n",
        "train_stfts, test_stfts = mean_stfts(df_train), mean_stfts(df_test)\n",
        "train_spec_contrasts, test_spec_contrasts = mean_spec_contrasts(df_train), mean_spec_contrasts(df_test)\n",
        "train_tonnetzs, test_tonnetzs = mean_tonnetzs(df_train), mean_tonnetzs(df_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfPtT0YdpttI"
      },
      "outputs": [],
      "source": [
        "train_mixed_f = []\n",
        "test_mixed_f = []\n",
        "\n",
        "for i in range(len(train_melspecs)):\n",
        "\n",
        "  train_mf = np.concatenate((train_melspecs[i],train_mfccs[i],train_stfts[i],\n",
        "                             train_spec_contrasts[i],train_tonnetzs[i])).tolist()\n",
        "\n",
        "  train_mixed_f.append(train_mf)\n",
        "\n",
        "for i in range(len(test_melspecs)):\n",
        "\n",
        "  test_mf = np.concatenate((test_melspecs[i],test_mfccs[i],test_stfts[i],\n",
        "                             test_spec_contrasts[i],test_tonnetzs[i])).tolist()\n",
        "\n",
        "  test_mixed_f.append(test_mf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8OT45sHpvBE"
      },
      "outputs": [],
      "source": [
        "train_mixed_f = torch.FloatTensor(train_mixed_f)\n",
        "test_mixed_f = torch.FloatTensor(test_mixed_f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VKtjykaqoIC",
        "outputId": "702b23d3-c09b-46ef-df74-9feca643c45d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1006, 193])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "train_mixed_f.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dafhP0mBpx21"
      },
      "outputs": [],
      "source": [
        "torch.save(train_mixed_f, '/content/drive/MyDrive/mixed_features/train_mf_toycar2.pt')\n",
        "torch.save( test_mixed_f, '/content/drive/MyDrive/mixed_features/test_mf_toycar2.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "SgjpeWy_RV1C"
      },
      "outputs": [],
      "source": [
        "train_mixed_f = torch.load('/content/drive/MyDrive/mixed_features/train_mf_valve0.pt')\n",
        "test_mixed_f = torch.load('/content/drive/MyDrive/mixed_features/test_mf_valve0.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEl3qOh-mZVK",
        "outputId": "bcb0afcd-e18f-49f0-90da-a26f50b5203d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([891, 193])"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ],
      "source": [
        "train_mixed_f.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "jWMPVGu1qiEq"
      },
      "outputs": [],
      "source": [
        "train_data = DataLoader(train_mixed_f, batch_size=32, shuffle = True)\n",
        "test_data = DataLoader(test_mixed_f, batch_size = 32, shuffle= False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "vNTBTRe6qnBq"
      },
      "outputs": [],
      "source": [
        "class UNet_FC(nn.Module):\n",
        "\n",
        "  def __init__(self, in_features):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.bn = nn.BatchNorm1d(128)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "    self.fc0 = nn.Linear(in_features=in_features,out_features=in_features)\n",
        "\n",
        "    # encoder\n",
        "    self.fc1 = nn.Linear(in_features=in_features, out_features=128)\n",
        "    self.fc2 = nn.Linear(in_features=128,out_features=128)\n",
        "    self.fc3 = nn.Linear(in_features=128, out_features=128)\n",
        "    self.fc4 = nn.Linear(in_features=128, out_features=128)\n",
        "    self.fc5 = nn.Linear(in_features=128, out_features=8)\n",
        "\n",
        "    # decoder\n",
        "    self.fc6 = nn.Linear(in_features=8, out_features=128)\n",
        "    self.fc7 = nn.Linear(in_features=128*2, out_features=128)\n",
        "    self.fc8 = nn.Linear(in_features=128*2, out_features=128)\n",
        "    self.fc9 = nn.Linear(in_features=128*2, out_features=128)\n",
        "\n",
        "    self.out = nn.Linear(in_features=128*2, out_features=in_features)\n",
        "\n",
        "  def encoder(self, x):\n",
        "    input = self.fc0(x)\n",
        "\n",
        "    x1 = self.relu(self.bn(self.fc1(input)))\n",
        "    x2 = self.relu(self.bn(self.fc2(x1)))\n",
        "    x3 = self.relu(self.bn(self.fc3(x2)))\n",
        "    x4 = self.relu(self.bn(self.fc4(x3)))\n",
        "    x5 = self.relu(self.fc5(x4))\n",
        "\n",
        "    return [x5, x4, x3, x2, x1]\n",
        "\n",
        "  def decoder(self, x):\n",
        "    x6 = self.relu(self.fc6(x[0]))\n",
        "    con1 = torch.cat((x6,x[1]), 1) \n",
        "    x7 = self.relu(self.bn(self.fc7(con1)))\n",
        "    con2 = torch.cat((x7,x[2]), 1)\n",
        "    x8 = self.relu(self.bn(self.fc8(con2)))\n",
        "    con3 = torch.cat((x8,x[3]), 1)\n",
        "    x9 = self.relu(self.bn(self.fc9(con3)))\n",
        "    con4 = torch.cat((x9,x[4]), 1)\n",
        "\n",
        "    x10 = self.out(con4)\n",
        "\n",
        "    return x10\n",
        "  \n",
        "  def forward(self, x):\n",
        "    # encoded = self.encoder(x)\n",
        "\n",
        "    # decoded = self.decoder(encoded)\n",
        "    input = self.fc0(x)\n",
        "\n",
        "    x1 = self.relu(self.bn(self.fc1(input)))\n",
        "    x2 = self.relu(self.bn(self.fc2(x1)))\n",
        "    x3 = self.relu(self.bn(self.fc3(x2)))\n",
        "    x4 = self.relu(self.bn(self.fc4(x3)))\n",
        "    x5 = self.relu(self.fc5(x4))\n",
        "\n",
        "    xy = [x5, x4, x3, x2, x1]\n",
        "\n",
        "    x6 = self.relu(self.fc6(xy[0]))\n",
        "    con1 = torch.cat((x6,xy[1]), 1) \n",
        "    x7 = self.relu(self.bn(self.fc7(con1)))\n",
        "    con2 = torch.cat((x7,xy[2]), 1)\n",
        "    x8 = self.relu(self.bn(self.fc8(con2)))\n",
        "    con3 = torch.cat((x8,xy[3]), 1)\n",
        "    x9 = self.relu(self.bn(self.fc9(con3)))\n",
        "    con4 = torch.cat((x9,xy[4]), 1)\n",
        "\n",
        "    x10 = self.out(con4)\n",
        "\n",
        "    # return decoded\n",
        "    return x10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "ZfgcBtQ3qn5l"
      },
      "outputs": [],
      "source": [
        "def train(model, optimizer, criterion, data_tr, data_val, scheduler = None,\n",
        "          epochs = 3000, device = 'cpu'):\n",
        "    # X_val, Y_val = next(iter(data_val))\n",
        "    losses = []\n",
        "    prev_avg_loss = 100000\n",
        "    for epoch in range(epochs):\n",
        "        train_avg_loss = 0\n",
        "        test_avg_loss = 0\n",
        "        # model.train()  # train mode\n",
        "        for batch in data_tr:\n",
        "          # data to device\n",
        "          batch = batch.to(device)\n",
        "          # set parameter gradients to zero\n",
        "          optimizer.zero_grad()\n",
        "          # forward\n",
        "          # print(Y_batch.shape)\n",
        "          predictions = model(batch)\n",
        "          loss = criterion(predictions, batch)\n",
        "          loss.backward() # backward-pass\n",
        "          optimizer.step()  # update weights\n",
        "          # calculate loss to show the user\n",
        "          if scheduler:\n",
        "            scheduler.step(loss)\n",
        "          train_avg_loss += loss / len(data_tr)\n",
        "\n",
        "        # model.eval()\n",
        "        for batch in data_val:\n",
        "          with torch.no_grad():\n",
        "            preds = model(batch.to(device)).cpu()\n",
        "            loss = criterion(preds,batch)\n",
        "            test_avg_loss += loss / len(data_val)\n",
        "                    \n",
        "        losses.append(train_avg_loss.item())\n",
        "        # if (epoch+1)%50 == 0:\n",
        "        print(\"{}/{} train_loss: {} test_loss:{}\".format(epoch+1, epochs, train_avg_loss, test_avg_loss))\n",
        "        # if test_avg_loss < 70:\n",
        "        #   break\n",
        "    return losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "ptkVTF55quOL"
      },
      "outputs": [],
      "source": [
        "unet = UNet_FC(in_features=193).to(device)\n",
        "optimizer = Adam(params = unet.parameters(), lr = 10e-3)\n",
        "# optimizer = Adam(params = unet.parameters())\n",
        "criterion = nn.MSELoss()\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, \n",
        "                                                       min_lr=10e-4, mode = 'min',\n",
        "                                                       patience = 30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkfmYl9oXhcB",
        "outputId": "a51a3997-d87a-4798-bd45-02f3208caadf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/3000 train_loss: 333662.21875 test_loss:335653.40625\n",
            "2/3000 train_loss: 324946.25 test_loss:322934.1875\n",
            "3/3000 train_loss: 308931.90625 test_loss:302348.71875\n",
            "4/3000 train_loss: 284780.875 test_loss:273718.0\n",
            "5/3000 train_loss: 253851.109375 test_loss:239605.3125\n",
            "6/3000 train_loss: 218879.03125 test_loss:203551.4375\n",
            "7/3000 train_loss: 182602.21875 test_loss:165705.4375\n",
            "8/3000 train_loss: 145166.828125 test_loss:128508.515625\n",
            "9/3000 train_loss: 109724.65625 test_loss:95479.5\n",
            "10/3000 train_loss: 80335.1015625 test_loss:67659.34375\n",
            "11/3000 train_loss: 54256.13671875 test_loss:44314.16796875\n",
            "12/3000 train_loss: 35104.07421875 test_loss:28078.09765625\n",
            "13/3000 train_loss: 20995.51171875 test_loss:16906.8671875\n",
            "14/3000 train_loss: 12155.15625 test_loss:9561.8798828125\n",
            "15/3000 train_loss: 6735.31494140625 test_loss:5529.9208984375\n",
            "16/3000 train_loss: 3922.173828125 test_loss:3793.693115234375\n",
            "17/3000 train_loss: 2350.059326171875 test_loss:2234.986572265625\n",
            "18/3000 train_loss: 1811.1334228515625 test_loss:1910.96337890625\n",
            "19/3000 train_loss: 1536.4542236328125 test_loss:1484.8292236328125\n",
            "20/3000 train_loss: 1391.544189453125 test_loss:1305.1103515625\n",
            "21/3000 train_loss: 1127.8699951171875 test_loss:1157.3626708984375\n",
            "22/3000 train_loss: 1061.310791015625 test_loss:1106.98828125\n",
            "23/3000 train_loss: 1045.7908935546875 test_loss:1073.92529296875\n",
            "24/3000 train_loss: 1325.5347900390625 test_loss:1085.846923828125\n",
            "25/3000 train_loss: 1096.818115234375 test_loss:1091.9453125\n",
            "26/3000 train_loss: 1002.1498413085938 test_loss:1095.6484375\n",
            "27/3000 train_loss: 1015.3182373046875 test_loss:1083.0892333984375\n",
            "28/3000 train_loss: 1018.7788696289062 test_loss:1081.8468017578125\n",
            "29/3000 train_loss: 1004.2835693359375 test_loss:1065.9716796875\n",
            "30/3000 train_loss: 1005.2212524414062 test_loss:1051.0577392578125\n",
            "31/3000 train_loss: 965.7495727539062 test_loss:1063.78564453125\n",
            "32/3000 train_loss: 935.0858764648438 test_loss:1051.587646484375\n",
            "33/3000 train_loss: 959.4509887695312 test_loss:1034.2410888671875\n",
            "34/3000 train_loss: 1003.9391479492188 test_loss:1020.9876708984375\n",
            "35/3000 train_loss: 978.3736572265625 test_loss:1022.40380859375\n",
            "36/3000 train_loss: 951.470703125 test_loss:1070.23291015625\n",
            "37/3000 train_loss: 973.2700805664062 test_loss:1005.486328125\n",
            "38/3000 train_loss: 954.4730224609375 test_loss:1038.76806640625\n",
            "39/3000 train_loss: 920.0128173828125 test_loss:1029.03857421875\n",
            "40/3000 train_loss: 964.1373901367188 test_loss:1019.1587524414062\n",
            "41/3000 train_loss: 931.843994140625 test_loss:1002.503662109375\n",
            "42/3000 train_loss: 925.4324340820312 test_loss:1035.419677734375\n",
            "43/3000 train_loss: 930.24072265625 test_loss:988.7094116210938\n",
            "44/3000 train_loss: 878.4457397460938 test_loss:991.3092041015625\n",
            "45/3000 train_loss: 938.9408569335938 test_loss:1002.3804931640625\n",
            "46/3000 train_loss: 901.3193969726562 test_loss:958.70947265625\n",
            "47/3000 train_loss: 898.300537109375 test_loss:964.4462280273438\n",
            "48/3000 train_loss: 880.8135375976562 test_loss:898.7886352539062\n",
            "49/3000 train_loss: 858.4830322265625 test_loss:891.1704711914062\n",
            "50/3000 train_loss: 924.4949951171875 test_loss:910.9059448242188\n",
            "51/3000 train_loss: 922.728515625 test_loss:958.9074096679688\n",
            "52/3000 train_loss: 899.7876586914062 test_loss:943.1533813476562\n",
            "53/3000 train_loss: 838.0599975585938 test_loss:970.908447265625\n",
            "54/3000 train_loss: 838.4472045898438 test_loss:927.5950927734375\n",
            "55/3000 train_loss: 902.8865356445312 test_loss:909.928466796875\n",
            "56/3000 train_loss: 862.0960693359375 test_loss:938.0647583007812\n",
            "57/3000 train_loss: 838.2610473632812 test_loss:882.955810546875\n",
            "58/3000 train_loss: 819.427490234375 test_loss:872.5662231445312\n",
            "59/3000 train_loss: 878.2142333984375 test_loss:916.31201171875\n",
            "60/3000 train_loss: 832.2724609375 test_loss:867.5543212890625\n",
            "61/3000 train_loss: 903.2252807617188 test_loss:938.0767822265625\n",
            "62/3000 train_loss: 873.4568481445312 test_loss:941.20947265625\n",
            "63/3000 train_loss: 854.3232421875 test_loss:938.5676879882812\n",
            "64/3000 train_loss: 849.4923095703125 test_loss:943.8455810546875\n",
            "65/3000 train_loss: 859.8056030273438 test_loss:911.830810546875\n",
            "66/3000 train_loss: 820.492919921875 test_loss:917.796630859375\n",
            "67/3000 train_loss: 803.5872192382812 test_loss:962.7459106445312\n",
            "68/3000 train_loss: 848.1690673828125 test_loss:938.5177001953125\n",
            "69/3000 train_loss: 786.4803466796875 test_loss:927.4458618164062\n",
            "70/3000 train_loss: 967.0897216796875 test_loss:882.0982666015625\n",
            "71/3000 train_loss: 768.8359985351562 test_loss:861.4528198242188\n",
            "72/3000 train_loss: 782.9631958007812 test_loss:854.644287109375\n",
            "73/3000 train_loss: 794.052001953125 test_loss:862.4293212890625\n",
            "74/3000 train_loss: 778.309326171875 test_loss:836.4796752929688\n",
            "75/3000 train_loss: 800.7825927734375 test_loss:832.3756103515625\n",
            "76/3000 train_loss: 790.1939086914062 test_loss:815.1668090820312\n",
            "77/3000 train_loss: 795.886474609375 test_loss:825.2200927734375\n",
            "78/3000 train_loss: 785.1312866210938 test_loss:818.6200561523438\n",
            "79/3000 train_loss: 742.454345703125 test_loss:808.1212158203125\n",
            "80/3000 train_loss: 816.0028076171875 test_loss:817.9521484375\n",
            "81/3000 train_loss: 774.2154541015625 test_loss:800.3368530273438\n",
            "82/3000 train_loss: 748.0526123046875 test_loss:790.6121215820312\n",
            "83/3000 train_loss: 740.0799560546875 test_loss:785.687255859375\n",
            "84/3000 train_loss: 773.321533203125 test_loss:802.4317626953125\n",
            "85/3000 train_loss: 766.7676391601562 test_loss:767.7406005859375\n",
            "86/3000 train_loss: 723.7891235351562 test_loss:763.8721313476562\n",
            "87/3000 train_loss: 718.883056640625 test_loss:762.6966552734375\n",
            "88/3000 train_loss: 729.9172973632812 test_loss:773.1119995117188\n",
            "89/3000 train_loss: 717.68408203125 test_loss:759.2578735351562\n",
            "90/3000 train_loss: 700.5936889648438 test_loss:772.9467163085938\n",
            "91/3000 train_loss: 758.9161376953125 test_loss:749.700439453125\n",
            "92/3000 train_loss: 725.02587890625 test_loss:740.4048461914062\n",
            "93/3000 train_loss: 697.6090087890625 test_loss:743.0142822265625\n",
            "94/3000 train_loss: 771.6904907226562 test_loss:756.536865234375\n",
            "95/3000 train_loss: 718.0359497070312 test_loss:748.4464111328125\n",
            "96/3000 train_loss: 695.1279907226562 test_loss:756.7214965820312\n",
            "97/3000 train_loss: 693.9390869140625 test_loss:731.3851318359375\n",
            "98/3000 train_loss: 660.5170288085938 test_loss:731.9688720703125\n",
            "99/3000 train_loss: 668.6883544921875 test_loss:726.7315673828125\n",
            "100/3000 train_loss: 709.8504028320312 test_loss:731.3969116210938\n",
            "101/3000 train_loss: 667.1539916992188 test_loss:723.9464721679688\n",
            "102/3000 train_loss: 666.8593139648438 test_loss:730.6583251953125\n",
            "103/3000 train_loss: 678.927978515625 test_loss:694.7080078125\n",
            "104/3000 train_loss: 701.1832885742188 test_loss:695.7487182617188\n",
            "105/3000 train_loss: 684.1989135742188 test_loss:689.6888427734375\n",
            "106/3000 train_loss: 677.8587646484375 test_loss:696.664306640625\n",
            "107/3000 train_loss: 680.6767578125 test_loss:700.0533447265625\n",
            "108/3000 train_loss: 676.0079956054688 test_loss:677.26171875\n",
            "109/3000 train_loss: 650.6751098632812 test_loss:678.6163940429688\n",
            "110/3000 train_loss: 630.4494018554688 test_loss:684.7686157226562\n",
            "111/3000 train_loss: 634.0857543945312 test_loss:678.9524536132812\n",
            "112/3000 train_loss: 643.2376708984375 test_loss:667.8306884765625\n",
            "113/3000 train_loss: 625.0018310546875 test_loss:666.485107421875\n",
            "114/3000 train_loss: 622.3081665039062 test_loss:667.6234130859375\n",
            "115/3000 train_loss: 707.6338500976562 test_loss:666.6130981445312\n",
            "116/3000 train_loss: 612.1868286132812 test_loss:658.6683349609375\n",
            "117/3000 train_loss: 615.800048828125 test_loss:646.8811645507812\n",
            "118/3000 train_loss: 603.1731567382812 test_loss:639.1084594726562\n",
            "119/3000 train_loss: 621.6678466796875 test_loss:627.9220581054688\n",
            "120/3000 train_loss: 607.54541015625 test_loss:636.489501953125\n",
            "121/3000 train_loss: 609.4849853515625 test_loss:668.2342529296875\n",
            "122/3000 train_loss: 616.6494140625 test_loss:636.286376953125\n",
            "123/3000 train_loss: 614.53271484375 test_loss:641.8693237304688\n",
            "124/3000 train_loss: 581.9063110351562 test_loss:631.8948364257812\n",
            "125/3000 train_loss: 644.3212890625 test_loss:626.0505981445312\n",
            "126/3000 train_loss: 606.1636962890625 test_loss:623.3599853515625\n",
            "127/3000 train_loss: 591.439453125 test_loss:633.25244140625\n",
            "128/3000 train_loss: 578.9971923828125 test_loss:628.501953125\n",
            "129/3000 train_loss: 600.7726440429688 test_loss:618.007080078125\n",
            "130/3000 train_loss: 629.745361328125 test_loss:619.6163940429688\n",
            "131/3000 train_loss: 608.2389526367188 test_loss:604.55126953125\n",
            "132/3000 train_loss: 586.857177734375 test_loss:619.2258911132812\n",
            "133/3000 train_loss: 585.2548828125 test_loss:593.6764526367188\n",
            "134/3000 train_loss: 569.7496948242188 test_loss:597.1144409179688\n",
            "135/3000 train_loss: 559.9520263671875 test_loss:599.6644897460938\n",
            "136/3000 train_loss: 584.4857177734375 test_loss:597.1388549804688\n",
            "137/3000 train_loss: 565.868896484375 test_loss:586.7125244140625\n",
            "138/3000 train_loss: 551.8949584960938 test_loss:588.7303466796875\n",
            "139/3000 train_loss: 561.5895385742188 test_loss:592.18603515625\n",
            "140/3000 train_loss: 572.4993896484375 test_loss:585.719482421875\n",
            "141/3000 train_loss: 572.1019287109375 test_loss:588.3934936523438\n",
            "142/3000 train_loss: 568.937255859375 test_loss:603.6585693359375\n",
            "143/3000 train_loss: 552.7985229492188 test_loss:597.5322875976562\n",
            "144/3000 train_loss: 547.7998046875 test_loss:584.607177734375\n",
            "145/3000 train_loss: 529.2096557617188 test_loss:587.5139770507812\n",
            "146/3000 train_loss: 521.6154174804688 test_loss:555.7952880859375\n",
            "147/3000 train_loss: 535.977783203125 test_loss:553.3697509765625\n",
            "148/3000 train_loss: 559.0322265625 test_loss:542.3361206054688\n",
            "149/3000 train_loss: 544.37646484375 test_loss:555.9779052734375\n",
            "150/3000 train_loss: 559.08056640625 test_loss:559.1530151367188\n",
            "151/3000 train_loss: 538.54931640625 test_loss:543.81982421875\n",
            "152/3000 train_loss: 521.28173828125 test_loss:532.4627075195312\n",
            "153/3000 train_loss: 520.7484130859375 test_loss:538.6943359375\n",
            "154/3000 train_loss: 549.295166015625 test_loss:542.9727783203125\n",
            "155/3000 train_loss: 520.5094604492188 test_loss:551.0477294921875\n",
            "156/3000 train_loss: 537.1312866210938 test_loss:524.9910278320312\n",
            "157/3000 train_loss: 496.4336242675781 test_loss:521.0625\n",
            "158/3000 train_loss: 515.9718627929688 test_loss:517.895751953125\n",
            "159/3000 train_loss: 519.7937622070312 test_loss:511.9132995605469\n",
            "160/3000 train_loss: 517.2422485351562 test_loss:517.3472900390625\n",
            "161/3000 train_loss: 493.4901123046875 test_loss:514.2821655273438\n",
            "162/3000 train_loss: 492.8197021484375 test_loss:511.5633544921875\n",
            "163/3000 train_loss: 501.49993896484375 test_loss:506.5527038574219\n",
            "164/3000 train_loss: 472.26318359375 test_loss:522.3580322265625\n",
            "165/3000 train_loss: 480.4353332519531 test_loss:498.3382568359375\n",
            "166/3000 train_loss: 504.1632385253906 test_loss:489.2122802734375\n",
            "167/3000 train_loss: 489.5931091308594 test_loss:499.83245849609375\n",
            "168/3000 train_loss: 475.1277160644531 test_loss:502.09912109375\n",
            "169/3000 train_loss: 481.419921875 test_loss:489.8729553222656\n",
            "170/3000 train_loss: 494.4054260253906 test_loss:480.1651611328125\n",
            "171/3000 train_loss: 455.2113952636719 test_loss:473.24432373046875\n",
            "172/3000 train_loss: 467.12249755859375 test_loss:491.7755126953125\n",
            "173/3000 train_loss: 436.2201843261719 test_loss:479.34808349609375\n",
            "174/3000 train_loss: 458.0225830078125 test_loss:471.3058776855469\n",
            "175/3000 train_loss: 477.1631164550781 test_loss:469.12646484375\n",
            "176/3000 train_loss: 474.66351318359375 test_loss:466.5101013183594\n",
            "177/3000 train_loss: 458.4397277832031 test_loss:480.48126220703125\n",
            "178/3000 train_loss: 446.4698181152344 test_loss:472.57745361328125\n",
            "179/3000 train_loss: 457.3016357421875 test_loss:463.82745361328125\n",
            "180/3000 train_loss: 430.3055419921875 test_loss:467.3604736328125\n",
            "181/3000 train_loss: 453.79791259765625 test_loss:461.6018981933594\n",
            "182/3000 train_loss: 458.1053466796875 test_loss:482.0455017089844\n",
            "183/3000 train_loss: 451.1122131347656 test_loss:461.9833984375\n",
            "184/3000 train_loss: 432.6462097167969 test_loss:454.82183837890625\n",
            "185/3000 train_loss: 444.49676513671875 test_loss:473.578125\n",
            "186/3000 train_loss: 458.79071044921875 test_loss:471.21044921875\n",
            "187/3000 train_loss: 441.936767578125 test_loss:456.7895202636719\n",
            "188/3000 train_loss: 410.92938232421875 test_loss:459.1930847167969\n",
            "189/3000 train_loss: 463.8301696777344 test_loss:467.34197998046875\n",
            "190/3000 train_loss: 425.2477111816406 test_loss:467.5692443847656\n",
            "191/3000 train_loss: 443.01422119140625 test_loss:465.3638000488281\n",
            "192/3000 train_loss: 421.01214599609375 test_loss:468.3294677734375\n",
            "193/3000 train_loss: 434.2764892578125 test_loss:459.9396057128906\n",
            "194/3000 train_loss: 425.97772216796875 test_loss:453.5648193359375\n",
            "195/3000 train_loss: 423.9851379394531 test_loss:451.07586669921875\n",
            "196/3000 train_loss: 472.7752380371094 test_loss:490.93096923828125\n",
            "197/3000 train_loss: 437.8141784667969 test_loss:454.61041259765625\n",
            "198/3000 train_loss: 418.94287109375 test_loss:439.9144287109375\n",
            "199/3000 train_loss: 417.183837890625 test_loss:432.164794921875\n",
            "200/3000 train_loss: 453.2761535644531 test_loss:425.3506164550781\n",
            "201/3000 train_loss: 410.758056640625 test_loss:433.18890380859375\n",
            "202/3000 train_loss: 392.9512634277344 test_loss:428.37066650390625\n",
            "203/3000 train_loss: 394.4466552734375 test_loss:420.71875\n",
            "204/3000 train_loss: 416.8072814941406 test_loss:424.8600158691406\n",
            "205/3000 train_loss: 401.756591796875 test_loss:429.51361083984375\n",
            "206/3000 train_loss: 425.7830810546875 test_loss:415.66314697265625\n",
            "207/3000 train_loss: 381.47509765625 test_loss:418.708740234375\n",
            "208/3000 train_loss: 402.2721252441406 test_loss:416.69647216796875\n",
            "209/3000 train_loss: 393.7186584472656 test_loss:420.69171142578125\n",
            "210/3000 train_loss: 412.850341796875 test_loss:418.20843505859375\n",
            "211/3000 train_loss: 389.73443603515625 test_loss:413.33148193359375\n",
            "212/3000 train_loss: 395.68878173828125 test_loss:421.0453186035156\n",
            "213/3000 train_loss: 388.7799072265625 test_loss:406.6791076660156\n",
            "214/3000 train_loss: 385.6216125488281 test_loss:404.61199951171875\n",
            "215/3000 train_loss: 371.9954528808594 test_loss:409.9044189453125\n",
            "216/3000 train_loss: 397.3761291503906 test_loss:416.092041015625\n",
            "217/3000 train_loss: 404.8831787109375 test_loss:406.5950622558594\n",
            "218/3000 train_loss: 385.6048583984375 test_loss:402.8789367675781\n",
            "219/3000 train_loss: 376.67742919921875 test_loss:393.0773620605469\n",
            "220/3000 train_loss: 367.1243896484375 test_loss:398.94866943359375\n",
            "221/3000 train_loss: 372.3296813964844 test_loss:397.1830749511719\n",
            "222/3000 train_loss: 399.9519958496094 test_loss:397.2929992675781\n",
            "223/3000 train_loss: 358.7217712402344 test_loss:388.43963623046875\n",
            "224/3000 train_loss: 383.82080078125 test_loss:421.0622863769531\n",
            "225/3000 train_loss: 360.1968078613281 test_loss:398.9214782714844\n",
            "226/3000 train_loss: 366.37353515625 test_loss:394.93927001953125\n",
            "227/3000 train_loss: 368.3826599121094 test_loss:395.4754638671875\n",
            "228/3000 train_loss: 370.6060791015625 test_loss:399.57513427734375\n",
            "229/3000 train_loss: 364.6026611328125 test_loss:387.6018371582031\n",
            "230/3000 train_loss: 378.720947265625 test_loss:398.220458984375\n",
            "231/3000 train_loss: 378.48699951171875 test_loss:386.03302001953125\n",
            "232/3000 train_loss: 357.3249816894531 test_loss:391.1085510253906\n",
            "233/3000 train_loss: 352.3350830078125 test_loss:376.3472900390625\n",
            "234/3000 train_loss: 367.1866149902344 test_loss:388.46142578125\n",
            "235/3000 train_loss: 372.93011474609375 test_loss:391.5733642578125\n",
            "236/3000 train_loss: 362.2222900390625 test_loss:412.3680419921875\n",
            "237/3000 train_loss: 378.596435546875 test_loss:373.8722229003906\n",
            "238/3000 train_loss: 344.5090637207031 test_loss:386.6528015136719\n",
            "239/3000 train_loss: 338.4414367675781 test_loss:381.1762390136719\n",
            "240/3000 train_loss: 348.275390625 test_loss:373.97454833984375\n",
            "241/3000 train_loss: 339.3654479980469 test_loss:376.8537292480469\n",
            "242/3000 train_loss: 350.083251953125 test_loss:365.43157958984375\n",
            "243/3000 train_loss: 348.585693359375 test_loss:365.9031982421875\n",
            "244/3000 train_loss: 371.58819580078125 test_loss:375.65289306640625\n",
            "245/3000 train_loss: 344.1904296875 test_loss:386.67681884765625\n",
            "246/3000 train_loss: 333.8634338378906 test_loss:368.9507141113281\n",
            "247/3000 train_loss: 335.35809326171875 test_loss:380.4269104003906\n",
            "248/3000 train_loss: 368.3209228515625 test_loss:373.1151123046875\n",
            "249/3000 train_loss: 330.693115234375 test_loss:351.927490234375\n",
            "250/3000 train_loss: 326.9018859863281 test_loss:370.2004699707031\n",
            "251/3000 train_loss: 339.9026794433594 test_loss:369.0419006347656\n",
            "252/3000 train_loss: 328.3853454589844 test_loss:364.08148193359375\n",
            "253/3000 train_loss: 331.7040710449219 test_loss:355.17987060546875\n",
            "254/3000 train_loss: 349.35406494140625 test_loss:362.5854797363281\n",
            "255/3000 train_loss: 343.24163818359375 test_loss:369.81524658203125\n",
            "256/3000 train_loss: 321.96087646484375 test_loss:357.48809814453125\n",
            "257/3000 train_loss: 321.74951171875 test_loss:340.651123046875\n",
            "258/3000 train_loss: 329.51422119140625 test_loss:343.16668701171875\n",
            "259/3000 train_loss: 312.1527099609375 test_loss:347.9056701660156\n",
            "260/3000 train_loss: 328.1199951171875 test_loss:339.0831298828125\n",
            "261/3000 train_loss: 340.244384765625 test_loss:345.97747802734375\n",
            "262/3000 train_loss: 323.6791687011719 test_loss:347.80670166015625\n",
            "263/3000 train_loss: 337.6800231933594 test_loss:356.57379150390625\n",
            "264/3000 train_loss: 353.5494079589844 test_loss:347.1730041503906\n",
            "265/3000 train_loss: 329.7225646972656 test_loss:348.9822692871094\n",
            "266/3000 train_loss: 322.7328796386719 test_loss:364.7698669433594\n",
            "267/3000 train_loss: 347.9330139160156 test_loss:339.29345703125\n",
            "268/3000 train_loss: 322.0525207519531 test_loss:342.45306396484375\n",
            "269/3000 train_loss: 299.6307678222656 test_loss:336.1944580078125\n",
            "270/3000 train_loss: 301.954833984375 test_loss:326.2538757324219\n",
            "271/3000 train_loss: 297.9527282714844 test_loss:324.6952209472656\n",
            "272/3000 train_loss: 318.4234619140625 test_loss:334.80035400390625\n",
            "273/3000 train_loss: 303.9094543457031 test_loss:351.0674133300781\n",
            "274/3000 train_loss: 304.9316711425781 test_loss:330.9045715332031\n",
            "275/3000 train_loss: 300.9958801269531 test_loss:330.1631164550781\n",
            "276/3000 train_loss: 306.4127502441406 test_loss:330.96435546875\n",
            "277/3000 train_loss: 345.53717041015625 test_loss:325.69158935546875\n",
            "278/3000 train_loss: 306.31494140625 test_loss:340.1037902832031\n",
            "279/3000 train_loss: 289.19110107421875 test_loss:332.3228759765625\n",
            "280/3000 train_loss: 288.44708251953125 test_loss:326.5484313964844\n",
            "281/3000 train_loss: 291.99310302734375 test_loss:331.6961975097656\n",
            "282/3000 train_loss: 288.74853515625 test_loss:325.77227783203125\n",
            "283/3000 train_loss: 273.1617126464844 test_loss:323.5576477050781\n",
            "284/3000 train_loss: 296.2717590332031 test_loss:320.44482421875\n",
            "285/3000 train_loss: 288.8865661621094 test_loss:310.3647766113281\n",
            "286/3000 train_loss: 270.29632568359375 test_loss:309.3239440917969\n",
            "287/3000 train_loss: 277.64617919921875 test_loss:306.8730773925781\n",
            "288/3000 train_loss: 292.8717346191406 test_loss:303.0186767578125\n",
            "289/3000 train_loss: 298.9306335449219 test_loss:304.1228332519531\n",
            "290/3000 train_loss: 303.34808349609375 test_loss:315.1734313964844\n",
            "291/3000 train_loss: 281.0847473144531 test_loss:305.0133361816406\n",
            "292/3000 train_loss: 282.5755615234375 test_loss:293.96136474609375\n",
            "293/3000 train_loss: 286.6351623535156 test_loss:292.6291809082031\n",
            "294/3000 train_loss: 279.60931396484375 test_loss:302.813720703125\n",
            "295/3000 train_loss: 286.5584411621094 test_loss:306.0178527832031\n",
            "296/3000 train_loss: 264.19134521484375 test_loss:308.75103759765625\n",
            "297/3000 train_loss: 284.2649841308594 test_loss:297.25604248046875\n",
            "298/3000 train_loss: 280.7322082519531 test_loss:296.6214904785156\n",
            "299/3000 train_loss: 253.6422882080078 test_loss:295.3507995605469\n",
            "300/3000 train_loss: 273.6631164550781 test_loss:309.3310241699219\n",
            "301/3000 train_loss: 281.444091796875 test_loss:304.4873046875\n",
            "302/3000 train_loss: 255.17100524902344 test_loss:317.5440979003906\n",
            "303/3000 train_loss: 277.8641662597656 test_loss:284.519287109375\n",
            "304/3000 train_loss: 265.8678283691406 test_loss:289.35064697265625\n",
            "305/3000 train_loss: 258.81787109375 test_loss:285.7407531738281\n",
            "306/3000 train_loss: 263.6814270019531 test_loss:301.5254211425781\n",
            "307/3000 train_loss: 289.7940368652344 test_loss:292.6968994140625\n",
            "308/3000 train_loss: 272.6447448730469 test_loss:287.084228515625\n",
            "309/3000 train_loss: 266.90521240234375 test_loss:285.4904479980469\n",
            "310/3000 train_loss: 265.6289978027344 test_loss:274.2359619140625\n",
            "311/3000 train_loss: 239.02883911132812 test_loss:287.2727355957031\n",
            "312/3000 train_loss: 264.5215148925781 test_loss:275.54815673828125\n",
            "313/3000 train_loss: 252.19175720214844 test_loss:281.0338439941406\n",
            "314/3000 train_loss: 257.2314758300781 test_loss:279.02740478515625\n",
            "315/3000 train_loss: 244.81419372558594 test_loss:276.40380859375\n",
            "316/3000 train_loss: 251.41139221191406 test_loss:273.38140869140625\n",
            "317/3000 train_loss: 241.7858428955078 test_loss:269.54241943359375\n",
            "318/3000 train_loss: 244.31370544433594 test_loss:292.54046630859375\n",
            "319/3000 train_loss: 243.3017578125 test_loss:272.0447692871094\n",
            "320/3000 train_loss: 253.53968811035156 test_loss:268.8329772949219\n",
            "321/3000 train_loss: 242.55413818359375 test_loss:264.06396484375\n",
            "322/3000 train_loss: 251.90525817871094 test_loss:265.5032043457031\n",
            "323/3000 train_loss: 245.9911651611328 test_loss:265.2801818847656\n",
            "324/3000 train_loss: 247.40846252441406 test_loss:262.5183410644531\n",
            "325/3000 train_loss: 236.6514129638672 test_loss:268.835693359375\n",
            "326/3000 train_loss: 245.5233612060547 test_loss:272.1092529296875\n",
            "327/3000 train_loss: 233.25341796875 test_loss:270.3775939941406\n",
            "328/3000 train_loss: 249.4365692138672 test_loss:255.9147491455078\n",
            "329/3000 train_loss: 251.16795349121094 test_loss:254.9434814453125\n",
            "330/3000 train_loss: 251.47207641601562 test_loss:261.7902526855469\n",
            "331/3000 train_loss: 243.2501983642578 test_loss:252.8304901123047\n",
            "332/3000 train_loss: 233.6455535888672 test_loss:256.4542236328125\n",
            "333/3000 train_loss: 240.20999145507812 test_loss:267.345947265625\n",
            "334/3000 train_loss: 238.88760375976562 test_loss:272.1849365234375\n",
            "335/3000 train_loss: 212.02882385253906 test_loss:268.5911560058594\n",
            "336/3000 train_loss: 232.31643676757812 test_loss:261.63067626953125\n",
            "337/3000 train_loss: 234.46986389160156 test_loss:263.2691650390625\n",
            "338/3000 train_loss: 234.76187133789062 test_loss:262.4022521972656\n",
            "339/3000 train_loss: 262.59124755859375 test_loss:264.5709533691406\n",
            "340/3000 train_loss: 229.26951599121094 test_loss:251.32131958007812\n",
            "341/3000 train_loss: 242.5591278076172 test_loss:247.65155029296875\n",
            "342/3000 train_loss: 246.1354217529297 test_loss:252.99685668945312\n",
            "343/3000 train_loss: 235.5696258544922 test_loss:257.3306579589844\n",
            "344/3000 train_loss: 235.8267059326172 test_loss:253.63729858398438\n",
            "345/3000 train_loss: 231.4074249267578 test_loss:260.74530029296875\n",
            "346/3000 train_loss: 211.86875915527344 test_loss:252.29751586914062\n",
            "347/3000 train_loss: 226.04022216796875 test_loss:239.95669555664062\n",
            "348/3000 train_loss: 223.23704528808594 test_loss:255.31698608398438\n",
            "349/3000 train_loss: 212.9971923828125 test_loss:246.8123779296875\n",
            "350/3000 train_loss: 227.90875244140625 test_loss:237.23519897460938\n",
            "351/3000 train_loss: 230.20306396484375 test_loss:236.12811279296875\n",
            "352/3000 train_loss: 239.184814453125 test_loss:243.7354736328125\n",
            "353/3000 train_loss: 231.4942169189453 test_loss:245.11224365234375\n",
            "354/3000 train_loss: 226.60623168945312 test_loss:243.16197204589844\n",
            "355/3000 train_loss: 211.86375427246094 test_loss:239.0520782470703\n",
            "356/3000 train_loss: 212.45706176757812 test_loss:247.9609832763672\n",
            "357/3000 train_loss: 216.58006286621094 test_loss:237.59555053710938\n",
            "358/3000 train_loss: 207.81748962402344 test_loss:243.71629333496094\n",
            "359/3000 train_loss: 199.7012481689453 test_loss:247.5643310546875\n",
            "360/3000 train_loss: 214.2986297607422 test_loss:233.12863159179688\n",
            "361/3000 train_loss: 213.864990234375 test_loss:245.54603576660156\n",
            "362/3000 train_loss: 243.67022705078125 test_loss:236.1062774658203\n",
            "363/3000 train_loss: 219.67825317382812 test_loss:248.34048461914062\n",
            "364/3000 train_loss: 227.062255859375 test_loss:264.15692138671875\n",
            "365/3000 train_loss: 229.1729736328125 test_loss:242.30458068847656\n",
            "366/3000 train_loss: 215.8048553466797 test_loss:253.5218963623047\n",
            "367/3000 train_loss: 207.25833129882812 test_loss:240.99021911621094\n",
            "368/3000 train_loss: 207.2069091796875 test_loss:237.59364318847656\n",
            "369/3000 train_loss: 230.6919708251953 test_loss:231.74659729003906\n",
            "370/3000 train_loss: 220.017822265625 test_loss:238.8233642578125\n",
            "371/3000 train_loss: 204.38714599609375 test_loss:249.24990844726562\n",
            "372/3000 train_loss: 211.61424255371094 test_loss:241.92591857910156\n",
            "373/3000 train_loss: 208.0370330810547 test_loss:234.7290802001953\n",
            "374/3000 train_loss: 209.39883422851562 test_loss:240.16917419433594\n",
            "375/3000 train_loss: 202.08563232421875 test_loss:244.53485107421875\n",
            "376/3000 train_loss: 230.31028747558594 test_loss:243.64791870117188\n",
            "377/3000 train_loss: 219.11318969726562 test_loss:241.40298461914062\n",
            "378/3000 train_loss: 202.237548828125 test_loss:242.155029296875\n",
            "379/3000 train_loss: 200.13973999023438 test_loss:232.40113830566406\n",
            "380/3000 train_loss: 211.20785522460938 test_loss:232.45584106445312\n",
            "381/3000 train_loss: 197.52218627929688 test_loss:236.08274841308594\n",
            "382/3000 train_loss: 196.3634796142578 test_loss:236.15467834472656\n",
            "383/3000 train_loss: 196.7381591796875 test_loss:237.7784423828125\n",
            "384/3000 train_loss: 198.2783203125 test_loss:231.30596923828125\n",
            "385/3000 train_loss: 193.63861083984375 test_loss:244.713134765625\n",
            "386/3000 train_loss: 213.011962890625 test_loss:240.2257080078125\n",
            "387/3000 train_loss: 202.85498046875 test_loss:225.78094482421875\n",
            "388/3000 train_loss: 191.71890258789062 test_loss:237.17823791503906\n",
            "389/3000 train_loss: 196.2933349609375 test_loss:238.13778686523438\n",
            "390/3000 train_loss: 208.02377319335938 test_loss:229.27207946777344\n",
            "391/3000 train_loss: 190.9433135986328 test_loss:240.9019317626953\n",
            "392/3000 train_loss: 194.281494140625 test_loss:232.35679626464844\n",
            "393/3000 train_loss: 224.7090301513672 test_loss:232.02366638183594\n",
            "394/3000 train_loss: 220.16525268554688 test_loss:225.78366088867188\n",
            "395/3000 train_loss: 202.89073181152344 test_loss:226.65000915527344\n",
            "396/3000 train_loss: 195.7039337158203 test_loss:235.47927856445312\n",
            "397/3000 train_loss: 193.33395385742188 test_loss:230.57785034179688\n",
            "398/3000 train_loss: 208.78945922851562 test_loss:240.3748779296875\n",
            "399/3000 train_loss: 184.15269470214844 test_loss:230.02987670898438\n",
            "400/3000 train_loss: 192.95448303222656 test_loss:227.17945861816406\n",
            "401/3000 train_loss: 192.96253967285156 test_loss:220.95091247558594\n",
            "402/3000 train_loss: 196.37623596191406 test_loss:225.60647583007812\n",
            "403/3000 train_loss: 183.22471618652344 test_loss:225.93536376953125\n",
            "404/3000 train_loss: 193.26596069335938 test_loss:222.3624267578125\n",
            "405/3000 train_loss: 188.1265106201172 test_loss:214.29940795898438\n",
            "406/3000 train_loss: 180.237060546875 test_loss:230.5970001220703\n",
            "407/3000 train_loss: 189.3417510986328 test_loss:219.90179443359375\n",
            "408/3000 train_loss: 191.5498504638672 test_loss:223.30735778808594\n",
            "409/3000 train_loss: 192.6114044189453 test_loss:231.913818359375\n",
            "410/3000 train_loss: 187.19248962402344 test_loss:220.7308807373047\n",
            "411/3000 train_loss: 190.01976013183594 test_loss:218.6375274658203\n",
            "412/3000 train_loss: 184.26922607421875 test_loss:234.1693572998047\n",
            "413/3000 train_loss: 187.1300811767578 test_loss:224.82211303710938\n",
            "414/3000 train_loss: 202.14137268066406 test_loss:227.78469848632812\n",
            "415/3000 train_loss: 181.47195434570312 test_loss:215.81500244140625\n",
            "416/3000 train_loss: 191.8114013671875 test_loss:227.2604217529297\n",
            "417/3000 train_loss: 190.96060180664062 test_loss:209.41082763671875\n",
            "418/3000 train_loss: 189.8104705810547 test_loss:230.7906951904297\n",
            "419/3000 train_loss: 184.29759216308594 test_loss:220.32359313964844\n",
            "420/3000 train_loss: 192.11465454101562 test_loss:220.03341674804688\n",
            "421/3000 train_loss: 192.7061004638672 test_loss:209.78115844726562\n",
            "422/3000 train_loss: 189.37132263183594 test_loss:238.38372802734375\n",
            "423/3000 train_loss: 203.1884765625 test_loss:230.142333984375\n",
            "424/3000 train_loss: 182.62307739257812 test_loss:214.79898071289062\n",
            "425/3000 train_loss: 189.29507446289062 test_loss:220.33819580078125\n",
            "426/3000 train_loss: 185.47512817382812 test_loss:223.7462158203125\n",
            "427/3000 train_loss: 186.2060546875 test_loss:224.36805725097656\n",
            "428/3000 train_loss: 206.75808715820312 test_loss:231.00575256347656\n",
            "429/3000 train_loss: 189.2863006591797 test_loss:221.76132202148438\n",
            "430/3000 train_loss: 183.314697265625 test_loss:228.35809326171875\n",
            "431/3000 train_loss: 210.4067840576172 test_loss:227.2609100341797\n",
            "432/3000 train_loss: 186.81739807128906 test_loss:213.79928588867188\n",
            "433/3000 train_loss: 181.01409912109375 test_loss:218.84616088867188\n",
            "434/3000 train_loss: 204.83242797851562 test_loss:211.13674926757812\n",
            "435/3000 train_loss: 181.237548828125 test_loss:212.17892456054688\n",
            "436/3000 train_loss: 191.50148010253906 test_loss:214.7535858154297\n",
            "437/3000 train_loss: 182.05426025390625 test_loss:227.64364624023438\n",
            "438/3000 train_loss: 191.23350524902344 test_loss:224.91339111328125\n",
            "439/3000 train_loss: 184.16612243652344 test_loss:221.63206481933594\n",
            "440/3000 train_loss: 199.92933654785156 test_loss:230.51882934570312\n",
            "441/3000 train_loss: 190.21881103515625 test_loss:211.10806274414062\n",
            "442/3000 train_loss: 177.43466186523438 test_loss:208.5015869140625\n",
            "443/3000 train_loss: 165.79290771484375 test_loss:214.9686279296875\n",
            "444/3000 train_loss: 179.1817169189453 test_loss:212.29281616210938\n",
            "445/3000 train_loss: 198.65438842773438 test_loss:209.45440673828125\n",
            "446/3000 train_loss: 190.77001953125 test_loss:207.06170654296875\n",
            "447/3000 train_loss: 170.10028076171875 test_loss:205.78358459472656\n",
            "448/3000 train_loss: 185.0835418701172 test_loss:209.60845947265625\n",
            "449/3000 train_loss: 192.42124938964844 test_loss:217.55502319335938\n",
            "450/3000 train_loss: 174.5099334716797 test_loss:219.50546264648438\n",
            "451/3000 train_loss: 188.4155731201172 test_loss:216.29881286621094\n",
            "452/3000 train_loss: 177.29197692871094 test_loss:217.94369506835938\n",
            "453/3000 train_loss: 179.14369201660156 test_loss:218.12799072265625\n",
            "454/3000 train_loss: 179.15184020996094 test_loss:223.779296875\n",
            "455/3000 train_loss: 170.4761962890625 test_loss:207.29928588867188\n",
            "456/3000 train_loss: 176.40792846679688 test_loss:209.55984497070312\n",
            "457/3000 train_loss: 192.70106506347656 test_loss:204.8467559814453\n",
            "458/3000 train_loss: 174.37142944335938 test_loss:200.52847290039062\n",
            "459/3000 train_loss: 168.95977783203125 test_loss:203.6299285888672\n",
            "460/3000 train_loss: 180.2021942138672 test_loss:211.58367919921875\n",
            "461/3000 train_loss: 190.27723693847656 test_loss:209.45269775390625\n",
            "462/3000 train_loss: 165.94607543945312 test_loss:214.84597778320312\n",
            "463/3000 train_loss: 186.0733642578125 test_loss:207.83766174316406\n",
            "464/3000 train_loss: 186.23011779785156 test_loss:220.2852783203125\n",
            "465/3000 train_loss: 172.1650848388672 test_loss:208.32247924804688\n",
            "466/3000 train_loss: 177.98851013183594 test_loss:209.71783447265625\n",
            "467/3000 train_loss: 178.03546142578125 test_loss:206.65487670898438\n",
            "468/3000 train_loss: 170.65040588378906 test_loss:209.67092895507812\n",
            "469/3000 train_loss: 179.01416015625 test_loss:215.5609893798828\n",
            "470/3000 train_loss: 175.7908935546875 test_loss:215.48208618164062\n",
            "471/3000 train_loss: 186.47972106933594 test_loss:208.8676300048828\n",
            "472/3000 train_loss: 174.2754669189453 test_loss:204.08673095703125\n",
            "473/3000 train_loss: 186.8126220703125 test_loss:206.357177734375\n",
            "474/3000 train_loss: 195.12574768066406 test_loss:207.85452270507812\n",
            "475/3000 train_loss: 169.2886199951172 test_loss:203.72012329101562\n",
            "476/3000 train_loss: 173.084228515625 test_loss:207.28814697265625\n",
            "477/3000 train_loss: 182.55410766601562 test_loss:202.64678955078125\n",
            "478/3000 train_loss: 168.34173583984375 test_loss:207.99887084960938\n",
            "479/3000 train_loss: 174.47947692871094 test_loss:199.325439453125\n",
            "480/3000 train_loss: 165.73733520507812 test_loss:209.96414184570312\n",
            "481/3000 train_loss: 174.34532165527344 test_loss:204.29188537597656\n",
            "482/3000 train_loss: 164.96035766601562 test_loss:205.92840576171875\n",
            "483/3000 train_loss: 178.45188903808594 test_loss:202.3629608154297\n",
            "484/3000 train_loss: 164.98670959472656 test_loss:208.0008087158203\n",
            "485/3000 train_loss: 183.9747772216797 test_loss:217.54104614257812\n",
            "486/3000 train_loss: 170.02894592285156 test_loss:204.6029052734375\n",
            "487/3000 train_loss: 168.60427856445312 test_loss:211.65695190429688\n",
            "488/3000 train_loss: 173.05601501464844 test_loss:211.48548889160156\n",
            "489/3000 train_loss: 169.00515747070312 test_loss:202.37481689453125\n",
            "490/3000 train_loss: 170.7515106201172 test_loss:201.29627990722656\n",
            "491/3000 train_loss: 167.7823486328125 test_loss:208.79397583007812\n",
            "492/3000 train_loss: 156.43504333496094 test_loss:209.00770568847656\n",
            "493/3000 train_loss: 168.5045166015625 test_loss:203.55467224121094\n",
            "494/3000 train_loss: 164.20375061035156 test_loss:203.84738159179688\n",
            "495/3000 train_loss: 181.03538513183594 test_loss:208.5174560546875\n",
            "496/3000 train_loss: 165.75965881347656 test_loss:205.7929229736328\n",
            "497/3000 train_loss: 182.2952880859375 test_loss:204.74627685546875\n",
            "498/3000 train_loss: 181.89842224121094 test_loss:207.61883544921875\n",
            "499/3000 train_loss: 161.38729858398438 test_loss:198.58981323242188\n",
            "500/3000 train_loss: 172.18324279785156 test_loss:206.6632537841797\n",
            "501/3000 train_loss: 177.8225555419922 test_loss:203.5029754638672\n",
            "502/3000 train_loss: 163.77615356445312 test_loss:197.84152221679688\n",
            "503/3000 train_loss: 167.10101318359375 test_loss:198.98191833496094\n",
            "504/3000 train_loss: 173.58982849121094 test_loss:213.85079956054688\n",
            "505/3000 train_loss: 174.67645263671875 test_loss:202.8419647216797\n",
            "506/3000 train_loss: 172.83486938476562 test_loss:195.5916748046875\n",
            "507/3000 train_loss: 171.3368377685547 test_loss:198.18411254882812\n",
            "508/3000 train_loss: 162.68678283691406 test_loss:197.47433471679688\n",
            "509/3000 train_loss: 168.6563720703125 test_loss:210.7353515625\n",
            "510/3000 train_loss: 163.03553771972656 test_loss:197.7038116455078\n",
            "511/3000 train_loss: 173.4489288330078 test_loss:201.61582946777344\n",
            "512/3000 train_loss: 164.50718688964844 test_loss:199.71424865722656\n",
            "513/3000 train_loss: 164.22958374023438 test_loss:196.33038330078125\n",
            "514/3000 train_loss: 160.6759490966797 test_loss:203.79150390625\n",
            "515/3000 train_loss: 165.95396423339844 test_loss:201.395263671875\n",
            "516/3000 train_loss: 183.38760375976562 test_loss:204.828857421875\n",
            "517/3000 train_loss: 172.97982788085938 test_loss:205.16815185546875\n",
            "518/3000 train_loss: 168.36178588867188 test_loss:199.99302673339844\n",
            "519/3000 train_loss: 173.08055114746094 test_loss:194.3341522216797\n",
            "520/3000 train_loss: 160.49722290039062 test_loss:201.2669219970703\n",
            "521/3000 train_loss: 157.61041259765625 test_loss:203.5349884033203\n",
            "522/3000 train_loss: 155.86831665039062 test_loss:199.10182189941406\n",
            "523/3000 train_loss: 154.31991577148438 test_loss:195.13003540039062\n",
            "524/3000 train_loss: 165.8517608642578 test_loss:197.20228576660156\n",
            "525/3000 train_loss: 175.47128295898438 test_loss:195.43972778320312\n",
            "526/3000 train_loss: 164.48065185546875 test_loss:198.18365478515625\n",
            "527/3000 train_loss: 169.07241821289062 test_loss:195.23606872558594\n",
            "528/3000 train_loss: 157.11085510253906 test_loss:194.36517333984375\n",
            "529/3000 train_loss: 160.17994689941406 test_loss:197.1165771484375\n",
            "530/3000 train_loss: 155.5465545654297 test_loss:197.17526245117188\n",
            "531/3000 train_loss: 155.91477966308594 test_loss:191.8240966796875\n",
            "532/3000 train_loss: 157.3925323486328 test_loss:197.40524291992188\n",
            "533/3000 train_loss: 167.69256591796875 test_loss:192.27081298828125\n",
            "534/3000 train_loss: 151.3092041015625 test_loss:186.6873321533203\n",
            "535/3000 train_loss: 171.0865478515625 test_loss:192.84620666503906\n",
            "536/3000 train_loss: 162.95919799804688 test_loss:190.35643005371094\n",
            "537/3000 train_loss: 171.0333709716797 test_loss:187.80349731445312\n",
            "538/3000 train_loss: 160.39083862304688 test_loss:201.32510375976562\n",
            "539/3000 train_loss: 159.4197540283203 test_loss:189.09335327148438\n",
            "540/3000 train_loss: 170.59056091308594 test_loss:200.48101806640625\n",
            "541/3000 train_loss: 158.72055053710938 test_loss:189.4133758544922\n",
            "542/3000 train_loss: 153.0435791015625 test_loss:190.1444854736328\n",
            "543/3000 train_loss: 169.63027954101562 test_loss:190.68414306640625\n",
            "544/3000 train_loss: 158.99319458007812 test_loss:195.2501983642578\n",
            "545/3000 train_loss: 145.51866149902344 test_loss:197.06552124023438\n",
            "546/3000 train_loss: 176.06643676757812 test_loss:192.33203125\n",
            "547/3000 train_loss: 159.0882568359375 test_loss:196.2044677734375\n",
            "548/3000 train_loss: 152.4852294921875 test_loss:198.82904052734375\n",
            "549/3000 train_loss: 158.61619567871094 test_loss:195.29257202148438\n",
            "550/3000 train_loss: 149.41558837890625 test_loss:197.75059509277344\n",
            "551/3000 train_loss: 153.4936065673828 test_loss:190.43222045898438\n",
            "552/3000 train_loss: 165.4631805419922 test_loss:182.8672637939453\n",
            "553/3000 train_loss: 150.03111267089844 test_loss:183.6692657470703\n",
            "554/3000 train_loss: 150.4241943359375 test_loss:187.03819274902344\n",
            "555/3000 train_loss: 148.5750732421875 test_loss:189.81063842773438\n",
            "556/3000 train_loss: 148.3632354736328 test_loss:193.3565673828125\n",
            "557/3000 train_loss: 144.77069091796875 test_loss:186.98321533203125\n",
            "558/3000 train_loss: 147.79002380371094 test_loss:185.21737670898438\n",
            "559/3000 train_loss: 169.37045288085938 test_loss:184.16403198242188\n",
            "560/3000 train_loss: 166.9842987060547 test_loss:191.4384002685547\n",
            "561/3000 train_loss: 161.86892700195312 test_loss:182.95550537109375\n",
            "562/3000 train_loss: 148.8341522216797 test_loss:182.19683837890625\n",
            "563/3000 train_loss: 156.19737243652344 test_loss:186.84451293945312\n",
            "564/3000 train_loss: 166.338134765625 test_loss:184.36541748046875\n",
            "565/3000 train_loss: 163.6509552001953 test_loss:185.9071502685547\n",
            "566/3000 train_loss: 153.97889709472656 test_loss:186.07479858398438\n",
            "567/3000 train_loss: 142.0513153076172 test_loss:182.07730102539062\n",
            "568/3000 train_loss: 146.41958618164062 test_loss:187.03067016601562\n",
            "569/3000 train_loss: 150.5266876220703 test_loss:194.03384399414062\n",
            "570/3000 train_loss: 161.56346130371094 test_loss:192.16708374023438\n",
            "571/3000 train_loss: 165.5660858154297 test_loss:180.81283569335938\n",
            "572/3000 train_loss: 169.94981384277344 test_loss:188.72714233398438\n",
            "573/3000 train_loss: 149.1272430419922 test_loss:181.53717041015625\n",
            "574/3000 train_loss: 142.9281768798828 test_loss:175.89474487304688\n",
            "575/3000 train_loss: 147.2115478515625 test_loss:176.8715362548828\n",
            "576/3000 train_loss: 159.57144165039062 test_loss:194.13160705566406\n",
            "577/3000 train_loss: 154.948486328125 test_loss:187.4524383544922\n",
            "578/3000 train_loss: 153.15573120117188 test_loss:189.37583923339844\n",
            "579/3000 train_loss: 145.68930053710938 test_loss:188.01129150390625\n",
            "580/3000 train_loss: 155.029052734375 test_loss:184.96536254882812\n",
            "581/3000 train_loss: 143.97300720214844 test_loss:177.06236267089844\n",
            "582/3000 train_loss: 153.8360137939453 test_loss:179.09222412109375\n",
            "583/3000 train_loss: 128.20484924316406 test_loss:183.8793487548828\n",
            "584/3000 train_loss: 142.1029052734375 test_loss:184.00823974609375\n",
            "585/3000 train_loss: 144.84193420410156 test_loss:180.73806762695312\n",
            "586/3000 train_loss: 147.4884033203125 test_loss:184.94290161132812\n",
            "587/3000 train_loss: 138.77403259277344 test_loss:183.3197021484375\n",
            "588/3000 train_loss: 140.38514709472656 test_loss:180.72750854492188\n",
            "589/3000 train_loss: 150.00064086914062 test_loss:190.0660400390625\n",
            "590/3000 train_loss: 155.60565185546875 test_loss:187.6858367919922\n",
            "591/3000 train_loss: 151.734375 test_loss:176.5768280029297\n",
            "592/3000 train_loss: 153.85037231445312 test_loss:184.24514770507812\n",
            "593/3000 train_loss: 134.44781494140625 test_loss:179.46014404296875\n",
            "594/3000 train_loss: 140.0835418701172 test_loss:177.64840698242188\n",
            "595/3000 train_loss: 161.828857421875 test_loss:193.22799682617188\n",
            "596/3000 train_loss: 145.755615234375 test_loss:183.22828674316406\n",
            "597/3000 train_loss: 146.9079132080078 test_loss:177.38414001464844\n",
            "598/3000 train_loss: 142.88026428222656 test_loss:169.80014038085938\n",
            "599/3000 train_loss: 144.3819122314453 test_loss:172.38121032714844\n",
            "600/3000 train_loss: 142.63204956054688 test_loss:182.2766571044922\n",
            "601/3000 train_loss: 146.21878051757812 test_loss:174.08233642578125\n",
            "602/3000 train_loss: 149.09414672851562 test_loss:177.72463989257812\n",
            "603/3000 train_loss: 153.0624542236328 test_loss:173.03778076171875\n",
            "604/3000 train_loss: 138.12147521972656 test_loss:178.64007568359375\n",
            "605/3000 train_loss: 139.39450073242188 test_loss:185.8001251220703\n",
            "606/3000 train_loss: 148.6461181640625 test_loss:182.15762329101562\n",
            "607/3000 train_loss: 144.68153381347656 test_loss:176.06300354003906\n",
            "608/3000 train_loss: 158.04629516601562 test_loss:175.74334716796875\n",
            "609/3000 train_loss: 139.71078491210938 test_loss:181.80665588378906\n",
            "610/3000 train_loss: 136.3249969482422 test_loss:174.8782958984375\n",
            "611/3000 train_loss: 149.57156372070312 test_loss:176.7611541748047\n",
            "612/3000 train_loss: 150.5290985107422 test_loss:176.99179077148438\n",
            "613/3000 train_loss: 133.82066345214844 test_loss:174.83340454101562\n",
            "614/3000 train_loss: 123.38142395019531 test_loss:167.5102081298828\n",
            "615/3000 train_loss: 158.54852294921875 test_loss:185.71322631835938\n",
            "616/3000 train_loss: 144.35877990722656 test_loss:172.90289306640625\n",
            "617/3000 train_loss: 148.1021270751953 test_loss:178.495849609375\n",
            "618/3000 train_loss: 144.5550537109375 test_loss:178.19503784179688\n",
            "619/3000 train_loss: 144.2487030029297 test_loss:174.24269104003906\n",
            "620/3000 train_loss: 128.98841857910156 test_loss:175.72854614257812\n",
            "621/3000 train_loss: 146.65020751953125 test_loss:170.2492218017578\n",
            "622/3000 train_loss: 130.4835662841797 test_loss:168.92921447753906\n",
            "623/3000 train_loss: 132.68572998046875 test_loss:170.5213623046875\n",
            "624/3000 train_loss: 125.98673248291016 test_loss:169.184326171875\n",
            "625/3000 train_loss: 141.81053161621094 test_loss:170.71746826171875\n",
            "626/3000 train_loss: 132.55015563964844 test_loss:167.74722290039062\n",
            "627/3000 train_loss: 146.7080078125 test_loss:170.59864807128906\n",
            "628/3000 train_loss: 136.71499633789062 test_loss:175.94044494628906\n",
            "629/3000 train_loss: 125.39096069335938 test_loss:172.07020568847656\n",
            "630/3000 train_loss: 148.5845184326172 test_loss:186.34999084472656\n",
            "631/3000 train_loss: 140.01126098632812 test_loss:173.8245849609375\n",
            "632/3000 train_loss: 134.668212890625 test_loss:171.4587860107422\n",
            "633/3000 train_loss: 143.10841369628906 test_loss:173.98341369628906\n",
            "634/3000 train_loss: 143.4926300048828 test_loss:170.68798828125\n",
            "635/3000 train_loss: 135.27503967285156 test_loss:180.05702209472656\n",
            "636/3000 train_loss: 134.6797637939453 test_loss:175.5939483642578\n",
            "637/3000 train_loss: 127.4424819946289 test_loss:169.26602172851562\n",
            "638/3000 train_loss: 141.92234802246094 test_loss:169.40858459472656\n",
            "639/3000 train_loss: 137.46446228027344 test_loss:176.02902221679688\n",
            "640/3000 train_loss: 143.78822326660156 test_loss:170.26846313476562\n",
            "641/3000 train_loss: 139.22894287109375 test_loss:165.37599182128906\n",
            "642/3000 train_loss: 125.39051818847656 test_loss:168.21218872070312\n",
            "643/3000 train_loss: 132.1129913330078 test_loss:173.28807067871094\n",
            "644/3000 train_loss: 132.58543395996094 test_loss:168.84934997558594\n",
            "645/3000 train_loss: 137.74331665039062 test_loss:170.61343383789062\n",
            "646/3000 train_loss: 121.0829849243164 test_loss:168.828369140625\n",
            "647/3000 train_loss: 135.8098602294922 test_loss:165.6038360595703\n",
            "648/3000 train_loss: 125.81608581542969 test_loss:172.0549774169922\n",
            "649/3000 train_loss: 132.71054077148438 test_loss:161.9939422607422\n",
            "650/3000 train_loss: 123.45506286621094 test_loss:169.3354949951172\n",
            "651/3000 train_loss: 141.9788055419922 test_loss:176.43392944335938\n",
            "652/3000 train_loss: 135.2975616455078 test_loss:166.6029052734375\n",
            "653/3000 train_loss: 138.9659423828125 test_loss:174.1552276611328\n",
            "654/3000 train_loss: 137.65333557128906 test_loss:164.8376007080078\n",
            "655/3000 train_loss: 127.95382690429688 test_loss:165.53553771972656\n",
            "656/3000 train_loss: 126.21553802490234 test_loss:174.64842224121094\n",
            "657/3000 train_loss: 145.4481201171875 test_loss:169.7793426513672\n",
            "658/3000 train_loss: 117.3601303100586 test_loss:169.43655395507812\n",
            "659/3000 train_loss: 121.80831146240234 test_loss:169.126220703125\n",
            "660/3000 train_loss: 127.34132385253906 test_loss:161.885986328125\n",
            "661/3000 train_loss: 137.22384643554688 test_loss:165.50057983398438\n",
            "662/3000 train_loss: 118.7356948852539 test_loss:166.46983337402344\n",
            "663/3000 train_loss: 148.64694213867188 test_loss:160.88766479492188\n",
            "664/3000 train_loss: 133.7972412109375 test_loss:172.1516571044922\n",
            "665/3000 train_loss: 133.28775024414062 test_loss:168.1235809326172\n",
            "666/3000 train_loss: 120.62297821044922 test_loss:177.5909423828125\n",
            "667/3000 train_loss: 133.04026794433594 test_loss:167.67539978027344\n",
            "668/3000 train_loss: 121.638671875 test_loss:169.7417755126953\n",
            "669/3000 train_loss: 126.4590072631836 test_loss:166.58567810058594\n",
            "670/3000 train_loss: 119.20652770996094 test_loss:159.8490447998047\n",
            "671/3000 train_loss: 118.35670471191406 test_loss:158.4714813232422\n",
            "672/3000 train_loss: 134.55245971679688 test_loss:159.48941040039062\n",
            "673/3000 train_loss: 124.84819030761719 test_loss:162.6436767578125\n",
            "674/3000 train_loss: 132.9393310546875 test_loss:155.3230438232422\n",
            "675/3000 train_loss: 116.0633773803711 test_loss:172.40643310546875\n",
            "676/3000 train_loss: 128.7763671875 test_loss:158.83934020996094\n",
            "677/3000 train_loss: 120.12964630126953 test_loss:166.8014678955078\n",
            "678/3000 train_loss: 121.68084716796875 test_loss:150.2189178466797\n",
            "679/3000 train_loss: 131.05441284179688 test_loss:161.2477569580078\n",
            "680/3000 train_loss: 129.80763244628906 test_loss:161.56089782714844\n",
            "681/3000 train_loss: 139.52708435058594 test_loss:163.7445526123047\n",
            "682/3000 train_loss: 124.5833511352539 test_loss:164.16824340820312\n",
            "683/3000 train_loss: 123.47488403320312 test_loss:162.42578125\n",
            "684/3000 train_loss: 127.72564697265625 test_loss:160.62171936035156\n",
            "685/3000 train_loss: 126.07316589355469 test_loss:161.0909423828125\n",
            "686/3000 train_loss: 124.64495849609375 test_loss:161.12615966796875\n",
            "687/3000 train_loss: 130.1012725830078 test_loss:156.5098876953125\n",
            "688/3000 train_loss: 136.2079620361328 test_loss:168.32948303222656\n",
            "689/3000 train_loss: 115.93445587158203 test_loss:160.08302307128906\n",
            "690/3000 train_loss: 141.238037109375 test_loss:167.53280639648438\n",
            "691/3000 train_loss: 116.1749038696289 test_loss:160.2377471923828\n",
            "692/3000 train_loss: 136.9207305908203 test_loss:169.47811889648438\n",
            "693/3000 train_loss: 121.74238586425781 test_loss:159.0810089111328\n",
            "694/3000 train_loss: 139.44412231445312 test_loss:168.27195739746094\n",
            "695/3000 train_loss: 133.2638702392578 test_loss:162.9603271484375\n",
            "696/3000 train_loss: 126.00005340576172 test_loss:170.6937255859375\n",
            "697/3000 train_loss: 127.05961608886719 test_loss:174.40699768066406\n",
            "698/3000 train_loss: 127.98145294189453 test_loss:156.02212524414062\n",
            "699/3000 train_loss: 121.30142974853516 test_loss:167.22227478027344\n",
            "700/3000 train_loss: 118.81717681884766 test_loss:168.9485321044922\n",
            "701/3000 train_loss: 131.45242309570312 test_loss:163.4874267578125\n",
            "702/3000 train_loss: 123.25635528564453 test_loss:158.51718139648438\n",
            "703/3000 train_loss: 119.50738525390625 test_loss:168.1554412841797\n",
            "704/3000 train_loss: 119.02276611328125 test_loss:155.6368865966797\n",
            "705/3000 train_loss: 125.66046905517578 test_loss:164.3517303466797\n",
            "706/3000 train_loss: 123.982177734375 test_loss:164.25437927246094\n",
            "707/3000 train_loss: 123.6336669921875 test_loss:168.99578857421875\n",
            "708/3000 train_loss: 121.91105651855469 test_loss:162.6282958984375\n",
            "709/3000 train_loss: 117.62444305419922 test_loss:164.4656982421875\n",
            "710/3000 train_loss: 117.8340072631836 test_loss:157.51968383789062\n",
            "711/3000 train_loss: 112.38758850097656 test_loss:161.66387939453125\n",
            "712/3000 train_loss: 126.09810638427734 test_loss:163.6543426513672\n",
            "713/3000 train_loss: 109.72791290283203 test_loss:161.6670379638672\n",
            "714/3000 train_loss: 112.96786499023438 test_loss:161.62547302246094\n",
            "715/3000 train_loss: 120.97569274902344 test_loss:158.51463317871094\n",
            "716/3000 train_loss: 119.69994354248047 test_loss:167.51487731933594\n",
            "717/3000 train_loss: 112.58904266357422 test_loss:148.79469299316406\n",
            "718/3000 train_loss: 116.50072479248047 test_loss:155.79263305664062\n",
            "719/3000 train_loss: 119.2294921875 test_loss:154.65597534179688\n",
            "720/3000 train_loss: 119.1888656616211 test_loss:155.75448608398438\n",
            "721/3000 train_loss: 117.0367202758789 test_loss:151.3916778564453\n",
            "722/3000 train_loss: 123.7784652709961 test_loss:156.92552185058594\n",
            "723/3000 train_loss: 122.08321380615234 test_loss:151.75177001953125\n",
            "724/3000 train_loss: 114.0519027709961 test_loss:158.68931579589844\n",
            "725/3000 train_loss: 130.90261840820312 test_loss:164.94580078125\n",
            "726/3000 train_loss: 127.02227783203125 test_loss:161.99853515625\n",
            "727/3000 train_loss: 115.92443084716797 test_loss:154.92373657226562\n",
            "728/3000 train_loss: 115.44441223144531 test_loss:155.2770538330078\n",
            "729/3000 train_loss: 105.66015625 test_loss:152.50270080566406\n",
            "730/3000 train_loss: 126.08387756347656 test_loss:156.48704528808594\n",
            "731/3000 train_loss: 123.57682800292969 test_loss:151.98403930664062\n",
            "732/3000 train_loss: 123.76852416992188 test_loss:151.6539764404297\n",
            "733/3000 train_loss: 112.48454284667969 test_loss:157.16815185546875\n",
            "734/3000 train_loss: 117.69841003417969 test_loss:154.65530395507812\n",
            "735/3000 train_loss: 124.9462890625 test_loss:171.53366088867188\n",
            "736/3000 train_loss: 116.33033752441406 test_loss:152.18496704101562\n",
            "737/3000 train_loss: 129.81166076660156 test_loss:148.60147094726562\n",
            "738/3000 train_loss: 114.17425537109375 test_loss:152.2371368408203\n",
            "739/3000 train_loss: 113.68946075439453 test_loss:151.50106811523438\n",
            "740/3000 train_loss: 115.72540283203125 test_loss:148.02891540527344\n",
            "741/3000 train_loss: 108.30030059814453 test_loss:144.55316162109375\n",
            "742/3000 train_loss: 123.9873046875 test_loss:160.87928771972656\n",
            "743/3000 train_loss: 119.22319793701172 test_loss:144.9680633544922\n",
            "744/3000 train_loss: 116.34931182861328 test_loss:155.86941528320312\n",
            "745/3000 train_loss: 126.08383178710938 test_loss:148.26806640625\n",
            "746/3000 train_loss: 127.27975463867188 test_loss:149.30857849121094\n",
            "747/3000 train_loss: 113.33025360107422 test_loss:153.56954956054688\n",
            "748/3000 train_loss: 113.14437866210938 test_loss:159.75538635253906\n",
            "749/3000 train_loss: 115.89816284179688 test_loss:146.0966033935547\n",
            "750/3000 train_loss: 117.25959014892578 test_loss:148.77198791503906\n",
            "751/3000 train_loss: 124.07299041748047 test_loss:153.66790771484375\n",
            "752/3000 train_loss: 121.57775115966797 test_loss:144.4231719970703\n",
            "753/3000 train_loss: 123.8232421875 test_loss:153.65049743652344\n",
            "754/3000 train_loss: 105.90013885498047 test_loss:151.453125\n",
            "755/3000 train_loss: 109.2995376586914 test_loss:148.45974731445312\n",
            "756/3000 train_loss: 124.68634796142578 test_loss:152.67637634277344\n",
            "757/3000 train_loss: 116.9099349975586 test_loss:153.9500274658203\n",
            "758/3000 train_loss: 110.63667297363281 test_loss:142.94149780273438\n",
            "759/3000 train_loss: 115.25091552734375 test_loss:154.07986450195312\n",
            "760/3000 train_loss: 119.34075927734375 test_loss:161.02415466308594\n",
            "761/3000 train_loss: 110.9930419921875 test_loss:152.23825073242188\n",
            "762/3000 train_loss: 119.54286193847656 test_loss:154.43870544433594\n",
            "763/3000 train_loss: 111.21392059326172 test_loss:148.30047607421875\n",
            "764/3000 train_loss: 110.7490463256836 test_loss:153.17733764648438\n",
            "765/3000 train_loss: 117.74388122558594 test_loss:142.19796752929688\n",
            "766/3000 train_loss: 110.81201934814453 test_loss:161.62786865234375\n",
            "767/3000 train_loss: 100.4225845336914 test_loss:148.87921142578125\n",
            "768/3000 train_loss: 104.58708190917969 test_loss:152.16334533691406\n",
            "769/3000 train_loss: 113.2754898071289 test_loss:149.72499084472656\n",
            "770/3000 train_loss: 110.05157470703125 test_loss:148.0224609375\n",
            "771/3000 train_loss: 104.622314453125 test_loss:146.40040588378906\n",
            "772/3000 train_loss: 120.399169921875 test_loss:142.1550750732422\n",
            "773/3000 train_loss: 108.76943969726562 test_loss:157.0357208251953\n",
            "774/3000 train_loss: 113.8431625366211 test_loss:145.92755126953125\n",
            "775/3000 train_loss: 119.21891021728516 test_loss:143.7875213623047\n",
            "776/3000 train_loss: 112.50259399414062 test_loss:138.14950561523438\n",
            "777/3000 train_loss: 114.00807189941406 test_loss:149.81260681152344\n",
            "778/3000 train_loss: 119.72727966308594 test_loss:153.61172485351562\n",
            "779/3000 train_loss: 112.00753021240234 test_loss:150.530029296875\n",
            "780/3000 train_loss: 110.04441833496094 test_loss:146.0689239501953\n",
            "781/3000 train_loss: 123.46404266357422 test_loss:149.56509399414062\n",
            "782/3000 train_loss: 103.25896453857422 test_loss:150.17825317382812\n",
            "783/3000 train_loss: 109.19798278808594 test_loss:158.46099853515625\n",
            "784/3000 train_loss: 110.22885131835938 test_loss:148.31854248046875\n",
            "785/3000 train_loss: 110.83675384521484 test_loss:147.057373046875\n",
            "786/3000 train_loss: 105.92481994628906 test_loss:147.25857543945312\n",
            "787/3000 train_loss: 104.69364929199219 test_loss:147.97007751464844\n",
            "788/3000 train_loss: 99.62490844726562 test_loss:148.144775390625\n",
            "789/3000 train_loss: 106.80645751953125 test_loss:147.6533966064453\n",
            "790/3000 train_loss: 99.03638458251953 test_loss:146.91436767578125\n",
            "791/3000 train_loss: 104.65047454833984 test_loss:154.66220092773438\n",
            "792/3000 train_loss: 101.27930450439453 test_loss:141.64613342285156\n",
            "793/3000 train_loss: 108.2764663696289 test_loss:142.57244873046875\n",
            "794/3000 train_loss: 113.51715087890625 test_loss:135.86036682128906\n",
            "795/3000 train_loss: 107.8200454711914 test_loss:143.76084899902344\n",
            "796/3000 train_loss: 104.43064880371094 test_loss:138.03982543945312\n",
            "797/3000 train_loss: 109.46379089355469 test_loss:144.51861572265625\n",
            "798/3000 train_loss: 108.83882904052734 test_loss:142.85064697265625\n",
            "799/3000 train_loss: 106.89623260498047 test_loss:146.59463500976562\n",
            "800/3000 train_loss: 102.5533676147461 test_loss:148.17852783203125\n",
            "801/3000 train_loss: 104.90074920654297 test_loss:146.16986083984375\n",
            "802/3000 train_loss: 115.96879577636719 test_loss:150.86204528808594\n",
            "803/3000 train_loss: 116.73735046386719 test_loss:149.07762145996094\n",
            "804/3000 train_loss: 96.73023986816406 test_loss:144.60519409179688\n",
            "805/3000 train_loss: 113.27517700195312 test_loss:141.39768981933594\n",
            "806/3000 train_loss: 105.2996826171875 test_loss:143.2291259765625\n",
            "807/3000 train_loss: 101.52842712402344 test_loss:139.63876342773438\n",
            "808/3000 train_loss: 109.18002319335938 test_loss:148.9451141357422\n",
            "809/3000 train_loss: 104.50921630859375 test_loss:146.82713317871094\n",
            "810/3000 train_loss: 108.98761749267578 test_loss:140.61080932617188\n",
            "811/3000 train_loss: 95.72830200195312 test_loss:142.6956024169922\n",
            "812/3000 train_loss: 98.45242309570312 test_loss:143.14614868164062\n",
            "813/3000 train_loss: 113.21043395996094 test_loss:143.3095245361328\n",
            "814/3000 train_loss: 106.99130249023438 test_loss:144.2937774658203\n",
            "815/3000 train_loss: 110.81584167480469 test_loss:141.26878356933594\n",
            "816/3000 train_loss: 100.40984344482422 test_loss:144.04452514648438\n",
            "817/3000 train_loss: 115.7409896850586 test_loss:149.3094940185547\n",
            "818/3000 train_loss: 104.21330261230469 test_loss:141.97998046875\n",
            "819/3000 train_loss: 103.85665893554688 test_loss:151.2046356201172\n",
            "820/3000 train_loss: 105.79057312011719 test_loss:152.8280029296875\n",
            "821/3000 train_loss: 107.38987731933594 test_loss:147.030029296875\n",
            "822/3000 train_loss: 105.39768981933594 test_loss:142.32298278808594\n",
            "823/3000 train_loss: 100.3883056640625 test_loss:142.8083038330078\n",
            "824/3000 train_loss: 105.36038970947266 test_loss:138.2644500732422\n",
            "825/3000 train_loss: 99.9380874633789 test_loss:139.6227569580078\n",
            "826/3000 train_loss: 99.44084167480469 test_loss:148.3721923828125\n",
            "827/3000 train_loss: 100.78611755371094 test_loss:141.6051788330078\n",
            "828/3000 train_loss: 106.18347930908203 test_loss:151.4305877685547\n",
            "829/3000 train_loss: 98.24432373046875 test_loss:152.9750518798828\n",
            "830/3000 train_loss: 94.31472778320312 test_loss:140.2926788330078\n",
            "831/3000 train_loss: 95.43693542480469 test_loss:140.4200897216797\n",
            "832/3000 train_loss: 96.41705322265625 test_loss:137.32154846191406\n",
            "833/3000 train_loss: 105.96083068847656 test_loss:144.89501953125\n",
            "834/3000 train_loss: 107.91133880615234 test_loss:149.68704223632812\n",
            "835/3000 train_loss: 102.1158447265625 test_loss:153.912109375\n",
            "836/3000 train_loss: 101.06584930419922 test_loss:144.38011169433594\n",
            "837/3000 train_loss: 103.0144271850586 test_loss:136.9861602783203\n",
            "838/3000 train_loss: 99.37716674804688 test_loss:154.52102661132812\n",
            "839/3000 train_loss: 92.64962005615234 test_loss:146.14285278320312\n",
            "840/3000 train_loss: 101.27586364746094 test_loss:136.70652770996094\n",
            "841/3000 train_loss: 89.9100112915039 test_loss:139.46881103515625\n",
            "842/3000 train_loss: 101.070068359375 test_loss:149.752197265625\n",
            "843/3000 train_loss: 110.17147064208984 test_loss:137.54437255859375\n",
            "844/3000 train_loss: 124.51309967041016 test_loss:134.3272705078125\n",
            "845/3000 train_loss: 102.77354431152344 test_loss:136.7126922607422\n",
            "846/3000 train_loss: 99.54605865478516 test_loss:131.76870727539062\n",
            "847/3000 train_loss: 100.72499084472656 test_loss:151.22061157226562\n",
            "848/3000 train_loss: 103.35531616210938 test_loss:135.23532104492188\n",
            "849/3000 train_loss: 95.04106903076172 test_loss:130.55323791503906\n",
            "850/3000 train_loss: 105.13551330566406 test_loss:139.71304321289062\n",
            "851/3000 train_loss: 102.18138122558594 test_loss:143.12925720214844\n",
            "852/3000 train_loss: 99.1708755493164 test_loss:144.25296020507812\n",
            "853/3000 train_loss: 110.90142059326172 test_loss:143.89390563964844\n",
            "854/3000 train_loss: 99.21817779541016 test_loss:142.66893005371094\n",
            "855/3000 train_loss: 99.01227569580078 test_loss:143.86988830566406\n",
            "856/3000 train_loss: 98.84834289550781 test_loss:139.29583740234375\n",
            "857/3000 train_loss: 94.50392150878906 test_loss:131.9730987548828\n",
            "858/3000 train_loss: 94.62229919433594 test_loss:139.93197631835938\n",
            "859/3000 train_loss: 97.0323486328125 test_loss:140.4986114501953\n",
            "860/3000 train_loss: 92.72908782958984 test_loss:151.4650115966797\n",
            "861/3000 train_loss: 96.66523742675781 test_loss:138.76441955566406\n",
            "862/3000 train_loss: 100.63786315917969 test_loss:136.2832794189453\n",
            "863/3000 train_loss: 95.21280670166016 test_loss:130.8791961669922\n",
            "864/3000 train_loss: 102.53952026367188 test_loss:137.4547882080078\n",
            "865/3000 train_loss: 100.8838882446289 test_loss:145.74024963378906\n",
            "866/3000 train_loss: 104.4607162475586 test_loss:144.47792053222656\n",
            "867/3000 train_loss: 101.28155517578125 test_loss:147.12669372558594\n",
            "868/3000 train_loss: 97.26858520507812 test_loss:134.24612426757812\n",
            "869/3000 train_loss: 104.29875946044922 test_loss:130.9496307373047\n",
            "870/3000 train_loss: 90.41810607910156 test_loss:129.5230712890625\n",
            "871/3000 train_loss: 92.44974517822266 test_loss:135.05177307128906\n",
            "872/3000 train_loss: 108.8327407836914 test_loss:138.75448608398438\n",
            "873/3000 train_loss: 100.47368621826172 test_loss:136.31781005859375\n",
            "874/3000 train_loss: 100.92142486572266 test_loss:139.96466064453125\n",
            "875/3000 train_loss: 93.98705291748047 test_loss:134.5603485107422\n",
            "876/3000 train_loss: 92.56389617919922 test_loss:134.4466552734375\n",
            "877/3000 train_loss: 104.45661926269531 test_loss:145.8641357421875\n",
            "878/3000 train_loss: 98.08969116210938 test_loss:137.74664306640625\n",
            "879/3000 train_loss: 92.32215881347656 test_loss:145.37205505371094\n",
            "880/3000 train_loss: 81.45532989501953 test_loss:137.9355926513672\n",
            "881/3000 train_loss: 100.2508773803711 test_loss:139.10702514648438\n",
            "882/3000 train_loss: 94.52342987060547 test_loss:147.34384155273438\n",
            "883/3000 train_loss: 94.0712890625 test_loss:139.5474395751953\n",
            "884/3000 train_loss: 87.54180908203125 test_loss:138.82069396972656\n",
            "885/3000 train_loss: 90.62220764160156 test_loss:133.3151397705078\n",
            "886/3000 train_loss: 96.151611328125 test_loss:139.9389190673828\n",
            "887/3000 train_loss: 99.63507080078125 test_loss:138.6322784423828\n",
            "888/3000 train_loss: 99.78898620605469 test_loss:136.18051147460938\n",
            "889/3000 train_loss: 98.37747192382812 test_loss:143.43849182128906\n",
            "890/3000 train_loss: 98.25183868408203 test_loss:138.9559326171875\n",
            "891/3000 train_loss: 99.74796295166016 test_loss:144.9186248779297\n",
            "892/3000 train_loss: 90.37596130371094 test_loss:139.66099548339844\n",
            "893/3000 train_loss: 102.59652709960938 test_loss:136.9399871826172\n",
            "894/3000 train_loss: 96.41383361816406 test_loss:133.73306274414062\n",
            "895/3000 train_loss: 91.63323211669922 test_loss:142.06385803222656\n",
            "896/3000 train_loss: 103.44026184082031 test_loss:130.14010620117188\n",
            "897/3000 train_loss: 98.62882232666016 test_loss:139.11212158203125\n",
            "898/3000 train_loss: 96.11864471435547 test_loss:131.6477508544922\n",
            "899/3000 train_loss: 88.49446868896484 test_loss:145.6253662109375\n",
            "900/3000 train_loss: 89.14068603515625 test_loss:135.4381103515625\n",
            "901/3000 train_loss: 91.14151763916016 test_loss:141.77703857421875\n",
            "902/3000 train_loss: 93.16302490234375 test_loss:138.98439025878906\n",
            "903/3000 train_loss: 91.3763427734375 test_loss:138.47891235351562\n",
            "904/3000 train_loss: 100.71044921875 test_loss:127.50595092773438\n",
            "905/3000 train_loss: 97.53056335449219 test_loss:133.33309936523438\n",
            "906/3000 train_loss: 88.86760711669922 test_loss:134.3231201171875\n",
            "907/3000 train_loss: 87.05172729492188 test_loss:133.3323516845703\n",
            "908/3000 train_loss: 96.58779907226562 test_loss:145.37344360351562\n",
            "909/3000 train_loss: 95.89790344238281 test_loss:141.41702270507812\n",
            "910/3000 train_loss: 100.39471435546875 test_loss:139.46170043945312\n",
            "911/3000 train_loss: 95.48015594482422 test_loss:146.08367919921875\n",
            "912/3000 train_loss: 88.29547119140625 test_loss:139.2783203125\n",
            "913/3000 train_loss: 89.66959381103516 test_loss:136.50088500976562\n",
            "914/3000 train_loss: 92.291259765625 test_loss:146.47434997558594\n",
            "915/3000 train_loss: 84.2637939453125 test_loss:137.65159606933594\n",
            "916/3000 train_loss: 99.37447357177734 test_loss:141.5186767578125\n",
            "917/3000 train_loss: 91.73863220214844 test_loss:140.93519592285156\n",
            "918/3000 train_loss: 91.8521728515625 test_loss:136.65599060058594\n",
            "919/3000 train_loss: 92.74576568603516 test_loss:136.31787109375\n",
            "920/3000 train_loss: 86.03634643554688 test_loss:137.75950622558594\n",
            "921/3000 train_loss: 93.373291015625 test_loss:131.57460021972656\n",
            "922/3000 train_loss: 99.16822814941406 test_loss:123.31329345703125\n",
            "923/3000 train_loss: 99.60050201416016 test_loss:140.84205627441406\n",
            "924/3000 train_loss: 88.07598876953125 test_loss:130.74539184570312\n",
            "925/3000 train_loss: 91.66576385498047 test_loss:134.44168090820312\n",
            "926/3000 train_loss: 98.06874084472656 test_loss:133.93190002441406\n",
            "927/3000 train_loss: 102.13656616210938 test_loss:131.76605224609375\n",
            "928/3000 train_loss: 88.68663787841797 test_loss:127.97509765625\n",
            "929/3000 train_loss: 89.93097686767578 test_loss:130.2161407470703\n",
            "930/3000 train_loss: 98.93125915527344 test_loss:137.30706787109375\n",
            "931/3000 train_loss: 91.66357421875 test_loss:138.45510864257812\n",
            "932/3000 train_loss: 90.7308578491211 test_loss:129.0908966064453\n",
            "933/3000 train_loss: 95.75471496582031 test_loss:136.09661865234375\n",
            "934/3000 train_loss: 98.97134399414062 test_loss:141.54051208496094\n",
            "935/3000 train_loss: 92.9351577758789 test_loss:133.10972595214844\n",
            "936/3000 train_loss: 86.17526245117188 test_loss:134.56182861328125\n",
            "937/3000 train_loss: 93.09577178955078 test_loss:137.69842529296875\n",
            "938/3000 train_loss: 95.83683013916016 test_loss:135.10879516601562\n",
            "939/3000 train_loss: 98.27462005615234 test_loss:133.72689819335938\n",
            "940/3000 train_loss: 88.56792449951172 test_loss:129.29071044921875\n",
            "941/3000 train_loss: 86.03248596191406 test_loss:125.71211242675781\n",
            "942/3000 train_loss: 89.35930633544922 test_loss:137.80357360839844\n",
            "943/3000 train_loss: 83.71702575683594 test_loss:131.5362091064453\n",
            "944/3000 train_loss: 80.06334686279297 test_loss:138.43296813964844\n",
            "945/3000 train_loss: 93.30378723144531 test_loss:123.9936752319336\n",
            "946/3000 train_loss: 91.4695053100586 test_loss:135.8328094482422\n",
            "947/3000 train_loss: 93.64392852783203 test_loss:126.99406433105469\n",
            "948/3000 train_loss: 89.03548431396484 test_loss:130.69473266601562\n",
            "949/3000 train_loss: 90.0236587524414 test_loss:132.6090087890625\n",
            "950/3000 train_loss: 96.12950134277344 test_loss:128.160888671875\n",
            "951/3000 train_loss: 95.73202514648438 test_loss:151.643798828125\n",
            "952/3000 train_loss: 91.72563934326172 test_loss:140.9207305908203\n",
            "953/3000 train_loss: 110.61668395996094 test_loss:136.47509765625\n",
            "954/3000 train_loss: 97.85252380371094 test_loss:129.1268768310547\n",
            "955/3000 train_loss: 87.8564224243164 test_loss:138.48008728027344\n",
            "956/3000 train_loss: 77.44820404052734 test_loss:132.61636352539062\n",
            "957/3000 train_loss: 94.96849822998047 test_loss:128.81991577148438\n",
            "958/3000 train_loss: 98.51746368408203 test_loss:129.28099060058594\n",
            "959/3000 train_loss: 89.75151062011719 test_loss:132.2984161376953\n",
            "960/3000 train_loss: 104.74014282226562 test_loss:147.2209930419922\n",
            "961/3000 train_loss: 86.38663482666016 test_loss:127.59336853027344\n",
            "962/3000 train_loss: 92.69087982177734 test_loss:126.26142883300781\n",
            "963/3000 train_loss: 85.2672119140625 test_loss:134.1752471923828\n",
            "964/3000 train_loss: 82.6488265991211 test_loss:131.3550262451172\n",
            "965/3000 train_loss: 93.84850311279297 test_loss:130.1897735595703\n",
            "966/3000 train_loss: 83.91841125488281 test_loss:122.396728515625\n",
            "967/3000 train_loss: 94.75517272949219 test_loss:138.2461700439453\n",
            "968/3000 train_loss: 89.72260284423828 test_loss:132.06777954101562\n",
            "969/3000 train_loss: 94.60540008544922 test_loss:129.1186981201172\n",
            "970/3000 train_loss: 90.52044677734375 test_loss:131.2660675048828\n",
            "971/3000 train_loss: 85.11947631835938 test_loss:126.0995101928711\n",
            "972/3000 train_loss: 96.66348266601562 test_loss:136.45875549316406\n",
            "973/3000 train_loss: 82.55381774902344 test_loss:124.81755065917969\n",
            "974/3000 train_loss: 83.69691467285156 test_loss:139.81253051757812\n",
            "975/3000 train_loss: 84.1250228881836 test_loss:132.2271270751953\n",
            "976/3000 train_loss: 89.30149841308594 test_loss:129.8372802734375\n",
            "977/3000 train_loss: 92.90174865722656 test_loss:132.39785766601562\n",
            "978/3000 train_loss: 91.38050079345703 test_loss:143.98812866210938\n",
            "979/3000 train_loss: 80.5199966430664 test_loss:132.173828125\n",
            "980/3000 train_loss: 97.19369506835938 test_loss:133.59217834472656\n",
            "981/3000 train_loss: 83.03675842285156 test_loss:126.21529388427734\n",
            "982/3000 train_loss: 86.93124389648438 test_loss:126.10246276855469\n",
            "983/3000 train_loss: 85.79904174804688 test_loss:126.24491882324219\n",
            "984/3000 train_loss: 81.25817108154297 test_loss:121.59870910644531\n",
            "985/3000 train_loss: 93.35630798339844 test_loss:131.7462921142578\n",
            "986/3000 train_loss: 84.8775634765625 test_loss:121.82211303710938\n",
            "987/3000 train_loss: 78.38337707519531 test_loss:132.6551055908203\n",
            "988/3000 train_loss: 87.77227020263672 test_loss:125.21807098388672\n",
            "989/3000 train_loss: 93.08363342285156 test_loss:145.20985412597656\n",
            "990/3000 train_loss: 88.77429962158203 test_loss:118.78997802734375\n",
            "991/3000 train_loss: 92.9620361328125 test_loss:131.39808654785156\n",
            "992/3000 train_loss: 91.70475006103516 test_loss:126.42125701904297\n",
            "993/3000 train_loss: 83.6421127319336 test_loss:124.78465270996094\n",
            "994/3000 train_loss: 78.16065979003906 test_loss:127.38723754882812\n",
            "995/3000 train_loss: 87.793212890625 test_loss:129.90213012695312\n",
            "996/3000 train_loss: 77.45707702636719 test_loss:128.9297637939453\n",
            "997/3000 train_loss: 89.6974868774414 test_loss:133.41725158691406\n",
            "998/3000 train_loss: 86.8751220703125 test_loss:132.15774536132812\n",
            "999/3000 train_loss: 91.93592071533203 test_loss:131.4880828857422\n",
            "1000/3000 train_loss: 87.51998901367188 test_loss:132.19854736328125\n",
            "1001/3000 train_loss: 92.06622314453125 test_loss:125.63343048095703\n",
            "1002/3000 train_loss: 84.28583526611328 test_loss:126.27299499511719\n",
            "1003/3000 train_loss: 85.51232147216797 test_loss:129.8971405029297\n",
            "1004/3000 train_loss: 91.56151580810547 test_loss:123.19967651367188\n",
            "1005/3000 train_loss: 91.22035217285156 test_loss:128.98883056640625\n",
            "1006/3000 train_loss: 81.34526062011719 test_loss:126.28599548339844\n",
            "1007/3000 train_loss: 78.02173614501953 test_loss:135.5543212890625\n",
            "1008/3000 train_loss: 87.98112487792969 test_loss:122.92079162597656\n",
            "1009/3000 train_loss: 81.23030853271484 test_loss:122.23918151855469\n",
            "1010/3000 train_loss: 77.70751953125 test_loss:123.34001159667969\n",
            "1011/3000 train_loss: 92.86186981201172 test_loss:124.15328979492188\n",
            "1012/3000 train_loss: 89.00083923339844 test_loss:141.77194213867188\n",
            "1013/3000 train_loss: 87.42364501953125 test_loss:124.10311126708984\n",
            "1014/3000 train_loss: 85.5150375366211 test_loss:123.13265991210938\n",
            "1015/3000 train_loss: 74.88520812988281 test_loss:126.90919494628906\n",
            "1016/3000 train_loss: 90.4310073852539 test_loss:129.81787109375\n",
            "1017/3000 train_loss: 83.22486877441406 test_loss:130.50274658203125\n",
            "1018/3000 train_loss: 82.15409088134766 test_loss:122.44206237792969\n",
            "1019/3000 train_loss: 90.22348022460938 test_loss:126.10044860839844\n",
            "1020/3000 train_loss: 81.74972534179688 test_loss:140.4959259033203\n",
            "1021/3000 train_loss: 85.758544921875 test_loss:124.58221435546875\n",
            "1022/3000 train_loss: 85.68017578125 test_loss:129.36048889160156\n",
            "1023/3000 train_loss: 87.9443588256836 test_loss:127.66266632080078\n",
            "1024/3000 train_loss: 79.97518920898438 test_loss:128.16107177734375\n",
            "1025/3000 train_loss: 89.4916763305664 test_loss:129.48626708984375\n",
            "1026/3000 train_loss: 86.11711120605469 test_loss:127.26481628417969\n",
            "1027/3000 train_loss: 82.00080108642578 test_loss:127.0072021484375\n",
            "1028/3000 train_loss: 91.91584014892578 test_loss:135.56285095214844\n",
            "1029/3000 train_loss: 83.03755187988281 test_loss:121.05006408691406\n",
            "1030/3000 train_loss: 76.38671112060547 test_loss:128.9358367919922\n",
            "1031/3000 train_loss: 72.88948822021484 test_loss:120.53160095214844\n",
            "1032/3000 train_loss: 75.78330993652344 test_loss:119.9393539428711\n",
            "1033/3000 train_loss: 85.58428192138672 test_loss:127.25782012939453\n",
            "1034/3000 train_loss: 87.95062255859375 test_loss:120.99652099609375\n",
            "1035/3000 train_loss: 81.50567626953125 test_loss:123.76600646972656\n",
            "1036/3000 train_loss: 87.44117736816406 test_loss:124.39944458007812\n",
            "1037/3000 train_loss: 91.63719177246094 test_loss:133.0000457763672\n",
            "1038/3000 train_loss: 85.80460357666016 test_loss:127.87971496582031\n",
            "1039/3000 train_loss: 97.13417053222656 test_loss:124.57392883300781\n",
            "1040/3000 train_loss: 78.99869537353516 test_loss:123.97845458984375\n",
            "1041/3000 train_loss: 83.90568542480469 test_loss:117.7231216430664\n",
            "1042/3000 train_loss: 78.9807357788086 test_loss:120.49610900878906\n",
            "1043/3000 train_loss: 80.46056365966797 test_loss:121.72833251953125\n",
            "1044/3000 train_loss: 73.87591552734375 test_loss:108.05096435546875\n",
            "1045/3000 train_loss: 88.5551986694336 test_loss:119.70958709716797\n",
            "1046/3000 train_loss: 86.3541259765625 test_loss:134.32337951660156\n",
            "1047/3000 train_loss: 90.80404663085938 test_loss:116.051513671875\n",
            "1048/3000 train_loss: 80.310546875 test_loss:123.47319793701172\n",
            "1049/3000 train_loss: 75.3719711303711 test_loss:126.08673095703125\n",
            "1050/3000 train_loss: 91.90694427490234 test_loss:125.4466781616211\n",
            "1051/3000 train_loss: 87.8892822265625 test_loss:144.409912109375\n",
            "1052/3000 train_loss: 86.80828094482422 test_loss:127.36213684082031\n",
            "1053/3000 train_loss: 79.88180541992188 test_loss:122.04435729980469\n",
            "1054/3000 train_loss: 75.76132202148438 test_loss:116.4393310546875\n",
            "1055/3000 train_loss: 80.08245849609375 test_loss:119.08463287353516\n",
            "1056/3000 train_loss: 81.73147583007812 test_loss:137.84866333007812\n",
            "1057/3000 train_loss: 81.4713363647461 test_loss:129.0084228515625\n",
            "1058/3000 train_loss: 81.61821746826172 test_loss:125.8021240234375\n",
            "1059/3000 train_loss: 79.40933990478516 test_loss:114.93551635742188\n",
            "1060/3000 train_loss: 79.55323791503906 test_loss:124.29561614990234\n",
            "1061/3000 train_loss: 81.9018325805664 test_loss:129.13426208496094\n",
            "1062/3000 train_loss: 89.2297134399414 test_loss:111.3728256225586\n",
            "1063/3000 train_loss: 84.29254913330078 test_loss:116.79913330078125\n",
            "1064/3000 train_loss: 77.76346588134766 test_loss:115.82742309570312\n",
            "1065/3000 train_loss: 83.63191986083984 test_loss:127.30621337890625\n",
            "1066/3000 train_loss: 80.26824951171875 test_loss:126.25349426269531\n",
            "1067/3000 train_loss: 78.92448425292969 test_loss:125.38043975830078\n",
            "1068/3000 train_loss: 88.19660186767578 test_loss:133.24319458007812\n",
            "1069/3000 train_loss: 99.88803100585938 test_loss:143.7371368408203\n",
            "1070/3000 train_loss: 85.67782592773438 test_loss:127.6021957397461\n",
            "1071/3000 train_loss: 72.39630126953125 test_loss:121.56831359863281\n",
            "1072/3000 train_loss: 78.19290161132812 test_loss:121.6162109375\n",
            "1073/3000 train_loss: 78.0302734375 test_loss:126.89611053466797\n",
            "1074/3000 train_loss: 81.07732391357422 test_loss:125.85643005371094\n",
            "1075/3000 train_loss: 79.32002258300781 test_loss:121.25604248046875\n",
            "1076/3000 train_loss: 82.0819320678711 test_loss:117.70206451416016\n",
            "1077/3000 train_loss: 88.5213623046875 test_loss:128.6448211669922\n",
            "1078/3000 train_loss: 80.25421142578125 test_loss:120.97733306884766\n",
            "1079/3000 train_loss: 70.59637451171875 test_loss:123.92034912109375\n",
            "1080/3000 train_loss: 79.46624755859375 test_loss:116.55644226074219\n",
            "1081/3000 train_loss: 70.4750747680664 test_loss:120.90287780761719\n",
            "1082/3000 train_loss: 74.01973724365234 test_loss:118.56843566894531\n",
            "1083/3000 train_loss: 80.31260681152344 test_loss:126.260498046875\n",
            "1084/3000 train_loss: 78.63822174072266 test_loss:135.84739685058594\n",
            "1085/3000 train_loss: 78.88593292236328 test_loss:122.82122802734375\n",
            "1086/3000 train_loss: 80.58428955078125 test_loss:125.42198944091797\n",
            "1087/3000 train_loss: 81.36649322509766 test_loss:120.87361907958984\n",
            "1088/3000 train_loss: 76.38121795654297 test_loss:121.6962890625\n",
            "1089/3000 train_loss: 81.61540222167969 test_loss:119.27286529541016\n",
            "1090/3000 train_loss: 85.38326263427734 test_loss:122.45777893066406\n",
            "1091/3000 train_loss: 83.41324615478516 test_loss:124.38292694091797\n",
            "1092/3000 train_loss: 85.0718002319336 test_loss:130.19589233398438\n",
            "1093/3000 train_loss: 82.09028625488281 test_loss:127.1907958984375\n",
            "1094/3000 train_loss: 90.39067840576172 test_loss:112.11752319335938\n",
            "1095/3000 train_loss: 89.19613647460938 test_loss:126.80847930908203\n",
            "1096/3000 train_loss: 76.54119110107422 test_loss:119.31318664550781\n",
            "1097/3000 train_loss: 83.54097747802734 test_loss:134.23104858398438\n",
            "1098/3000 train_loss: 95.61888885498047 test_loss:120.4783935546875\n",
            "1099/3000 train_loss: 79.12674713134766 test_loss:122.04130554199219\n",
            "1100/3000 train_loss: 71.01802825927734 test_loss:119.07472229003906\n",
            "1101/3000 train_loss: 69.22064208984375 test_loss:117.25402069091797\n",
            "1102/3000 train_loss: 77.92044830322266 test_loss:118.15934753417969\n",
            "1103/3000 train_loss: 88.6224594116211 test_loss:115.7010726928711\n",
            "1104/3000 train_loss: 80.31491088867188 test_loss:123.33690643310547\n",
            "1105/3000 train_loss: 77.8553466796875 test_loss:116.13331604003906\n",
            "1106/3000 train_loss: 73.46414947509766 test_loss:119.02366638183594\n",
            "1107/3000 train_loss: 78.73912811279297 test_loss:126.4521255493164\n",
            "1108/3000 train_loss: 78.73989868164062 test_loss:116.29351806640625\n",
            "1109/3000 train_loss: 76.16526794433594 test_loss:111.14323425292969\n",
            "1110/3000 train_loss: 74.1535415649414 test_loss:129.18727111816406\n",
            "1111/3000 train_loss: 74.89749908447266 test_loss:121.18865966796875\n",
            "1112/3000 train_loss: 68.36106872558594 test_loss:119.46100616455078\n",
            "1113/3000 train_loss: 79.6661148071289 test_loss:117.35592651367188\n",
            "1114/3000 train_loss: 83.80838012695312 test_loss:128.54208374023438\n",
            "1115/3000 train_loss: 82.76081085205078 test_loss:127.04415893554688\n",
            "1116/3000 train_loss: 75.09258270263672 test_loss:123.36629486083984\n",
            "1117/3000 train_loss: 73.50215911865234 test_loss:120.95942687988281\n",
            "1118/3000 train_loss: 88.7424087524414 test_loss:136.65809631347656\n",
            "1119/3000 train_loss: 71.5178451538086 test_loss:121.7337417602539\n",
            "1120/3000 train_loss: 71.5864486694336 test_loss:127.4323501586914\n",
            "1121/3000 train_loss: 70.50190734863281 test_loss:116.60232543945312\n",
            "1122/3000 train_loss: 77.70257568359375 test_loss:143.31005859375\n",
            "1123/3000 train_loss: 85.10111999511719 test_loss:125.56163024902344\n",
            "1124/3000 train_loss: 77.64730834960938 test_loss:124.30610656738281\n",
            "1125/3000 train_loss: 78.12727355957031 test_loss:126.31914520263672\n",
            "1126/3000 train_loss: 76.5107650756836 test_loss:119.60031127929688\n",
            "1127/3000 train_loss: 77.7935562133789 test_loss:118.18207550048828\n",
            "1128/3000 train_loss: 95.34671783447266 test_loss:125.53256225585938\n",
            "1129/3000 train_loss: 79.54033660888672 test_loss:129.1094970703125\n",
            "1130/3000 train_loss: 80.8271255493164 test_loss:130.7777099609375\n",
            "1131/3000 train_loss: 77.1097183227539 test_loss:126.5245361328125\n",
            "1132/3000 train_loss: 84.21430206298828 test_loss:122.27849578857422\n",
            "1133/3000 train_loss: 79.36389923095703 test_loss:127.14447021484375\n",
            "1134/3000 train_loss: 77.39025115966797 test_loss:122.19445037841797\n",
            "1135/3000 train_loss: 69.27924346923828 test_loss:117.19895935058594\n",
            "1136/3000 train_loss: 82.78892517089844 test_loss:133.4978790283203\n",
            "1137/3000 train_loss: 69.63029479980469 test_loss:126.95105743408203\n",
            "1138/3000 train_loss: 85.45645141601562 test_loss:125.75723266601562\n",
            "1139/3000 train_loss: 87.30931854248047 test_loss:129.76466369628906\n",
            "1140/3000 train_loss: 73.69207000732422 test_loss:127.92181396484375\n",
            "1141/3000 train_loss: 83.01612854003906 test_loss:125.6611328125\n",
            "1142/3000 train_loss: 91.07896423339844 test_loss:136.3946533203125\n",
            "1143/3000 train_loss: 86.9496078491211 test_loss:143.8063507080078\n",
            "1144/3000 train_loss: 72.51688385009766 test_loss:124.57669067382812\n",
            "1145/3000 train_loss: 77.8836441040039 test_loss:126.62579345703125\n",
            "1146/3000 train_loss: 71.86637878417969 test_loss:119.90631103515625\n",
            "1147/3000 train_loss: 86.22230529785156 test_loss:132.82009887695312\n",
            "1148/3000 train_loss: 74.31868743896484 test_loss:135.79330444335938\n",
            "1149/3000 train_loss: 68.36634063720703 test_loss:119.71688079833984\n",
            "1150/3000 train_loss: 91.8862533569336 test_loss:126.27117156982422\n",
            "1151/3000 train_loss: 82.42155456542969 test_loss:134.38714599609375\n",
            "1152/3000 train_loss: 83.40702819824219 test_loss:129.85377502441406\n",
            "1153/3000 train_loss: 82.11128234863281 test_loss:119.95697021484375\n",
            "1154/3000 train_loss: 71.70243835449219 test_loss:121.8553695678711\n",
            "1155/3000 train_loss: 74.18169403076172 test_loss:122.87171936035156\n",
            "1156/3000 train_loss: 76.20772552490234 test_loss:116.32989501953125\n",
            "1157/3000 train_loss: 73.82019805908203 test_loss:121.86485290527344\n",
            "1158/3000 train_loss: 83.9349136352539 test_loss:132.11473083496094\n",
            "1159/3000 train_loss: 66.38574981689453 test_loss:116.0791244506836\n",
            "1160/3000 train_loss: 73.1434326171875 test_loss:119.23919677734375\n",
            "1161/3000 train_loss: 79.6627197265625 test_loss:121.91835021972656\n",
            "1162/3000 train_loss: 76.52700805664062 test_loss:122.33688354492188\n",
            "1163/3000 train_loss: 69.663330078125 test_loss:130.04721069335938\n",
            "1164/3000 train_loss: 76.18122100830078 test_loss:119.83635711669922\n",
            "1165/3000 train_loss: 72.20259094238281 test_loss:129.19046020507812\n",
            "1166/3000 train_loss: 86.94654846191406 test_loss:122.49830627441406\n",
            "1167/3000 train_loss: 78.61093139648438 test_loss:121.49127197265625\n",
            "1168/3000 train_loss: 73.41805267333984 test_loss:125.75298309326172\n",
            "1169/3000 train_loss: 69.99137878417969 test_loss:112.07853698730469\n",
            "1170/3000 train_loss: 75.3709945678711 test_loss:108.28150939941406\n",
            "1171/3000 train_loss: 69.85528564453125 test_loss:126.57539367675781\n",
            "1172/3000 train_loss: 71.38951873779297 test_loss:118.39675903320312\n",
            "1173/3000 train_loss: 76.2828598022461 test_loss:117.14657592773438\n",
            "1174/3000 train_loss: 74.17085266113281 test_loss:126.2901382446289\n",
            "1175/3000 train_loss: 83.53824615478516 test_loss:120.54457092285156\n",
            "1176/3000 train_loss: 67.22125244140625 test_loss:116.72384643554688\n",
            "1177/3000 train_loss: 65.25086212158203 test_loss:124.30062866210938\n",
            "1178/3000 train_loss: 78.96025085449219 test_loss:126.71039581298828\n",
            "1179/3000 train_loss: 75.13495635986328 test_loss:117.35441589355469\n",
            "1180/3000 train_loss: 66.37394714355469 test_loss:130.76681518554688\n",
            "1181/3000 train_loss: 68.56310272216797 test_loss:118.14795684814453\n",
            "1182/3000 train_loss: 80.60050201416016 test_loss:120.96649932861328\n",
            "1183/3000 train_loss: 66.830810546875 test_loss:126.34736633300781\n",
            "1184/3000 train_loss: 79.62088775634766 test_loss:123.7777328491211\n",
            "1185/3000 train_loss: 71.56009674072266 test_loss:119.64353942871094\n",
            "1186/3000 train_loss: 73.43978118896484 test_loss:121.67118835449219\n",
            "1187/3000 train_loss: 72.83599853515625 test_loss:112.30271911621094\n",
            "1188/3000 train_loss: 78.13936614990234 test_loss:122.80039978027344\n",
            "1189/3000 train_loss: 80.25128173828125 test_loss:110.65771484375\n",
            "1190/3000 train_loss: 89.68372344970703 test_loss:121.62481689453125\n",
            "1191/3000 train_loss: 72.66226196289062 test_loss:120.92830657958984\n",
            "1192/3000 train_loss: 80.06928253173828 test_loss:127.56891632080078\n",
            "1193/3000 train_loss: 71.86415100097656 test_loss:117.21282958984375\n",
            "1194/3000 train_loss: 77.56244659423828 test_loss:122.89299774169922\n",
            "1195/3000 train_loss: 78.4642105102539 test_loss:118.01018524169922\n",
            "1196/3000 train_loss: 77.4154281616211 test_loss:110.00131225585938\n",
            "1197/3000 train_loss: 82.93191528320312 test_loss:124.26573181152344\n",
            "1198/3000 train_loss: 77.39059448242188 test_loss:119.9404067993164\n",
            "1199/3000 train_loss: 70.85787963867188 test_loss:124.81136322021484\n",
            "1200/3000 train_loss: 68.59439849853516 test_loss:115.84532928466797\n",
            "1201/3000 train_loss: 80.05239868164062 test_loss:120.11442565917969\n",
            "1202/3000 train_loss: 78.83647918701172 test_loss:121.11717224121094\n",
            "1203/3000 train_loss: 74.7154312133789 test_loss:111.00848388671875\n",
            "1204/3000 train_loss: 75.11905670166016 test_loss:116.86860656738281\n",
            "1205/3000 train_loss: 76.45388793945312 test_loss:117.62792205810547\n",
            "1206/3000 train_loss: 78.8842544555664 test_loss:133.69444274902344\n",
            "1207/3000 train_loss: 89.75700378417969 test_loss:116.86204528808594\n",
            "1208/3000 train_loss: 71.06708526611328 test_loss:121.34394073486328\n",
            "1209/3000 train_loss: 77.47064208984375 test_loss:117.54193115234375\n",
            "1210/3000 train_loss: 75.0877914428711 test_loss:129.9353790283203\n",
            "1211/3000 train_loss: 79.05941772460938 test_loss:118.18769073486328\n",
            "1212/3000 train_loss: 81.2590103149414 test_loss:122.8250503540039\n",
            "1213/3000 train_loss: 78.75271606445312 test_loss:118.11329650878906\n",
            "1214/3000 train_loss: 88.66210174560547 test_loss:121.34852600097656\n",
            "1215/3000 train_loss: 69.61627960205078 test_loss:117.40046691894531\n",
            "1216/3000 train_loss: 84.67015075683594 test_loss:125.82609558105469\n",
            "1217/3000 train_loss: 79.31466674804688 test_loss:110.81198120117188\n",
            "1218/3000 train_loss: 64.4009017944336 test_loss:110.95832061767578\n",
            "1219/3000 train_loss: 70.10726165771484 test_loss:126.53843688964844\n",
            "1220/3000 train_loss: 75.5714111328125 test_loss:122.48628234863281\n",
            "1221/3000 train_loss: 70.22515869140625 test_loss:120.9571762084961\n",
            "1222/3000 train_loss: 67.53661346435547 test_loss:126.11541748046875\n",
            "1223/3000 train_loss: 70.49347686767578 test_loss:122.97328186035156\n",
            "1224/3000 train_loss: 75.54743194580078 test_loss:115.94993591308594\n",
            "1225/3000 train_loss: 83.2110366821289 test_loss:114.53607940673828\n",
            "1226/3000 train_loss: 66.18466186523438 test_loss:120.27262878417969\n",
            "1227/3000 train_loss: 73.40778350830078 test_loss:120.37628173828125\n",
            "1228/3000 train_loss: 64.44669342041016 test_loss:116.21620178222656\n",
            "1229/3000 train_loss: 85.45420837402344 test_loss:111.15308380126953\n",
            "1230/3000 train_loss: 81.55332946777344 test_loss:117.5799560546875\n",
            "1231/3000 train_loss: 70.02748107910156 test_loss:116.44964599609375\n",
            "1232/3000 train_loss: 80.33687591552734 test_loss:139.32847595214844\n",
            "1233/3000 train_loss: 80.68543243408203 test_loss:123.97721099853516\n",
            "1234/3000 train_loss: 68.85777282714844 test_loss:122.68966674804688\n",
            "1235/3000 train_loss: 69.1741714477539 test_loss:115.12315368652344\n",
            "1236/3000 train_loss: 74.15612030029297 test_loss:117.35250854492188\n",
            "1237/3000 train_loss: 76.17558288574219 test_loss:116.21873474121094\n",
            "1238/3000 train_loss: 78.05777740478516 test_loss:121.62600708007812\n",
            "1239/3000 train_loss: 78.40595245361328 test_loss:121.37310028076172\n",
            "1240/3000 train_loss: 85.28546142578125 test_loss:136.95095825195312\n",
            "1241/3000 train_loss: 81.73729705810547 test_loss:113.17106628417969\n",
            "1242/3000 train_loss: 82.56954193115234 test_loss:124.05216979980469\n",
            "1243/3000 train_loss: 68.00297546386719 test_loss:113.8147964477539\n",
            "1244/3000 train_loss: 69.7779312133789 test_loss:112.96664428710938\n",
            "1245/3000 train_loss: 73.99565887451172 test_loss:115.99494934082031\n",
            "1246/3000 train_loss: 69.8214111328125 test_loss:111.56391906738281\n",
            "1247/3000 train_loss: 64.18645477294922 test_loss:114.93429565429688\n",
            "1248/3000 train_loss: 71.17266082763672 test_loss:115.68461608886719\n",
            "1249/3000 train_loss: 73.84403228759766 test_loss:127.6328125\n",
            "1250/3000 train_loss: 68.86644744873047 test_loss:118.67396545410156\n",
            "1251/3000 train_loss: 74.65943908691406 test_loss:125.98702239990234\n",
            "1252/3000 train_loss: 73.28160095214844 test_loss:125.96800231933594\n",
            "1253/3000 train_loss: 69.46437072753906 test_loss:119.50878143310547\n",
            "1254/3000 train_loss: 68.5545425415039 test_loss:135.1898956298828\n",
            "1255/3000 train_loss: 69.08621215820312 test_loss:112.98137664794922\n",
            "1256/3000 train_loss: 73.27702331542969 test_loss:118.4330062866211\n",
            "1257/3000 train_loss: 73.78428649902344 test_loss:123.64205169677734\n",
            "1258/3000 train_loss: 77.77935791015625 test_loss:117.0073013305664\n",
            "1259/3000 train_loss: 70.02322387695312 test_loss:111.0498275756836\n",
            "1260/3000 train_loss: 72.34593200683594 test_loss:120.53145599365234\n",
            "1261/3000 train_loss: 71.61094665527344 test_loss:124.37468719482422\n",
            "1262/3000 train_loss: 79.01789093017578 test_loss:123.2559585571289\n",
            "1263/3000 train_loss: 73.91053771972656 test_loss:114.64055633544922\n",
            "1264/3000 train_loss: 69.5853500366211 test_loss:124.95347595214844\n",
            "1265/3000 train_loss: 80.84645080566406 test_loss:113.37646484375\n",
            "1266/3000 train_loss: 70.32426452636719 test_loss:108.38481140136719\n",
            "1267/3000 train_loss: 73.93598937988281 test_loss:117.279296875\n",
            "1268/3000 train_loss: 70.88382720947266 test_loss:115.59281158447266\n",
            "1269/3000 train_loss: 68.05464172363281 test_loss:118.35682678222656\n",
            "1270/3000 train_loss: 68.00057220458984 test_loss:113.86929321289062\n",
            "1271/3000 train_loss: 70.38945007324219 test_loss:110.08544921875\n",
            "1272/3000 train_loss: 72.04501342773438 test_loss:119.84821319580078\n",
            "1273/3000 train_loss: 68.45805358886719 test_loss:116.92025756835938\n",
            "1274/3000 train_loss: 63.218955993652344 test_loss:120.72506713867188\n",
            "1275/3000 train_loss: 71.52545166015625 test_loss:117.67403411865234\n",
            "1276/3000 train_loss: 67.0604476928711 test_loss:128.77801513671875\n",
            "1277/3000 train_loss: 78.0277328491211 test_loss:122.52568054199219\n",
            "1278/3000 train_loss: 63.355865478515625 test_loss:116.4786605834961\n",
            "1279/3000 train_loss: 77.45188903808594 test_loss:125.08082580566406\n",
            "1280/3000 train_loss: 66.78236389160156 test_loss:133.3792724609375\n",
            "1281/3000 train_loss: 70.84358215332031 test_loss:120.34005737304688\n",
            "1282/3000 train_loss: 78.50627899169922 test_loss:121.54753112792969\n",
            "1283/3000 train_loss: 77.23439025878906 test_loss:117.44930267333984\n",
            "1284/3000 train_loss: 75.22245788574219 test_loss:128.43226623535156\n",
            "1285/3000 train_loss: 68.53121185302734 test_loss:119.50880432128906\n",
            "1286/3000 train_loss: 66.37000274658203 test_loss:115.11332702636719\n",
            "1287/3000 train_loss: 74.37477111816406 test_loss:126.92521667480469\n",
            "1288/3000 train_loss: 76.27997589111328 test_loss:125.08206939697266\n",
            "1289/3000 train_loss: 77.4761734008789 test_loss:126.67076110839844\n",
            "1290/3000 train_loss: 73.14506530761719 test_loss:125.38592529296875\n",
            "1291/3000 train_loss: 66.56071472167969 test_loss:110.33697509765625\n",
            "1292/3000 train_loss: 66.73504638671875 test_loss:119.04766845703125\n",
            "1293/3000 train_loss: 70.03120422363281 test_loss:117.52311706542969\n",
            "1294/3000 train_loss: 67.91661834716797 test_loss:112.64712524414062\n",
            "1295/3000 train_loss: 65.65143585205078 test_loss:118.17461395263672\n",
            "1296/3000 train_loss: 74.14253234863281 test_loss:112.66856384277344\n",
            "1297/3000 train_loss: 68.29828643798828 test_loss:118.21673583984375\n",
            "1298/3000 train_loss: 74.37105560302734 test_loss:124.04063415527344\n",
            "1299/3000 train_loss: 71.45950317382812 test_loss:111.2305679321289\n",
            "1300/3000 train_loss: 62.57948684692383 test_loss:124.44403076171875\n",
            "1301/3000 train_loss: 76.75397491455078 test_loss:119.62399291992188\n",
            "1302/3000 train_loss: 74.23538208007812 test_loss:117.38385009765625\n",
            "1303/3000 train_loss: 71.06192016601562 test_loss:111.98595428466797\n",
            "1304/3000 train_loss: 72.71672821044922 test_loss:124.90498352050781\n",
            "1305/3000 train_loss: 64.91102600097656 test_loss:116.87150573730469\n",
            "1306/3000 train_loss: 65.3689193725586 test_loss:124.6138916015625\n",
            "1307/3000 train_loss: 77.59375762939453 test_loss:113.24943542480469\n",
            "1308/3000 train_loss: 74.71100616455078 test_loss:119.76188659667969\n",
            "1309/3000 train_loss: 61.337100982666016 test_loss:116.64720153808594\n",
            "1310/3000 train_loss: 71.30230712890625 test_loss:124.4599609375\n",
            "1311/3000 train_loss: 83.38114166259766 test_loss:115.70652770996094\n",
            "1312/3000 train_loss: 70.54601287841797 test_loss:129.66026306152344\n",
            "1313/3000 train_loss: 67.00723266601562 test_loss:116.9853286743164\n",
            "1314/3000 train_loss: 85.72976684570312 test_loss:114.12307739257812\n",
            "1315/3000 train_loss: 73.24034881591797 test_loss:133.34234619140625\n",
            "1316/3000 train_loss: 65.89344787597656 test_loss:119.91371154785156\n",
            "1317/3000 train_loss: 66.80156707763672 test_loss:117.30020904541016\n",
            "1318/3000 train_loss: 75.06491088867188 test_loss:120.79725646972656\n",
            "1319/3000 train_loss: 80.07303619384766 test_loss:116.47084045410156\n",
            "1320/3000 train_loss: 67.5383071899414 test_loss:114.6923828125\n",
            "1321/3000 train_loss: 63.7882194519043 test_loss:125.47125244140625\n",
            "1322/3000 train_loss: 69.638427734375 test_loss:115.97920989990234\n",
            "1323/3000 train_loss: 71.45957946777344 test_loss:121.9701156616211\n",
            "1324/3000 train_loss: 63.24696731567383 test_loss:114.94937896728516\n",
            "1325/3000 train_loss: 64.63336181640625 test_loss:118.18293762207031\n",
            "1326/3000 train_loss: 69.78568267822266 test_loss:107.3082275390625\n",
            "1327/3000 train_loss: 65.4261703491211 test_loss:106.23529052734375\n",
            "1328/3000 train_loss: 84.1052474975586 test_loss:120.23247528076172\n",
            "1329/3000 train_loss: 66.46111297607422 test_loss:111.59955596923828\n",
            "1330/3000 train_loss: 74.65116882324219 test_loss:111.39042663574219\n",
            "1331/3000 train_loss: 63.65679931640625 test_loss:117.12924194335938\n",
            "1332/3000 train_loss: 70.48174285888672 test_loss:122.18194580078125\n",
            "1333/3000 train_loss: 83.28279113769531 test_loss:128.05809020996094\n",
            "1334/3000 train_loss: 72.28849029541016 test_loss:124.72373962402344\n",
            "1335/3000 train_loss: 68.08108520507812 test_loss:119.98175048828125\n",
            "1336/3000 train_loss: 74.88719940185547 test_loss:115.58674621582031\n",
            "1337/3000 train_loss: 77.78247833251953 test_loss:123.632568359375\n",
            "1338/3000 train_loss: 72.18970489501953 test_loss:109.23118591308594\n",
            "1339/3000 train_loss: 66.48931884765625 test_loss:116.13943481445312\n",
            "1340/3000 train_loss: 66.23395538330078 test_loss:110.80070495605469\n",
            "1341/3000 train_loss: 61.5069465637207 test_loss:118.71470642089844\n",
            "1342/3000 train_loss: 67.68988037109375 test_loss:114.64349365234375\n",
            "1343/3000 train_loss: 79.95508575439453 test_loss:148.57022094726562\n",
            "1344/3000 train_loss: 73.02522277832031 test_loss:117.50399780273438\n",
            "1345/3000 train_loss: 60.784271240234375 test_loss:115.34437561035156\n",
            "1346/3000 train_loss: 68.8272705078125 test_loss:136.0770721435547\n",
            "1347/3000 train_loss: 68.8880844116211 test_loss:115.5859146118164\n",
            "1348/3000 train_loss: 73.1668472290039 test_loss:114.72126007080078\n",
            "1349/3000 train_loss: 80.22865295410156 test_loss:122.59211730957031\n",
            "1350/3000 train_loss: 71.39356994628906 test_loss:124.28350830078125\n",
            "1351/3000 train_loss: 66.2049789428711 test_loss:117.90917205810547\n",
            "1352/3000 train_loss: 73.95084381103516 test_loss:128.5968017578125\n",
            "1353/3000 train_loss: 57.038047790527344 test_loss:115.07804870605469\n",
            "1354/3000 train_loss: 62.45539474487305 test_loss:114.1297378540039\n",
            "1355/3000 train_loss: 71.36058807373047 test_loss:111.42296600341797\n",
            "1356/3000 train_loss: 75.60081481933594 test_loss:115.6721420288086\n",
            "1357/3000 train_loss: 65.02003479003906 test_loss:115.18213653564453\n",
            "1358/3000 train_loss: 64.33331298828125 test_loss:121.74885559082031\n",
            "1359/3000 train_loss: 68.34075164794922 test_loss:112.11376190185547\n",
            "1360/3000 train_loss: 70.25349426269531 test_loss:122.14079284667969\n",
            "1361/3000 train_loss: 67.97039794921875 test_loss:106.72439575195312\n",
            "1362/3000 train_loss: 70.8663101196289 test_loss:118.49002075195312\n",
            "1363/3000 train_loss: 73.29991912841797 test_loss:118.68278503417969\n",
            "1364/3000 train_loss: 68.01533508300781 test_loss:121.12797546386719\n",
            "1365/3000 train_loss: 64.45648956298828 test_loss:106.22276306152344\n",
            "1366/3000 train_loss: 76.40055084228516 test_loss:125.66431427001953\n",
            "1367/3000 train_loss: 69.427001953125 test_loss:114.95762634277344\n",
            "1368/3000 train_loss: 66.09513854980469 test_loss:114.91423034667969\n",
            "1369/3000 train_loss: 76.10601806640625 test_loss:125.76245880126953\n",
            "1370/3000 train_loss: 73.28115844726562 test_loss:116.61172485351562\n",
            "1371/3000 train_loss: 70.0087890625 test_loss:122.20220184326172\n",
            "1372/3000 train_loss: 65.46009826660156 test_loss:103.3747787475586\n",
            "1373/3000 train_loss: 67.60279846191406 test_loss:117.77392578125\n",
            "1374/3000 train_loss: 72.40457153320312 test_loss:116.46434783935547\n",
            "1375/3000 train_loss: 66.49274444580078 test_loss:118.94092559814453\n",
            "1376/3000 train_loss: 70.2042007446289 test_loss:114.3237533569336\n",
            "1377/3000 train_loss: 73.37332153320312 test_loss:116.2854995727539\n",
            "1378/3000 train_loss: 67.28129577636719 test_loss:122.26939392089844\n",
            "1379/3000 train_loss: 66.40928649902344 test_loss:116.71722412109375\n",
            "1380/3000 train_loss: 66.97941589355469 test_loss:117.83244323730469\n",
            "1381/3000 train_loss: 74.64151000976562 test_loss:114.75831604003906\n",
            "1382/3000 train_loss: 69.09085083007812 test_loss:107.44828796386719\n",
            "1383/3000 train_loss: 69.83503723144531 test_loss:105.17059326171875\n",
            "1384/3000 train_loss: 76.6323471069336 test_loss:118.12425994873047\n",
            "1385/3000 train_loss: 73.83829498291016 test_loss:108.65546417236328\n",
            "1386/3000 train_loss: 60.56913375854492 test_loss:116.98193359375\n",
            "1387/3000 train_loss: 68.50354766845703 test_loss:114.9419937133789\n",
            "1388/3000 train_loss: 65.45307922363281 test_loss:106.11376953125\n",
            "1389/3000 train_loss: 69.04569244384766 test_loss:110.24765014648438\n",
            "1390/3000 train_loss: 76.48686218261719 test_loss:114.57498931884766\n",
            "1391/3000 train_loss: 73.69219970703125 test_loss:119.44120025634766\n",
            "1392/3000 train_loss: 63.679237365722656 test_loss:117.17147064208984\n",
            "1393/3000 train_loss: 66.09825134277344 test_loss:112.99078369140625\n",
            "1394/3000 train_loss: 60.68674850463867 test_loss:104.35183715820312\n",
            "1395/3000 train_loss: 73.7610855102539 test_loss:129.57017517089844\n",
            "1396/3000 train_loss: 64.31372833251953 test_loss:111.49005889892578\n",
            "1397/3000 train_loss: 60.96149444580078 test_loss:106.15130615234375\n",
            "1398/3000 train_loss: 74.18921661376953 test_loss:111.2085189819336\n",
            "1399/3000 train_loss: 67.53294372558594 test_loss:116.70951843261719\n",
            "1400/3000 train_loss: 64.60737609863281 test_loss:120.99811553955078\n",
            "1401/3000 train_loss: 73.2215805053711 test_loss:107.93875122070312\n",
            "1402/3000 train_loss: 68.45825958251953 test_loss:115.84326171875\n",
            "1403/3000 train_loss: 68.48655700683594 test_loss:105.8563232421875\n",
            "1404/3000 train_loss: 66.43531799316406 test_loss:114.73592376708984\n",
            "1405/3000 train_loss: 67.4906005859375 test_loss:117.71788024902344\n",
            "1406/3000 train_loss: 63.87806701660156 test_loss:107.16390228271484\n",
            "1407/3000 train_loss: 63.51795959472656 test_loss:126.7632064819336\n",
            "1408/3000 train_loss: 62.41347122192383 test_loss:119.32904815673828\n",
            "1409/3000 train_loss: 69.27957916259766 test_loss:110.75843811035156\n",
            "1410/3000 train_loss: 70.52356719970703 test_loss:103.0193862915039\n",
            "1411/3000 train_loss: 69.63401794433594 test_loss:105.03912353515625\n",
            "1412/3000 train_loss: 66.81415557861328 test_loss:119.5990982055664\n",
            "1413/3000 train_loss: 65.43412017822266 test_loss:118.12814331054688\n",
            "1414/3000 train_loss: 70.46953582763672 test_loss:117.09928894042969\n",
            "1415/3000 train_loss: 71.99420166015625 test_loss:121.15327453613281\n",
            "1416/3000 train_loss: 69.49605560302734 test_loss:113.83706665039062\n",
            "1417/3000 train_loss: 65.62062072753906 test_loss:113.42543029785156\n",
            "1418/3000 train_loss: 72.15199279785156 test_loss:110.63953399658203\n",
            "1419/3000 train_loss: 74.19872283935547 test_loss:122.22893524169922\n",
            "1420/3000 train_loss: 61.56696701049805 test_loss:115.65519714355469\n",
            "1421/3000 train_loss: 73.22061920166016 test_loss:117.46720123291016\n",
            "1422/3000 train_loss: 73.03959655761719 test_loss:112.83139038085938\n",
            "1423/3000 train_loss: 63.18593215942383 test_loss:101.92353820800781\n",
            "1424/3000 train_loss: 59.44501495361328 test_loss:123.23953247070312\n",
            "1425/3000 train_loss: 64.3797607421875 test_loss:105.55683898925781\n",
            "1426/3000 train_loss: 72.39396667480469 test_loss:115.27318572998047\n",
            "1427/3000 train_loss: 72.61445617675781 test_loss:116.79595184326172\n",
            "1428/3000 train_loss: 74.38018035888672 test_loss:131.67518615722656\n",
            "1429/3000 train_loss: 79.25926971435547 test_loss:103.65644836425781\n",
            "1430/3000 train_loss: 57.203125 test_loss:111.41114807128906\n",
            "1431/3000 train_loss: 59.65939712524414 test_loss:102.41331481933594\n",
            "1432/3000 train_loss: 60.51359939575195 test_loss:114.96882629394531\n",
            "1433/3000 train_loss: 73.25210571289062 test_loss:107.2702407836914\n",
            "1434/3000 train_loss: 72.32959747314453 test_loss:114.67884826660156\n",
            "1435/3000 train_loss: 60.798431396484375 test_loss:119.7061538696289\n",
            "1436/3000 train_loss: 69.38507843017578 test_loss:109.81521606445312\n",
            "1437/3000 train_loss: 64.8520736694336 test_loss:118.8565673828125\n",
            "1438/3000 train_loss: 72.44510650634766 test_loss:108.20242309570312\n",
            "1439/3000 train_loss: 64.46947479248047 test_loss:133.86463928222656\n",
            "1440/3000 train_loss: 70.88838958740234 test_loss:128.06494140625\n",
            "1441/3000 train_loss: 58.239864349365234 test_loss:123.31368255615234\n",
            "1442/3000 train_loss: 68.53176879882812 test_loss:120.564697265625\n",
            "1443/3000 train_loss: 62.54109573364258 test_loss:119.31285095214844\n",
            "1444/3000 train_loss: 64.9734115600586 test_loss:110.21751403808594\n",
            "1445/3000 train_loss: 69.38822174072266 test_loss:116.66632080078125\n",
            "1446/3000 train_loss: 70.25935363769531 test_loss:118.21483612060547\n",
            "1447/3000 train_loss: 65.31666564941406 test_loss:105.89945983886719\n",
            "1448/3000 train_loss: 73.6092529296875 test_loss:122.71211242675781\n",
            "1449/3000 train_loss: 64.97245788574219 test_loss:106.08443450927734\n",
            "1450/3000 train_loss: 73.86036682128906 test_loss:112.02934265136719\n",
            "1451/3000 train_loss: 61.787296295166016 test_loss:117.41913604736328\n",
            "1452/3000 train_loss: 66.38773345947266 test_loss:109.19181823730469\n",
            "1453/3000 train_loss: 66.53448486328125 test_loss:118.04827880859375\n",
            "1454/3000 train_loss: 61.614463806152344 test_loss:111.3756332397461\n",
            "1455/3000 train_loss: 67.91410064697266 test_loss:117.43270874023438\n",
            "1456/3000 train_loss: 66.3499755859375 test_loss:120.85881042480469\n",
            "1457/3000 train_loss: 70.82157135009766 test_loss:111.27265167236328\n",
            "1458/3000 train_loss: 70.75366973876953 test_loss:122.68838500976562\n",
            "1459/3000 train_loss: 67.35360717773438 test_loss:114.65719604492188\n",
            "1460/3000 train_loss: 73.35840606689453 test_loss:119.8359603881836\n",
            "1461/3000 train_loss: 62.191650390625 test_loss:119.32748413085938\n",
            "1462/3000 train_loss: 67.79971313476562 test_loss:111.24081420898438\n",
            "1463/3000 train_loss: 65.33897399902344 test_loss:121.17884826660156\n",
            "1464/3000 train_loss: 61.16823196411133 test_loss:107.39960479736328\n",
            "1465/3000 train_loss: 64.31753540039062 test_loss:120.39912414550781\n",
            "1466/3000 train_loss: 61.79750061035156 test_loss:120.75285339355469\n",
            "1467/3000 train_loss: 60.67644119262695 test_loss:108.47943115234375\n",
            "1468/3000 train_loss: 72.90213012695312 test_loss:102.81623840332031\n",
            "1469/3000 train_loss: 68.97821044921875 test_loss:118.98208618164062\n",
            "1470/3000 train_loss: 54.141929626464844 test_loss:118.02407836914062\n",
            "1471/3000 train_loss: 60.939083099365234 test_loss:107.2158203125\n",
            "1472/3000 train_loss: 68.65470886230469 test_loss:127.1150894165039\n",
            "1473/3000 train_loss: 61.65222930908203 test_loss:113.3102798461914\n",
            "1474/3000 train_loss: 75.1393814086914 test_loss:121.39566802978516\n",
            "1475/3000 train_loss: 64.14055633544922 test_loss:115.56226348876953\n",
            "1476/3000 train_loss: 60.782352447509766 test_loss:113.09614562988281\n",
            "1477/3000 train_loss: 69.51325988769531 test_loss:123.7489013671875\n",
            "1478/3000 train_loss: 67.44161224365234 test_loss:105.2901611328125\n",
            "1479/3000 train_loss: 80.78069305419922 test_loss:122.47140502929688\n",
            "1480/3000 train_loss: 75.15689849853516 test_loss:103.47367858886719\n",
            "1481/3000 train_loss: 57.21276092529297 test_loss:117.28617858886719\n",
            "1482/3000 train_loss: 60.04977798461914 test_loss:107.98896789550781\n",
            "1483/3000 train_loss: 64.92768096923828 test_loss:114.29680633544922\n",
            "1484/3000 train_loss: 63.91584396362305 test_loss:112.99384307861328\n",
            "1485/3000 train_loss: 68.6400375366211 test_loss:110.27113342285156\n",
            "1486/3000 train_loss: 56.584625244140625 test_loss:126.0989761352539\n",
            "1487/3000 train_loss: 64.66004943847656 test_loss:103.48507690429688\n",
            "1488/3000 train_loss: 66.86473846435547 test_loss:111.29846954345703\n",
            "1489/3000 train_loss: 57.20079803466797 test_loss:112.14846801757812\n",
            "1490/3000 train_loss: 60.594261169433594 test_loss:108.75816345214844\n",
            "1491/3000 train_loss: 61.62942123413086 test_loss:133.70606994628906\n",
            "1492/3000 train_loss: 60.499149322509766 test_loss:108.64126586914062\n",
            "1493/3000 train_loss: 68.41816711425781 test_loss:109.45767974853516\n",
            "1494/3000 train_loss: 66.73152160644531 test_loss:104.94078063964844\n",
            "1495/3000 train_loss: 76.91831970214844 test_loss:118.5856704711914\n",
            "1496/3000 train_loss: 60.37049865722656 test_loss:120.75648498535156\n",
            "1497/3000 train_loss: 61.64073944091797 test_loss:113.82329559326172\n",
            "1498/3000 train_loss: 64.90998840332031 test_loss:115.25377655029297\n",
            "1499/3000 train_loss: 66.68964385986328 test_loss:117.28333282470703\n",
            "1500/3000 train_loss: 59.96491241455078 test_loss:114.08831787109375\n",
            "1501/3000 train_loss: 67.83590698242188 test_loss:117.57220458984375\n",
            "1502/3000 train_loss: 59.719512939453125 test_loss:112.61537170410156\n",
            "1503/3000 train_loss: 65.23158264160156 test_loss:120.4717025756836\n",
            "1504/3000 train_loss: 71.55591583251953 test_loss:113.55282592773438\n",
            "1505/3000 train_loss: 67.17781829833984 test_loss:110.07719421386719\n",
            "1506/3000 train_loss: 64.33982849121094 test_loss:110.76263427734375\n",
            "1507/3000 train_loss: 61.77197265625 test_loss:105.55192565917969\n",
            "1508/3000 train_loss: 63.33518600463867 test_loss:115.90465545654297\n",
            "1509/3000 train_loss: 57.816795349121094 test_loss:101.99380493164062\n",
            "1510/3000 train_loss: 58.408077239990234 test_loss:106.49085998535156\n",
            "1511/3000 train_loss: 66.64261627197266 test_loss:108.65135192871094\n",
            "1512/3000 train_loss: 62.92974090576172 test_loss:114.50631713867188\n",
            "1513/3000 train_loss: 67.96057891845703 test_loss:111.67466735839844\n",
            "1514/3000 train_loss: 65.04387664794922 test_loss:106.43922424316406\n",
            "1515/3000 train_loss: 67.03172302246094 test_loss:112.85491180419922\n",
            "1516/3000 train_loss: 72.34418487548828 test_loss:118.98883056640625\n",
            "1517/3000 train_loss: 63.50971603393555 test_loss:120.57437133789062\n",
            "1518/3000 train_loss: 62.52311706542969 test_loss:114.97579956054688\n",
            "1519/3000 train_loss: 60.22846603393555 test_loss:115.84901428222656\n",
            "1520/3000 train_loss: 63.650840759277344 test_loss:114.09158325195312\n",
            "1521/3000 train_loss: 68.45221710205078 test_loss:113.46563720703125\n",
            "1522/3000 train_loss: 67.48941802978516 test_loss:109.31295776367188\n",
            "1523/3000 train_loss: 74.6559066772461 test_loss:122.03947448730469\n",
            "1524/3000 train_loss: 61.528533935546875 test_loss:110.71318817138672\n",
            "1525/3000 train_loss: 67.71725463867188 test_loss:99.69208526611328\n",
            "1526/3000 train_loss: 74.58930969238281 test_loss:113.78700256347656\n",
            "1527/3000 train_loss: 67.54816436767578 test_loss:110.9775161743164\n",
            "1528/3000 train_loss: 61.4384651184082 test_loss:104.34432983398438\n",
            "1529/3000 train_loss: 63.7529411315918 test_loss:103.0478515625\n",
            "1530/3000 train_loss: 68.46548461914062 test_loss:105.63302612304688\n",
            "1531/3000 train_loss: 58.38829040527344 test_loss:102.63325500488281\n",
            "1532/3000 train_loss: 66.91445922851562 test_loss:122.51251220703125\n",
            "1533/3000 train_loss: 65.48986053466797 test_loss:109.74728393554688\n",
            "1534/3000 train_loss: 63.110023498535156 test_loss:119.25811767578125\n",
            "1535/3000 train_loss: 65.3873291015625 test_loss:115.65614318847656\n",
            "1536/3000 train_loss: 61.57944869995117 test_loss:109.55968475341797\n",
            "1537/3000 train_loss: 61.89739990234375 test_loss:106.38362121582031\n",
            "1538/3000 train_loss: 65.88339233398438 test_loss:110.74386596679688\n",
            "1539/3000 train_loss: 58.998985290527344 test_loss:113.9140625\n",
            "1540/3000 train_loss: 70.12677764892578 test_loss:102.93083190917969\n",
            "1541/3000 train_loss: 70.8005599975586 test_loss:101.70193481445312\n",
            "1542/3000 train_loss: 68.62255096435547 test_loss:108.05418395996094\n",
            "1543/3000 train_loss: 62.90816879272461 test_loss:117.54379272460938\n",
            "1544/3000 train_loss: 67.23451232910156 test_loss:111.01216125488281\n",
            "1545/3000 train_loss: 71.51748657226562 test_loss:111.67951965332031\n",
            "1546/3000 train_loss: 67.72816467285156 test_loss:106.04719543457031\n",
            "1547/3000 train_loss: 62.53997802734375 test_loss:114.39413452148438\n",
            "1548/3000 train_loss: 57.83978271484375 test_loss:111.66056060791016\n",
            "1549/3000 train_loss: 55.4200439453125 test_loss:104.61631774902344\n",
            "1550/3000 train_loss: 67.48963165283203 test_loss:107.42707824707031\n",
            "1551/3000 train_loss: 68.2523193359375 test_loss:115.40071868896484\n",
            "1552/3000 train_loss: 65.29701232910156 test_loss:109.22981262207031\n",
            "1553/3000 train_loss: 62.9730339050293 test_loss:100.72250366210938\n",
            "1554/3000 train_loss: 73.48908233642578 test_loss:112.26162719726562\n",
            "1555/3000 train_loss: 65.73750305175781 test_loss:118.1102066040039\n",
            "1556/3000 train_loss: 58.40056228637695 test_loss:106.01033020019531\n",
            "1557/3000 train_loss: 67.04318237304688 test_loss:106.4332275390625\n",
            "1558/3000 train_loss: 67.50850677490234 test_loss:111.89187622070312\n",
            "1559/3000 train_loss: 61.17796325683594 test_loss:118.61040496826172\n",
            "1560/3000 train_loss: 60.10771179199219 test_loss:106.63907623291016\n",
            "1561/3000 train_loss: 53.49933624267578 test_loss:111.61587524414062\n",
            "1562/3000 train_loss: 68.9982681274414 test_loss:109.59086608886719\n",
            "1563/3000 train_loss: 59.17611312866211 test_loss:114.66546630859375\n",
            "1564/3000 train_loss: 63.60966873168945 test_loss:112.49529266357422\n",
            "1565/3000 train_loss: 67.04496765136719 test_loss:116.0315933227539\n",
            "1566/3000 train_loss: 58.41635513305664 test_loss:106.91899108886719\n",
            "1567/3000 train_loss: 61.84699249267578 test_loss:108.76011657714844\n",
            "1568/3000 train_loss: 67.8531494140625 test_loss:119.88543701171875\n",
            "1569/3000 train_loss: 68.44620513916016 test_loss:112.57072448730469\n",
            "1570/3000 train_loss: 59.212581634521484 test_loss:112.3475341796875\n",
            "1571/3000 train_loss: 62.37274932861328 test_loss:99.13294982910156\n",
            "1572/3000 train_loss: 59.343379974365234 test_loss:111.20191192626953\n",
            "1573/3000 train_loss: 58.83418655395508 test_loss:108.65849304199219\n",
            "1574/3000 train_loss: 63.47901153564453 test_loss:110.06889343261719\n",
            "1575/3000 train_loss: 64.2313003540039 test_loss:109.77178955078125\n",
            "1576/3000 train_loss: 68.19207000732422 test_loss:111.5224380493164\n",
            "1577/3000 train_loss: 60.39853286743164 test_loss:104.88436889648438\n",
            "1578/3000 train_loss: 63.69783401489258 test_loss:110.97264099121094\n",
            "1579/3000 train_loss: 63.18025207519531 test_loss:114.51944732666016\n",
            "1580/3000 train_loss: 57.32692337036133 test_loss:106.7711181640625\n",
            "1581/3000 train_loss: 58.00742721557617 test_loss:106.3830795288086\n",
            "1582/3000 train_loss: 66.766357421875 test_loss:111.77815246582031\n",
            "1583/3000 train_loss: 58.6430549621582 test_loss:111.95143127441406\n",
            "1584/3000 train_loss: 59.40054702758789 test_loss:114.40412139892578\n",
            "1585/3000 train_loss: 60.2650146484375 test_loss:121.6151123046875\n",
            "1586/3000 train_loss: 52.96858215332031 test_loss:109.4212646484375\n",
            "1587/3000 train_loss: 64.55047607421875 test_loss:122.38846588134766\n",
            "1588/3000 train_loss: 69.08148956298828 test_loss:112.22796630859375\n",
            "1589/3000 train_loss: 62.307071685791016 test_loss:118.42098236083984\n",
            "1590/3000 train_loss: 57.008750915527344 test_loss:116.80086517333984\n",
            "1591/3000 train_loss: 66.4218521118164 test_loss:115.94462585449219\n",
            "1592/3000 train_loss: 58.666996002197266 test_loss:114.15997314453125\n",
            "1593/3000 train_loss: 66.97566223144531 test_loss:105.4763412475586\n",
            "1594/3000 train_loss: 61.161808013916016 test_loss:111.04192352294922\n",
            "1595/3000 train_loss: 58.47439193725586 test_loss:106.5018310546875\n",
            "1596/3000 train_loss: 63.95529556274414 test_loss:114.75728607177734\n",
            "1597/3000 train_loss: 61.91484832763672 test_loss:102.98881530761719\n",
            "1598/3000 train_loss: 64.77178192138672 test_loss:103.61422729492188\n",
            "1599/3000 train_loss: 65.91643524169922 test_loss:109.35045623779297\n",
            "1600/3000 train_loss: 59.913658142089844 test_loss:117.60871124267578\n",
            "1601/3000 train_loss: 57.033119201660156 test_loss:103.78418731689453\n",
            "1602/3000 train_loss: 64.40757751464844 test_loss:103.32550048828125\n",
            "1603/3000 train_loss: 59.660438537597656 test_loss:109.12659454345703\n",
            "1604/3000 train_loss: 57.645591735839844 test_loss:107.71428680419922\n",
            "1605/3000 train_loss: 62.382476806640625 test_loss:122.56755828857422\n",
            "1606/3000 train_loss: 69.89400482177734 test_loss:107.94933319091797\n",
            "1607/3000 train_loss: 63.69863510131836 test_loss:125.3941879272461\n",
            "1608/3000 train_loss: 62.782806396484375 test_loss:106.0908432006836\n",
            "1609/3000 train_loss: 66.80738067626953 test_loss:123.04054260253906\n",
            "1610/3000 train_loss: 70.13153076171875 test_loss:107.17170715332031\n",
            "1611/3000 train_loss: 65.09764862060547 test_loss:122.91415405273438\n",
            "1612/3000 train_loss: 58.615055084228516 test_loss:110.17781066894531\n",
            "1613/3000 train_loss: 60.58474349975586 test_loss:107.29224395751953\n",
            "1614/3000 train_loss: 71.23224639892578 test_loss:112.19388580322266\n",
            "1615/3000 train_loss: 58.444664001464844 test_loss:122.5640869140625\n",
            "1616/3000 train_loss: 61.351497650146484 test_loss:124.7268295288086\n",
            "1617/3000 train_loss: 64.51647186279297 test_loss:126.99745178222656\n",
            "1618/3000 train_loss: 66.25543212890625 test_loss:126.91749572753906\n",
            "1619/3000 train_loss: 67.34779357910156 test_loss:119.4167251586914\n",
            "1620/3000 train_loss: 65.87406158447266 test_loss:124.70606994628906\n",
            "1621/3000 train_loss: 59.510398864746094 test_loss:109.01880645751953\n",
            "1622/3000 train_loss: 66.59789276123047 test_loss:124.19499206542969\n",
            "1623/3000 train_loss: 67.58570098876953 test_loss:113.37642669677734\n",
            "1624/3000 train_loss: 61.51939392089844 test_loss:117.01873779296875\n",
            "1625/3000 train_loss: 66.18800354003906 test_loss:112.05207061767578\n",
            "1626/3000 train_loss: 57.76069641113281 test_loss:112.38145446777344\n",
            "1627/3000 train_loss: 66.7963638305664 test_loss:121.87657165527344\n",
            "1628/3000 train_loss: 63.2962646484375 test_loss:126.59127044677734\n",
            "1629/3000 train_loss: 55.99811935424805 test_loss:109.75239562988281\n",
            "1630/3000 train_loss: 67.40972900390625 test_loss:140.0377655029297\n",
            "1631/3000 train_loss: 66.04403686523438 test_loss:113.11420440673828\n",
            "1632/3000 train_loss: 61.1308479309082 test_loss:112.03080749511719\n",
            "1633/3000 train_loss: 58.488677978515625 test_loss:124.32212829589844\n",
            "1634/3000 train_loss: 60.909889221191406 test_loss:112.40689086914062\n",
            "1635/3000 train_loss: 70.88016510009766 test_loss:119.799560546875\n",
            "1636/3000 train_loss: 60.49245834350586 test_loss:119.65025329589844\n",
            "1637/3000 train_loss: 75.56636810302734 test_loss:105.94313049316406\n",
            "1638/3000 train_loss: 50.82179260253906 test_loss:124.81019592285156\n",
            "1639/3000 train_loss: 66.42271423339844 test_loss:127.09767150878906\n",
            "1640/3000 train_loss: 63.8707275390625 test_loss:111.80987548828125\n",
            "1641/3000 train_loss: 60.89693069458008 test_loss:107.49256896972656\n",
            "1642/3000 train_loss: 59.946102142333984 test_loss:114.74615478515625\n",
            "1643/3000 train_loss: 64.31494140625 test_loss:124.64988708496094\n",
            "1644/3000 train_loss: 61.3917236328125 test_loss:114.58939361572266\n",
            "1645/3000 train_loss: 59.89705276489258 test_loss:108.45623779296875\n",
            "1646/3000 train_loss: 61.00453567504883 test_loss:103.794677734375\n",
            "1647/3000 train_loss: 60.946903228759766 test_loss:125.75694274902344\n",
            "1648/3000 train_loss: 58.97814178466797 test_loss:115.81646728515625\n",
            "1649/3000 train_loss: 62.125572204589844 test_loss:107.68830871582031\n",
            "1650/3000 train_loss: 61.352481842041016 test_loss:108.42235565185547\n",
            "1651/3000 train_loss: 72.4468994140625 test_loss:122.88285827636719\n",
            "1652/3000 train_loss: 67.10543060302734 test_loss:116.82183837890625\n",
            "1653/3000 train_loss: 57.82019805908203 test_loss:103.58084106445312\n",
            "1654/3000 train_loss: 60.19613265991211 test_loss:105.88525390625\n",
            "1655/3000 train_loss: 74.45109558105469 test_loss:126.02714538574219\n",
            "1656/3000 train_loss: 66.82730102539062 test_loss:116.8900146484375\n",
            "1657/3000 train_loss: 67.01646423339844 test_loss:103.46524047851562\n",
            "1658/3000 train_loss: 61.68099594116211 test_loss:111.91801452636719\n",
            "1659/3000 train_loss: 59.978275299072266 test_loss:110.43115234375\n",
            "1660/3000 train_loss: 57.307350158691406 test_loss:115.28800964355469\n",
            "1661/3000 train_loss: 62.248695373535156 test_loss:115.57540893554688\n",
            "1662/3000 train_loss: 54.894046783447266 test_loss:111.47494506835938\n",
            "1663/3000 train_loss: 52.06175231933594 test_loss:110.69247436523438\n",
            "1664/3000 train_loss: 62.33783721923828 test_loss:111.89429473876953\n",
            "1665/3000 train_loss: 57.504058837890625 test_loss:120.21524047851562\n",
            "1666/3000 train_loss: 73.51673126220703 test_loss:100.01055145263672\n",
            "1667/3000 train_loss: 65.29745483398438 test_loss:122.00304412841797\n",
            "1668/3000 train_loss: 55.65505599975586 test_loss:108.51535034179688\n",
            "1669/3000 train_loss: 67.09285736083984 test_loss:106.93122100830078\n",
            "1670/3000 train_loss: 74.48757934570312 test_loss:116.00597381591797\n",
            "1671/3000 train_loss: 66.25160217285156 test_loss:101.40292358398438\n",
            "1672/3000 train_loss: 60.62040328979492 test_loss:113.30781555175781\n",
            "1673/3000 train_loss: 57.88389587402344 test_loss:109.07655334472656\n",
            "1674/3000 train_loss: 57.610069274902344 test_loss:103.92880249023438\n",
            "1675/3000 train_loss: 58.51469802856445 test_loss:108.01063537597656\n",
            "1676/3000 train_loss: 55.91322326660156 test_loss:116.566162109375\n",
            "1677/3000 train_loss: 61.72933578491211 test_loss:114.00060272216797\n",
            "1678/3000 train_loss: 61.534461975097656 test_loss:126.79602813720703\n",
            "1679/3000 train_loss: 56.051025390625 test_loss:111.446533203125\n",
            "1680/3000 train_loss: 56.172447204589844 test_loss:122.99537658691406\n",
            "1681/3000 train_loss: 61.48909378051758 test_loss:117.68936157226562\n",
            "1682/3000 train_loss: 58.92875671386719 test_loss:118.77174377441406\n",
            "1683/3000 train_loss: 60.56550979614258 test_loss:103.2731704711914\n",
            "1684/3000 train_loss: 70.744873046875 test_loss:113.97760009765625\n",
            "1685/3000 train_loss: 55.6644401550293 test_loss:107.61288452148438\n",
            "1686/3000 train_loss: 65.57779693603516 test_loss:110.8092041015625\n",
            "1687/3000 train_loss: 64.34837341308594 test_loss:128.2779998779297\n",
            "1688/3000 train_loss: 52.86745071411133 test_loss:119.60623168945312\n",
            "1689/3000 train_loss: 60.08622741699219 test_loss:114.76908111572266\n",
            "1690/3000 train_loss: 57.02783966064453 test_loss:110.18219757080078\n",
            "1691/3000 train_loss: 56.19971466064453 test_loss:124.50468444824219\n",
            "1692/3000 train_loss: 59.101402282714844 test_loss:111.10971069335938\n",
            "1693/3000 train_loss: 64.79064178466797 test_loss:118.51087951660156\n",
            "1694/3000 train_loss: 61.63360595703125 test_loss:101.82482147216797\n",
            "1695/3000 train_loss: 51.818416595458984 test_loss:112.25873565673828\n",
            "1696/3000 train_loss: 58.874290466308594 test_loss:126.90780639648438\n",
            "1697/3000 train_loss: 59.120140075683594 test_loss:112.48097229003906\n",
            "1698/3000 train_loss: 64.04808044433594 test_loss:110.16036224365234\n",
            "1699/3000 train_loss: 56.53152084350586 test_loss:105.91201782226562\n",
            "1700/3000 train_loss: 68.12356567382812 test_loss:118.21162414550781\n",
            "1701/3000 train_loss: 55.25920867919922 test_loss:101.64361572265625\n",
            "1702/3000 train_loss: 64.73800659179688 test_loss:104.38614654541016\n",
            "1703/3000 train_loss: 57.6279182434082 test_loss:119.4893798828125\n",
            "1704/3000 train_loss: 55.97957992553711 test_loss:108.07320404052734\n",
            "1705/3000 train_loss: 54.44508743286133 test_loss:117.87067413330078\n",
            "1706/3000 train_loss: 57.08116149902344 test_loss:112.40972900390625\n",
            "1707/3000 train_loss: 70.35083770751953 test_loss:113.2934799194336\n",
            "1708/3000 train_loss: 58.9517936706543 test_loss:119.16563415527344\n",
            "1709/3000 train_loss: 62.394683837890625 test_loss:130.46934509277344\n",
            "1710/3000 train_loss: 60.514259338378906 test_loss:103.35057067871094\n",
            "1711/3000 train_loss: 57.22164535522461 test_loss:103.89022064208984\n",
            "1712/3000 train_loss: 59.331417083740234 test_loss:113.9236068725586\n",
            "1713/3000 train_loss: 64.17680358886719 test_loss:118.61590576171875\n",
            "1714/3000 train_loss: 61.364261627197266 test_loss:119.49540710449219\n",
            "1715/3000 train_loss: 54.26242446899414 test_loss:118.49453735351562\n",
            "1716/3000 train_loss: 53.492881774902344 test_loss:112.39012908935547\n",
            "1717/3000 train_loss: 56.489070892333984 test_loss:114.25254821777344\n",
            "1718/3000 train_loss: 54.65589904785156 test_loss:114.11721801757812\n",
            "1719/3000 train_loss: 59.89794921875 test_loss:114.0172119140625\n",
            "1720/3000 train_loss: 60.91850280761719 test_loss:105.97126770019531\n",
            "1721/3000 train_loss: 58.962562561035156 test_loss:105.34965515136719\n",
            "1722/3000 train_loss: 63.267337799072266 test_loss:109.39190673828125\n",
            "1723/3000 train_loss: 62.0908088684082 test_loss:135.28814697265625\n",
            "1724/3000 train_loss: 60.302833557128906 test_loss:106.02059173583984\n",
            "1725/3000 train_loss: 53.85618591308594 test_loss:113.77072143554688\n",
            "1726/3000 train_loss: 64.69590759277344 test_loss:116.36666870117188\n",
            "1727/3000 train_loss: 59.20752716064453 test_loss:103.4410171508789\n",
            "1728/3000 train_loss: 59.63269805908203 test_loss:118.94241333007812\n",
            "1729/3000 train_loss: 56.45585250854492 test_loss:107.80228424072266\n",
            "1730/3000 train_loss: 58.64580154418945 test_loss:108.33988952636719\n",
            "1731/3000 train_loss: 68.78890991210938 test_loss:104.90258026123047\n",
            "1732/3000 train_loss: 60.20948028564453 test_loss:101.19865417480469\n",
            "1733/3000 train_loss: 61.0864143371582 test_loss:112.67179870605469\n",
            "1734/3000 train_loss: 58.905029296875 test_loss:113.20724487304688\n",
            "1735/3000 train_loss: 66.54592895507812 test_loss:116.5280990600586\n",
            "1736/3000 train_loss: 67.4294662475586 test_loss:121.0553207397461\n",
            "1737/3000 train_loss: 55.70402908325195 test_loss:121.18351745605469\n",
            "1738/3000 train_loss: 68.060302734375 test_loss:126.33636474609375\n",
            "1739/3000 train_loss: 62.13265609741211 test_loss:112.8654556274414\n",
            "1740/3000 train_loss: 55.37030792236328 test_loss:100.77020263671875\n",
            "1741/3000 train_loss: 62.372169494628906 test_loss:118.23393249511719\n",
            "1742/3000 train_loss: 67.977294921875 test_loss:101.62039184570312\n",
            "1743/3000 train_loss: 64.07319641113281 test_loss:114.60620880126953\n",
            "1744/3000 train_loss: 72.86210632324219 test_loss:104.35893249511719\n",
            "1745/3000 train_loss: 61.205833435058594 test_loss:115.26715850830078\n",
            "1746/3000 train_loss: 68.42320251464844 test_loss:119.35075378417969\n",
            "1747/3000 train_loss: 62.042484283447266 test_loss:113.64337158203125\n",
            "1748/3000 train_loss: 62.542537689208984 test_loss:121.51679992675781\n",
            "1749/3000 train_loss: 60.59613037109375 test_loss:104.5989761352539\n",
            "1750/3000 train_loss: 56.97894287109375 test_loss:123.83157348632812\n",
            "1751/3000 train_loss: 63.136131286621094 test_loss:104.17198181152344\n",
            "1752/3000 train_loss: 62.74205017089844 test_loss:100.56903076171875\n",
            "1753/3000 train_loss: 61.933773040771484 test_loss:107.78089141845703\n",
            "1754/3000 train_loss: 61.27962112426758 test_loss:115.46753692626953\n",
            "1755/3000 train_loss: 58.06046676635742 test_loss:104.19166564941406\n",
            "1756/3000 train_loss: 59.63896179199219 test_loss:105.81947326660156\n",
            "1757/3000 train_loss: 52.6453857421875 test_loss:103.03754425048828\n",
            "1758/3000 train_loss: 47.87073516845703 test_loss:111.45150756835938\n",
            "1759/3000 train_loss: 53.59742736816406 test_loss:108.22270965576172\n",
            "1760/3000 train_loss: 62.289039611816406 test_loss:109.17225646972656\n",
            "1761/3000 train_loss: 57.956024169921875 test_loss:103.939453125\n",
            "1762/3000 train_loss: 57.467918395996094 test_loss:99.80392456054688\n",
            "1763/3000 train_loss: 61.06630325317383 test_loss:109.47179412841797\n",
            "1764/3000 train_loss: 62.853515625 test_loss:123.95243835449219\n",
            "1765/3000 train_loss: 60.124481201171875 test_loss:119.17857360839844\n",
            "1766/3000 train_loss: 61.185462951660156 test_loss:105.60844421386719\n",
            "1767/3000 train_loss: 54.32456588745117 test_loss:124.21754455566406\n",
            "1768/3000 train_loss: 54.52265930175781 test_loss:109.6218032836914\n",
            "1769/3000 train_loss: 54.24966812133789 test_loss:108.91816711425781\n",
            "1770/3000 train_loss: 56.62405014038086 test_loss:103.83140563964844\n",
            "1771/3000 train_loss: 53.89130401611328 test_loss:107.34087371826172\n",
            "1772/3000 train_loss: 66.2347183227539 test_loss:109.18670654296875\n",
            "1773/3000 train_loss: 49.00505065917969 test_loss:103.60966491699219\n",
            "1774/3000 train_loss: 50.508304595947266 test_loss:102.24736785888672\n",
            "1775/3000 train_loss: 59.27413558959961 test_loss:97.82228088378906\n",
            "1776/3000 train_loss: 59.642372131347656 test_loss:104.96315002441406\n",
            "1777/3000 train_loss: 60.990234375 test_loss:106.64978790283203\n",
            "1778/3000 train_loss: 59.750160217285156 test_loss:119.49787139892578\n",
            "1779/3000 train_loss: 52.53912353515625 test_loss:99.26162719726562\n",
            "1780/3000 train_loss: 66.2939453125 test_loss:109.94880676269531\n",
            "1781/3000 train_loss: 54.78965759277344 test_loss:122.43759155273438\n",
            "1782/3000 train_loss: 62.18592834472656 test_loss:94.5517578125\n",
            "1783/3000 train_loss: 65.68904113769531 test_loss:125.24664306640625\n",
            "1784/3000 train_loss: 55.22830581665039 test_loss:110.72901916503906\n",
            "1785/3000 train_loss: 63.72365188598633 test_loss:122.49483489990234\n",
            "1786/3000 train_loss: 60.04301452636719 test_loss:108.52418518066406\n",
            "1787/3000 train_loss: 51.53948974609375 test_loss:107.62147521972656\n",
            "1788/3000 train_loss: 63.09236526489258 test_loss:110.43414306640625\n",
            "1789/3000 train_loss: 59.7855110168457 test_loss:101.60984802246094\n",
            "1790/3000 train_loss: 57.61919403076172 test_loss:110.78707122802734\n",
            "1791/3000 train_loss: 51.59199523925781 test_loss:111.10957336425781\n",
            "1792/3000 train_loss: 55.94586944580078 test_loss:122.30595397949219\n",
            "1793/3000 train_loss: 56.177249908447266 test_loss:108.36885833740234\n",
            "1794/3000 train_loss: 56.2868766784668 test_loss:104.9857406616211\n",
            "1795/3000 train_loss: 65.25601959228516 test_loss:104.60865020751953\n",
            "1796/3000 train_loss: 51.14751052856445 test_loss:106.80001068115234\n",
            "1797/3000 train_loss: 54.26898193359375 test_loss:100.51100158691406\n",
            "1798/3000 train_loss: 55.70599365234375 test_loss:105.10652923583984\n",
            "1799/3000 train_loss: 53.65083694458008 test_loss:110.44140625\n",
            "1800/3000 train_loss: 68.5082015991211 test_loss:94.68153381347656\n",
            "1801/3000 train_loss: 51.96760559082031 test_loss:112.59815216064453\n",
            "1802/3000 train_loss: 61.06802749633789 test_loss:110.67282104492188\n",
            "1803/3000 train_loss: 50.89183807373047 test_loss:117.5118179321289\n",
            "1804/3000 train_loss: 58.711669921875 test_loss:105.07256317138672\n",
            "1805/3000 train_loss: 56.30268859863281 test_loss:111.0792007446289\n",
            "1806/3000 train_loss: 65.47111511230469 test_loss:113.04054260253906\n",
            "1807/3000 train_loss: 55.51565170288086 test_loss:105.87458801269531\n",
            "1808/3000 train_loss: 63.55800247192383 test_loss:102.77510070800781\n",
            "1809/3000 train_loss: 56.349342346191406 test_loss:104.89897918701172\n",
            "1810/3000 train_loss: 55.66226577758789 test_loss:106.23528289794922\n",
            "1811/3000 train_loss: 54.0831184387207 test_loss:101.70671081542969\n",
            "1812/3000 train_loss: 52.64193344116211 test_loss:113.67036437988281\n",
            "1813/3000 train_loss: 59.80257797241211 test_loss:111.31483459472656\n",
            "1814/3000 train_loss: 56.65687942504883 test_loss:120.09310913085938\n",
            "1815/3000 train_loss: 55.239139556884766 test_loss:113.61683654785156\n",
            "1816/3000 train_loss: 60.12999725341797 test_loss:110.50706481933594\n",
            "1817/3000 train_loss: 57.209197998046875 test_loss:103.72775268554688\n",
            "1818/3000 train_loss: 55.881675720214844 test_loss:113.4582748413086\n",
            "1819/3000 train_loss: 56.95131301879883 test_loss:110.42803955078125\n",
            "1820/3000 train_loss: 61.94810104370117 test_loss:102.15632629394531\n",
            "1821/3000 train_loss: 65.45301818847656 test_loss:121.30397033691406\n",
            "1822/3000 train_loss: 57.789573669433594 test_loss:109.75579833984375\n",
            "1823/3000 train_loss: 49.582523345947266 test_loss:121.54093933105469\n",
            "1824/3000 train_loss: 54.554439544677734 test_loss:99.19737243652344\n",
            "1825/3000 train_loss: 56.19054412841797 test_loss:112.84634399414062\n",
            "1826/3000 train_loss: 55.23162841796875 test_loss:107.86955261230469\n",
            "1827/3000 train_loss: 54.53793716430664 test_loss:99.18484497070312\n",
            "1828/3000 train_loss: 57.59658432006836 test_loss:121.20036315917969\n",
            "1829/3000 train_loss: 57.87807083129883 test_loss:98.33473205566406\n",
            "1830/3000 train_loss: 56.14433670043945 test_loss:104.80826568603516\n",
            "1831/3000 train_loss: 64.04621124267578 test_loss:113.94132995605469\n",
            "1832/3000 train_loss: 53.72724914550781 test_loss:105.042724609375\n",
            "1833/3000 train_loss: 52.08396911621094 test_loss:98.16178894042969\n",
            "1834/3000 train_loss: 54.25115966796875 test_loss:112.42059326171875\n",
            "1835/3000 train_loss: 53.497493743896484 test_loss:111.36186218261719\n",
            "1836/3000 train_loss: 60.87031555175781 test_loss:124.36720275878906\n",
            "1837/3000 train_loss: 58.814918518066406 test_loss:99.01611328125\n",
            "1838/3000 train_loss: 51.96303176879883 test_loss:106.66593933105469\n",
            "1839/3000 train_loss: 57.64963150024414 test_loss:119.10623168945312\n",
            "1840/3000 train_loss: 64.05316925048828 test_loss:97.54478454589844\n",
            "1841/3000 train_loss: 53.81122970581055 test_loss:101.26608276367188\n",
            "1842/3000 train_loss: 52.48043441772461 test_loss:102.68020629882812\n",
            "1843/3000 train_loss: 50.18667221069336 test_loss:100.81636047363281\n",
            "1844/3000 train_loss: 56.61200714111328 test_loss:105.93600463867188\n",
            "1845/3000 train_loss: 57.78091812133789 test_loss:102.87300109863281\n",
            "1846/3000 train_loss: 51.49730682373047 test_loss:103.99116516113281\n",
            "1847/3000 train_loss: 57.968299865722656 test_loss:111.44830322265625\n",
            "1848/3000 train_loss: 59.79197692871094 test_loss:112.3004379272461\n",
            "1849/3000 train_loss: 62.81652069091797 test_loss:99.53057861328125\n",
            "1850/3000 train_loss: 54.653053283691406 test_loss:130.0432891845703\n",
            "1851/3000 train_loss: 62.3712272644043 test_loss:111.83279418945312\n",
            "1852/3000 train_loss: 51.68861770629883 test_loss:104.14627075195312\n",
            "1853/3000 train_loss: 52.19985580444336 test_loss:109.63089752197266\n",
            "1854/3000 train_loss: 57.95515823364258 test_loss:97.77251434326172\n",
            "1855/3000 train_loss: 50.50480651855469 test_loss:108.1912612915039\n",
            "1856/3000 train_loss: 61.88889694213867 test_loss:100.37625122070312\n",
            "1857/3000 train_loss: 47.84532928466797 test_loss:108.12661743164062\n",
            "1858/3000 train_loss: 58.88215255737305 test_loss:96.87860107421875\n",
            "1859/3000 train_loss: 57.52265930175781 test_loss:110.71589660644531\n",
            "1860/3000 train_loss: 51.46240997314453 test_loss:101.88768005371094\n",
            "1861/3000 train_loss: 55.237037658691406 test_loss:122.779541015625\n",
            "1862/3000 train_loss: 55.38385009765625 test_loss:114.15357971191406\n",
            "1863/3000 train_loss: 58.10309982299805 test_loss:110.15101623535156\n",
            "1864/3000 train_loss: 56.15863037109375 test_loss:101.90547943115234\n",
            "1865/3000 train_loss: 55.95249557495117 test_loss:95.53643798828125\n",
            "1866/3000 train_loss: 54.88371658325195 test_loss:109.79389953613281\n",
            "1867/3000 train_loss: 54.335914611816406 test_loss:106.61033630371094\n",
            "1868/3000 train_loss: 60.9219970703125 test_loss:121.92349243164062\n",
            "1869/3000 train_loss: 46.96293640136719 test_loss:110.34495544433594\n",
            "1870/3000 train_loss: 56.82358932495117 test_loss:102.51091003417969\n",
            "1871/3000 train_loss: 55.04512023925781 test_loss:109.92894744873047\n",
            "1872/3000 train_loss: 65.9531478881836 test_loss:103.73393249511719\n",
            "1873/3000 train_loss: 54.71298599243164 test_loss:112.67955017089844\n",
            "1874/3000 train_loss: 51.710628509521484 test_loss:108.55696868896484\n",
            "1875/3000 train_loss: 59.48964309692383 test_loss:104.37834930419922\n",
            "1876/3000 train_loss: 55.90871810913086 test_loss:107.67694091796875\n",
            "1877/3000 train_loss: 56.164695739746094 test_loss:95.99597930908203\n",
            "1878/3000 train_loss: 55.597476959228516 test_loss:100.77108001708984\n",
            "1879/3000 train_loss: 53.43467330932617 test_loss:99.0500717163086\n",
            "1880/3000 train_loss: 51.23044204711914 test_loss:102.95877075195312\n",
            "1881/3000 train_loss: 62.888267517089844 test_loss:99.30455780029297\n",
            "1882/3000 train_loss: 55.44480514526367 test_loss:106.26803588867188\n",
            "1883/3000 train_loss: 50.27059555053711 test_loss:105.31515502929688\n",
            "1884/3000 train_loss: 57.78105926513672 test_loss:102.37425994873047\n",
            "1885/3000 train_loss: 52.66004180908203 test_loss:93.78201293945312\n",
            "1886/3000 train_loss: 58.968894958496094 test_loss:110.4292984008789\n",
            "1887/3000 train_loss: 59.7869758605957 test_loss:117.26301574707031\n",
            "1888/3000 train_loss: 67.39642333984375 test_loss:102.95024108886719\n",
            "1889/3000 train_loss: 58.00947189331055 test_loss:109.96697998046875\n",
            "1890/3000 train_loss: 58.2265739440918 test_loss:99.45869445800781\n",
            "1891/3000 train_loss: 47.90006637573242 test_loss:103.77460479736328\n",
            "1892/3000 train_loss: 52.91886520385742 test_loss:121.21865844726562\n",
            "1893/3000 train_loss: 62.61544418334961 test_loss:96.94480895996094\n",
            "1894/3000 train_loss: 54.06463623046875 test_loss:98.54887390136719\n",
            "1895/3000 train_loss: 59.524627685546875 test_loss:115.71934509277344\n",
            "1896/3000 train_loss: 55.26648712158203 test_loss:115.25389099121094\n",
            "1897/3000 train_loss: 64.67680358886719 test_loss:105.64789581298828\n",
            "1898/3000 train_loss: 62.262481689453125 test_loss:96.61703491210938\n",
            "1899/3000 train_loss: 70.00703430175781 test_loss:94.52086639404297\n",
            "1900/3000 train_loss: 57.555992126464844 test_loss:101.23096466064453\n",
            "1901/3000 train_loss: 57.791542053222656 test_loss:94.37431335449219\n",
            "1902/3000 train_loss: 56.66426467895508 test_loss:104.91366577148438\n",
            "1903/3000 train_loss: 50.691280364990234 test_loss:101.55208587646484\n",
            "1904/3000 train_loss: 56.48613739013672 test_loss:105.64677429199219\n",
            "1905/3000 train_loss: 47.111976623535156 test_loss:96.84681701660156\n",
            "1906/3000 train_loss: 54.92759323120117 test_loss:103.63250732421875\n",
            "1907/3000 train_loss: 48.26171875 test_loss:105.65973663330078\n",
            "1908/3000 train_loss: 53.243839263916016 test_loss:112.64816284179688\n",
            "1909/3000 train_loss: 58.45878601074219 test_loss:99.99331665039062\n",
            "1910/3000 train_loss: 64.06502532958984 test_loss:112.84990692138672\n",
            "1911/3000 train_loss: 56.399295806884766 test_loss:105.5364990234375\n",
            "1912/3000 train_loss: 52.12264633178711 test_loss:109.70246124267578\n",
            "1913/3000 train_loss: 58.69252014160156 test_loss:112.07279968261719\n",
            "1914/3000 train_loss: 55.1142463684082 test_loss:101.52984619140625\n",
            "1915/3000 train_loss: 51.02204132080078 test_loss:105.52546691894531\n",
            "1916/3000 train_loss: 55.64457321166992 test_loss:100.99238586425781\n",
            "1917/3000 train_loss: 52.68364715576172 test_loss:109.52580261230469\n",
            "1918/3000 train_loss: 57.01988983154297 test_loss:97.85926818847656\n",
            "1919/3000 train_loss: 54.80278778076172 test_loss:121.03666687011719\n",
            "1920/3000 train_loss: 60.40380096435547 test_loss:128.87576293945312\n",
            "1921/3000 train_loss: 59.92902374267578 test_loss:107.32325744628906\n",
            "1922/3000 train_loss: 58.0327033996582 test_loss:99.78216552734375\n",
            "1923/3000 train_loss: 50.4723014831543 test_loss:99.18388366699219\n",
            "1924/3000 train_loss: 54.960628509521484 test_loss:108.69990539550781\n",
            "1925/3000 train_loss: 58.199344635009766 test_loss:111.95264434814453\n",
            "1926/3000 train_loss: 48.63256072998047 test_loss:108.19383239746094\n",
            "1927/3000 train_loss: 54.748878479003906 test_loss:102.13977813720703\n",
            "1928/3000 train_loss: 54.67715835571289 test_loss:104.82307434082031\n",
            "1929/3000 train_loss: 53.50794219970703 test_loss:104.88465118408203\n",
            "1930/3000 train_loss: 57.461299896240234 test_loss:96.95710754394531\n",
            "1931/3000 train_loss: 50.68378829956055 test_loss:113.99516296386719\n",
            "1932/3000 train_loss: 51.12645721435547 test_loss:113.05785369873047\n",
            "1933/3000 train_loss: 53.52941131591797 test_loss:100.33845520019531\n",
            "1934/3000 train_loss: 57.078304290771484 test_loss:118.90460205078125\n",
            "1935/3000 train_loss: 54.81483459472656 test_loss:115.95134735107422\n",
            "1936/3000 train_loss: 53.78249740600586 test_loss:110.43482971191406\n",
            "1937/3000 train_loss: 68.90949249267578 test_loss:108.11457824707031\n",
            "1938/3000 train_loss: 49.62132263183594 test_loss:103.3564453125\n",
            "1939/3000 train_loss: 57.767513275146484 test_loss:111.02090454101562\n",
            "1940/3000 train_loss: 53.60918045043945 test_loss:99.88292694091797\n",
            "1941/3000 train_loss: 58.862579345703125 test_loss:102.87738037109375\n",
            "1942/3000 train_loss: 55.21023178100586 test_loss:112.94671630859375\n",
            "1943/3000 train_loss: 59.903743743896484 test_loss:124.00363159179688\n",
            "1944/3000 train_loss: 54.45559310913086 test_loss:98.59384155273438\n",
            "1945/3000 train_loss: 54.54013442993164 test_loss:109.15321350097656\n",
            "1946/3000 train_loss: 50.065040588378906 test_loss:102.79686737060547\n",
            "1947/3000 train_loss: 52.64338302612305 test_loss:104.19248962402344\n",
            "1948/3000 train_loss: 54.13206481933594 test_loss:100.41460418701172\n",
            "1949/3000 train_loss: 53.49000930786133 test_loss:94.80827331542969\n",
            "1950/3000 train_loss: 48.631675720214844 test_loss:112.05923461914062\n",
            "1951/3000 train_loss: 56.95403289794922 test_loss:100.55799865722656\n",
            "1952/3000 train_loss: 55.99404525756836 test_loss:100.95304107666016\n",
            "1953/3000 train_loss: 56.96230697631836 test_loss:105.04219818115234\n",
            "1954/3000 train_loss: 55.641456604003906 test_loss:96.29887390136719\n",
            "1955/3000 train_loss: 67.12103271484375 test_loss:102.27964782714844\n",
            "1956/3000 train_loss: 50.2291145324707 test_loss:109.16140747070312\n",
            "1957/3000 train_loss: 53.71003723144531 test_loss:106.39350128173828\n",
            "1958/3000 train_loss: 56.416969299316406 test_loss:101.34362030029297\n",
            "1959/3000 train_loss: 53.86330795288086 test_loss:96.52899169921875\n",
            "1960/3000 train_loss: 54.39799118041992 test_loss:100.73095703125\n",
            "1961/3000 train_loss: 51.91349792480469 test_loss:103.97514343261719\n",
            "1962/3000 train_loss: 57.32783126831055 test_loss:115.83464050292969\n",
            "1963/3000 train_loss: 52.487430572509766 test_loss:102.64803314208984\n",
            "1964/3000 train_loss: 70.87549591064453 test_loss:110.18158721923828\n",
            "1965/3000 train_loss: 55.4913330078125 test_loss:106.25418090820312\n",
            "1966/3000 train_loss: 51.61695098876953 test_loss:103.77536010742188\n",
            "1967/3000 train_loss: 50.42193603515625 test_loss:99.164306640625\n",
            "1968/3000 train_loss: 53.51338195800781 test_loss:104.46099853515625\n",
            "1969/3000 train_loss: 47.84839630126953 test_loss:101.20414733886719\n",
            "1970/3000 train_loss: 66.08662414550781 test_loss:96.12797546386719\n",
            "1971/3000 train_loss: 49.54115676879883 test_loss:118.70599365234375\n",
            "1972/3000 train_loss: 56.020172119140625 test_loss:101.84044647216797\n",
            "1973/3000 train_loss: 58.130794525146484 test_loss:94.98210906982422\n",
            "1974/3000 train_loss: 52.53929901123047 test_loss:100.4480972290039\n",
            "1975/3000 train_loss: 53.70070266723633 test_loss:95.98307800292969\n",
            "1976/3000 train_loss: 48.03826141357422 test_loss:100.6407470703125\n",
            "1977/3000 train_loss: 54.39073944091797 test_loss:107.35521697998047\n",
            "1978/3000 train_loss: 57.07096862792969 test_loss:105.70714569091797\n",
            "1979/3000 train_loss: 55.65937042236328 test_loss:102.29473876953125\n",
            "1980/3000 train_loss: 48.42795944213867 test_loss:101.55484008789062\n",
            "1981/3000 train_loss: 55.296871185302734 test_loss:108.12421417236328\n",
            "1982/3000 train_loss: 47.727657318115234 test_loss:107.16641998291016\n",
            "1983/3000 train_loss: 61.508262634277344 test_loss:100.3671646118164\n",
            "1984/3000 train_loss: 49.982940673828125 test_loss:92.08126068115234\n",
            "1985/3000 train_loss: 52.991554260253906 test_loss:107.98273468017578\n",
            "1986/3000 train_loss: 51.0238151550293 test_loss:96.21031188964844\n",
            "1987/3000 train_loss: 51.79069900512695 test_loss:108.40237426757812\n",
            "1988/3000 train_loss: 53.06694793701172 test_loss:98.01193237304688\n",
            "1989/3000 train_loss: 52.00117492675781 test_loss:100.110107421875\n",
            "1990/3000 train_loss: 52.044471740722656 test_loss:111.060302734375\n",
            "1991/3000 train_loss: 53.331600189208984 test_loss:110.62065124511719\n",
            "1992/3000 train_loss: 49.13554382324219 test_loss:103.83283996582031\n",
            "1993/3000 train_loss: 53.95859146118164 test_loss:108.26628112792969\n",
            "1994/3000 train_loss: 53.379390716552734 test_loss:102.9917221069336\n",
            "1995/3000 train_loss: 57.950313568115234 test_loss:108.64490509033203\n",
            "1996/3000 train_loss: 52.758853912353516 test_loss:104.37753295898438\n",
            "1997/3000 train_loss: 52.86192321777344 test_loss:113.11407470703125\n",
            "1998/3000 train_loss: 51.918190002441406 test_loss:106.7970962524414\n",
            "1999/3000 train_loss: 58.254356384277344 test_loss:109.52377319335938\n",
            "2000/3000 train_loss: 48.542747497558594 test_loss:105.08869934082031\n",
            "2001/3000 train_loss: 47.92523193359375 test_loss:110.744873046875\n",
            "2002/3000 train_loss: 51.40132522583008 test_loss:104.57901763916016\n",
            "2003/3000 train_loss: 54.436859130859375 test_loss:105.93082427978516\n",
            "2004/3000 train_loss: 55.84516143798828 test_loss:111.8769302368164\n",
            "2005/3000 train_loss: 49.31113052368164 test_loss:118.33839416503906\n",
            "2006/3000 train_loss: 54.110252380371094 test_loss:102.72005462646484\n",
            "2007/3000 train_loss: 45.785301208496094 test_loss:110.55728149414062\n",
            "2008/3000 train_loss: 56.28185272216797 test_loss:99.93560791015625\n",
            "2009/3000 train_loss: 55.83033752441406 test_loss:105.74835205078125\n",
            "2010/3000 train_loss: 53.98612976074219 test_loss:108.92024230957031\n",
            "2011/3000 train_loss: 52.51016616821289 test_loss:113.75566101074219\n",
            "2012/3000 train_loss: 49.29578399658203 test_loss:105.83160400390625\n",
            "2013/3000 train_loss: 51.09489059448242 test_loss:107.19114685058594\n",
            "2014/3000 train_loss: 52.97632598876953 test_loss:105.95809173583984\n",
            "2015/3000 train_loss: 54.40143966674805 test_loss:109.64949035644531\n",
            "2016/3000 train_loss: 48.093990325927734 test_loss:107.5397720336914\n",
            "2017/3000 train_loss: 60.1186408996582 test_loss:114.08682250976562\n",
            "2018/3000 train_loss: 57.68103790283203 test_loss:100.43414306640625\n",
            "2019/3000 train_loss: 51.91737365722656 test_loss:96.96827697753906\n",
            "2020/3000 train_loss: 56.20039749145508 test_loss:102.45436096191406\n",
            "2021/3000 train_loss: 60.076107025146484 test_loss:99.45384216308594\n",
            "2022/3000 train_loss: 50.625282287597656 test_loss:95.58226776123047\n",
            "2023/3000 train_loss: 51.90093994140625 test_loss:101.79562377929688\n",
            "2024/3000 train_loss: 57.31814193725586 test_loss:102.76813507080078\n",
            "2025/3000 train_loss: 53.33860778808594 test_loss:101.08087158203125\n",
            "2026/3000 train_loss: 54.83051681518555 test_loss:105.93080139160156\n",
            "2027/3000 train_loss: 52.66948318481445 test_loss:112.35926818847656\n",
            "2028/3000 train_loss: 67.71295166015625 test_loss:109.3676986694336\n",
            "2029/3000 train_loss: 52.993980407714844 test_loss:118.62305450439453\n",
            "2030/3000 train_loss: 52.1912727355957 test_loss:97.69068908691406\n",
            "2031/3000 train_loss: 52.51213073730469 test_loss:105.85845947265625\n",
            "2032/3000 train_loss: 52.79643249511719 test_loss:101.83452606201172\n",
            "2033/3000 train_loss: 57.99940872192383 test_loss:102.54414367675781\n",
            "2034/3000 train_loss: 51.97486877441406 test_loss:112.41824340820312\n",
            "2035/3000 train_loss: 54.2302360534668 test_loss:101.74750518798828\n",
            "2036/3000 train_loss: 57.39957046508789 test_loss:95.25032043457031\n",
            "2037/3000 train_loss: 51.84122848510742 test_loss:109.43577575683594\n",
            "2038/3000 train_loss: 47.317420959472656 test_loss:101.23146057128906\n",
            "2039/3000 train_loss: 55.17000961303711 test_loss:99.65254211425781\n",
            "2040/3000 train_loss: 54.29091262817383 test_loss:97.4890365600586\n",
            "2041/3000 train_loss: 60.79130554199219 test_loss:105.62596893310547\n",
            "2042/3000 train_loss: 50.22956085205078 test_loss:97.08905029296875\n",
            "2043/3000 train_loss: 57.65758514404297 test_loss:110.10613250732422\n",
            "2044/3000 train_loss: 54.87253952026367 test_loss:108.27408599853516\n",
            "2045/3000 train_loss: 58.3785514831543 test_loss:124.21532440185547\n",
            "2046/3000 train_loss: 54.185340881347656 test_loss:103.07907104492188\n",
            "2047/3000 train_loss: 45.702388763427734 test_loss:117.94549560546875\n",
            "2048/3000 train_loss: 52.69329833984375 test_loss:102.14544677734375\n",
            "2049/3000 train_loss: 53.51219940185547 test_loss:108.64309692382812\n",
            "2050/3000 train_loss: 47.7508544921875 test_loss:102.23955535888672\n",
            "2051/3000 train_loss: 46.48358917236328 test_loss:100.901611328125\n",
            "2052/3000 train_loss: 50.334075927734375 test_loss:107.30670928955078\n",
            "2053/3000 train_loss: 52.84215545654297 test_loss:110.43402862548828\n",
            "2054/3000 train_loss: 56.656768798828125 test_loss:100.36446380615234\n",
            "2055/3000 train_loss: 46.93989944458008 test_loss:108.50921630859375\n",
            "2056/3000 train_loss: 65.1854476928711 test_loss:112.5347671508789\n",
            "2057/3000 train_loss: 52.58279800415039 test_loss:102.43843078613281\n",
            "2058/3000 train_loss: 52.51541519165039 test_loss:107.04844665527344\n",
            "2059/3000 train_loss: 52.995609283447266 test_loss:112.944580078125\n",
            "2060/3000 train_loss: 53.0584831237793 test_loss:114.84696960449219\n",
            "2061/3000 train_loss: 52.82847213745117 test_loss:104.3802490234375\n",
            "2062/3000 train_loss: 52.00640106201172 test_loss:113.20782470703125\n",
            "2063/3000 train_loss: 47.67515182495117 test_loss:96.17838287353516\n",
            "2064/3000 train_loss: 52.58026885986328 test_loss:104.40721130371094\n",
            "2065/3000 train_loss: 52.981170654296875 test_loss:121.44153594970703\n",
            "2066/3000 train_loss: 53.334388732910156 test_loss:111.79801940917969\n",
            "2067/3000 train_loss: 59.22774887084961 test_loss:115.21866607666016\n",
            "2068/3000 train_loss: 50.29536437988281 test_loss:98.75885009765625\n",
            "2069/3000 train_loss: 46.18992614746094 test_loss:97.9424057006836\n",
            "2070/3000 train_loss: 58.090850830078125 test_loss:108.45993041992188\n",
            "2071/3000 train_loss: 54.633602142333984 test_loss:103.52091217041016\n",
            "2072/3000 train_loss: 57.23360061645508 test_loss:99.855224609375\n",
            "2073/3000 train_loss: 58.348731994628906 test_loss:100.37298583984375\n",
            "2074/3000 train_loss: 52.09922790527344 test_loss:118.58839416503906\n",
            "2075/3000 train_loss: 57.36223220825195 test_loss:119.7434310913086\n",
            "2076/3000 train_loss: 55.78221130371094 test_loss:109.20339965820312\n",
            "2077/3000 train_loss: 52.621646881103516 test_loss:106.5843505859375\n",
            "2078/3000 train_loss: 51.844581604003906 test_loss:100.68658447265625\n",
            "2079/3000 train_loss: 54.02962112426758 test_loss:96.48336791992188\n",
            "2080/3000 train_loss: 56.832008361816406 test_loss:108.57255554199219\n",
            "2081/3000 train_loss: 50.59756088256836 test_loss:110.8611068725586\n",
            "2082/3000 train_loss: 58.60472106933594 test_loss:105.15602111816406\n",
            "2083/3000 train_loss: 51.10885238647461 test_loss:106.07131958007812\n",
            "2084/3000 train_loss: 49.644317626953125 test_loss:106.05330657958984\n",
            "2085/3000 train_loss: 57.84052658081055 test_loss:95.55914306640625\n",
            "2086/3000 train_loss: 51.62031173706055 test_loss:123.32335662841797\n",
            "2087/3000 train_loss: 53.025821685791016 test_loss:107.92523193359375\n",
            "2088/3000 train_loss: 48.69504165649414 test_loss:105.58344268798828\n",
            "2089/3000 train_loss: 51.820457458496094 test_loss:95.8506851196289\n",
            "2090/3000 train_loss: 48.762882232666016 test_loss:108.75829315185547\n",
            "2091/3000 train_loss: 48.80061340332031 test_loss:102.16554260253906\n",
            "2092/3000 train_loss: 55.50423812866211 test_loss:118.02903747558594\n",
            "2093/3000 train_loss: 48.95163345336914 test_loss:103.60733032226562\n",
            "2094/3000 train_loss: 52.20826721191406 test_loss:117.69119262695312\n",
            "2095/3000 train_loss: 48.084537506103516 test_loss:100.00294494628906\n",
            "2096/3000 train_loss: 50.75321960449219 test_loss:116.50748443603516\n",
            "2097/3000 train_loss: 46.3851432800293 test_loss:106.8743896484375\n",
            "2098/3000 train_loss: 50.10889434814453 test_loss:112.25459289550781\n",
            "2099/3000 train_loss: 52.44392395019531 test_loss:110.92967224121094\n",
            "2100/3000 train_loss: 54.22080612182617 test_loss:110.76921081542969\n",
            "2101/3000 train_loss: 47.13883590698242 test_loss:110.76219177246094\n",
            "2102/3000 train_loss: 52.12349319458008 test_loss:111.71885681152344\n",
            "2103/3000 train_loss: 41.5045166015625 test_loss:112.85688781738281\n",
            "2104/3000 train_loss: 48.77207565307617 test_loss:99.34861755371094\n",
            "2105/3000 train_loss: 48.87693786621094 test_loss:93.10751342773438\n",
            "2106/3000 train_loss: 56.56378173828125 test_loss:102.89594268798828\n",
            "2107/3000 train_loss: 46.722816467285156 test_loss:90.80097198486328\n",
            "2108/3000 train_loss: 49.86506652832031 test_loss:114.77351379394531\n",
            "2109/3000 train_loss: 46.895301818847656 test_loss:99.57281494140625\n",
            "2110/3000 train_loss: 57.634925842285156 test_loss:94.66114044189453\n",
            "2111/3000 train_loss: 52.87776565551758 test_loss:107.19930267333984\n",
            "2112/3000 train_loss: 45.022010803222656 test_loss:106.4266128540039\n",
            "2113/3000 train_loss: 49.1646728515625 test_loss:97.1852798461914\n",
            "2114/3000 train_loss: 53.988380432128906 test_loss:100.75852966308594\n",
            "2115/3000 train_loss: 46.475399017333984 test_loss:102.51971435546875\n",
            "2116/3000 train_loss: 50.21781539916992 test_loss:94.32038879394531\n",
            "2117/3000 train_loss: 55.50687026977539 test_loss:97.25437927246094\n",
            "2118/3000 train_loss: 49.69218826293945 test_loss:98.7001953125\n",
            "2119/3000 train_loss: 54.830352783203125 test_loss:97.06153869628906\n",
            "2120/3000 train_loss: 53.096988677978516 test_loss:104.66560363769531\n",
            "2121/3000 train_loss: 49.80107879638672 test_loss:112.6746826171875\n",
            "2122/3000 train_loss: 48.47300338745117 test_loss:106.67391204833984\n",
            "2123/3000 train_loss: 53.55132293701172 test_loss:98.3393783569336\n",
            "2124/3000 train_loss: 56.38719940185547 test_loss:109.6697769165039\n",
            "2125/3000 train_loss: 49.46522521972656 test_loss:101.89885711669922\n",
            "2126/3000 train_loss: 54.36701202392578 test_loss:117.04115295410156\n",
            "2127/3000 train_loss: 47.93959426879883 test_loss:109.22481536865234\n",
            "2128/3000 train_loss: 56.21949005126953 test_loss:111.33399963378906\n",
            "2129/3000 train_loss: 48.48503875732422 test_loss:104.75665283203125\n",
            "2130/3000 train_loss: 54.582027435302734 test_loss:100.05377197265625\n",
            "2131/3000 train_loss: 57.61117172241211 test_loss:110.11946868896484\n",
            "2132/3000 train_loss: 51.23684310913086 test_loss:109.88784790039062\n",
            "2133/3000 train_loss: 60.4478874206543 test_loss:122.7093505859375\n",
            "2134/3000 train_loss: 50.85000991821289 test_loss:117.76799774169922\n",
            "2135/3000 train_loss: 52.39699172973633 test_loss:96.87996673583984\n",
            "2136/3000 train_loss: 55.1197624206543 test_loss:99.44158935546875\n",
            "2137/3000 train_loss: 54.43061447143555 test_loss:114.05610656738281\n",
            "2138/3000 train_loss: 57.818565368652344 test_loss:101.31044006347656\n",
            "2139/3000 train_loss: 60.96345138549805 test_loss:93.77304077148438\n",
            "2140/3000 train_loss: 50.26521301269531 test_loss:107.52611541748047\n",
            "2141/3000 train_loss: 51.338558197021484 test_loss:97.29116821289062\n",
            "2142/3000 train_loss: 48.57241439819336 test_loss:102.03630065917969\n",
            "2143/3000 train_loss: 59.12602615356445 test_loss:117.19322967529297\n",
            "2144/3000 train_loss: 47.598777770996094 test_loss:102.81858825683594\n",
            "2145/3000 train_loss: 55.60908889770508 test_loss:91.66644287109375\n",
            "2146/3000 train_loss: 59.84800720214844 test_loss:110.2327880859375\n",
            "2147/3000 train_loss: 47.45777893066406 test_loss:108.244140625\n",
            "2148/3000 train_loss: 58.57526397705078 test_loss:98.38218688964844\n",
            "2149/3000 train_loss: 46.899227142333984 test_loss:103.54261779785156\n",
            "2150/3000 train_loss: 46.368228912353516 test_loss:101.51905059814453\n",
            "2151/3000 train_loss: 59.42687225341797 test_loss:106.17866516113281\n",
            "2152/3000 train_loss: 62.09995651245117 test_loss:115.1142578125\n",
            "2153/3000 train_loss: 43.212196350097656 test_loss:95.08431243896484\n",
            "2154/3000 train_loss: 44.60549545288086 test_loss:102.88129425048828\n",
            "2155/3000 train_loss: 47.18587112426758 test_loss:90.3935317993164\n",
            "2156/3000 train_loss: 46.60393524169922 test_loss:112.22057342529297\n",
            "2157/3000 train_loss: 44.68779754638672 test_loss:93.26136779785156\n",
            "2158/3000 train_loss: 50.698760986328125 test_loss:104.94364166259766\n",
            "2159/3000 train_loss: 48.08857727050781 test_loss:111.16926574707031\n",
            "2160/3000 train_loss: 48.359622955322266 test_loss:102.3992691040039\n",
            "2161/3000 train_loss: 55.09064865112305 test_loss:97.6073226928711\n",
            "2162/3000 train_loss: 52.08636474609375 test_loss:109.91900634765625\n",
            "2163/3000 train_loss: 50.777984619140625 test_loss:98.93601989746094\n",
            "2164/3000 train_loss: 46.4526252746582 test_loss:99.59184265136719\n",
            "2165/3000 train_loss: 47.35676574707031 test_loss:97.85078430175781\n",
            "2166/3000 train_loss: 47.57759475708008 test_loss:103.46952819824219\n",
            "2167/3000 train_loss: 46.59363555908203 test_loss:104.83200073242188\n",
            "2168/3000 train_loss: 49.006500244140625 test_loss:108.57911682128906\n",
            "2169/3000 train_loss: 62.0740966796875 test_loss:112.324951171875\n",
            "2170/3000 train_loss: 45.183189392089844 test_loss:110.3619384765625\n",
            "2171/3000 train_loss: 47.11726379394531 test_loss:105.3617935180664\n",
            "2172/3000 train_loss: 49.96245574951172 test_loss:122.86829376220703\n",
            "2173/3000 train_loss: 55.18889617919922 test_loss:100.27194213867188\n",
            "2174/3000 train_loss: 47.72404098510742 test_loss:94.16325378417969\n",
            "2175/3000 train_loss: 49.63427734375 test_loss:109.10060119628906\n",
            "2176/3000 train_loss: 55.48861312866211 test_loss:109.46546936035156\n",
            "2177/3000 train_loss: 50.607635498046875 test_loss:91.21674346923828\n",
            "2178/3000 train_loss: 49.08727264404297 test_loss:112.98157501220703\n",
            "2179/3000 train_loss: 44.61176300048828 test_loss:100.69169616699219\n",
            "2180/3000 train_loss: 55.737815856933594 test_loss:97.88352966308594\n",
            "2181/3000 train_loss: 51.158775329589844 test_loss:99.52391052246094\n",
            "2182/3000 train_loss: 47.52300262451172 test_loss:111.82015991210938\n",
            "2183/3000 train_loss: 52.03154373168945 test_loss:107.20391845703125\n",
            "2184/3000 train_loss: 52.511470794677734 test_loss:115.39201354980469\n",
            "2185/3000 train_loss: 56.79800796508789 test_loss:109.91500854492188\n",
            "2186/3000 train_loss: 54.077735900878906 test_loss:111.32230377197266\n",
            "2187/3000 train_loss: 49.229129791259766 test_loss:102.77415466308594\n",
            "2188/3000 train_loss: 49.201236724853516 test_loss:96.47764587402344\n",
            "2189/3000 train_loss: 51.477882385253906 test_loss:108.09317016601562\n",
            "2190/3000 train_loss: 50.18804168701172 test_loss:103.26091766357422\n",
            "2191/3000 train_loss: 55.540313720703125 test_loss:105.40884399414062\n",
            "2192/3000 train_loss: 51.034767150878906 test_loss:117.4247817993164\n",
            "2193/3000 train_loss: 52.541282653808594 test_loss:103.31130981445312\n",
            "2194/3000 train_loss: 49.81793212890625 test_loss:99.37789916992188\n",
            "2195/3000 train_loss: 41.95491409301758 test_loss:103.60247802734375\n",
            "2196/3000 train_loss: 57.37514114379883 test_loss:120.95260620117188\n",
            "2197/3000 train_loss: 53.020355224609375 test_loss:112.11016845703125\n",
            "2198/3000 train_loss: 48.651771545410156 test_loss:110.05757904052734\n",
            "2199/3000 train_loss: 58.44908905029297 test_loss:102.83657836914062\n",
            "2200/3000 train_loss: 49.13086700439453 test_loss:96.72733306884766\n",
            "2201/3000 train_loss: 58.47148895263672 test_loss:97.86516571044922\n",
            "2202/3000 train_loss: 49.53099822998047 test_loss:100.21802520751953\n",
            "2203/3000 train_loss: 53.36922836303711 test_loss:99.54651641845703\n",
            "2204/3000 train_loss: 55.573482513427734 test_loss:109.94563293457031\n",
            "2205/3000 train_loss: 45.883514404296875 test_loss:102.213623046875\n",
            "2206/3000 train_loss: 46.04009246826172 test_loss:108.83772277832031\n",
            "2207/3000 train_loss: 51.170989990234375 test_loss:97.65216064453125\n",
            "2208/3000 train_loss: 50.45475769042969 test_loss:97.5802230834961\n",
            "2209/3000 train_loss: 48.83477783203125 test_loss:113.9985580444336\n",
            "2210/3000 train_loss: 51.12403106689453 test_loss:93.84163665771484\n",
            "2211/3000 train_loss: 55.90427780151367 test_loss:104.35116577148438\n",
            "2212/3000 train_loss: 50.85497283935547 test_loss:99.17671203613281\n",
            "2213/3000 train_loss: 50.503787994384766 test_loss:102.09398651123047\n",
            "2214/3000 train_loss: 49.0057258605957 test_loss:118.95817565917969\n",
            "2215/3000 train_loss: 47.41969680786133 test_loss:104.56523132324219\n",
            "2216/3000 train_loss: 49.84848403930664 test_loss:113.64728546142578\n",
            "2217/3000 train_loss: 53.98345947265625 test_loss:98.26667785644531\n",
            "2218/3000 train_loss: 63.957759857177734 test_loss:114.62816619873047\n",
            "2219/3000 train_loss: 46.37511444091797 test_loss:107.41912841796875\n",
            "2220/3000 train_loss: 46.62377166748047 test_loss:101.89533233642578\n",
            "2221/3000 train_loss: 53.91229248046875 test_loss:101.67202758789062\n",
            "2222/3000 train_loss: 51.144100189208984 test_loss:107.49040985107422\n",
            "2223/3000 train_loss: 52.8159065246582 test_loss:105.4754409790039\n",
            "2224/3000 train_loss: 55.277469635009766 test_loss:101.32388305664062\n",
            "2225/3000 train_loss: 48.76578903198242 test_loss:97.49589538574219\n",
            "2226/3000 train_loss: 46.707298278808594 test_loss:101.03382110595703\n",
            "2227/3000 train_loss: 63.700157165527344 test_loss:103.23773956298828\n",
            "2228/3000 train_loss: 49.244789123535156 test_loss:104.7963638305664\n",
            "2229/3000 train_loss: 52.83696365356445 test_loss:94.63702392578125\n",
            "2230/3000 train_loss: 58.55302429199219 test_loss:98.36416625976562\n",
            "2231/3000 train_loss: 49.33506393432617 test_loss:100.95606231689453\n",
            "2232/3000 train_loss: 49.9210090637207 test_loss:102.61285400390625\n",
            "2233/3000 train_loss: 50.61858367919922 test_loss:100.18033599853516\n",
            "2234/3000 train_loss: 46.98088455200195 test_loss:110.2648696899414\n",
            "2235/3000 train_loss: 43.72960662841797 test_loss:95.96540832519531\n",
            "2236/3000 train_loss: 46.48558044433594 test_loss:103.72249603271484\n",
            "2237/3000 train_loss: 45.00241470336914 test_loss:102.0370864868164\n",
            "2238/3000 train_loss: 54.88288497924805 test_loss:98.26139068603516\n",
            "2239/3000 train_loss: 47.98755645751953 test_loss:98.36671447753906\n",
            "2240/3000 train_loss: 44.4306526184082 test_loss:99.88014221191406\n",
            "2241/3000 train_loss: 50.09034729003906 test_loss:108.45404815673828\n",
            "2242/3000 train_loss: 54.36349868774414 test_loss:98.28034210205078\n",
            "2243/3000 train_loss: 49.3362922668457 test_loss:98.35433197021484\n",
            "2244/3000 train_loss: 54.96923828125 test_loss:89.14808654785156\n",
            "2245/3000 train_loss: 47.40981674194336 test_loss:109.29146575927734\n",
            "2246/3000 train_loss: 48.20940017700195 test_loss:99.48004913330078\n",
            "2247/3000 train_loss: 48.39216995239258 test_loss:97.9842758178711\n",
            "2248/3000 train_loss: 46.885658264160156 test_loss:103.74850463867188\n",
            "2249/3000 train_loss: 47.5439567565918 test_loss:108.15690612792969\n",
            "2250/3000 train_loss: 50.62925720214844 test_loss:97.78981018066406\n",
            "2251/3000 train_loss: 49.084388732910156 test_loss:113.36663055419922\n",
            "2252/3000 train_loss: 51.10525131225586 test_loss:108.3084945678711\n",
            "2253/3000 train_loss: 47.907833099365234 test_loss:103.91680908203125\n",
            "2254/3000 train_loss: 49.50009536743164 test_loss:105.05953216552734\n",
            "2255/3000 train_loss: 48.454429626464844 test_loss:97.02742004394531\n",
            "2256/3000 train_loss: 51.10372543334961 test_loss:98.85829162597656\n",
            "2257/3000 train_loss: 53.386634826660156 test_loss:106.88111114501953\n",
            "2258/3000 train_loss: 53.764163970947266 test_loss:114.2820053100586\n",
            "2259/3000 train_loss: 47.431060791015625 test_loss:101.61671447753906\n",
            "2260/3000 train_loss: 42.90647888183594 test_loss:91.50397491455078\n",
            "2261/3000 train_loss: 50.358062744140625 test_loss:96.19304656982422\n",
            "2262/3000 train_loss: 47.69377899169922 test_loss:103.4095230102539\n",
            "2263/3000 train_loss: 47.84141540527344 test_loss:94.61605834960938\n",
            "2264/3000 train_loss: 43.56096267700195 test_loss:97.53619384765625\n",
            "2265/3000 train_loss: 47.41378402709961 test_loss:97.10894012451172\n",
            "2266/3000 train_loss: 57.381752014160156 test_loss:95.73817443847656\n",
            "2267/3000 train_loss: 52.15620040893555 test_loss:109.29793548583984\n",
            "2268/3000 train_loss: 46.24201965332031 test_loss:98.26144409179688\n",
            "2269/3000 train_loss: 51.54663848876953 test_loss:109.15819549560547\n",
            "2270/3000 train_loss: 52.43345260620117 test_loss:104.82833862304688\n",
            "2271/3000 train_loss: 48.06342315673828 test_loss:102.06349182128906\n",
            "2272/3000 train_loss: 50.71612548828125 test_loss:93.405517578125\n",
            "2273/3000 train_loss: 49.562103271484375 test_loss:103.25901794433594\n",
            "2274/3000 train_loss: 48.9191780090332 test_loss:103.01097106933594\n",
            "2275/3000 train_loss: 49.88432312011719 test_loss:108.25636291503906\n",
            "2276/3000 train_loss: 49.894405364990234 test_loss:97.19467163085938\n",
            "2277/3000 train_loss: 47.00933074951172 test_loss:103.28726196289062\n",
            "2278/3000 train_loss: 49.371849060058594 test_loss:104.072509765625\n",
            "2279/3000 train_loss: 53.777740478515625 test_loss:116.64540100097656\n",
            "2280/3000 train_loss: 47.95166015625 test_loss:95.65611267089844\n",
            "2281/3000 train_loss: 46.226776123046875 test_loss:109.51165771484375\n",
            "2282/3000 train_loss: 45.30815887451172 test_loss:91.32337951660156\n",
            "2283/3000 train_loss: 55.41803741455078 test_loss:107.3940658569336\n",
            "2284/3000 train_loss: 54.547183990478516 test_loss:108.10153198242188\n",
            "2285/3000 train_loss: 52.51261901855469 test_loss:118.26814270019531\n",
            "2286/3000 train_loss: 44.42812728881836 test_loss:106.71607971191406\n",
            "2287/3000 train_loss: 50.44239044189453 test_loss:106.11669158935547\n",
            "2288/3000 train_loss: 54.33584976196289 test_loss:102.9044418334961\n",
            "2289/3000 train_loss: 47.75831985473633 test_loss:96.96442413330078\n",
            "2290/3000 train_loss: 44.735443115234375 test_loss:103.6529769897461\n",
            "2291/3000 train_loss: 54.89072036743164 test_loss:100.08403778076172\n",
            "2292/3000 train_loss: 42.59697341918945 test_loss:110.92457580566406\n",
            "2293/3000 train_loss: 50.08983612060547 test_loss:102.39193725585938\n",
            "2294/3000 train_loss: 55.35770034790039 test_loss:93.10791015625\n",
            "2295/3000 train_loss: 59.666969299316406 test_loss:98.5577392578125\n",
            "2296/3000 train_loss: 51.655094146728516 test_loss:95.21332550048828\n",
            "2297/3000 train_loss: 49.424442291259766 test_loss:96.79203796386719\n",
            "2298/3000 train_loss: 52.9925651550293 test_loss:101.3708267211914\n",
            "2299/3000 train_loss: 59.678409576416016 test_loss:100.89248657226562\n",
            "2300/3000 train_loss: 50.54856491088867 test_loss:91.6103515625\n",
            "2301/3000 train_loss: 57.33942413330078 test_loss:99.90276336669922\n",
            "2302/3000 train_loss: 48.990135192871094 test_loss:110.32318878173828\n",
            "2303/3000 train_loss: 45.183616638183594 test_loss:103.37022399902344\n",
            "2304/3000 train_loss: 49.524723052978516 test_loss:102.78563690185547\n",
            "2305/3000 train_loss: 49.86433029174805 test_loss:122.12616729736328\n",
            "2306/3000 train_loss: 46.94275665283203 test_loss:113.25373840332031\n",
            "2307/3000 train_loss: 48.253326416015625 test_loss:86.56259155273438\n",
            "2308/3000 train_loss: 46.38175582885742 test_loss:95.10552978515625\n",
            "2309/3000 train_loss: 43.52277755737305 test_loss:95.95447540283203\n",
            "2310/3000 train_loss: 49.154624938964844 test_loss:99.22184753417969\n",
            "2311/3000 train_loss: 51.714744567871094 test_loss:100.80751037597656\n",
            "2312/3000 train_loss: 49.08285903930664 test_loss:106.61890411376953\n",
            "2313/3000 train_loss: 43.5250129699707 test_loss:95.91766357421875\n",
            "2314/3000 train_loss: 53.59066390991211 test_loss:90.30099487304688\n",
            "2315/3000 train_loss: 50.505126953125 test_loss:95.91569519042969\n",
            "2316/3000 train_loss: 46.61140823364258 test_loss:98.1441650390625\n",
            "2317/3000 train_loss: 48.60271072387695 test_loss:100.52596282958984\n",
            "2318/3000 train_loss: 53.540504455566406 test_loss:102.30941772460938\n",
            "2319/3000 train_loss: 41.58891296386719 test_loss:98.82962036132812\n",
            "2320/3000 train_loss: 52.17882537841797 test_loss:115.31782531738281\n",
            "2321/3000 train_loss: 48.796470642089844 test_loss:90.12983703613281\n",
            "2322/3000 train_loss: 43.33706283569336 test_loss:96.41722106933594\n",
            "2323/3000 train_loss: 46.6668586730957 test_loss:92.88789367675781\n",
            "2324/3000 train_loss: 49.47176742553711 test_loss:97.60281372070312\n",
            "2325/3000 train_loss: 48.44068908691406 test_loss:102.12663269042969\n",
            "2326/3000 train_loss: 43.31232833862305 test_loss:95.98604583740234\n",
            "2327/3000 train_loss: 54.24101257324219 test_loss:108.38079833984375\n",
            "2328/3000 train_loss: 49.04949951171875 test_loss:110.66891479492188\n",
            "2329/3000 train_loss: 52.011600494384766 test_loss:93.92634582519531\n",
            "2330/3000 train_loss: 68.87141418457031 test_loss:91.57125091552734\n",
            "2331/3000 train_loss: 47.239017486572266 test_loss:89.35221862792969\n",
            "2332/3000 train_loss: 51.621063232421875 test_loss:100.9827880859375\n",
            "2333/3000 train_loss: 52.92089080810547 test_loss:96.92386627197266\n",
            "2334/3000 train_loss: 51.542510986328125 test_loss:111.05250549316406\n",
            "2335/3000 train_loss: 49.9356803894043 test_loss:106.91542053222656\n",
            "2336/3000 train_loss: 49.0382080078125 test_loss:109.28701782226562\n",
            "2337/3000 train_loss: 49.339759826660156 test_loss:95.52949523925781\n",
            "2338/3000 train_loss: 47.03226089477539 test_loss:96.24099731445312\n",
            "2339/3000 train_loss: 42.232017517089844 test_loss:90.3771743774414\n",
            "2340/3000 train_loss: 43.71956253051758 test_loss:89.94569396972656\n",
            "2341/3000 train_loss: 40.465206146240234 test_loss:96.14820861816406\n",
            "2342/3000 train_loss: 45.45176696777344 test_loss:101.25032043457031\n",
            "2343/3000 train_loss: 43.11000061035156 test_loss:103.08161926269531\n",
            "2344/3000 train_loss: 45.57347106933594 test_loss:99.62586975097656\n",
            "2345/3000 train_loss: 47.73072052001953 test_loss:91.81871795654297\n",
            "2346/3000 train_loss: 46.25725555419922 test_loss:98.05130004882812\n",
            "2347/3000 train_loss: 41.772186279296875 test_loss:111.659912109375\n",
            "2348/3000 train_loss: 53.14466094970703 test_loss:98.255859375\n",
            "2349/3000 train_loss: 48.58523178100586 test_loss:100.16439056396484\n",
            "2350/3000 train_loss: 43.39045715332031 test_loss:91.69710540771484\n",
            "2351/3000 train_loss: 45.30567169189453 test_loss:89.7267074584961\n",
            "2352/3000 train_loss: 45.06764602661133 test_loss:87.21318817138672\n",
            "2353/3000 train_loss: 51.68876647949219 test_loss:92.48753356933594\n",
            "2354/3000 train_loss: 54.54084396362305 test_loss:102.58406066894531\n",
            "2355/3000 train_loss: 49.99734115600586 test_loss:108.94267272949219\n",
            "2356/3000 train_loss: 42.16184616088867 test_loss:94.14602661132812\n",
            "2357/3000 train_loss: 43.37159729003906 test_loss:103.88159942626953\n",
            "2358/3000 train_loss: 44.5594596862793 test_loss:106.50001525878906\n",
            "2359/3000 train_loss: 49.61809539794922 test_loss:96.48121643066406\n",
            "2360/3000 train_loss: 45.80144119262695 test_loss:98.55592346191406\n",
            "2361/3000 train_loss: 49.970848083496094 test_loss:94.32089233398438\n",
            "2362/3000 train_loss: 45.9293327331543 test_loss:109.39796447753906\n",
            "2363/3000 train_loss: 44.490413665771484 test_loss:108.20997619628906\n",
            "2364/3000 train_loss: 45.47780227661133 test_loss:100.24604797363281\n",
            "2365/3000 train_loss: 55.994842529296875 test_loss:107.10401916503906\n",
            "2366/3000 train_loss: 50.01079177856445 test_loss:113.17770385742188\n",
            "2367/3000 train_loss: 45.89525604248047 test_loss:119.02849578857422\n",
            "2368/3000 train_loss: 56.08774948120117 test_loss:112.37896728515625\n",
            "2369/3000 train_loss: 51.899898529052734 test_loss:90.6560287475586\n",
            "2370/3000 train_loss: 45.809654235839844 test_loss:90.9974594116211\n",
            "2371/3000 train_loss: 44.70913314819336 test_loss:121.5172119140625\n",
            "2372/3000 train_loss: 44.78387451171875 test_loss:92.83665466308594\n",
            "2373/3000 train_loss: 45.02804183959961 test_loss:97.44978332519531\n",
            "2374/3000 train_loss: 47.92574691772461 test_loss:101.72035217285156\n",
            "2375/3000 train_loss: 43.027339935302734 test_loss:103.682861328125\n",
            "2376/3000 train_loss: 47.83586502075195 test_loss:109.78445434570312\n",
            "2377/3000 train_loss: 48.85344314575195 test_loss:103.88436126708984\n",
            "2378/3000 train_loss: 49.422080993652344 test_loss:113.90926361083984\n",
            "2379/3000 train_loss: 44.10200500488281 test_loss:93.4049301147461\n",
            "2380/3000 train_loss: 53.556861877441406 test_loss:89.75188446044922\n",
            "2381/3000 train_loss: 50.377418518066406 test_loss:101.39360809326172\n",
            "2382/3000 train_loss: 49.703880310058594 test_loss:104.14802551269531\n",
            "2383/3000 train_loss: 47.81047821044922 test_loss:100.14773559570312\n",
            "2384/3000 train_loss: 46.11695861816406 test_loss:93.3101806640625\n",
            "2385/3000 train_loss: 59.26700973510742 test_loss:92.75654602050781\n",
            "2386/3000 train_loss: 52.62956237792969 test_loss:98.18157958984375\n",
            "2387/3000 train_loss: 53.17594909667969 test_loss:111.5702133178711\n",
            "2388/3000 train_loss: 45.95514678955078 test_loss:94.40861511230469\n",
            "2389/3000 train_loss: 47.40678787231445 test_loss:100.28678894042969\n",
            "2390/3000 train_loss: 46.75257873535156 test_loss:96.87503814697266\n",
            "2391/3000 train_loss: 43.01578140258789 test_loss:96.87394714355469\n",
            "2392/3000 train_loss: 44.9859619140625 test_loss:99.74714660644531\n",
            "2393/3000 train_loss: 51.29505920410156 test_loss:99.1751708984375\n",
            "2394/3000 train_loss: 45.63665008544922 test_loss:105.0204849243164\n",
            "2395/3000 train_loss: 45.35124969482422 test_loss:100.864501953125\n",
            "2396/3000 train_loss: 45.347171783447266 test_loss:100.58125305175781\n",
            "2397/3000 train_loss: 44.06542205810547 test_loss:93.53646850585938\n",
            "2398/3000 train_loss: 45.91419982910156 test_loss:109.45289611816406\n",
            "2399/3000 train_loss: 43.3406867980957 test_loss:100.75404357910156\n",
            "2400/3000 train_loss: 50.462947845458984 test_loss:108.20763397216797\n",
            "2401/3000 train_loss: 48.29743194580078 test_loss:100.38128662109375\n",
            "2402/3000 train_loss: 45.380523681640625 test_loss:104.21791076660156\n",
            "2403/3000 train_loss: 52.3189697265625 test_loss:92.66333770751953\n",
            "2404/3000 train_loss: 41.45152282714844 test_loss:112.4616470336914\n",
            "2405/3000 train_loss: 45.904911041259766 test_loss:89.1390380859375\n",
            "2406/3000 train_loss: 53.909942626953125 test_loss:109.51237487792969\n",
            "2407/3000 train_loss: 43.6906852722168 test_loss:115.5955810546875\n",
            "2408/3000 train_loss: 51.62577819824219 test_loss:107.3586654663086\n",
            "2409/3000 train_loss: 53.48068618774414 test_loss:95.9179458618164\n",
            "2410/3000 train_loss: 48.43179702758789 test_loss:96.26321411132812\n",
            "2411/3000 train_loss: 48.59282302856445 test_loss:85.11302185058594\n",
            "2412/3000 train_loss: 48.63270950317383 test_loss:100.2176513671875\n",
            "2413/3000 train_loss: 41.83036422729492 test_loss:111.00507354736328\n",
            "2414/3000 train_loss: 49.75290298461914 test_loss:101.15834045410156\n",
            "2415/3000 train_loss: 44.01720428466797 test_loss:103.3897933959961\n",
            "2416/3000 train_loss: 46.9984130859375 test_loss:110.48452758789062\n",
            "2417/3000 train_loss: 49.68021011352539 test_loss:101.91088104248047\n",
            "2418/3000 train_loss: 44.53075408935547 test_loss:111.6982192993164\n",
            "2419/3000 train_loss: 49.41448211669922 test_loss:106.52925872802734\n",
            "2420/3000 train_loss: 43.169647216796875 test_loss:109.60018920898438\n",
            "2421/3000 train_loss: 48.89547348022461 test_loss:102.62080383300781\n",
            "2422/3000 train_loss: 41.51657485961914 test_loss:117.57482147216797\n",
            "2423/3000 train_loss: 53.61148452758789 test_loss:92.25506591796875\n",
            "2424/3000 train_loss: 45.15400314331055 test_loss:105.13420104980469\n",
            "2425/3000 train_loss: 50.13839340209961 test_loss:96.66236114501953\n",
            "2426/3000 train_loss: 49.34279251098633 test_loss:111.34765625\n",
            "2427/3000 train_loss: 48.956050872802734 test_loss:89.83731842041016\n",
            "2428/3000 train_loss: 39.350555419921875 test_loss:97.31756591796875\n",
            "2429/3000 train_loss: 41.75691223144531 test_loss:93.55650329589844\n",
            "2430/3000 train_loss: 57.44963836669922 test_loss:103.10641479492188\n",
            "2431/3000 train_loss: 50.94221496582031 test_loss:104.19390106201172\n",
            "2432/3000 train_loss: 49.3956298828125 test_loss:94.38055419921875\n",
            "2433/3000 train_loss: 45.777069091796875 test_loss:95.87431335449219\n",
            "2434/3000 train_loss: 53.34394836425781 test_loss:98.5427474975586\n",
            "2435/3000 train_loss: 48.41019821166992 test_loss:96.63059997558594\n",
            "2436/3000 train_loss: 50.782066345214844 test_loss:87.69286346435547\n",
            "2437/3000 train_loss: 47.63957977294922 test_loss:96.15936279296875\n",
            "2438/3000 train_loss: 45.10531997680664 test_loss:93.50592041015625\n",
            "2439/3000 train_loss: 47.730255126953125 test_loss:91.6144790649414\n",
            "2440/3000 train_loss: 46.56310272216797 test_loss:96.23971557617188\n",
            "2441/3000 train_loss: 41.74320602416992 test_loss:91.47879791259766\n",
            "2442/3000 train_loss: 42.52762985229492 test_loss:96.42939758300781\n",
            "2443/3000 train_loss: 42.250465393066406 test_loss:103.37307739257812\n",
            "2444/3000 train_loss: 49.393150329589844 test_loss:109.48265075683594\n",
            "2445/3000 train_loss: 53.260799407958984 test_loss:118.10542297363281\n",
            "2446/3000 train_loss: 47.917388916015625 test_loss:107.3037338256836\n",
            "2447/3000 train_loss: 47.96510696411133 test_loss:115.41481018066406\n",
            "2448/3000 train_loss: 53.61948013305664 test_loss:115.6185531616211\n",
            "2449/3000 train_loss: 45.301082611083984 test_loss:113.75567626953125\n",
            "2450/3000 train_loss: 56.718204498291016 test_loss:113.00822448730469\n",
            "2451/3000 train_loss: 42.87399673461914 test_loss:100.15530395507812\n",
            "2452/3000 train_loss: 42.76577377319336 test_loss:96.06200408935547\n",
            "2453/3000 train_loss: 47.89838409423828 test_loss:99.2114486694336\n",
            "2454/3000 train_loss: 48.40575408935547 test_loss:97.41542053222656\n",
            "2455/3000 train_loss: 61.33237838745117 test_loss:96.48881530761719\n",
            "2456/3000 train_loss: 49.20607376098633 test_loss:114.8763427734375\n",
            "2457/3000 train_loss: 42.37641143798828 test_loss:100.6385726928711\n",
            "2458/3000 train_loss: 44.87501525878906 test_loss:105.82322692871094\n",
            "2459/3000 train_loss: 43.63565444946289 test_loss:104.486572265625\n",
            "2460/3000 train_loss: 46.44382095336914 test_loss:99.15133666992188\n",
            "2461/3000 train_loss: 52.11021423339844 test_loss:86.62401580810547\n",
            "2462/3000 train_loss: 48.65727233886719 test_loss:96.48875427246094\n",
            "2463/3000 train_loss: 48.65148162841797 test_loss:98.53311920166016\n",
            "2464/3000 train_loss: 48.57395553588867 test_loss:97.23599243164062\n",
            "2465/3000 train_loss: 46.96419906616211 test_loss:94.69054412841797\n",
            "2466/3000 train_loss: 50.379676818847656 test_loss:98.33869934082031\n",
            "2467/3000 train_loss: 46.40214538574219 test_loss:111.70650482177734\n",
            "2468/3000 train_loss: 45.192787170410156 test_loss:91.50457000732422\n",
            "2469/3000 train_loss: 51.6924934387207 test_loss:100.88152313232422\n",
            "2470/3000 train_loss: 44.478031158447266 test_loss:98.50119018554688\n",
            "2471/3000 train_loss: 48.21072006225586 test_loss:101.4234848022461\n",
            "2472/3000 train_loss: 46.149234771728516 test_loss:90.52658081054688\n",
            "2473/3000 train_loss: 44.60451126098633 test_loss:94.94591522216797\n",
            "2474/3000 train_loss: 50.136051177978516 test_loss:87.66273498535156\n",
            "2475/3000 train_loss: 47.142433166503906 test_loss:91.97439575195312\n",
            "2476/3000 train_loss: 47.28562927246094 test_loss:95.85967254638672\n",
            "2477/3000 train_loss: 41.48475646972656 test_loss:95.13948059082031\n",
            "2478/3000 train_loss: 43.27333068847656 test_loss:100.09357452392578\n",
            "2479/3000 train_loss: 49.594078063964844 test_loss:90.04998016357422\n",
            "2480/3000 train_loss: 51.93852233886719 test_loss:102.253662109375\n",
            "2481/3000 train_loss: 39.71537780761719 test_loss:96.49901580810547\n",
            "2482/3000 train_loss: 39.7293586730957 test_loss:101.37835693359375\n",
            "2483/3000 train_loss: 44.009681701660156 test_loss:103.93639373779297\n",
            "2484/3000 train_loss: 48.311065673828125 test_loss:94.16572570800781\n",
            "2485/3000 train_loss: 47.47664260864258 test_loss:107.72636413574219\n",
            "2486/3000 train_loss: 47.33015823364258 test_loss:98.21734619140625\n",
            "2487/3000 train_loss: 44.26292037963867 test_loss:93.75892639160156\n",
            "2488/3000 train_loss: 44.47361373901367 test_loss:87.81429290771484\n",
            "2489/3000 train_loss: 50.055450439453125 test_loss:89.88520050048828\n",
            "2490/3000 train_loss: 50.086509704589844 test_loss:97.24398803710938\n",
            "2491/3000 train_loss: 49.7119255065918 test_loss:112.49140930175781\n",
            "2492/3000 train_loss: 40.38665008544922 test_loss:84.96118927001953\n",
            "2493/3000 train_loss: 42.98491287231445 test_loss:85.4970703125\n",
            "2494/3000 train_loss: 51.45341491699219 test_loss:88.38484954833984\n",
            "2495/3000 train_loss: 53.87217330932617 test_loss:87.54634094238281\n",
            "2496/3000 train_loss: 45.945919036865234 test_loss:94.55931854248047\n",
            "2497/3000 train_loss: 45.04304504394531 test_loss:86.34756469726562\n",
            "2498/3000 train_loss: 44.206722259521484 test_loss:98.18417358398438\n",
            "2499/3000 train_loss: 48.15330123901367 test_loss:89.65039825439453\n",
            "2500/3000 train_loss: 51.67886734008789 test_loss:100.34099578857422\n",
            "2501/3000 train_loss: 38.81472396850586 test_loss:88.39869689941406\n",
            "2502/3000 train_loss: 47.919891357421875 test_loss:101.88502502441406\n",
            "2503/3000 train_loss: 43.45964813232422 test_loss:98.38958740234375\n",
            "2504/3000 train_loss: 43.49482345581055 test_loss:108.21340942382812\n",
            "2505/3000 train_loss: 39.30343246459961 test_loss:95.71409606933594\n",
            "2506/3000 train_loss: 46.70891189575195 test_loss:85.14994049072266\n",
            "2507/3000 train_loss: 45.28642272949219 test_loss:91.94308471679688\n",
            "2508/3000 train_loss: 39.26349639892578 test_loss:88.79518127441406\n",
            "2509/3000 train_loss: 41.215057373046875 test_loss:97.38087463378906\n",
            "2510/3000 train_loss: 45.902748107910156 test_loss:105.33576965332031\n",
            "2511/3000 train_loss: 43.34672164916992 test_loss:95.50464630126953\n",
            "2512/3000 train_loss: 43.863372802734375 test_loss:95.65149688720703\n",
            "2513/3000 train_loss: 40.40631103515625 test_loss:89.7072525024414\n",
            "2514/3000 train_loss: 44.61333465576172 test_loss:88.06085968017578\n",
            "2515/3000 train_loss: 42.93379592895508 test_loss:87.84651184082031\n",
            "2516/3000 train_loss: 47.35841751098633 test_loss:89.80850219726562\n",
            "2517/3000 train_loss: 47.0700798034668 test_loss:80.24324798583984\n",
            "2518/3000 train_loss: 56.27571105957031 test_loss:112.81333923339844\n",
            "2519/3000 train_loss: 43.61420440673828 test_loss:101.35818481445312\n",
            "2520/3000 train_loss: 47.807044982910156 test_loss:89.39513397216797\n",
            "2521/3000 train_loss: 48.95875930786133 test_loss:87.33184814453125\n",
            "2522/3000 train_loss: 54.419891357421875 test_loss:92.3634262084961\n",
            "2523/3000 train_loss: 48.183998107910156 test_loss:97.03471374511719\n",
            "2524/3000 train_loss: 42.79074478149414 test_loss:91.18016052246094\n",
            "2525/3000 train_loss: 41.598594665527344 test_loss:101.52095031738281\n",
            "2526/3000 train_loss: 42.829444885253906 test_loss:108.11203002929688\n",
            "2527/3000 train_loss: 42.78345489501953 test_loss:102.54998779296875\n",
            "2528/3000 train_loss: 41.88235855102539 test_loss:102.74822998046875\n",
            "2529/3000 train_loss: 48.33399200439453 test_loss:99.36421203613281\n",
            "2530/3000 train_loss: 42.802066802978516 test_loss:98.90580749511719\n",
            "2531/3000 train_loss: 44.473426818847656 test_loss:96.69564056396484\n",
            "2532/3000 train_loss: 46.513343811035156 test_loss:93.55046844482422\n",
            "2533/3000 train_loss: 46.501312255859375 test_loss:95.85826873779297\n",
            "2534/3000 train_loss: 50.20781707763672 test_loss:108.27548217773438\n",
            "2535/3000 train_loss: 45.291194915771484 test_loss:94.02749633789062\n",
            "2536/3000 train_loss: 44.06806182861328 test_loss:107.11640930175781\n",
            "2537/3000 train_loss: 43.555057525634766 test_loss:97.38047790527344\n",
            "2538/3000 train_loss: 49.2238883972168 test_loss:87.71179962158203\n",
            "2539/3000 train_loss: 53.59356689453125 test_loss:88.42194366455078\n",
            "2540/3000 train_loss: 44.25372314453125 test_loss:95.34465789794922\n",
            "2541/3000 train_loss: 50.797752380371094 test_loss:90.30680847167969\n",
            "2542/3000 train_loss: 47.290367126464844 test_loss:91.56060028076172\n",
            "2543/3000 train_loss: 39.618492126464844 test_loss:92.36337280273438\n",
            "2544/3000 train_loss: 48.99826431274414 test_loss:104.66062927246094\n",
            "2545/3000 train_loss: 39.44203567504883 test_loss:98.02371215820312\n",
            "2546/3000 train_loss: 44.59526824951172 test_loss:92.22640991210938\n",
            "2547/3000 train_loss: 44.14299774169922 test_loss:92.51081848144531\n",
            "2548/3000 train_loss: 43.520713806152344 test_loss:94.51747131347656\n",
            "2549/3000 train_loss: 43.679256439208984 test_loss:99.88018798828125\n",
            "2550/3000 train_loss: 50.61594009399414 test_loss:98.68873596191406\n",
            "2551/3000 train_loss: 43.569557189941406 test_loss:97.73976135253906\n",
            "2552/3000 train_loss: 48.27593994140625 test_loss:108.62030029296875\n",
            "2553/3000 train_loss: 41.77109909057617 test_loss:93.79371643066406\n",
            "2554/3000 train_loss: 46.728816986083984 test_loss:91.41027069091797\n",
            "2555/3000 train_loss: 46.164710998535156 test_loss:94.69437408447266\n",
            "2556/3000 train_loss: 47.695884704589844 test_loss:89.04806518554688\n",
            "2557/3000 train_loss: 45.66782760620117 test_loss:87.02945709228516\n",
            "2558/3000 train_loss: 50.74571990966797 test_loss:90.55903625488281\n",
            "2559/3000 train_loss: 46.51532745361328 test_loss:95.9684829711914\n",
            "2560/3000 train_loss: 38.99968719482422 test_loss:88.99819946289062\n",
            "2561/3000 train_loss: 38.12044143676758 test_loss:92.78266906738281\n",
            "2562/3000 train_loss: 44.921905517578125 test_loss:96.09024047851562\n",
            "2563/3000 train_loss: 44.95817565917969 test_loss:99.0777816772461\n",
            "2564/3000 train_loss: 49.21525955200195 test_loss:85.60174560546875\n",
            "2565/3000 train_loss: 50.150917053222656 test_loss:96.04325866699219\n",
            "2566/3000 train_loss: 49.20412826538086 test_loss:101.540771484375\n",
            "2567/3000 train_loss: 48.087459564208984 test_loss:79.40673065185547\n",
            "2568/3000 train_loss: 44.81065368652344 test_loss:99.28375244140625\n",
            "2569/3000 train_loss: 43.68416976928711 test_loss:90.83380126953125\n",
            "2570/3000 train_loss: 46.660484313964844 test_loss:102.16305541992188\n",
            "2571/3000 train_loss: 57.009307861328125 test_loss:105.96231842041016\n",
            "2572/3000 train_loss: 44.356266021728516 test_loss:87.44862365722656\n",
            "2573/3000 train_loss: 46.05482864379883 test_loss:81.94158172607422\n",
            "2574/3000 train_loss: 49.035552978515625 test_loss:94.18174743652344\n",
            "2575/3000 train_loss: 47.533084869384766 test_loss:92.16102600097656\n",
            "2576/3000 train_loss: 50.447784423828125 test_loss:109.84817504882812\n",
            "2577/3000 train_loss: 55.741424560546875 test_loss:107.23432922363281\n",
            "2578/3000 train_loss: 43.831871032714844 test_loss:99.91370391845703\n",
            "2579/3000 train_loss: 48.51045608520508 test_loss:90.88297271728516\n",
            "2580/3000 train_loss: 46.2950439453125 test_loss:95.75919342041016\n",
            "2581/3000 train_loss: 45.766849517822266 test_loss:96.873779296875\n",
            "2582/3000 train_loss: 45.2932014465332 test_loss:102.59893798828125\n",
            "2583/3000 train_loss: 43.13883972167969 test_loss:89.22129821777344\n",
            "2584/3000 train_loss: 53.42930221557617 test_loss:100.50993347167969\n",
            "2585/3000 train_loss: 45.29660415649414 test_loss:93.59266662597656\n",
            "2586/3000 train_loss: 49.3187255859375 test_loss:89.84629821777344\n",
            "2587/3000 train_loss: 42.69501495361328 test_loss:83.66527557373047\n",
            "2588/3000 train_loss: 45.98353576660156 test_loss:85.40784454345703\n",
            "2589/3000 train_loss: 46.878173828125 test_loss:95.89236450195312\n",
            "2590/3000 train_loss: 42.26206970214844 test_loss:97.02595520019531\n",
            "2591/3000 train_loss: 36.57310104370117 test_loss:94.04000854492188\n",
            "2592/3000 train_loss: 43.33580017089844 test_loss:88.35890197753906\n",
            "2593/3000 train_loss: 40.83185577392578 test_loss:108.13722229003906\n",
            "2594/3000 train_loss: 39.26909637451172 test_loss:88.19954681396484\n",
            "2595/3000 train_loss: 51.14143753051758 test_loss:88.11448669433594\n",
            "2596/3000 train_loss: 45.06172561645508 test_loss:96.77071380615234\n",
            "2597/3000 train_loss: 41.756874084472656 test_loss:95.90353393554688\n",
            "2598/3000 train_loss: 46.05014419555664 test_loss:85.1159439086914\n",
            "2599/3000 train_loss: 38.65625 test_loss:101.11727905273438\n",
            "2600/3000 train_loss: 44.364166259765625 test_loss:98.83769989013672\n",
            "2601/3000 train_loss: 46.068817138671875 test_loss:99.95557403564453\n",
            "2602/3000 train_loss: 46.02068328857422 test_loss:89.95147705078125\n",
            "2603/3000 train_loss: 46.627708435058594 test_loss:84.016845703125\n",
            "2604/3000 train_loss: 41.21785354614258 test_loss:92.72795104980469\n",
            "2605/3000 train_loss: 41.81655502319336 test_loss:94.55867004394531\n",
            "2606/3000 train_loss: 42.90757369995117 test_loss:90.46987915039062\n",
            "2607/3000 train_loss: 43.03965377807617 test_loss:98.72538757324219\n",
            "2608/3000 train_loss: 44.83885955810547 test_loss:89.7573471069336\n",
            "2609/3000 train_loss: 48.7904167175293 test_loss:96.25072479248047\n",
            "2610/3000 train_loss: 42.978206634521484 test_loss:114.49681854248047\n",
            "2611/3000 train_loss: 52.61336135864258 test_loss:99.28473663330078\n",
            "2612/3000 train_loss: 51.35660934448242 test_loss:98.38504028320312\n",
            "2613/3000 train_loss: 39.32337951660156 test_loss:93.23957824707031\n",
            "2614/3000 train_loss: 43.5923957824707 test_loss:110.15994262695312\n",
            "2615/3000 train_loss: 39.59004211425781 test_loss:93.537109375\n",
            "2616/3000 train_loss: 47.56987380981445 test_loss:82.03913879394531\n",
            "2617/3000 train_loss: 45.8149299621582 test_loss:102.34523010253906\n",
            "2618/3000 train_loss: 42.37002182006836 test_loss:104.73007202148438\n",
            "2619/3000 train_loss: 44.634483337402344 test_loss:82.4544906616211\n",
            "2620/3000 train_loss: 38.27485275268555 test_loss:101.38832092285156\n",
            "2621/3000 train_loss: 46.8070068359375 test_loss:88.38119506835938\n",
            "2622/3000 train_loss: 44.393272399902344 test_loss:80.76771545410156\n",
            "2623/3000 train_loss: 43.251121520996094 test_loss:87.39004516601562\n",
            "2624/3000 train_loss: 44.92927551269531 test_loss:87.02930450439453\n",
            "2625/3000 train_loss: 47.89083480834961 test_loss:103.23922729492188\n",
            "2626/3000 train_loss: 45.336769104003906 test_loss:96.0705795288086\n",
            "2627/3000 train_loss: 48.00598907470703 test_loss:97.48594665527344\n",
            "2628/3000 train_loss: 45.134647369384766 test_loss:89.11212921142578\n",
            "2629/3000 train_loss: 45.03506088256836 test_loss:98.54623413085938\n",
            "2630/3000 train_loss: 42.89869689941406 test_loss:81.31114959716797\n",
            "2631/3000 train_loss: 44.34335708618164 test_loss:86.31967163085938\n",
            "2632/3000 train_loss: 43.17951583862305 test_loss:86.75175476074219\n",
            "2633/3000 train_loss: 47.086727142333984 test_loss:88.94528198242188\n",
            "2634/3000 train_loss: 39.33799743652344 test_loss:85.55300903320312\n",
            "2635/3000 train_loss: 39.64541244506836 test_loss:95.5588150024414\n",
            "2636/3000 train_loss: 47.92142868041992 test_loss:97.29150390625\n",
            "2637/3000 train_loss: 47.64411926269531 test_loss:100.28524780273438\n",
            "2638/3000 train_loss: 42.951168060302734 test_loss:94.88276672363281\n",
            "2639/3000 train_loss: 45.22900390625 test_loss:87.98151397705078\n",
            "2640/3000 train_loss: 48.00362014770508 test_loss:84.42943572998047\n",
            "2641/3000 train_loss: 42.44671630859375 test_loss:99.08770751953125\n",
            "2642/3000 train_loss: 44.098018646240234 test_loss:94.69049072265625\n",
            "2643/3000 train_loss: 43.9780387878418 test_loss:94.04513549804688\n",
            "2644/3000 train_loss: 48.22439956665039 test_loss:98.5327377319336\n",
            "2645/3000 train_loss: 41.886287689208984 test_loss:93.0885009765625\n",
            "2646/3000 train_loss: 48.27602767944336 test_loss:87.67845916748047\n",
            "2647/3000 train_loss: 45.53459167480469 test_loss:91.30577087402344\n",
            "2648/3000 train_loss: 41.14596939086914 test_loss:94.52566528320312\n",
            "2649/3000 train_loss: 40.28450393676758 test_loss:90.50706481933594\n",
            "2650/3000 train_loss: 45.29404067993164 test_loss:81.99600219726562\n",
            "2651/3000 train_loss: 57.05826950073242 test_loss:99.3963623046875\n",
            "2652/3000 train_loss: 43.76066207885742 test_loss:110.5744400024414\n",
            "2653/3000 train_loss: 49.556671142578125 test_loss:97.58155059814453\n",
            "2654/3000 train_loss: 39.60160446166992 test_loss:91.94509887695312\n",
            "2655/3000 train_loss: 40.94694137573242 test_loss:107.34893035888672\n",
            "2656/3000 train_loss: 42.048702239990234 test_loss:98.55656433105469\n",
            "2657/3000 train_loss: 53.748619079589844 test_loss:88.05757141113281\n",
            "2658/3000 train_loss: 39.37847137451172 test_loss:101.50691223144531\n",
            "2659/3000 train_loss: 42.644744873046875 test_loss:87.21713256835938\n",
            "2660/3000 train_loss: 42.70258712768555 test_loss:109.67633819580078\n",
            "2661/3000 train_loss: 47.10322189331055 test_loss:86.69969940185547\n",
            "2662/3000 train_loss: 47.192298889160156 test_loss:90.1390151977539\n",
            "2663/3000 train_loss: 40.70438003540039 test_loss:94.46369934082031\n",
            "2664/3000 train_loss: 44.54815673828125 test_loss:93.94757080078125\n",
            "2665/3000 train_loss: 43.57444763183594 test_loss:85.05026245117188\n",
            "2666/3000 train_loss: 49.20347595214844 test_loss:80.74331665039062\n",
            "2667/3000 train_loss: 42.67730712890625 test_loss:92.51872253417969\n",
            "2668/3000 train_loss: 47.18923568725586 test_loss:94.26127624511719\n",
            "2669/3000 train_loss: 41.68647003173828 test_loss:98.56710815429688\n",
            "2670/3000 train_loss: 43.70060729980469 test_loss:92.85955810546875\n",
            "2671/3000 train_loss: 48.40874099731445 test_loss:106.66414642333984\n",
            "2672/3000 train_loss: 38.959312438964844 test_loss:101.07302856445312\n",
            "2673/3000 train_loss: 44.49797058105469 test_loss:104.11181640625\n",
            "2674/3000 train_loss: 45.69831848144531 test_loss:85.03083038330078\n",
            "2675/3000 train_loss: 34.9734001159668 test_loss:98.28584289550781\n",
            "2676/3000 train_loss: 42.390228271484375 test_loss:91.81745910644531\n",
            "2677/3000 train_loss: 45.95479202270508 test_loss:87.81135559082031\n",
            "2678/3000 train_loss: 43.46101379394531 test_loss:87.86212158203125\n",
            "2679/3000 train_loss: 42.10679244995117 test_loss:90.48535919189453\n",
            "2680/3000 train_loss: 45.76610565185547 test_loss:91.06173706054688\n",
            "2681/3000 train_loss: 46.82499694824219 test_loss:98.34534454345703\n",
            "2682/3000 train_loss: 45.286766052246094 test_loss:101.7075424194336\n",
            "2683/3000 train_loss: 45.19333267211914 test_loss:102.04161071777344\n",
            "2684/3000 train_loss: 43.40497970581055 test_loss:91.60931396484375\n",
            "2685/3000 train_loss: 40.599464416503906 test_loss:93.15809631347656\n",
            "2686/3000 train_loss: 48.72869873046875 test_loss:87.94924926757812\n",
            "2687/3000 train_loss: 47.282554626464844 test_loss:90.61193084716797\n",
            "2688/3000 train_loss: 46.25271987915039 test_loss:93.56884765625\n",
            "2689/3000 train_loss: 39.47315216064453 test_loss:95.99736022949219\n",
            "2690/3000 train_loss: 45.114967346191406 test_loss:88.75050354003906\n",
            "2691/3000 train_loss: 39.176517486572266 test_loss:99.46641540527344\n",
            "2692/3000 train_loss: 41.14085388183594 test_loss:93.4795150756836\n",
            "2693/3000 train_loss: 45.407470703125 test_loss:99.18232727050781\n",
            "2694/3000 train_loss: 43.133323669433594 test_loss:87.82475280761719\n",
            "2695/3000 train_loss: 43.528961181640625 test_loss:91.1904296875\n",
            "2696/3000 train_loss: 51.595035552978516 test_loss:97.31775665283203\n",
            "2697/3000 train_loss: 47.521827697753906 test_loss:96.7771987915039\n",
            "2698/3000 train_loss: 51.935176849365234 test_loss:124.36589813232422\n",
            "2699/3000 train_loss: 43.682456970214844 test_loss:94.16325378417969\n",
            "2700/3000 train_loss: 45.49989700317383 test_loss:94.0636215209961\n",
            "2701/3000 train_loss: 42.658721923828125 test_loss:87.77981567382812\n",
            "2702/3000 train_loss: 44.88397979736328 test_loss:102.12498474121094\n",
            "2703/3000 train_loss: 46.1273307800293 test_loss:96.29359436035156\n",
            "2704/3000 train_loss: 45.13848876953125 test_loss:90.73191833496094\n",
            "2705/3000 train_loss: 40.60858917236328 test_loss:89.40071105957031\n",
            "2706/3000 train_loss: 44.87544250488281 test_loss:88.28715515136719\n",
            "2707/3000 train_loss: 50.67274856567383 test_loss:93.56346893310547\n",
            "2708/3000 train_loss: 42.18878936767578 test_loss:98.22468566894531\n",
            "2709/3000 train_loss: 51.793556213378906 test_loss:100.63577270507812\n",
            "2710/3000 train_loss: 45.159889221191406 test_loss:109.9403076171875\n",
            "2711/3000 train_loss: 43.023189544677734 test_loss:93.65690612792969\n",
            "2712/3000 train_loss: 43.241539001464844 test_loss:106.65104675292969\n",
            "2713/3000 train_loss: 43.4400749206543 test_loss:78.5522232055664\n",
            "2714/3000 train_loss: 42.22891616821289 test_loss:89.74504089355469\n",
            "2715/3000 train_loss: 51.186683654785156 test_loss:92.52000427246094\n",
            "2716/3000 train_loss: 40.505943298339844 test_loss:93.690185546875\n",
            "2717/3000 train_loss: 40.87331771850586 test_loss:94.6373291015625\n",
            "2718/3000 train_loss: 42.535640716552734 test_loss:93.7383041381836\n",
            "2719/3000 train_loss: 45.24006271362305 test_loss:97.91752624511719\n",
            "2720/3000 train_loss: 41.0410270690918 test_loss:84.45406341552734\n",
            "2721/3000 train_loss: 51.357078552246094 test_loss:92.4040756225586\n",
            "2722/3000 train_loss: 43.04806137084961 test_loss:83.17788696289062\n",
            "2723/3000 train_loss: 44.439430236816406 test_loss:90.64166259765625\n",
            "2724/3000 train_loss: 40.979618072509766 test_loss:105.14588928222656\n",
            "2725/3000 train_loss: 48.06438064575195 test_loss:85.4101333618164\n",
            "2726/3000 train_loss: 46.1811408996582 test_loss:78.71885681152344\n",
            "2727/3000 train_loss: 45.607086181640625 test_loss:88.56820678710938\n",
            "2728/3000 train_loss: 47.89298629760742 test_loss:86.92587280273438\n",
            "2729/3000 train_loss: 43.70651626586914 test_loss:87.60368347167969\n",
            "2730/3000 train_loss: 45.91490936279297 test_loss:104.62391662597656\n",
            "2731/3000 train_loss: 39.66278076171875 test_loss:92.45429992675781\n",
            "2732/3000 train_loss: 49.280738830566406 test_loss:96.16616821289062\n",
            "2733/3000 train_loss: 43.06609344482422 test_loss:98.18602752685547\n",
            "2734/3000 train_loss: 43.81466293334961 test_loss:95.50753784179688\n",
            "2735/3000 train_loss: 51.055294036865234 test_loss:94.57215881347656\n",
            "2736/3000 train_loss: 43.4160270690918 test_loss:97.54411315917969\n",
            "2737/3000 train_loss: 41.830596923828125 test_loss:95.7711410522461\n",
            "2738/3000 train_loss: 45.860103607177734 test_loss:98.70013427734375\n",
            "2739/3000 train_loss: 42.607208251953125 test_loss:92.57318115234375\n",
            "2740/3000 train_loss: 38.863338470458984 test_loss:84.51271057128906\n",
            "2741/3000 train_loss: 39.823097229003906 test_loss:93.38685607910156\n",
            "2742/3000 train_loss: 42.15826416015625 test_loss:99.61569213867188\n",
            "2743/3000 train_loss: 43.947750091552734 test_loss:91.20832824707031\n",
            "2744/3000 train_loss: 42.90830612182617 test_loss:105.68748474121094\n",
            "2745/3000 train_loss: 41.221649169921875 test_loss:98.66232299804688\n",
            "2746/3000 train_loss: 42.26475524902344 test_loss:109.79842376708984\n",
            "2747/3000 train_loss: 41.373985290527344 test_loss:86.14749145507812\n",
            "2748/3000 train_loss: 41.53263854980469 test_loss:84.47099304199219\n",
            "2749/3000 train_loss: 45.81796646118164 test_loss:92.84440612792969\n",
            "2750/3000 train_loss: 40.22191619873047 test_loss:111.23527526855469\n",
            "2751/3000 train_loss: 47.00387191772461 test_loss:87.13888549804688\n",
            "2752/3000 train_loss: 45.15081787109375 test_loss:95.32124328613281\n",
            "2753/3000 train_loss: 42.62733840942383 test_loss:89.30879211425781\n",
            "2754/3000 train_loss: 44.161800384521484 test_loss:92.81983184814453\n",
            "2755/3000 train_loss: 40.1729621887207 test_loss:95.59686279296875\n",
            "2756/3000 train_loss: 42.14632797241211 test_loss:84.67178344726562\n",
            "2757/3000 train_loss: 41.48542404174805 test_loss:94.61021423339844\n",
            "2758/3000 train_loss: 45.1339111328125 test_loss:88.55586242675781\n",
            "2759/3000 train_loss: 39.25033950805664 test_loss:92.00802612304688\n",
            "2760/3000 train_loss: 49.88547897338867 test_loss:97.46070861816406\n",
            "2761/3000 train_loss: 48.472740173339844 test_loss:93.22470092773438\n",
            "2762/3000 train_loss: 40.760841369628906 test_loss:101.11976623535156\n",
            "2763/3000 train_loss: 46.412906646728516 test_loss:109.56954956054688\n",
            "2764/3000 train_loss: 51.81047821044922 test_loss:98.51799011230469\n",
            "2765/3000 train_loss: 41.77273178100586 test_loss:86.06071472167969\n",
            "2766/3000 train_loss: 43.87678527832031 test_loss:96.04800415039062\n",
            "2767/3000 train_loss: 39.25086212158203 test_loss:89.317626953125\n",
            "2768/3000 train_loss: 45.775962829589844 test_loss:111.44984436035156\n",
            "2769/3000 train_loss: 38.949974060058594 test_loss:90.27586364746094\n",
            "2770/3000 train_loss: 39.9925651550293 test_loss:83.55118560791016\n",
            "2771/3000 train_loss: 41.95933532714844 test_loss:92.91084289550781\n",
            "2772/3000 train_loss: 40.39054489135742 test_loss:95.03413391113281\n",
            "2773/3000 train_loss: 43.03215789794922 test_loss:90.9960708618164\n",
            "2774/3000 train_loss: 48.604454040527344 test_loss:86.71175384521484\n",
            "2775/3000 train_loss: 44.87357711791992 test_loss:88.31295776367188\n",
            "2776/3000 train_loss: 47.15761184692383 test_loss:94.82833099365234\n",
            "2777/3000 train_loss: 48.8020133972168 test_loss:105.50177764892578\n",
            "2778/3000 train_loss: 39.82209014892578 test_loss:96.10023498535156\n",
            "2779/3000 train_loss: 37.229942321777344 test_loss:90.11198425292969\n",
            "2780/3000 train_loss: 48.478668212890625 test_loss:92.0767593383789\n",
            "2781/3000 train_loss: 48.85402297973633 test_loss:89.49186706542969\n",
            "2782/3000 train_loss: 35.495731353759766 test_loss:86.79691314697266\n",
            "2783/3000 train_loss: 43.764678955078125 test_loss:94.08695983886719\n",
            "2784/3000 train_loss: 42.382415771484375 test_loss:82.70440673828125\n",
            "2785/3000 train_loss: 43.86146926879883 test_loss:84.6920166015625\n",
            "2786/3000 train_loss: 44.348575592041016 test_loss:87.85836791992188\n",
            "2787/3000 train_loss: 35.222747802734375 test_loss:90.8653564453125\n",
            "2788/3000 train_loss: 38.15919876098633 test_loss:86.37281799316406\n",
            "2789/3000 train_loss: 41.758602142333984 test_loss:96.66272735595703\n",
            "2790/3000 train_loss: 42.673187255859375 test_loss:88.10725402832031\n",
            "2791/3000 train_loss: 43.13116455078125 test_loss:92.03761291503906\n",
            "2792/3000 train_loss: 46.207130432128906 test_loss:95.31936645507812\n",
            "2793/3000 train_loss: 41.77015686035156 test_loss:87.67920684814453\n",
            "2794/3000 train_loss: 42.5633659362793 test_loss:97.39613342285156\n",
            "2795/3000 train_loss: 39.33140563964844 test_loss:92.9373779296875\n",
            "2796/3000 train_loss: 47.031063079833984 test_loss:91.8670654296875\n",
            "2797/3000 train_loss: 39.5679817199707 test_loss:87.78997802734375\n",
            "2798/3000 train_loss: 41.05100631713867 test_loss:103.34776306152344\n",
            "2799/3000 train_loss: 45.893131256103516 test_loss:97.35591125488281\n",
            "2800/3000 train_loss: 42.772789001464844 test_loss:95.971923828125\n",
            "2801/3000 train_loss: 39.477012634277344 test_loss:92.32807159423828\n",
            "2802/3000 train_loss: 42.1226806640625 test_loss:82.45468139648438\n",
            "2803/3000 train_loss: 46.46881103515625 test_loss:84.96061706542969\n",
            "2804/3000 train_loss: 41.777408599853516 test_loss:83.1042251586914\n",
            "2805/3000 train_loss: 40.57268142700195 test_loss:85.03681182861328\n",
            "2806/3000 train_loss: 46.363807678222656 test_loss:90.91181182861328\n",
            "2807/3000 train_loss: 39.669654846191406 test_loss:89.46859741210938\n",
            "2808/3000 train_loss: 45.074676513671875 test_loss:88.52310180664062\n",
            "2809/3000 train_loss: 39.68404006958008 test_loss:91.26616668701172\n",
            "2810/3000 train_loss: 47.47208023071289 test_loss:91.20879364013672\n",
            "2811/3000 train_loss: 42.215599060058594 test_loss:89.6351318359375\n",
            "2812/3000 train_loss: 40.3680305480957 test_loss:84.43531799316406\n",
            "2813/3000 train_loss: 44.05531692504883 test_loss:81.36933135986328\n",
            "2814/3000 train_loss: 43.25651931762695 test_loss:94.15400695800781\n",
            "2815/3000 train_loss: 46.41039276123047 test_loss:91.70491027832031\n",
            "2816/3000 train_loss: 46.70568084716797 test_loss:100.53427124023438\n",
            "2817/3000 train_loss: 45.2289924621582 test_loss:87.43070983886719\n",
            "2818/3000 train_loss: 42.16590881347656 test_loss:88.43709564208984\n",
            "2819/3000 train_loss: 44.993194580078125 test_loss:88.57148742675781\n",
            "2820/3000 train_loss: 42.51496887207031 test_loss:92.60978698730469\n",
            "2821/3000 train_loss: 38.31632614135742 test_loss:88.64176940917969\n",
            "2822/3000 train_loss: 44.81237030029297 test_loss:98.69287109375\n",
            "2823/3000 train_loss: 41.3433952331543 test_loss:96.0009994506836\n",
            "2824/3000 train_loss: 46.60109329223633 test_loss:96.0460205078125\n",
            "2825/3000 train_loss: 39.18333053588867 test_loss:86.0491714477539\n",
            "2826/3000 train_loss: 36.70418167114258 test_loss:91.00917053222656\n",
            "2827/3000 train_loss: 40.21529769897461 test_loss:97.84471130371094\n",
            "2828/3000 train_loss: 39.1084098815918 test_loss:94.99888610839844\n",
            "2829/3000 train_loss: 44.374027252197266 test_loss:94.54363250732422\n",
            "2830/3000 train_loss: 45.44359588623047 test_loss:101.5805435180664\n",
            "2831/3000 train_loss: 42.708065032958984 test_loss:83.49969482421875\n",
            "2832/3000 train_loss: 38.46330261230469 test_loss:110.81532287597656\n",
            "2833/3000 train_loss: 44.275306701660156 test_loss:92.02867889404297\n",
            "2834/3000 train_loss: 45.319305419921875 test_loss:103.05648040771484\n",
            "2835/3000 train_loss: 38.86159896850586 test_loss:106.20561218261719\n",
            "2836/3000 train_loss: 48.76361846923828 test_loss:106.22733306884766\n",
            "2837/3000 train_loss: 45.105018615722656 test_loss:117.0848388671875\n",
            "2838/3000 train_loss: 38.9247932434082 test_loss:85.9485855102539\n",
            "2839/3000 train_loss: 44.98566436767578 test_loss:90.3128662109375\n",
            "2840/3000 train_loss: 37.192623138427734 test_loss:99.5071029663086\n",
            "2841/3000 train_loss: 43.08955383300781 test_loss:101.43566131591797\n",
            "2842/3000 train_loss: 42.53425979614258 test_loss:103.18679809570312\n",
            "2843/3000 train_loss: 39.20491027832031 test_loss:88.57874298095703\n",
            "2844/3000 train_loss: 44.465274810791016 test_loss:110.18705749511719\n",
            "2845/3000 train_loss: 46.46682357788086 test_loss:110.2087631225586\n",
            "2846/3000 train_loss: 40.37520217895508 test_loss:98.23822021484375\n",
            "2847/3000 train_loss: 52.96017837524414 test_loss:93.69596862792969\n",
            "2848/3000 train_loss: 38.234798431396484 test_loss:94.70823669433594\n",
            "2849/3000 train_loss: 37.831260681152344 test_loss:109.02621459960938\n",
            "2850/3000 train_loss: 45.10410690307617 test_loss:98.90933227539062\n",
            "2851/3000 train_loss: 45.888038635253906 test_loss:81.86608123779297\n",
            "2852/3000 train_loss: 43.90399169921875 test_loss:88.26976776123047\n",
            "2853/3000 train_loss: 40.3845329284668 test_loss:90.38119506835938\n",
            "2854/3000 train_loss: 39.87235641479492 test_loss:98.04902648925781\n",
            "2855/3000 train_loss: 40.29404830932617 test_loss:103.8157730102539\n",
            "2856/3000 train_loss: 51.6176872253418 test_loss:94.70588684082031\n",
            "2857/3000 train_loss: 40.1483039855957 test_loss:89.39120483398438\n",
            "2858/3000 train_loss: 41.72704315185547 test_loss:90.23585510253906\n",
            "2859/3000 train_loss: 47.14841079711914 test_loss:84.7668685913086\n",
            "2860/3000 train_loss: 43.5920295715332 test_loss:81.0069351196289\n",
            "2861/3000 train_loss: 43.205448150634766 test_loss:87.61779022216797\n",
            "2862/3000 train_loss: 45.35390090942383 test_loss:88.62589263916016\n",
            "2863/3000 train_loss: 41.555145263671875 test_loss:89.73793029785156\n",
            "2864/3000 train_loss: 38.49705123901367 test_loss:107.37694549560547\n",
            "2865/3000 train_loss: 44.91128921508789 test_loss:85.18182373046875\n",
            "2866/3000 train_loss: 45.08856201171875 test_loss:82.31452941894531\n",
            "2867/3000 train_loss: 45.05808639526367 test_loss:87.23897552490234\n",
            "2868/3000 train_loss: 44.17940902709961 test_loss:93.0054702758789\n",
            "2869/3000 train_loss: 42.049991607666016 test_loss:87.21965026855469\n",
            "2870/3000 train_loss: 46.43018341064453 test_loss:106.47188568115234\n",
            "2871/3000 train_loss: 47.031959533691406 test_loss:115.92280578613281\n",
            "2872/3000 train_loss: 52.325775146484375 test_loss:89.38946533203125\n",
            "2873/3000 train_loss: 48.98430633544922 test_loss:97.47398376464844\n",
            "2874/3000 train_loss: 44.1383171081543 test_loss:99.42481994628906\n",
            "2875/3000 train_loss: 38.93239212036133 test_loss:107.14033508300781\n",
            "2876/3000 train_loss: 40.82496643066406 test_loss:96.50973510742188\n",
            "2877/3000 train_loss: 45.643577575683594 test_loss:96.62507629394531\n",
            "2878/3000 train_loss: 42.00484848022461 test_loss:98.4947509765625\n",
            "2879/3000 train_loss: 38.72850799560547 test_loss:87.8709716796875\n",
            "2880/3000 train_loss: 40.27981948852539 test_loss:82.86363983154297\n",
            "2881/3000 train_loss: 39.976531982421875 test_loss:100.76837158203125\n",
            "2882/3000 train_loss: 39.34027862548828 test_loss:90.15998840332031\n",
            "2883/3000 train_loss: 48.3203010559082 test_loss:96.4345703125\n",
            "2884/3000 train_loss: 42.27446365356445 test_loss:93.49268341064453\n",
            "2885/3000 train_loss: 41.83095169067383 test_loss:94.48371124267578\n",
            "2886/3000 train_loss: 43.749935150146484 test_loss:98.67129516601562\n",
            "2887/3000 train_loss: 36.957271575927734 test_loss:90.69222259521484\n",
            "2888/3000 train_loss: 43.604248046875 test_loss:92.04049682617188\n",
            "2889/3000 train_loss: 44.9962272644043 test_loss:90.34049987792969\n",
            "2890/3000 train_loss: 43.583194732666016 test_loss:91.6731185913086\n",
            "2891/3000 train_loss: 51.40020751953125 test_loss:95.65267944335938\n",
            "2892/3000 train_loss: 40.71110153198242 test_loss:84.11236572265625\n",
            "2893/3000 train_loss: 40.283992767333984 test_loss:96.3387222290039\n",
            "2894/3000 train_loss: 43.28047561645508 test_loss:90.7970962524414\n",
            "2895/3000 train_loss: 44.804256439208984 test_loss:83.36592102050781\n",
            "2896/3000 train_loss: 39.747718811035156 test_loss:83.44225311279297\n",
            "2897/3000 train_loss: 42.29806900024414 test_loss:94.79589080810547\n",
            "2898/3000 train_loss: 38.19508743286133 test_loss:90.96562194824219\n",
            "2899/3000 train_loss: 48.279266357421875 test_loss:98.14627075195312\n",
            "2900/3000 train_loss: 42.12077713012695 test_loss:89.28408813476562\n",
            "2901/3000 train_loss: 37.789798736572266 test_loss:85.07601165771484\n",
            "2902/3000 train_loss: 45.79342269897461 test_loss:93.04034423828125\n",
            "2903/3000 train_loss: 44.253570556640625 test_loss:84.10787963867188\n",
            "2904/3000 train_loss: 37.867767333984375 test_loss:88.88323974609375\n",
            "2905/3000 train_loss: 40.817298889160156 test_loss:84.44168090820312\n",
            "2906/3000 train_loss: 38.55305862426758 test_loss:92.2830581665039\n",
            "2907/3000 train_loss: 46.858646392822266 test_loss:94.91943359375\n",
            "2908/3000 train_loss: 47.71633529663086 test_loss:103.24002075195312\n",
            "2909/3000 train_loss: 44.08476638793945 test_loss:92.43331909179688\n",
            "2910/3000 train_loss: 39.641605377197266 test_loss:106.67302703857422\n",
            "2911/3000 train_loss: 39.208866119384766 test_loss:90.60881805419922\n",
            "2912/3000 train_loss: 41.92316818237305 test_loss:101.25836944580078\n",
            "2913/3000 train_loss: 39.661537170410156 test_loss:96.44795227050781\n",
            "2914/3000 train_loss: 37.66874313354492 test_loss:89.52911376953125\n",
            "2915/3000 train_loss: 40.61451721191406 test_loss:94.25243377685547\n",
            "2916/3000 train_loss: 45.57558059692383 test_loss:86.89985656738281\n",
            "2917/3000 train_loss: 45.56991958618164 test_loss:89.08270263671875\n",
            "2918/3000 train_loss: 40.39082336425781 test_loss:83.75469207763672\n",
            "2919/3000 train_loss: 39.409061431884766 test_loss:96.03880310058594\n",
            "2920/3000 train_loss: 39.21120071411133 test_loss:95.03144836425781\n",
            "2921/3000 train_loss: 43.90901184082031 test_loss:91.91719818115234\n",
            "2922/3000 train_loss: 40.28532791137695 test_loss:95.42803192138672\n",
            "2923/3000 train_loss: 47.33396911621094 test_loss:87.25161743164062\n",
            "2924/3000 train_loss: 47.9425048828125 test_loss:99.8064193725586\n",
            "2925/3000 train_loss: 43.15665054321289 test_loss:96.35618591308594\n",
            "2926/3000 train_loss: 44.03382873535156 test_loss:98.0694808959961\n",
            "2927/3000 train_loss: 41.548282623291016 test_loss:101.32066345214844\n",
            "2928/3000 train_loss: 51.11503601074219 test_loss:99.95526123046875\n",
            "2929/3000 train_loss: 39.49213409423828 test_loss:104.91227722167969\n",
            "2930/3000 train_loss: 40.28671646118164 test_loss:106.29198455810547\n",
            "2931/3000 train_loss: 44.25776672363281 test_loss:103.01046752929688\n",
            "2932/3000 train_loss: 38.37542724609375 test_loss:95.69828796386719\n",
            "2933/3000 train_loss: 43.07383346557617 test_loss:103.05461120605469\n",
            "2934/3000 train_loss: 42.36574935913086 test_loss:100.22523498535156\n",
            "2935/3000 train_loss: 41.96723175048828 test_loss:85.33406829833984\n",
            "2936/3000 train_loss: 39.433109283447266 test_loss:101.35567474365234\n",
            "2937/3000 train_loss: 45.146915435791016 test_loss:103.75375366210938\n",
            "2938/3000 train_loss: 40.51045227050781 test_loss:90.4171371459961\n",
            "2939/3000 train_loss: 41.4100341796875 test_loss:94.48405456542969\n",
            "2940/3000 train_loss: 37.70493698120117 test_loss:97.06562042236328\n",
            "2941/3000 train_loss: 35.069915771484375 test_loss:102.26451110839844\n",
            "2942/3000 train_loss: 39.94712448120117 test_loss:90.59107971191406\n",
            "2943/3000 train_loss: 42.5709228515625 test_loss:99.67013549804688\n",
            "2944/3000 train_loss: 40.675479888916016 test_loss:108.54621887207031\n",
            "2945/3000 train_loss: 40.31409454345703 test_loss:103.03117370605469\n",
            "2946/3000 train_loss: 49.28878402709961 test_loss:87.4384765625\n",
            "2947/3000 train_loss: 39.47597122192383 test_loss:96.42789459228516\n",
            "2948/3000 train_loss: 45.81861877441406 test_loss:91.61132049560547\n",
            "2949/3000 train_loss: 40.24486541748047 test_loss:95.03434753417969\n",
            "2950/3000 train_loss: 45.155723571777344 test_loss:105.6632308959961\n",
            "2951/3000 train_loss: 38.261810302734375 test_loss:101.55728912353516\n",
            "2952/3000 train_loss: 42.9821662902832 test_loss:89.0933837890625\n",
            "2953/3000 train_loss: 43.116119384765625 test_loss:93.44332885742188\n",
            "2954/3000 train_loss: 39.28861999511719 test_loss:95.37791442871094\n",
            "2955/3000 train_loss: 41.6467170715332 test_loss:81.6312484741211\n",
            "2956/3000 train_loss: 47.228355407714844 test_loss:103.39779663085938\n",
            "2957/3000 train_loss: 46.74592971801758 test_loss:101.97187042236328\n",
            "2958/3000 train_loss: 47.11128234863281 test_loss:95.33277893066406\n",
            "2959/3000 train_loss: 42.8762092590332 test_loss:90.28069305419922\n",
            "2960/3000 train_loss: 42.97673034667969 test_loss:96.2853012084961\n",
            "2961/3000 train_loss: 44.315242767333984 test_loss:85.60581970214844\n",
            "2962/3000 train_loss: 40.17083740234375 test_loss:89.51372528076172\n",
            "2963/3000 train_loss: 40.84415817260742 test_loss:97.40365600585938\n",
            "2964/3000 train_loss: 39.036048889160156 test_loss:84.32219696044922\n",
            "2965/3000 train_loss: 40.02607345581055 test_loss:84.2591323852539\n",
            "2966/3000 train_loss: 36.94127655029297 test_loss:83.76133728027344\n",
            "2967/3000 train_loss: 51.69977951049805 test_loss:89.05418395996094\n",
            "2968/3000 train_loss: 46.933380126953125 test_loss:97.95501708984375\n",
            "2969/3000 train_loss: 42.805763244628906 test_loss:94.8642349243164\n",
            "2970/3000 train_loss: 37.72291946411133 test_loss:94.76004028320312\n",
            "2971/3000 train_loss: 37.196468353271484 test_loss:108.18392181396484\n",
            "2972/3000 train_loss: 41.73928451538086 test_loss:109.50344848632812\n",
            "2973/3000 train_loss: 35.884925842285156 test_loss:97.87281799316406\n",
            "2974/3000 train_loss: 40.09738540649414 test_loss:112.67823028564453\n",
            "2975/3000 train_loss: 44.072872161865234 test_loss:91.65176391601562\n",
            "2976/3000 train_loss: 39.3838005065918 test_loss:90.15487670898438\n",
            "2977/3000 train_loss: 39.05254364013672 test_loss:90.29615020751953\n",
            "2978/3000 train_loss: 41.939849853515625 test_loss:99.36634826660156\n",
            "2979/3000 train_loss: 40.88100051879883 test_loss:116.78334045410156\n",
            "2980/3000 train_loss: 49.60625076293945 test_loss:87.79342651367188\n",
            "2981/3000 train_loss: 38.086421966552734 test_loss:82.93673706054688\n",
            "2982/3000 train_loss: 44.64177703857422 test_loss:83.02310180664062\n",
            "2983/3000 train_loss: 42.127891540527344 test_loss:85.97319793701172\n",
            "2984/3000 train_loss: 38.20802307128906 test_loss:81.91796875\n",
            "2985/3000 train_loss: 40.987945556640625 test_loss:94.39466857910156\n",
            "2986/3000 train_loss: 41.93614959716797 test_loss:93.73171997070312\n",
            "2987/3000 train_loss: 37.06180953979492 test_loss:100.7999267578125\n",
            "2988/3000 train_loss: 42.291908264160156 test_loss:86.72681427001953\n",
            "2989/3000 train_loss: 39.59841537475586 test_loss:86.81887817382812\n",
            "2990/3000 train_loss: 37.56611251831055 test_loss:89.72640991210938\n",
            "2991/3000 train_loss: 38.37344741821289 test_loss:91.1684341430664\n",
            "2992/3000 train_loss: 37.258140563964844 test_loss:88.65115356445312\n",
            "2993/3000 train_loss: 41.64118957519531 test_loss:97.10453033447266\n",
            "2994/3000 train_loss: 39.579254150390625 test_loss:93.30461883544922\n",
            "2995/3000 train_loss: 43.56258010864258 test_loss:87.69501495361328\n",
            "2996/3000 train_loss: 37.59381866455078 test_loss:85.29139709472656\n",
            "2997/3000 train_loss: 42.34674072265625 test_loss:87.75894165039062\n",
            "2998/3000 train_loss: 38.94483184814453 test_loss:88.52352905273438\n",
            "2999/3000 train_loss: 44.617794036865234 test_loss:90.79328155517578\n",
            "3000/3000 train_loss: 44.72031021118164 test_loss:79.24736785888672\n"
          ]
        }
      ],
      "source": [
        "losses = train(model = unet, optimizer = optimizer, criterion=criterion, data_tr=train_data,\n",
        "               data_val = test_data, scheduler = scheduler,device = device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "6Ew7_F0-q7aL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1532a83c-e461-4b19-8181-41b30354ffb9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(79.2474)"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ],
      "source": [
        "avg_loss = 0\n",
        "# unet.eval()\n",
        "# unet.train()\n",
        "preds = []\n",
        "i = 0\n",
        "test_anomaly_losses = []\n",
        "test_normal_losses = []\n",
        "test_losses = []\n",
        "test_real = y_test.tolist()\n",
        "for batch in test_data:\n",
        "  with torch.no_grad():\n",
        "    # unet.train()\n",
        "    predictions = unet(batch.to(device)).cpu()\n",
        "    preds.append(predictions)\n",
        "  loss = criterion(predictions, batch.cpu())\n",
        "  for j in range(len(predictions)):\n",
        "    if int(y_test[i]) == 1:\n",
        "      test_anomaly_losses.append(float(criterion(predictions[j], batch[j])))\n",
        "    else:\n",
        "      test_normal_losses.append(float(criterion(predictions[j], batch[j])))\n",
        "    i += 1\n",
        "    test_losses.append(criterion(predictions[j], batch[j]))\n",
        "  # print(loss)\n",
        "  # print(loss)\n",
        "  avg_loss += loss / len(test_data)\n",
        "# avg_loss\n",
        "\n",
        "avg_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "VpDKorrRso9o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cfb5b8a-2ef2-44f7-facf-f732f018483f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(110.82398070006812, 40.45492704391479)"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ],
      "source": [
        "sum(test_anomaly_losses)/len(test_anomaly_losses) , sum(test_normal_losses)/len(test_normal_losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "cJE0-57Qts3E"
      },
      "outputs": [],
      "source": [
        "# torch.save(unet, \"unet_fan2_2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LEvbZKYuh7J",
        "outputId": "fa2822de-da66-426e-c32d-f6ee11c07ce6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8253781512605042\n"
          ]
        }
      ],
      "source": [
        "results = []\n",
        "vals = np.arange(40, 100, 0.5).tolist()\n",
        "for threshold in vals:\n",
        "  preds = []\n",
        "  for j in range(len(test_losses)):\n",
        "    if test_losses[j] > threshold:\n",
        "      preds.append(1)\n",
        "    else:\n",
        "      preds.append(0)\n",
        "  \n",
        "  results.append(roc_auc_score(test_real,preds))\n",
        "\n",
        "print(max(results))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "W4H4vpFX35yK"
      },
      "outputs": [],
      "source": [
        "def get_logmelspectrogram(waveform):\n",
        "    melspec = librosa.feature.melspectrogram(y=waveform.numpy(), hop_length=250, n_mels = 304)\n",
        "\n",
        "    logmelspec = librosa.power_to_db(melspec)\n",
        "\n",
        "    return logmelspec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vo1-S_qcuUZR"
      },
      "outputs": [],
      "source": [
        "# train_logmelspecs, test_logmelspecs = mean_logmelspecs(df_train), mean_logmelspecs(df_test)\n",
        "train_data1 = []\n",
        "for wave in df_train:\n",
        "  train_data1.append(get_logmelspectrogram(wave)[0])\n",
        "\n",
        "test_data1 = []\n",
        "for wave in df_test:\n",
        "  test_data1.append(get_logmelspectrogram(wave)[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGd9oI5IEVMx",
        "outputId": "73a79d4e-9e50-4b29-e7ea-55ee60a08c22"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-30-68ec04120629>:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  train_data1 = torch.FloatTensor(train_data1)\n"
          ]
        }
      ],
      "source": [
        "train_data1 = torch.FloatTensor(train_data1)\n",
        "test_data1 = torch.FloatTensor(test_data1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMOi9331OVb4"
      },
      "outputs": [],
      "source": [
        "train_logs = DataLoader(train_data1.reshape(916*304,641),batch_size = 304)\n",
        "test_logs = DataLoader(test_data1.reshape(459*304,641),batch_size = 304)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9rD6tuI1rfe"
      },
      "outputs": [],
      "source": [
        "unet1 = UNet_FC(in_features=641).to(device)\n",
        "optimizer1 = Adam(params = unet1.parameters(), lr = 10e-3)\n",
        "# optimizer = Adam(params = unet.parameters())\n",
        "criterion1 = nn.MSELoss()\n",
        "scheduler1 = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer1, factor=0.5, \n",
        "                                                       min_lr=10e-4, mode = 'min',\n",
        "                                                       patience = 30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qr6k85ma3ftD",
        "outputId": "67174f45-79f9-41f1-8959-a0c75a2b17fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 train_loss: 21.703369140625 test_loss:17.446470260620117\n"
          ]
        }
      ],
      "source": [
        "losses = train(model = unet1, optimizer = optimizer1, criterion=criterion1, data_tr=train_logs,\n",
        "               data_val = test_logs, device = device, epochs = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrPbpKgSPx7v",
        "outputId": "a7bd10e8-e2bd-4579-8211-d6eaaa879711"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(17.4465)"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "avg_loss = 0\n",
        "# unet.eval()\n",
        "# unet.train()\n",
        "preds = []\n",
        "i = 0\n",
        "test_anomaly_losses = []\n",
        "test_normal_losses = []\n",
        "test_losses = []\n",
        "test_real = y_test.tolist()\n",
        "for batch in test_logs:\n",
        "  with torch.no_grad():\n",
        "    # unet.train()\n",
        "    predictions = unet1(batch.to(device)).cpu()\n",
        "    preds.append(predictions)\n",
        "    loss = criterion(predictions, batch.cpu())\n",
        "    test_losses.append(loss)\n",
        "    if int(y_test[i]) == 1:\n",
        "      test_anomaly_losses.append(loss)\n",
        "    else:\n",
        "      test_normal_losses.append(loss)\n",
        "    i += 1\n",
        "  # print(loss)\n",
        "  # print(loss)\n",
        "  avg_loss += loss / len(test_logs)\n",
        "# avg_loss\n",
        "\n",
        "avg_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Z5Z1XYFN_x2",
        "outputId": "9d767817-5525-443f-db72-2588edb4cfbc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor(18.0466), tensor(15.2920))"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sum(test_anomaly_losses)/len(test_anomaly_losses) , sum(test_normal_losses)/len(test_normal_losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "er74WfG7P_B1",
        "outputId": "884ef8f3-8bfe-4d71-bc01-534f5d5a0a03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.5875626740947075\n"
          ]
        }
      ],
      "source": [
        "results = []\n",
        "vals = np.arange(10, 21, 0.1).tolist()\n",
        "for threshold in vals:\n",
        "  preds = []\n",
        "  for j in range(len(test_losses)):\n",
        "    if test_losses[j] > threshold:\n",
        "      preds.append(1)\n",
        "    else:\n",
        "      preds.append(0)\n",
        "  \n",
        "  results.append(roc_auc_score(test_real,preds))\n",
        "\n",
        "print(max(results))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QaSSqG8SbAw2"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}