{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "9SStKf4G0V5H"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "import torchaudio\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from skimage.util import img_as_ubyte\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import io\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_XtxbKLZq5KX",
    "outputId": "0c6de58c-f736-440b-85ce-a6ad2ae77c29"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CYxHegIM0Z4i",
    "outputId": "8b35e1ba-3d36-4aca-c569-e94fbad47a14"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive/\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "h9DATQwS0ivD"
   },
   "outputs": [],
   "source": [
    "class MimiiDataset(Dataset):\n",
    "    def __init__(self,audio_dir, n_fft = 1024, win_length = 1024,\n",
    "                 hop_length = 512,power = 2,n_mels = 128,pad_mode = 'reflect',\n",
    "                 sr = 16000,center = True,norm = None):\n",
    "      \n",
    "        super(MimiiDataset, self).__init__()\n",
    "        self.audio_dir = audio_dir\n",
    "        self.n_mels = n_mels\n",
    "        self.n_fft = n_fft\n",
    "        self.win_length = win_length\n",
    "        self.hop_length = hop_length\n",
    "        self.power = power\n",
    "        self.pad_mode = pad_mode\n",
    "        self.sr = sr\n",
    "        self.center = center\n",
    "        self.norm = norm\n",
    "\n",
    "    def get_files(self):\n",
    "       return self.train_files, self.test_files\n",
    "    \n",
    "    def get_data(self,device, id):\n",
    "        \n",
    "        self.train_files, self.train_labels = self._train_file_list(device, id)\n",
    "        self.test_files, self.test_labels = self._test_file_list(device, id)\n",
    "        \n",
    "        self.train_data = self.get_audios(self.train_files)\n",
    "        self.test_data = self.get_audios(self.test_files)\n",
    "        \n",
    "        return self.train_data, self.test_data, self.train_labels, self.test_labels\n",
    "    \n",
    "    def _train_file_list(self, device, id):\n",
    "        query = os.path.abspath(\n",
    "            f\"{self.audio_dir}/{device}/train/normal_id_0{id}*.wav\"\n",
    "        )\n",
    "        train_normal_files = sorted(glob.glob(query))\n",
    "        train_normal_labels = np.zeros(len(train_normal_files))\n",
    "        \n",
    "        query = os.path.abspath(\n",
    "                f\"{self.audio_dir}/{device}/train/anomaly_id_0{id}*.wav\"\n",
    "            )\n",
    "        train_anomaly_files = sorted(glob.glob(query))\n",
    "        train_anomaly_labels = np.ones(len(train_anomaly_files))\n",
    "        \n",
    "        train_file_list = np.concatenate((train_normal_files, train_anomaly_files), axis=0)\n",
    "        train_labels = np.concatenate((train_normal_labels, train_anomaly_labels), axis=0)\n",
    "        \n",
    "        return train_file_list, train_labels\n",
    "    \n",
    "    def _test_file_list(self, device, id):     \n",
    "        query = os.path.abspath(\n",
    "            f\"{self.audio_dir}/{device}/test/normal_id_0{id}*.wav\"\n",
    "            )\n",
    "        test_normal_files = sorted(glob.glob(query))\n",
    "        test_normal_labels = np.zeros(len(test_normal_files))\n",
    "        \n",
    "        query = os.path.abspath(\n",
    "            f\"{self.audio_dir}/{device}/test/anomaly_id_0{id}*.wav\"\n",
    "            )\n",
    "        test_anomaly_files = sorted(glob.glob(query))\n",
    "        test_anomaly_labels = np.ones(len(test_anomaly_files))\n",
    "        \n",
    "        test_file_list = np.concatenate((test_normal_files, \n",
    "                                          test_anomaly_files), axis=0)\n",
    "        test_labels = np.concatenate((test_normal_labels,\n",
    "                                      test_anomaly_labels), axis=0)\n",
    "          \n",
    "        return test_file_list, test_labels\n",
    "\n",
    "    def normalize(self,tensor):\n",
    "        tensor_minusmean = tensor - tensor.mean()\n",
    "        return tensor_minusmean/np.absolute(tensor_minusmean).max()\n",
    "\n",
    "    def make0min(self,tensornd):\n",
    "        tensor = tensornd.numpy()\n",
    "        res = np.where(tensor == 0, 1E-19 , tensor)\n",
    "        return torch.from_numpy(res)\n",
    "\n",
    "    def spectrogrameToImage(self,specgram):\n",
    "        # specgram = torchaudio.transforms.MelSpectrogram(n_fft=1024, win_length=1024, \n",
    "        #                                                 hop_length=512, power=2, \n",
    "        #                                                 normalized=True, n_mels=128)(waveform )\n",
    "        specgram= self.make0min(specgram)\n",
    "        specgram = specgram.log2()[0,:,:].numpy()\n",
    "        \n",
    "        tr2image = transforms.Compose([transforms.ToPILImage()])\n",
    "\n",
    "        specgram= self.normalize(specgram)\n",
    "        # specgram = img_as_ubyte(specgram)\n",
    "        specgramImage = tr2image(specgram)\n",
    "        return specgramImage\n",
    "\n",
    "    def get_logmelspectrogram(self, waveform):\n",
    "        melspec = librosa.feature.melspectrogram(\n",
    "          n_fft=self.n_fft, win_length=self.win_length, hop_length=self.hop_length,\n",
    "          power=self.power,n_mels=self.n_mels,pad_mode=self.pad_mode,sr=self.sr,\n",
    "          center=self.center,norm=self.norm,htk=True,\n",
    "          y = waveform.numpy()\n",
    "        )\n",
    "\n",
    "        logmelspec = librosa.power_to_db(melspec)\n",
    "\n",
    "        return logmelspec\n",
    "\n",
    "    def get_melspectrogram(self,waveform):\n",
    "        melspec = librosa.feature.melspectrogram(\n",
    "            n_fft=self.n_fft, win_length=self.win_length, hop_length=self.hop_length,\n",
    "            power=self.power,n_mels=self.n_mels,pad_mode=self.pad_mode,sr=self.sr,\n",
    "            center=self.center,norm=self.norm,htk=True,\n",
    "            y = waveform.numpy()\n",
    "        )\n",
    "\n",
    "        return melspec\n",
    "    \n",
    "    def get_mfcc(self,waveform):\n",
    "        mfcc = librosa.feature.mfcc(    \n",
    "            n_fft=self.n_fft, win_length=self.win_length, \n",
    "            hop_length=self.hop_length,pad_mode=self.pad_mode,sr=self.sr,\n",
    "            center=self.center,norm=self.norm,n_mfcc=40,\n",
    "            y = waveform.numpy()\n",
    "        )\n",
    "\n",
    "        return mfcc\n",
    "\n",
    "    def get_chroma_stft(self,waveform):\n",
    "        stft = librosa.feature.chroma_stft(\n",
    "            n_fft=self.n_fft, win_length=self.win_length, \n",
    "            hop_length=self.hop_length,pad_mode=self.pad_mode,sr=self.sr,\n",
    "            center=self.center,norm=self.norm,n_chroma=12,\n",
    "            y=waveform.numpy()\n",
    "        )\n",
    "\n",
    "        return stft\n",
    "\n",
    "    def get_spectral_contrast(self,waveform):\n",
    "        spec_contrast = librosa.feature.spectral_contrast(    \n",
    "            n_fft=self.n_fft, win_length=self.win_length,center=self.center,\n",
    "            hop_length=self.hop_length,pad_mode=self.pad_mode,sr=self.sr,\n",
    "            y = waveform.numpy()\n",
    "        )\n",
    "\n",
    "        return spec_contrast\n",
    "    \n",
    "    def get_tonnetz(self,waveform):\n",
    "        harmonic = librosa.effects.harmonic(waveform.numpy())\n",
    "        tonnetz = librosa.feature.tonnetz(y=harmonic,sr=self.sr)\n",
    "\n",
    "        return tonnetz\n",
    "\n",
    "    def get_audios(self, file_list):\n",
    "        data = []\n",
    "        for i in range(len(file_list)):\n",
    "          y, sr = torchaudio.load(file_list[i])  \n",
    "          data.append(y)\n",
    "\n",
    "        return data\n",
    "    def _derive_data(self, file_list):\n",
    "        train_data = []\n",
    "        test_data = []\n",
    "        train_mode = True\n",
    "        for file_list in [self.train_files, self.test_files]:\n",
    "          tr2tensor = transforms.Compose([transforms.PILToTensor()])\n",
    "          data = []\n",
    "          for j in range(len(file_list)):\n",
    "            y, sr = torchaudio.load(file_list[j])  \n",
    "            spec = self.get_melspectrogram(y)\n",
    "            spec = self.spectrogrameToImage(spec)\n",
    "            spec = spec.convert('RGB')\n",
    "            vectors = tr2tensor(spec)\n",
    "            if train_mode:     \n",
    "              train_data.append(vectors)\n",
    "            else:\n",
    "              test_data.append(vectors)\n",
    "            \n",
    "          train_mode = False\n",
    "                \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "S96soeIc0o13"
   },
   "outputs": [],
   "source": [
    "dataset = MimiiDataset('/content/drive/MyDrive/mimii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "id": "Gn2zdn92doi1"
   },
   "outputs": [],
   "source": [
    "_, _, y_train, y_test = dataset.get_data('valve', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "id": "SgjpeWy_RV1C"
   },
   "outputs": [],
   "source": [
    "train_mixed_f = torch.load('/content/drive/MyDrive/mixed_features/train_mf_valve2.pt')\n",
    "test_mixed_f = torch.load('/content/drive/MyDrive/mixed_features/test_mf_valve2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "id": "jWMPVGu1qiEq"
   },
   "outputs": [],
   "source": [
    "train_data = DataLoader(train_mixed_f, batch_size=64, shuffle = True)\n",
    "test_data = DataLoader(test_mixed_f, batch_size = 64, shuffle= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "id": "vNTBTRe6qnBq"
   },
   "outputs": [],
   "source": [
    "class UNet_FC(nn.Module):\n",
    "\n",
    "  def __init__(self, in_features):\n",
    "    super().__init__()\n",
    "    \n",
    "    self.bn = nn.BatchNorm1d(128)\n",
    "    self.relu = nn.ReLU()\n",
    "\n",
    "    self.fc0 = nn.Linear(in_features=in_features,out_features=in_features)\n",
    "\n",
    "    # encoder\n",
    "    self.fc1 = nn.Linear(in_features=in_features, out_features=128)\n",
    "    self.fc2 = nn.Linear(in_features=128,out_features=128)\n",
    "    self.fc3 = nn.Linear(in_features=128, out_features=128)\n",
    "    self.fc4 = nn.Linear(in_features=128, out_features=128)\n",
    "    self.fc5 = nn.Linear(in_features=128, out_features=8)\n",
    "\n",
    "    # decoder\n",
    "    self.fc6 = nn.Linear(in_features=8, out_features=128)\n",
    "    self.fc7 = nn.Linear(in_features=128*2, out_features=128)\n",
    "    self.fc8 = nn.Linear(in_features=128*2, out_features=128)\n",
    "    self.fc9 = nn.Linear(in_features=128*2, out_features=128)\n",
    "\n",
    "    self.out = nn.Linear(in_features=128*2, out_features=in_features)\n",
    "\n",
    "  def forward(self, x):\n",
    "    input = self.fc0(x)\n",
    "\n",
    "    x1 = self.relu(self.bn(self.fc1(input)))\n",
    "    x2 = self.relu(self.bn(self.fc2(x1)))\n",
    "    x3 = self.relu(self.bn(self.fc3(x2)))\n",
    "    x4 = self.relu(self.bn(self.fc4(x3)))\n",
    "    x5 = self.relu(self.fc5(x4))\n",
    "\n",
    "    xy = [x5, x4, x3, x2, x1]\n",
    "\n",
    "    x6 = self.relu(self.fc6(xy[0]))\n",
    "    con1 = torch.cat((x6,xy[1]), 1) \n",
    "    x7 = self.relu(self.bn(self.fc7(con1)))\n",
    "    con2 = torch.cat((x7,xy[2]), 1)\n",
    "    x8 = self.relu(self.bn(self.fc8(con2)))\n",
    "    con3 = torch.cat((x8,xy[3]), 1)\n",
    "    x9 = self.relu(self.bn(self.fc9(con3)))\n",
    "    con4 = torch.cat((x9,xy[4]), 1)\n",
    "\n",
    "    x10 = self.out(con4)\n",
    "\n",
    "    # return decoded\n",
    "    return x10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "id": "ZfgcBtQ3qn5l"
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, data_tr, data_val, scheduler = None,\n",
    "          epochs = 3000, device = 'cpu'):\n",
    "    # X_val, Y_val = next(iter(data_val))\n",
    "    losses = []\n",
    "    prev_avg_loss = 100000\n",
    "    for epoch in range(epochs):\n",
    "        train_avg_loss = 0\n",
    "        test_avg_loss = 0\n",
    "        # model.train()  # train mode\n",
    "        for batch in data_tr:\n",
    "          # data to device\n",
    "          batch = batch.to(device)\n",
    "          # set parameter gradients to zero\n",
    "          optimizer.zero_grad()\n",
    "          # forward\n",
    "          # print(Y_batch.shape)\n",
    "          predictions = model(batch)\n",
    "          loss = criterion(predictions, batch)\n",
    "          loss.backward() # backward-pass\n",
    "          optimizer.step()  # update weights\n",
    "          # calculate loss to show the user\n",
    "          if scheduler:\n",
    "            scheduler.step(loss)\n",
    "          train_avg_loss += loss / len(data_tr)\n",
    "\n",
    "        # model.eval()\n",
    "        for batch in data_val:\n",
    "          with torch.no_grad():\n",
    "            preds = model(batch.to(device)).cpu()\n",
    "            loss = criterion(preds,batch)\n",
    "            test_avg_loss += loss / len(data_val)\n",
    "                    \n",
    "        losses.append(train_avg_loss.item())\n",
    "        # if (epoch+1)%50 == 0:\n",
    "        print(\"{}/{} train_loss: {} test_loss:{}\".format(epoch+1, epochs, train_avg_loss, test_avg_loss))\n",
    "        # if test_avg_loss < 70:\n",
    "        #   break\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "id": "ptkVTF55quOL"
   },
   "outputs": [],
   "source": [
    "unet = UNet_FC(in_features=193).to(device)\n",
    "optimizer = Adam(params = unet.parameters(), lr = 10e-3)\n",
    "# optimizer = Adam(params = unet.parameters())\n",
    "criterion = nn.MSELoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, \n",
    "                                                       min_lr=10e-4, mode = 'min',\n",
    "                                                       patience = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QkfmYl9oXhcB",
    "outputId": "1ac8aea0-c567-44cb-817b-688002acb930"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1/3000 train_loss: 336611.9375 test_loss:338024.375\n",
      "2/3000 train_loss: 334308.46875 test_loss:335905.125\n",
      "3/3000 train_loss: 332609.6875 test_loss:333004.9375\n",
      "4/3000 train_loss: 330330.5 test_loss:329252.65625\n",
      "5/3000 train_loss: 324359.3125 test_loss:324264.5\n",
      "6/3000 train_loss: 319423.53125 test_loss:318388.84375\n",
      "7/3000 train_loss: 313554.5 test_loss:311201.84375\n",
      "8/3000 train_loss: 305879.28125 test_loss:303162.5625\n",
      "9/3000 train_loss: 298075.25 test_loss:294045.4375\n",
      "10/3000 train_loss: 287972.40625 test_loss:284011.8125\n",
      "11/3000 train_loss: 278006.78125 test_loss:273291.375\n",
      "12/3000 train_loss: 266738.625 test_loss:261795.578125\n",
      "13/3000 train_loss: 254923.578125 test_loss:249860.28125\n",
      "14/3000 train_loss: 243291.296875 test_loss:237245.140625\n",
      "15/3000 train_loss: 230088.53125 test_loss:224502.8125\n",
      "16/3000 train_loss: 217001.015625 test_loss:211205.375\n",
      "17/3000 train_loss: 203691.875 test_loss:197702.8125\n",
      "18/3000 train_loss: 191319.078125 test_loss:184780.984375\n",
      "19/3000 train_loss: 177361.59375 test_loss:170755.6875\n",
      "20/3000 train_loss: 164245.265625 test_loss:157348.125\n",
      "21/3000 train_loss: 150243.3125 test_loss:143991.859375\n",
      "22/3000 train_loss: 137938.3125 test_loss:131454.234375\n",
      "23/3000 train_loss: 124903.7890625 test_loss:118161.015625\n",
      "24/3000 train_loss: 111922.5390625 test_loss:106458.234375\n",
      "25/3000 train_loss: 99749.2109375 test_loss:94932.5234375\n",
      "26/3000 train_loss: 88255.6796875 test_loss:83234.28125\n",
      "27/3000 train_loss: 77221.3046875 test_loss:72498.203125\n",
      "28/3000 train_loss: 67803.8359375 test_loss:63213.22265625\n",
      "29/3000 train_loss: 58194.02734375 test_loss:54439.77734375\n",
      "30/3000 train_loss: 50245.6796875 test_loss:46425.375\n",
      "31/3000 train_loss: 42318.12890625 test_loss:39203.390625\n",
      "32/3000 train_loss: 35494.19921875 test_loss:32980.390625\n",
      "33/3000 train_loss: 29569.78125 test_loss:27981.01171875\n",
      "34/3000 train_loss: 24676.798828125 test_loss:23233.00390625\n",
      "35/3000 train_loss: 20686.314453125 test_loss:19470.205078125\n",
      "36/3000 train_loss: 16720.44921875 test_loss:15568.0\n",
      "37/3000 train_loss: 13370.798828125 test_loss:12805.44921875\n",
      "38/3000 train_loss: 11595.7919921875 test_loss:10128.2431640625\n",
      "39/3000 train_loss: 8928.40625 test_loss:8272.591796875\n",
      "40/3000 train_loss: 7071.76904296875 test_loss:6454.25732421875\n",
      "41/3000 train_loss: 5665.7333984375 test_loss:5491.712890625\n",
      "42/3000 train_loss: 4767.80224609375 test_loss:4329.220703125\n",
      "43/3000 train_loss: 3893.4775390625 test_loss:3538.86669921875\n",
      "44/3000 train_loss: 2920.17529296875 test_loss:2908.53369140625\n",
      "45/3000 train_loss: 2617.789794921875 test_loss:2362.9638671875\n",
      "46/3000 train_loss: 1942.7960205078125 test_loss:1942.44189453125\n",
      "47/3000 train_loss: 1616.5032958984375 test_loss:1750.95166015625\n",
      "48/3000 train_loss: 1386.96337890625 test_loss:1595.8193359375\n",
      "49/3000 train_loss: 1298.1004638671875 test_loss:1300.8336181640625\n",
      "50/3000 train_loss: 1135.6439208984375 test_loss:1210.099365234375\n",
      "51/3000 train_loss: 1118.212646484375 test_loss:1500.253173828125\n",
      "52/3000 train_loss: 992.4578247070312 test_loss:1204.0335693359375\n",
      "53/3000 train_loss: 910.1619262695312 test_loss:866.87109375\n",
      "54/3000 train_loss: 808.5470581054688 test_loss:892.5068359375\n",
      "55/3000 train_loss: 813.88232421875 test_loss:852.1884765625\n",
      "56/3000 train_loss: 812.0380859375 test_loss:1263.51904296875\n",
      "57/3000 train_loss: 838.0220336914062 test_loss:771.0731811523438\n",
      "58/3000 train_loss: 701.5952758789062 test_loss:840.8165283203125\n",
      "59/3000 train_loss: 670.0820922851562 test_loss:685.9442138671875\n",
      "60/3000 train_loss: 902.0557250976562 test_loss:953.0457153320312\n",
      "61/3000 train_loss: 899.7646484375 test_loss:815.7372436523438\n",
      "62/3000 train_loss: 734.9369506835938 test_loss:724.5015258789062\n",
      "63/3000 train_loss: 701.0604858398438 test_loss:770.6245727539062\n",
      "64/3000 train_loss: 674.6646118164062 test_loss:707.31982421875\n",
      "65/3000 train_loss: 562.3841552734375 test_loss:712.472412109375\n",
      "66/3000 train_loss: 505.09735107421875 test_loss:666.595703125\n",
      "67/3000 train_loss: 493.0753173828125 test_loss:679.13525390625\n",
      "68/3000 train_loss: 525.9705810546875 test_loss:657.6694946289062\n",
      "69/3000 train_loss: 524.63134765625 test_loss:701.6862182617188\n",
      "70/3000 train_loss: 626.4779052734375 test_loss:652.681640625\n",
      "71/3000 train_loss: 485.2275695800781 test_loss:654.41015625\n",
      "72/3000 train_loss: 490.7005310058594 test_loss:663.7257080078125\n",
      "73/3000 train_loss: 455.6861877441406 test_loss:648.1409912109375\n",
      "74/3000 train_loss: 497.20635986328125 test_loss:687.01171875\n",
      "75/3000 train_loss: 482.74127197265625 test_loss:590.4662475585938\n",
      "76/3000 train_loss: 442.6404113769531 test_loss:621.2119750976562\n",
      "77/3000 train_loss: 459.2416687011719 test_loss:587.3333740234375\n",
      "78/3000 train_loss: 509.49310302734375 test_loss:587.322265625\n",
      "79/3000 train_loss: 446.28216552734375 test_loss:582.126220703125\n",
      "80/3000 train_loss: 459.402587890625 test_loss:604.253173828125\n",
      "81/3000 train_loss: 410.6694030761719 test_loss:587.9542236328125\n",
      "82/3000 train_loss: 443.6775817871094 test_loss:581.9498291015625\n",
      "83/3000 train_loss: 447.28424072265625 test_loss:584.2010498046875\n",
      "84/3000 train_loss: 421.9493103027344 test_loss:606.87841796875\n",
      "85/3000 train_loss: 423.1904602050781 test_loss:595.885986328125\n",
      "86/3000 train_loss: 477.1852111816406 test_loss:579.5562744140625\n",
      "87/3000 train_loss: 443.7030334472656 test_loss:605.7738037109375\n",
      "88/3000 train_loss: 431.7041015625 test_loss:584.6390380859375\n",
      "89/3000 train_loss: 419.25640869140625 test_loss:592.36376953125\n",
      "90/3000 train_loss: 423.39105224609375 test_loss:590.4611206054688\n",
      "91/3000 train_loss: 435.52069091796875 test_loss:583.4508666992188\n",
      "92/3000 train_loss: 437.1043395996094 test_loss:577.70263671875\n",
      "93/3000 train_loss: 405.9032897949219 test_loss:583.5875244140625\n",
      "94/3000 train_loss: 402.5484619140625 test_loss:584.5010986328125\n",
      "95/3000 train_loss: 449.7380676269531 test_loss:583.8404541015625\n",
      "96/3000 train_loss: 428.95758056640625 test_loss:583.720458984375\n",
      "97/3000 train_loss: 425.45245361328125 test_loss:580.0001220703125\n",
      "98/3000 train_loss: 434.348388671875 test_loss:585.990478515625\n",
      "99/3000 train_loss: 420.96746826171875 test_loss:575.3309326171875\n",
      "100/3000 train_loss: 415.1755676269531 test_loss:576.1201171875\n",
      "101/3000 train_loss: 406.9583435058594 test_loss:586.322021484375\n",
      "102/3000 train_loss: 427.46527099609375 test_loss:568.7805786132812\n",
      "103/3000 train_loss: 430.364990234375 test_loss:587.9637451171875\n",
      "104/3000 train_loss: 443.3452453613281 test_loss:591.2302856445312\n",
      "105/3000 train_loss: 445.8172607421875 test_loss:566.4312744140625\n",
      "106/3000 train_loss: 444.40679931640625 test_loss:574.853271484375\n",
      "107/3000 train_loss: 439.3153991699219 test_loss:587.5884399414062\n",
      "108/3000 train_loss: 412.4005126953125 test_loss:587.7953491210938\n",
      "109/3000 train_loss: 442.5830078125 test_loss:574.5235595703125\n",
      "110/3000 train_loss: 426.473876953125 test_loss:586.5180053710938\n",
      "111/3000 train_loss: 434.7078552246094 test_loss:589.6810302734375\n",
      "112/3000 train_loss: 424.7353820800781 test_loss:571.2881469726562\n",
      "113/3000 train_loss: 415.91522216796875 test_loss:595.4757690429688\n",
      "114/3000 train_loss: 409.9883728027344 test_loss:574.6717529296875\n",
      "115/3000 train_loss: 457.263916015625 test_loss:584.209716796875\n",
      "116/3000 train_loss: 450.572021484375 test_loss:579.8798828125\n",
      "117/3000 train_loss: 409.9666442871094 test_loss:573.7569580078125\n",
      "118/3000 train_loss: 393.29949951171875 test_loss:587.8521118164062\n",
      "119/3000 train_loss: 473.64447021484375 test_loss:580.9928588867188\n",
      "120/3000 train_loss: 459.45379638671875 test_loss:571.6080322265625\n",
      "121/3000 train_loss: 432.4304504394531 test_loss:595.9312744140625\n",
      "122/3000 train_loss: 409.5875549316406 test_loss:589.3677978515625\n",
      "123/3000 train_loss: 403.53424072265625 test_loss:575.166259765625\n",
      "124/3000 train_loss: 434.9208068847656 test_loss:574.4899291992188\n",
      "125/3000 train_loss: 409.4094543457031 test_loss:567.52099609375\n",
      "126/3000 train_loss: 437.44293212890625 test_loss:583.68603515625\n",
      "127/3000 train_loss: 406.7244873046875 test_loss:563.8875732421875\n",
      "128/3000 train_loss: 395.5369873046875 test_loss:575.7572021484375\n",
      "129/3000 train_loss: 419.437744140625 test_loss:571.6925048828125\n",
      "130/3000 train_loss: 391.50311279296875 test_loss:571.5565185546875\n",
      "131/3000 train_loss: 397.0989685058594 test_loss:564.6470947265625\n",
      "132/3000 train_loss: 367.87091064453125 test_loss:576.4776611328125\n",
      "133/3000 train_loss: 396.0052795410156 test_loss:570.8545532226562\n",
      "134/3000 train_loss: 453.99169921875 test_loss:573.8206176757812\n",
      "135/3000 train_loss: 414.2195129394531 test_loss:555.967529296875\n",
      "136/3000 train_loss: 407.037841796875 test_loss:557.9744873046875\n",
      "137/3000 train_loss: 404.2148132324219 test_loss:564.1973876953125\n",
      "138/3000 train_loss: 402.42108154296875 test_loss:546.3670654296875\n",
      "139/3000 train_loss: 416.0673522949219 test_loss:570.0301513671875\n",
      "140/3000 train_loss: 414.6706237792969 test_loss:558.8782958984375\n",
      "141/3000 train_loss: 393.303466796875 test_loss:547.8927001953125\n",
      "142/3000 train_loss: 389.85400390625 test_loss:562.0337524414062\n",
      "143/3000 train_loss: 414.8307800292969 test_loss:553.1116333007812\n",
      "144/3000 train_loss: 424.69464111328125 test_loss:574.2281494140625\n",
      "145/3000 train_loss: 420.58184814453125 test_loss:556.400634765625\n",
      "146/3000 train_loss: 420.1626892089844 test_loss:558.7403564453125\n",
      "147/3000 train_loss: 458.19537353515625 test_loss:554.124755859375\n",
      "148/3000 train_loss: 407.52276611328125 test_loss:567.758544921875\n",
      "149/3000 train_loss: 435.72540283203125 test_loss:557.8472290039062\n",
      "150/3000 train_loss: 439.9793701171875 test_loss:571.1820068359375\n",
      "151/3000 train_loss: 389.1746520996094 test_loss:554.62255859375\n",
      "152/3000 train_loss: 443.9871520996094 test_loss:580.128662109375\n",
      "153/3000 train_loss: 368.73895263671875 test_loss:556.9256591796875\n",
      "154/3000 train_loss: 401.7386779785156 test_loss:570.478759765625\n",
      "155/3000 train_loss: 423.17218017578125 test_loss:555.53759765625\n",
      "156/3000 train_loss: 401.57989501953125 test_loss:566.2637329101562\n",
      "157/3000 train_loss: 393.1455993652344 test_loss:550.992919921875\n",
      "158/3000 train_loss: 413.7904357910156 test_loss:553.0709228515625\n",
      "159/3000 train_loss: 401.895263671875 test_loss:562.9090576171875\n",
      "160/3000 train_loss: 359.8573303222656 test_loss:561.4061279296875\n",
      "161/3000 train_loss: 378.7608337402344 test_loss:546.1154174804688\n",
      "162/3000 train_loss: 425.29461669921875 test_loss:560.0703125\n",
      "163/3000 train_loss: 388.8536682128906 test_loss:543.779541015625\n",
      "164/3000 train_loss: 363.0895080566406 test_loss:557.4930419921875\n",
      "165/3000 train_loss: 424.6306457519531 test_loss:550.4440307617188\n",
      "166/3000 train_loss: 399.33306884765625 test_loss:560.7982177734375\n",
      "167/3000 train_loss: 380.220703125 test_loss:540.5460205078125\n",
      "168/3000 train_loss: 376.9317321777344 test_loss:538.349365234375\n",
      "169/3000 train_loss: 365.41510009765625 test_loss:547.4586181640625\n",
      "170/3000 train_loss: 371.3050537109375 test_loss:541.0338134765625\n",
      "171/3000 train_loss: 375.32098388671875 test_loss:559.1898803710938\n",
      "172/3000 train_loss: 374.923828125 test_loss:539.7933349609375\n",
      "173/3000 train_loss: 369.458984375 test_loss:560.4671630859375\n",
      "174/3000 train_loss: 366.6015930175781 test_loss:539.3499145507812\n",
      "175/3000 train_loss: 448.14837646484375 test_loss:551.888671875\n",
      "176/3000 train_loss: 422.3212890625 test_loss:550.235595703125\n",
      "177/3000 train_loss: 404.1300354003906 test_loss:542.5271606445312\n",
      "178/3000 train_loss: 391.8514404296875 test_loss:550.6827392578125\n",
      "179/3000 train_loss: 389.9098815917969 test_loss:536.6732788085938\n",
      "180/3000 train_loss: 387.2445983886719 test_loss:552.4624633789062\n",
      "181/3000 train_loss: 373.152587890625 test_loss:547.62890625\n",
      "182/3000 train_loss: 343.673583984375 test_loss:539.985595703125\n",
      "183/3000 train_loss: 373.38067626953125 test_loss:542.9935302734375\n",
      "184/3000 train_loss: 374.5385437011719 test_loss:549.0599365234375\n",
      "185/3000 train_loss: 363.5067138671875 test_loss:533.07275390625\n",
      "186/3000 train_loss: 398.3807067871094 test_loss:565.370361328125\n",
      "187/3000 train_loss: 412.3931884765625 test_loss:538.450927734375\n",
      "188/3000 train_loss: 372.5658874511719 test_loss:533.58203125\n",
      "189/3000 train_loss: 347.62042236328125 test_loss:551.907470703125\n",
      "190/3000 train_loss: 348.3002014160156 test_loss:528.706298828125\n",
      "191/3000 train_loss: 390.78173828125 test_loss:526.2452392578125\n",
      "192/3000 train_loss: 362.3070068359375 test_loss:529.9146118164062\n",
      "193/3000 train_loss: 368.4969177246094 test_loss:537.070556640625\n",
      "194/3000 train_loss: 364.03643798828125 test_loss:525.8822021484375\n",
      "195/3000 train_loss: 385.2214050292969 test_loss:555.73046875\n",
      "196/3000 train_loss: 382.3388366699219 test_loss:529.6781005859375\n",
      "197/3000 train_loss: 370.7906799316406 test_loss:539.3753662109375\n",
      "198/3000 train_loss: 358.1670837402344 test_loss:522.3941040039062\n",
      "199/3000 train_loss: 347.63214111328125 test_loss:522.76318359375\n",
      "200/3000 train_loss: 383.2240905761719 test_loss:526.869873046875\n",
      "201/3000 train_loss: 393.1697692871094 test_loss:538.061279296875\n",
      "202/3000 train_loss: 364.8622131347656 test_loss:524.121826171875\n",
      "203/3000 train_loss: 353.09991455078125 test_loss:525.2757568359375\n",
      "204/3000 train_loss: 390.4605712890625 test_loss:512.8711547851562\n",
      "205/3000 train_loss: 393.837646484375 test_loss:536.2852783203125\n",
      "206/3000 train_loss: 356.0889587402344 test_loss:521.2276000976562\n",
      "207/3000 train_loss: 371.64215087890625 test_loss:528.076416015625\n",
      "208/3000 train_loss: 347.5946960449219 test_loss:515.846435546875\n",
      "209/3000 train_loss: 381.01605224609375 test_loss:531.1712646484375\n",
      "210/3000 train_loss: 412.30523681640625 test_loss:514.5914306640625\n",
      "211/3000 train_loss: 370.7706604003906 test_loss:534.8565673828125\n",
      "212/3000 train_loss: 385.90869140625 test_loss:526.0166015625\n",
      "213/3000 train_loss: 373.8602600097656 test_loss:515.6952514648438\n",
      "214/3000 train_loss: 350.2718505859375 test_loss:512.7855834960938\n",
      "215/3000 train_loss: 359.43878173828125 test_loss:501.9013671875\n",
      "216/3000 train_loss: 371.4217834472656 test_loss:513.5257568359375\n",
      "217/3000 train_loss: 357.53302001953125 test_loss:507.82940673828125\n",
      "218/3000 train_loss: 352.1846008300781 test_loss:509.8896789550781\n",
      "219/3000 train_loss: 350.8930969238281 test_loss:508.14031982421875\n",
      "220/3000 train_loss: 362.00201416015625 test_loss:500.14495849609375\n",
      "221/3000 train_loss: 379.8815002441406 test_loss:507.0621032714844\n",
      "222/3000 train_loss: 384.17022705078125 test_loss:509.2451171875\n",
      "223/3000 train_loss: 348.2188415527344 test_loss:499.5069885253906\n",
      "224/3000 train_loss: 370.0546875 test_loss:512.93408203125\n",
      "225/3000 train_loss: 341.97027587890625 test_loss:510.1676940917969\n",
      "226/3000 train_loss: 345.038818359375 test_loss:519.3175048828125\n",
      "227/3000 train_loss: 334.53216552734375 test_loss:501.8992004394531\n",
      "228/3000 train_loss: 357.2735595703125 test_loss:519.7239990234375\n",
      "229/3000 train_loss: 347.12762451171875 test_loss:505.90625\n",
      "230/3000 train_loss: 348.0906982421875 test_loss:512.57666015625\n",
      "231/3000 train_loss: 353.8490295410156 test_loss:507.74578857421875\n",
      "232/3000 train_loss: 454.13311767578125 test_loss:512.806640625\n",
      "233/3000 train_loss: 367.51544189453125 test_loss:510.8673095703125\n",
      "234/3000 train_loss: 372.6864013671875 test_loss:519.2276000976562\n",
      "235/3000 train_loss: 342.8067321777344 test_loss:501.63519287109375\n",
      "236/3000 train_loss: 372.14691162109375 test_loss:493.3448486328125\n",
      "237/3000 train_loss: 354.2065734863281 test_loss:492.627685546875\n",
      "238/3000 train_loss: 347.59490966796875 test_loss:500.55792236328125\n",
      "239/3000 train_loss: 352.479736328125 test_loss:495.51904296875\n",
      "240/3000 train_loss: 339.6401062011719 test_loss:495.6714782714844\n",
      "241/3000 train_loss: 357.4482116699219 test_loss:486.84332275390625\n",
      "242/3000 train_loss: 363.2993469238281 test_loss:496.0985107421875\n",
      "243/3000 train_loss: 380.48883056640625 test_loss:477.88555908203125\n",
      "244/3000 train_loss: 339.1599426269531 test_loss:488.90643310546875\n",
      "245/3000 train_loss: 337.9737854003906 test_loss:482.72003173828125\n",
      "246/3000 train_loss: 374.9197692871094 test_loss:488.0224609375\n",
      "247/3000 train_loss: 398.4814758300781 test_loss:499.4105224609375\n",
      "248/3000 train_loss: 354.7551574707031 test_loss:491.2508850097656\n",
      "249/3000 train_loss: 333.38983154296875 test_loss:484.9383239746094\n",
      "250/3000 train_loss: 366.2325744628906 test_loss:486.39837646484375\n",
      "251/3000 train_loss: 352.91802978515625 test_loss:482.6887512207031\n",
      "252/3000 train_loss: 380.2452392578125 test_loss:477.1837463378906\n",
      "253/3000 train_loss: 342.7440490722656 test_loss:477.803955078125\n",
      "254/3000 train_loss: 340.7921447753906 test_loss:470.75848388671875\n",
      "255/3000 train_loss: 344.44171142578125 test_loss:477.36328125\n",
      "256/3000 train_loss: 360.433837890625 test_loss:465.6648254394531\n",
      "257/3000 train_loss: 336.8615417480469 test_loss:475.1869201660156\n",
      "258/3000 train_loss: 353.59307861328125 test_loss:465.947998046875\n",
      "259/3000 train_loss: 410.64312744140625 test_loss:471.73785400390625\n",
      "260/3000 train_loss: 308.31005859375 test_loss:465.7550048828125\n",
      "261/3000 train_loss: 359.7489013671875 test_loss:464.2159423828125\n",
      "262/3000 train_loss: 326.559814453125 test_loss:463.6288146972656\n",
      "263/3000 train_loss: 326.30450439453125 test_loss:485.65771484375\n",
      "264/3000 train_loss: 330.0382385253906 test_loss:465.37274169921875\n",
      "265/3000 train_loss: 334.184814453125 test_loss:473.7728271484375\n",
      "266/3000 train_loss: 325.0416564941406 test_loss:474.81231689453125\n",
      "267/3000 train_loss: 314.1767272949219 test_loss:460.9562072753906\n",
      "268/3000 train_loss: 334.23260498046875 test_loss:479.4613952636719\n",
      "269/3000 train_loss: 366.3396301269531 test_loss:460.8642272949219\n",
      "270/3000 train_loss: 321.6515197753906 test_loss:461.9267578125\n",
      "271/3000 train_loss: 338.00201416015625 test_loss:454.95361328125\n",
      "272/3000 train_loss: 316.7249755859375 test_loss:453.79949951171875\n",
      "273/3000 train_loss: 319.208984375 test_loss:454.42510986328125\n",
      "274/3000 train_loss: 351.4822082519531 test_loss:452.486083984375\n",
      "275/3000 train_loss: 383.7589416503906 test_loss:453.70599365234375\n",
      "276/3000 train_loss: 340.90185546875 test_loss:460.81951904296875\n",
      "277/3000 train_loss: 317.6323547363281 test_loss:452.3365478515625\n",
      "278/3000 train_loss: 326.28375244140625 test_loss:456.45660400390625\n",
      "279/3000 train_loss: 307.0797119140625 test_loss:446.24444580078125\n",
      "280/3000 train_loss: 304.6119079589844 test_loss:454.7313232421875\n",
      "281/3000 train_loss: 332.8230895996094 test_loss:447.65643310546875\n",
      "282/3000 train_loss: 337.3735656738281 test_loss:455.07427978515625\n",
      "283/3000 train_loss: 362.01611328125 test_loss:454.51470947265625\n",
      "284/3000 train_loss: 313.7042236328125 test_loss:454.96527099609375\n",
      "285/3000 train_loss: 362.11688232421875 test_loss:451.32012939453125\n",
      "286/3000 train_loss: 319.8070373535156 test_loss:451.91510009765625\n",
      "287/3000 train_loss: 317.06781005859375 test_loss:452.0697326660156\n",
      "288/3000 train_loss: 310.2450866699219 test_loss:459.2264404296875\n",
      "289/3000 train_loss: 331.4393615722656 test_loss:439.973388671875\n",
      "290/3000 train_loss: 302.7246398925781 test_loss:444.8244934082031\n",
      "291/3000 train_loss: 316.9349060058594 test_loss:437.8575134277344\n",
      "292/3000 train_loss: 323.94647216796875 test_loss:440.12060546875\n",
      "293/3000 train_loss: 300.0007629394531 test_loss:432.90625\n",
      "294/3000 train_loss: 305.635986328125 test_loss:439.18121337890625\n",
      "295/3000 train_loss: 307.45953369140625 test_loss:445.2965087890625\n",
      "296/3000 train_loss: 305.76495361328125 test_loss:437.59100341796875\n",
      "297/3000 train_loss: 311.7369079589844 test_loss:440.45599365234375\n",
      "298/3000 train_loss: 325.630126953125 test_loss:438.92620849609375\n",
      "299/3000 train_loss: 316.46044921875 test_loss:456.969970703125\n",
      "300/3000 train_loss: 309.770751953125 test_loss:431.6935729980469\n",
      "301/3000 train_loss: 323.87115478515625 test_loss:459.28143310546875\n",
      "302/3000 train_loss: 329.6510314941406 test_loss:442.67254638671875\n",
      "303/3000 train_loss: 308.06756591796875 test_loss:440.41278076171875\n",
      "304/3000 train_loss: 328.3929138183594 test_loss:454.21453857421875\n",
      "305/3000 train_loss: 309.82318115234375 test_loss:437.8816223144531\n",
      "306/3000 train_loss: 313.1402587890625 test_loss:439.30340576171875\n",
      "307/3000 train_loss: 315.5289611816406 test_loss:441.8822326660156\n",
      "308/3000 train_loss: 309.6175537109375 test_loss:442.0218505859375\n",
      "309/3000 train_loss: 311.529296875 test_loss:426.66796875\n",
      "310/3000 train_loss: 298.1269226074219 test_loss:434.1910400390625\n",
      "311/3000 train_loss: 307.493896484375 test_loss:425.618896484375\n",
      "312/3000 train_loss: 299.469482421875 test_loss:425.53680419921875\n",
      "313/3000 train_loss: 317.7884521484375 test_loss:425.4190979003906\n",
      "314/3000 train_loss: 356.877197265625 test_loss:429.148681640625\n",
      "315/3000 train_loss: 317.4028625488281 test_loss:423.5528564453125\n",
      "316/3000 train_loss: 292.6614990234375 test_loss:425.7054443359375\n",
      "317/3000 train_loss: 345.6479187011719 test_loss:429.8542175292969\n",
      "318/3000 train_loss: 308.85211181640625 test_loss:421.491455078125\n",
      "319/3000 train_loss: 288.6629638671875 test_loss:424.8932189941406\n",
      "320/3000 train_loss: 304.9101867675781 test_loss:428.9655456542969\n",
      "321/3000 train_loss: 302.09246826171875 test_loss:421.1446533203125\n",
      "322/3000 train_loss: 296.23699951171875 test_loss:420.4251708984375\n",
      "323/3000 train_loss: 300.6107482910156 test_loss:423.43890380859375\n",
      "324/3000 train_loss: 281.809814453125 test_loss:410.072265625\n",
      "325/3000 train_loss: 300.1673583984375 test_loss:422.38818359375\n",
      "326/3000 train_loss: 306.5616455078125 test_loss:403.9287109375\n",
      "327/3000 train_loss: 284.6153564453125 test_loss:411.43084716796875\n",
      "328/3000 train_loss: 283.16497802734375 test_loss:410.12518310546875\n",
      "329/3000 train_loss: 307.77978515625 test_loss:414.0745849609375\n",
      "330/3000 train_loss: 309.71728515625 test_loss:410.5352478027344\n",
      "331/3000 train_loss: 307.3895263671875 test_loss:400.44891357421875\n",
      "332/3000 train_loss: 305.41143798828125 test_loss:404.28466796875\n",
      "333/3000 train_loss: 292.50848388671875 test_loss:399.4662780761719\n",
      "334/3000 train_loss: 283.6558837890625 test_loss:398.1266174316406\n",
      "335/3000 train_loss: 308.7129211425781 test_loss:425.13519287109375\n",
      "336/3000 train_loss: 310.63153076171875 test_loss:404.0023498535156\n",
      "337/3000 train_loss: 266.28472900390625 test_loss:396.3472900390625\n",
      "338/3000 train_loss: 277.544189453125 test_loss:407.96466064453125\n",
      "339/3000 train_loss: 300.4382019042969 test_loss:393.28851318359375\n",
      "340/3000 train_loss: 310.822265625 test_loss:393.4473571777344\n",
      "341/3000 train_loss: 320.5193786621094 test_loss:408.91168212890625\n",
      "342/3000 train_loss: 299.408203125 test_loss:392.7450256347656\n",
      "343/3000 train_loss: 291.57220458984375 test_loss:393.1280822753906\n",
      "344/3000 train_loss: 272.92083740234375 test_loss:402.8832702636719\n",
      "345/3000 train_loss: 288.5599365234375 test_loss:389.2264404296875\n",
      "346/3000 train_loss: 282.6175231933594 test_loss:393.75079345703125\n",
      "347/3000 train_loss: 281.4940490722656 test_loss:411.3040771484375\n",
      "348/3000 train_loss: 286.6038818359375 test_loss:392.8488464355469\n",
      "349/3000 train_loss: 277.6764221191406 test_loss:389.829833984375\n",
      "350/3000 train_loss: 288.76812744140625 test_loss:400.0174560546875\n",
      "351/3000 train_loss: 296.0080871582031 test_loss:388.7691650390625\n",
      "352/3000 train_loss: 285.8797607421875 test_loss:425.67010498046875\n",
      "353/3000 train_loss: 285.9013671875 test_loss:393.1108703613281\n",
      "354/3000 train_loss: 292.14251708984375 test_loss:389.33673095703125\n",
      "355/3000 train_loss: 258.1038513183594 test_loss:398.2840270996094\n",
      "356/3000 train_loss: 279.33544921875 test_loss:381.87127685546875\n",
      "357/3000 train_loss: 273.1942138671875 test_loss:387.6610412597656\n",
      "358/3000 train_loss: 264.4273986816406 test_loss:389.5428466796875\n",
      "359/3000 train_loss: 278.2338562011719 test_loss:380.12823486328125\n",
      "360/3000 train_loss: 295.18658447265625 test_loss:394.63629150390625\n",
      "361/3000 train_loss: 260.85467529296875 test_loss:381.0559997558594\n",
      "362/3000 train_loss: 267.30072021484375 test_loss:384.2298583984375\n",
      "363/3000 train_loss: 267.3902587890625 test_loss:379.4881591796875\n",
      "364/3000 train_loss: 328.99200439453125 test_loss:374.9494934082031\n",
      "365/3000 train_loss: 321.73931884765625 test_loss:391.5244140625\n",
      "366/3000 train_loss: 265.9949951171875 test_loss:373.7989807128906\n",
      "367/3000 train_loss: 270.0368957519531 test_loss:388.95281982421875\n",
      "368/3000 train_loss: 258.25885009765625 test_loss:371.13818359375\n",
      "369/3000 train_loss: 265.9828186035156 test_loss:375.79852294921875\n",
      "370/3000 train_loss: 287.43572998046875 test_loss:377.7418212890625\n",
      "371/3000 train_loss: 276.62994384765625 test_loss:369.06988525390625\n",
      "372/3000 train_loss: 273.1454162597656 test_loss:374.01025390625\n",
      "373/3000 train_loss: 277.1829833984375 test_loss:368.11004638671875\n",
      "374/3000 train_loss: 279.00494384765625 test_loss:368.00164794921875\n",
      "375/3000 train_loss: 264.2507629394531 test_loss:365.31243896484375\n",
      "376/3000 train_loss: 278.1387023925781 test_loss:364.84893798828125\n",
      "377/3000 train_loss: 257.25482177734375 test_loss:355.160400390625\n",
      "378/3000 train_loss: 269.75555419921875 test_loss:366.22064208984375\n",
      "379/3000 train_loss: 286.782958984375 test_loss:352.543701171875\n",
      "380/3000 train_loss: 262.0052185058594 test_loss:362.71588134765625\n",
      "381/3000 train_loss: 264.148193359375 test_loss:355.9533996582031\n",
      "382/3000 train_loss: 255.643310546875 test_loss:358.0704345703125\n",
      "383/3000 train_loss: 245.92926025390625 test_loss:362.80975341796875\n",
      "384/3000 train_loss: 263.70721435546875 test_loss:350.30230712890625\n",
      "385/3000 train_loss: 263.08758544921875 test_loss:366.6507873535156\n",
      "386/3000 train_loss: 255.78814697265625 test_loss:357.202880859375\n",
      "387/3000 train_loss: 270.2933349609375 test_loss:355.04498291015625\n",
      "388/3000 train_loss: 251.24386596679688 test_loss:366.2030944824219\n",
      "389/3000 train_loss: 266.8252868652344 test_loss:346.98846435546875\n",
      "390/3000 train_loss: 233.02647399902344 test_loss:346.60333251953125\n",
      "391/3000 train_loss: 252.1859130859375 test_loss:358.34490966796875\n",
      "392/3000 train_loss: 264.4733581542969 test_loss:349.20684814453125\n",
      "393/3000 train_loss: 236.57545471191406 test_loss:345.308837890625\n",
      "394/3000 train_loss: 257.4749450683594 test_loss:343.16302490234375\n",
      "395/3000 train_loss: 265.760986328125 test_loss:342.70416259765625\n",
      "396/3000 train_loss: 248.98672485351562 test_loss:348.688232421875\n",
      "397/3000 train_loss: 241.95082092285156 test_loss:343.4376220703125\n",
      "398/3000 train_loss: 242.48565673828125 test_loss:335.89886474609375\n",
      "399/3000 train_loss: 256.965576171875 test_loss:337.2691650390625\n",
      "400/3000 train_loss: 236.53756713867188 test_loss:343.136474609375\n",
      "401/3000 train_loss: 244.0962677001953 test_loss:339.75982666015625\n",
      "402/3000 train_loss: 233.3990936279297 test_loss:329.23944091796875\n",
      "403/3000 train_loss: 267.31854248046875 test_loss:338.7982177734375\n",
      "404/3000 train_loss: 274.38641357421875 test_loss:332.99542236328125\n",
      "405/3000 train_loss: 243.34507751464844 test_loss:332.93878173828125\n",
      "406/3000 train_loss: 231.15126037597656 test_loss:341.9562072753906\n",
      "407/3000 train_loss: 279.95849609375 test_loss:340.95428466796875\n",
      "408/3000 train_loss: 264.122802734375 test_loss:340.4591064453125\n",
      "409/3000 train_loss: 234.90838623046875 test_loss:332.6187744140625\n",
      "410/3000 train_loss: 229.20074462890625 test_loss:336.36041259765625\n",
      "411/3000 train_loss: 251.0029754638672 test_loss:339.0151062011719\n",
      "412/3000 train_loss: 227.67572021484375 test_loss:332.7806091308594\n",
      "413/3000 train_loss: 228.41664123535156 test_loss:330.99237060546875\n",
      "414/3000 train_loss: 269.3004455566406 test_loss:333.76287841796875\n",
      "415/3000 train_loss: 263.77142333984375 test_loss:335.9833679199219\n",
      "416/3000 train_loss: 249.70199584960938 test_loss:322.22064208984375\n",
      "417/3000 train_loss: 259.2023010253906 test_loss:363.8695068359375\n",
      "418/3000 train_loss: 237.44229125976562 test_loss:324.92779541015625\n",
      "419/3000 train_loss: 261.7349548339844 test_loss:325.6171569824219\n",
      "420/3000 train_loss: 269.9329833984375 test_loss:352.3477783203125\n",
      "421/3000 train_loss: 254.5067901611328 test_loss:321.8502197265625\n",
      "422/3000 train_loss: 299.99285888671875 test_loss:316.3471374511719\n",
      "423/3000 train_loss: 235.817138671875 test_loss:317.134033203125\n",
      "424/3000 train_loss: 215.85704040527344 test_loss:327.4793395996094\n",
      "425/3000 train_loss: 226.19093322753906 test_loss:322.6930847167969\n",
      "426/3000 train_loss: 244.31129455566406 test_loss:318.1363830566406\n",
      "427/3000 train_loss: 226.7698211669922 test_loss:322.6893310546875\n",
      "428/3000 train_loss: 242.0463409423828 test_loss:314.5960693359375\n",
      "429/3000 train_loss: 228.41513061523438 test_loss:319.66064453125\n",
      "430/3000 train_loss: 257.6883850097656 test_loss:316.5856628417969\n",
      "431/3000 train_loss: 232.88539123535156 test_loss:316.48150634765625\n",
      "432/3000 train_loss: 228.76202392578125 test_loss:315.37823486328125\n",
      "433/3000 train_loss: 208.59791564941406 test_loss:325.640625\n",
      "434/3000 train_loss: 213.31654357910156 test_loss:319.20953369140625\n",
      "435/3000 train_loss: 229.22230529785156 test_loss:315.1530456542969\n",
      "436/3000 train_loss: 234.69015502929688 test_loss:330.0527648925781\n",
      "437/3000 train_loss: 213.73556518554688 test_loss:319.13885498046875\n",
      "438/3000 train_loss: 237.70947265625 test_loss:339.21685791015625\n",
      "439/3000 train_loss: 219.68214416503906 test_loss:311.1028747558594\n",
      "440/3000 train_loss: 227.93502807617188 test_loss:313.2397155761719\n",
      "441/3000 train_loss: 223.8705291748047 test_loss:314.97637939453125\n",
      "442/3000 train_loss: 214.42770385742188 test_loss:315.3431701660156\n",
      "443/3000 train_loss: 241.55984497070312 test_loss:308.37945556640625\n",
      "444/3000 train_loss: 231.9400634765625 test_loss:305.16546630859375\n",
      "445/3000 train_loss: 225.20339965820312 test_loss:307.5966491699219\n",
      "446/3000 train_loss: 251.22943115234375 test_loss:308.05517578125\n",
      "447/3000 train_loss: 222.9542999267578 test_loss:307.289794921875\n",
      "448/3000 train_loss: 223.6874237060547 test_loss:313.0201110839844\n",
      "449/3000 train_loss: 233.5902099609375 test_loss:307.27825927734375\n",
      "450/3000 train_loss: 222.61773681640625 test_loss:298.867919921875\n",
      "451/3000 train_loss: 209.11915588378906 test_loss:300.18450927734375\n",
      "452/3000 train_loss: 229.16526794433594 test_loss:303.3846435546875\n",
      "453/3000 train_loss: 203.6387481689453 test_loss:298.7012939453125\n",
      "454/3000 train_loss: 218.8636932373047 test_loss:297.2464294433594\n",
      "455/3000 train_loss: 226.68453979492188 test_loss:314.9925842285156\n",
      "456/3000 train_loss: 219.0133819580078 test_loss:300.12652587890625\n",
      "457/3000 train_loss: 225.1551513671875 test_loss:294.68829345703125\n",
      "458/3000 train_loss: 237.297119140625 test_loss:304.8963928222656\n",
      "459/3000 train_loss: 220.28280639648438 test_loss:299.4159240722656\n",
      "460/3000 train_loss: 207.57025146484375 test_loss:306.6432800292969\n",
      "461/3000 train_loss: 219.93734741210938 test_loss:293.8074645996094\n",
      "462/3000 train_loss: 202.34725952148438 test_loss:295.6592102050781\n",
      "463/3000 train_loss: 211.04454040527344 test_loss:296.81243896484375\n",
      "464/3000 train_loss: 216.25839233398438 test_loss:295.84185791015625\n",
      "465/3000 train_loss: 212.2966766357422 test_loss:310.10076904296875\n",
      "466/3000 train_loss: 231.880859375 test_loss:301.1422119140625\n",
      "467/3000 train_loss: 227.41909790039062 test_loss:298.93701171875\n",
      "468/3000 train_loss: 216.32284545898438 test_loss:302.10748291015625\n",
      "469/3000 train_loss: 291.1936340332031 test_loss:325.3912353515625\n",
      "470/3000 train_loss: 214.34881591796875 test_loss:301.7255859375\n",
      "471/3000 train_loss: 223.5048370361328 test_loss:311.8406677246094\n",
      "472/3000 train_loss: 224.61917114257812 test_loss:308.533935546875\n",
      "473/3000 train_loss: 211.98501586914062 test_loss:302.98748779296875\n",
      "474/3000 train_loss: 208.17662048339844 test_loss:294.5611877441406\n",
      "475/3000 train_loss: 193.17410278320312 test_loss:293.3856201171875\n",
      "476/3000 train_loss: 205.22433471679688 test_loss:301.9051513671875\n",
      "477/3000 train_loss: 212.50390625 test_loss:307.2793273925781\n",
      "478/3000 train_loss: 199.12149047851562 test_loss:291.89935302734375\n",
      "479/3000 train_loss: 212.06100463867188 test_loss:307.0039367675781\n",
      "480/3000 train_loss: 184.31866455078125 test_loss:297.3946533203125\n",
      "481/3000 train_loss: 210.61122131347656 test_loss:293.12109375\n",
      "482/3000 train_loss: 208.0441131591797 test_loss:292.2239990234375\n",
      "483/3000 train_loss: 229.12420654296875 test_loss:299.9830322265625\n",
      "484/3000 train_loss: 228.1688690185547 test_loss:303.5419616699219\n",
      "485/3000 train_loss: 241.78448486328125 test_loss:305.6510925292969\n",
      "486/3000 train_loss: 200.86598205566406 test_loss:300.7872314453125\n",
      "487/3000 train_loss: 202.66143798828125 test_loss:285.3162841796875\n",
      "488/3000 train_loss: 198.83551025390625 test_loss:291.37103271484375\n",
      "489/3000 train_loss: 205.03878784179688 test_loss:286.28216552734375\n",
      "490/3000 train_loss: 195.09881591796875 test_loss:282.42950439453125\n",
      "491/3000 train_loss: 192.5467987060547 test_loss:276.6618347167969\n",
      "492/3000 train_loss: 192.0748291015625 test_loss:305.93524169921875\n",
      "493/3000 train_loss: 211.28074645996094 test_loss:276.80743408203125\n",
      "494/3000 train_loss: 175.44432067871094 test_loss:288.160888671875\n",
      "495/3000 train_loss: 189.4044189453125 test_loss:297.52716064453125\n",
      "496/3000 train_loss: 195.98130798339844 test_loss:275.5906677246094\n",
      "497/3000 train_loss: 189.9373321533203 test_loss:281.5751647949219\n",
      "498/3000 train_loss: 234.21255493164062 test_loss:277.56304931640625\n",
      "499/3000 train_loss: 246.46241760253906 test_loss:287.5704650878906\n",
      "500/3000 train_loss: 237.95184326171875 test_loss:274.06170654296875\n",
      "501/3000 train_loss: 215.6011505126953 test_loss:275.52337646484375\n",
      "502/3000 train_loss: 200.38230895996094 test_loss:275.3025207519531\n",
      "503/3000 train_loss: 202.3386993408203 test_loss:281.3782653808594\n",
      "504/3000 train_loss: 190.6327667236328 test_loss:270.1969909667969\n",
      "505/3000 train_loss: 190.54638671875 test_loss:274.03839111328125\n",
      "506/3000 train_loss: 196.03433227539062 test_loss:277.6377868652344\n",
      "507/3000 train_loss: 211.8068084716797 test_loss:277.1214294433594\n",
      "508/3000 train_loss: 243.1934356689453 test_loss:268.4697570800781\n",
      "509/3000 train_loss: 204.07350158691406 test_loss:268.142822265625\n",
      "510/3000 train_loss: 199.15408325195312 test_loss:277.3413391113281\n",
      "511/3000 train_loss: 214.32022094726562 test_loss:270.19512939453125\n",
      "512/3000 train_loss: 186.13027954101562 test_loss:261.30389404296875\n",
      "513/3000 train_loss: 202.46165466308594 test_loss:267.5166931152344\n",
      "514/3000 train_loss: 185.99041748046875 test_loss:261.448974609375\n",
      "515/3000 train_loss: 197.84146118164062 test_loss:260.8074645996094\n",
      "516/3000 train_loss: 183.6826629638672 test_loss:260.70867919921875\n",
      "517/3000 train_loss: 201.33834838867188 test_loss:259.9930419921875\n",
      "518/3000 train_loss: 183.8726806640625 test_loss:259.5115966796875\n",
      "519/3000 train_loss: 176.9798126220703 test_loss:265.39447021484375\n",
      "520/3000 train_loss: 176.46266174316406 test_loss:261.007080078125\n",
      "521/3000 train_loss: 216.73756408691406 test_loss:271.21221923828125\n",
      "522/3000 train_loss: 203.17047119140625 test_loss:268.92376708984375\n",
      "523/3000 train_loss: 215.1083984375 test_loss:263.8304138183594\n",
      "524/3000 train_loss: 168.75936889648438 test_loss:266.4410095214844\n",
      "525/3000 train_loss: 180.87693786621094 test_loss:255.105224609375\n",
      "526/3000 train_loss: 184.6552276611328 test_loss:255.67904663085938\n",
      "527/3000 train_loss: 225.64822387695312 test_loss:265.468017578125\n",
      "528/3000 train_loss: 188.83023071289062 test_loss:254.13812255859375\n",
      "529/3000 train_loss: 181.13729858398438 test_loss:263.95208740234375\n",
      "530/3000 train_loss: 170.32664489746094 test_loss:259.24139404296875\n",
      "531/3000 train_loss: 174.14109802246094 test_loss:258.0699462890625\n",
      "532/3000 train_loss: 199.36614990234375 test_loss:263.3127746582031\n",
      "533/3000 train_loss: 182.64004516601562 test_loss:258.30303955078125\n",
      "534/3000 train_loss: 176.9168243408203 test_loss:254.42672729492188\n",
      "535/3000 train_loss: 196.31114196777344 test_loss:254.60574340820312\n",
      "536/3000 train_loss: 178.52952575683594 test_loss:259.3194885253906\n",
      "537/3000 train_loss: 179.66433715820312 test_loss:263.9924011230469\n",
      "538/3000 train_loss: 174.71823120117188 test_loss:256.6326904296875\n",
      "539/3000 train_loss: 176.78756713867188 test_loss:260.5950622558594\n",
      "540/3000 train_loss: 184.75531005859375 test_loss:257.18194580078125\n",
      "541/3000 train_loss: 172.49655151367188 test_loss:254.7283935546875\n",
      "542/3000 train_loss: 173.55838012695312 test_loss:260.24725341796875\n",
      "543/3000 train_loss: 185.2010955810547 test_loss:258.4638977050781\n",
      "544/3000 train_loss: 183.54962158203125 test_loss:249.19891357421875\n",
      "545/3000 train_loss: 193.9025115966797 test_loss:253.13206481933594\n",
      "546/3000 train_loss: 174.72378540039062 test_loss:262.1947021484375\n",
      "547/3000 train_loss: 182.0801544189453 test_loss:249.74429321289062\n",
      "548/3000 train_loss: 215.43841552734375 test_loss:260.8617248535156\n",
      "549/3000 train_loss: 176.3347930908203 test_loss:265.4937744140625\n",
      "550/3000 train_loss: 188.44329833984375 test_loss:255.706298828125\n",
      "551/3000 train_loss: 182.85464477539062 test_loss:249.88455200195312\n",
      "552/3000 train_loss: 171.9716339111328 test_loss:251.39060974121094\n",
      "553/3000 train_loss: 184.6850128173828 test_loss:250.79885864257812\n",
      "554/3000 train_loss: 191.10508728027344 test_loss:262.17010498046875\n",
      "555/3000 train_loss: 187.03366088867188 test_loss:256.6131591796875\n",
      "556/3000 train_loss: 180.2537078857422 test_loss:263.84820556640625\n",
      "557/3000 train_loss: 195.31207275390625 test_loss:262.9007263183594\n",
      "558/3000 train_loss: 194.2965087890625 test_loss:252.69813537597656\n",
      "559/3000 train_loss: 171.8168487548828 test_loss:245.94358825683594\n",
      "560/3000 train_loss: 176.7239990234375 test_loss:247.54006958007812\n",
      "561/3000 train_loss: 183.85287475585938 test_loss:246.86923217773438\n",
      "562/3000 train_loss: 163.80885314941406 test_loss:247.91065979003906\n",
      "563/3000 train_loss: 175.24240112304688 test_loss:244.46542358398438\n",
      "564/3000 train_loss: 179.3112030029297 test_loss:244.43063354492188\n",
      "565/3000 train_loss: 168.8379364013672 test_loss:249.83103942871094\n",
      "566/3000 train_loss: 192.0712890625 test_loss:251.59161376953125\n",
      "567/3000 train_loss: 170.59481811523438 test_loss:245.66696166992188\n",
      "568/3000 train_loss: 176.4158935546875 test_loss:243.1275634765625\n",
      "569/3000 train_loss: 173.59213256835938 test_loss:248.64529418945312\n",
      "570/3000 train_loss: 174.2535400390625 test_loss:257.27020263671875\n",
      "571/3000 train_loss: 176.59719848632812 test_loss:243.4796905517578\n",
      "572/3000 train_loss: 180.8352813720703 test_loss:262.00390625\n",
      "573/3000 train_loss: 187.19517517089844 test_loss:244.14886474609375\n",
      "574/3000 train_loss: 216.6680145263672 test_loss:255.47283935546875\n",
      "575/3000 train_loss: 186.57867431640625 test_loss:248.0648193359375\n",
      "576/3000 train_loss: 162.07913208007812 test_loss:257.0399169921875\n",
      "577/3000 train_loss: 190.92095947265625 test_loss:244.70285034179688\n",
      "578/3000 train_loss: 175.60263061523438 test_loss:256.3935852050781\n",
      "579/3000 train_loss: 178.51031494140625 test_loss:248.241455078125\n",
      "580/3000 train_loss: 171.5560302734375 test_loss:261.6768798828125\n",
      "581/3000 train_loss: 176.3102264404297 test_loss:253.976806640625\n",
      "582/3000 train_loss: 171.30166625976562 test_loss:242.14035034179688\n",
      "583/3000 train_loss: 163.06532287597656 test_loss:245.86825561523438\n",
      "584/3000 train_loss: 195.7353515625 test_loss:239.07098388671875\n",
      "585/3000 train_loss: 157.13046264648438 test_loss:250.4071502685547\n",
      "586/3000 train_loss: 156.16400146484375 test_loss:240.85263061523438\n",
      "587/3000 train_loss: 184.56195068359375 test_loss:241.36038208007812\n",
      "588/3000 train_loss: 180.99778747558594 test_loss:255.143798828125\n",
      "589/3000 train_loss: 163.9574737548828 test_loss:251.19573974609375\n",
      "590/3000 train_loss: 170.08108520507812 test_loss:250.2459716796875\n",
      "591/3000 train_loss: 173.062255859375 test_loss:236.51974487304688\n",
      "592/3000 train_loss: 178.4446258544922 test_loss:241.88809204101562\n",
      "593/3000 train_loss: 163.91958618164062 test_loss:244.4415740966797\n",
      "594/3000 train_loss: 183.22256469726562 test_loss:244.73458862304688\n",
      "595/3000 train_loss: 150.08761596679688 test_loss:244.1485595703125\n",
      "596/3000 train_loss: 172.27574157714844 test_loss:245.27935791015625\n",
      "597/3000 train_loss: 175.20303344726562 test_loss:240.04759216308594\n",
      "598/3000 train_loss: 174.87632751464844 test_loss:258.7809143066406\n",
      "599/3000 train_loss: 173.44451904296875 test_loss:239.22036743164062\n",
      "600/3000 train_loss: 171.68955993652344 test_loss:243.63250732421875\n",
      "601/3000 train_loss: 173.60060119628906 test_loss:247.04437255859375\n",
      "602/3000 train_loss: 175.35556030273438 test_loss:241.24227905273438\n",
      "603/3000 train_loss: 168.80148315429688 test_loss:241.04632568359375\n",
      "604/3000 train_loss: 171.4154510498047 test_loss:237.78903198242188\n",
      "605/3000 train_loss: 190.41238403320312 test_loss:244.90988159179688\n",
      "606/3000 train_loss: 180.6422882080078 test_loss:258.0797119140625\n",
      "607/3000 train_loss: 163.91510009765625 test_loss:240.62905883789062\n",
      "608/3000 train_loss: 177.80535888671875 test_loss:235.49270629882812\n",
      "609/3000 train_loss: 183.718017578125 test_loss:242.17462158203125\n",
      "610/3000 train_loss: 169.1543426513672 test_loss:254.9243927001953\n",
      "611/3000 train_loss: 187.51724243164062 test_loss:229.9746856689453\n",
      "612/3000 train_loss: 178.19052124023438 test_loss:234.35830688476562\n",
      "613/3000 train_loss: 153.56178283691406 test_loss:234.55816650390625\n",
      "614/3000 train_loss: 155.2633514404297 test_loss:257.92315673828125\n",
      "615/3000 train_loss: 163.85008239746094 test_loss:231.75082397460938\n",
      "616/3000 train_loss: 168.3053741455078 test_loss:232.8392791748047\n",
      "617/3000 train_loss: 179.97496032714844 test_loss:229.8189697265625\n",
      "618/3000 train_loss: 172.56332397460938 test_loss:237.89059448242188\n",
      "619/3000 train_loss: 192.87298583984375 test_loss:233.63665771484375\n",
      "620/3000 train_loss: 207.85775756835938 test_loss:236.60174560546875\n",
      "621/3000 train_loss: 156.5069122314453 test_loss:238.1063232421875\n",
      "622/3000 train_loss: 166.80039978027344 test_loss:224.57626342773438\n",
      "623/3000 train_loss: 162.82534790039062 test_loss:235.40223693847656\n",
      "624/3000 train_loss: 170.5615692138672 test_loss:224.19500732421875\n",
      "625/3000 train_loss: 165.4821014404297 test_loss:225.00741577148438\n",
      "626/3000 train_loss: 165.37513732910156 test_loss:235.48284912109375\n",
      "627/3000 train_loss: 160.94749450683594 test_loss:228.03793334960938\n",
      "628/3000 train_loss: 160.49607849121094 test_loss:228.4068145751953\n",
      "629/3000 train_loss: 157.11302185058594 test_loss:239.03237915039062\n",
      "630/3000 train_loss: 164.2414093017578 test_loss:223.27825927734375\n",
      "631/3000 train_loss: 202.41238403320312 test_loss:227.89060974121094\n",
      "632/3000 train_loss: 154.04739379882812 test_loss:231.23521423339844\n",
      "633/3000 train_loss: 159.5909423828125 test_loss:228.8909912109375\n",
      "634/3000 train_loss: 149.02127075195312 test_loss:218.0425567626953\n",
      "635/3000 train_loss: 158.3944854736328 test_loss:224.17269897460938\n",
      "636/3000 train_loss: 179.086669921875 test_loss:225.51473999023438\n",
      "637/3000 train_loss: 148.52769470214844 test_loss:220.35205078125\n",
      "638/3000 train_loss: 162.86001586914062 test_loss:226.84429931640625\n",
      "639/3000 train_loss: 154.42930603027344 test_loss:227.27345275878906\n",
      "640/3000 train_loss: 151.97543334960938 test_loss:226.80780029296875\n",
      "641/3000 train_loss: 168.72972106933594 test_loss:224.85296630859375\n",
      "642/3000 train_loss: 169.67845153808594 test_loss:224.37330627441406\n",
      "643/3000 train_loss: 192.58522033691406 test_loss:234.37295532226562\n",
      "644/3000 train_loss: 198.14187622070312 test_loss:227.66452026367188\n",
      "645/3000 train_loss: 173.26937866210938 test_loss:220.76144409179688\n",
      "646/3000 train_loss: 175.02859497070312 test_loss:225.30572509765625\n",
      "647/3000 train_loss: 166.1735076904297 test_loss:227.5466766357422\n",
      "648/3000 train_loss: 175.94332885742188 test_loss:229.0758819580078\n",
      "649/3000 train_loss: 142.08567810058594 test_loss:221.83609008789062\n",
      "650/3000 train_loss: 164.45664978027344 test_loss:228.97183227539062\n",
      "651/3000 train_loss: 156.91311645507812 test_loss:225.39987182617188\n",
      "652/3000 train_loss: 154.0022430419922 test_loss:224.12796020507812\n",
      "653/3000 train_loss: 164.47607421875 test_loss:222.42535400390625\n",
      "654/3000 train_loss: 174.0545654296875 test_loss:234.65980529785156\n",
      "655/3000 train_loss: 160.66685485839844 test_loss:219.05938720703125\n",
      "656/3000 train_loss: 159.33131408691406 test_loss:224.34642028808594\n",
      "657/3000 train_loss: 154.67381286621094 test_loss:224.1800537109375\n",
      "658/3000 train_loss: 161.7079620361328 test_loss:220.93255615234375\n",
      "659/3000 train_loss: 158.4394989013672 test_loss:226.485595703125\n",
      "660/3000 train_loss: 155.31430053710938 test_loss:225.40240478515625\n",
      "661/3000 train_loss: 159.17649841308594 test_loss:227.771484375\n",
      "662/3000 train_loss: 153.26370239257812 test_loss:220.85743713378906\n",
      "663/3000 train_loss: 158.34280395507812 test_loss:239.14849853515625\n",
      "664/3000 train_loss: 176.1688232421875 test_loss:220.99798583984375\n",
      "665/3000 train_loss: 144.8292236328125 test_loss:223.97235107421875\n",
      "666/3000 train_loss: 148.4071044921875 test_loss:218.82936096191406\n",
      "667/3000 train_loss: 147.94374084472656 test_loss:216.87301635742188\n",
      "668/3000 train_loss: 167.28431701660156 test_loss:218.00242614746094\n",
      "669/3000 train_loss: 158.60208129882812 test_loss:216.48724365234375\n",
      "670/3000 train_loss: 180.14453125 test_loss:227.10728454589844\n",
      "671/3000 train_loss: 166.849365234375 test_loss:219.74708557128906\n",
      "672/3000 train_loss: 154.92686462402344 test_loss:219.76864624023438\n",
      "673/3000 train_loss: 174.70289611816406 test_loss:216.2495574951172\n",
      "674/3000 train_loss: 153.97752380371094 test_loss:217.99090576171875\n",
      "675/3000 train_loss: 157.05331420898438 test_loss:220.14312744140625\n",
      "676/3000 train_loss: 161.1131591796875 test_loss:218.470947265625\n",
      "677/3000 train_loss: 152.6800537109375 test_loss:218.90695190429688\n",
      "678/3000 train_loss: 163.76248168945312 test_loss:229.45361328125\n",
      "679/3000 train_loss: 173.16067504882812 test_loss:212.58883666992188\n",
      "680/3000 train_loss: 164.4553680419922 test_loss:219.08848571777344\n",
      "681/3000 train_loss: 156.3511199951172 test_loss:217.62477111816406\n",
      "682/3000 train_loss: 159.5628204345703 test_loss:221.90182495117188\n",
      "683/3000 train_loss: 152.85365295410156 test_loss:218.60113525390625\n",
      "684/3000 train_loss: 165.86575317382812 test_loss:219.47128295898438\n",
      "685/3000 train_loss: 162.49903869628906 test_loss:212.977294921875\n",
      "686/3000 train_loss: 145.56857299804688 test_loss:219.89479064941406\n",
      "687/3000 train_loss: 153.3300018310547 test_loss:224.68728637695312\n",
      "688/3000 train_loss: 150.04515075683594 test_loss:220.0758056640625\n",
      "689/3000 train_loss: 166.0182342529297 test_loss:217.18075561523438\n",
      "690/3000 train_loss: 149.568603515625 test_loss:214.93002319335938\n",
      "691/3000 train_loss: 161.1591033935547 test_loss:215.15774536132812\n",
      "692/3000 train_loss: 150.01226806640625 test_loss:214.16195678710938\n",
      "693/3000 train_loss: 169.6476593017578 test_loss:225.70127868652344\n",
      "694/3000 train_loss: 173.75531005859375 test_loss:221.41696166992188\n",
      "695/3000 train_loss: 152.2188262939453 test_loss:211.37210083007812\n",
      "696/3000 train_loss: 155.86683654785156 test_loss:214.5230712890625\n",
      "697/3000 train_loss: 151.39950561523438 test_loss:211.58828735351562\n",
      "698/3000 train_loss: 151.46636962890625 test_loss:212.93820190429688\n",
      "699/3000 train_loss: 138.31924438476562 test_loss:209.59600830078125\n",
      "700/3000 train_loss: 153.33395385742188 test_loss:210.53961181640625\n",
      "701/3000 train_loss: 162.58734130859375 test_loss:209.9009246826172\n",
      "702/3000 train_loss: 147.8983154296875 test_loss:209.57327270507812\n",
      "703/3000 train_loss: 149.9627227783203 test_loss:217.1057586669922\n",
      "704/3000 train_loss: 176.04595947265625 test_loss:218.32571411132812\n",
      "705/3000 train_loss: 160.6132049560547 test_loss:219.54324340820312\n",
      "706/3000 train_loss: 153.0604705810547 test_loss:209.65269470214844\n",
      "707/3000 train_loss: 146.53656005859375 test_loss:204.69601440429688\n",
      "708/3000 train_loss: 149.3269500732422 test_loss:220.63858032226562\n",
      "709/3000 train_loss: 162.76480102539062 test_loss:214.4638671875\n",
      "710/3000 train_loss: 143.00421142578125 test_loss:207.58670043945312\n",
      "711/3000 train_loss: 156.3339385986328 test_loss:210.1678466796875\n",
      "712/3000 train_loss: 154.0901641845703 test_loss:213.41525268554688\n",
      "713/3000 train_loss: 140.43785095214844 test_loss:210.80673217773438\n",
      "714/3000 train_loss: 162.56468200683594 test_loss:209.20510864257812\n",
      "715/3000 train_loss: 146.6504364013672 test_loss:216.19091796875\n",
      "716/3000 train_loss: 159.73178100585938 test_loss:216.45663452148438\n",
      "717/3000 train_loss: 146.97161865234375 test_loss:215.0604248046875\n",
      "718/3000 train_loss: 170.27256774902344 test_loss:207.89447021484375\n",
      "719/3000 train_loss: 161.6838836669922 test_loss:214.28326416015625\n",
      "720/3000 train_loss: 146.33299255371094 test_loss:210.41299438476562\n",
      "721/3000 train_loss: 162.30996704101562 test_loss:226.64974975585938\n",
      "722/3000 train_loss: 174.14404296875 test_loss:212.44345092773438\n",
      "723/3000 train_loss: 152.62039184570312 test_loss:207.35971069335938\n",
      "724/3000 train_loss: 154.64129638671875 test_loss:212.19577026367188\n",
      "725/3000 train_loss: 142.76268005371094 test_loss:213.12103271484375\n",
      "726/3000 train_loss: 158.74102783203125 test_loss:208.81748962402344\n",
      "727/3000 train_loss: 147.3720703125 test_loss:228.26150512695312\n",
      "728/3000 train_loss: 156.60308837890625 test_loss:212.97535705566406\n",
      "729/3000 train_loss: 155.671875 test_loss:212.37255859375\n",
      "730/3000 train_loss: 154.96258544921875 test_loss:215.06533813476562\n",
      "731/3000 train_loss: 153.47085571289062 test_loss:212.2610626220703\n",
      "732/3000 train_loss: 160.89588928222656 test_loss:225.9012451171875\n",
      "733/3000 train_loss: 157.25933837890625 test_loss:214.3240966796875\n",
      "734/3000 train_loss: 156.3453826904297 test_loss:212.89820861816406\n",
      "735/3000 train_loss: 173.08541870117188 test_loss:218.04327392578125\n",
      "736/3000 train_loss: 151.49026489257812 test_loss:210.59915161132812\n",
      "737/3000 train_loss: 152.2879638671875 test_loss:209.781982421875\n",
      "738/3000 train_loss: 151.26776123046875 test_loss:214.33929443359375\n",
      "739/3000 train_loss: 147.81675720214844 test_loss:208.2720489501953\n",
      "740/3000 train_loss: 139.8514404296875 test_loss:204.41500854492188\n",
      "741/3000 train_loss: 143.28970336914062 test_loss:208.67962646484375\n",
      "742/3000 train_loss: 140.53414916992188 test_loss:208.78945922851562\n",
      "743/3000 train_loss: 156.1048126220703 test_loss:210.26113891601562\n",
      "744/3000 train_loss: 140.91998291015625 test_loss:205.99514770507812\n",
      "745/3000 train_loss: 156.11221313476562 test_loss:210.0136260986328\n",
      "746/3000 train_loss: 149.75747680664062 test_loss:210.1972198486328\n",
      "747/3000 train_loss: 146.6427001953125 test_loss:207.20077514648438\n",
      "748/3000 train_loss: 162.3208770751953 test_loss:216.39129638671875\n",
      "749/3000 train_loss: 159.6537322998047 test_loss:208.3712615966797\n",
      "750/3000 train_loss: 161.89527893066406 test_loss:211.24542236328125\n",
      "751/3000 train_loss: 149.7884979248047 test_loss:206.99276733398438\n",
      "752/3000 train_loss: 184.60342407226562 test_loss:203.966552734375\n",
      "753/3000 train_loss: 133.28488159179688 test_loss:208.76760864257812\n",
      "754/3000 train_loss: 136.76622009277344 test_loss:205.60032653808594\n",
      "755/3000 train_loss: 145.42593383789062 test_loss:205.41073608398438\n",
      "756/3000 train_loss: 157.0381317138672 test_loss:209.83509826660156\n",
      "757/3000 train_loss: 142.36053466796875 test_loss:208.1357879638672\n",
      "758/3000 train_loss: 158.76548767089844 test_loss:207.19384765625\n",
      "759/3000 train_loss: 151.26895141601562 test_loss:208.17623901367188\n",
      "760/3000 train_loss: 148.7037811279297 test_loss:205.5585479736328\n",
      "761/3000 train_loss: 156.0382843017578 test_loss:206.2227325439453\n",
      "762/3000 train_loss: 183.56710815429688 test_loss:213.10830688476562\n",
      "763/3000 train_loss: 142.05636596679688 test_loss:209.2462158203125\n",
      "764/3000 train_loss: 143.73294067382812 test_loss:204.87069702148438\n",
      "765/3000 train_loss: 147.30723571777344 test_loss:206.69113159179688\n",
      "766/3000 train_loss: 156.01611328125 test_loss:202.36793518066406\n",
      "767/3000 train_loss: 138.9447479248047 test_loss:201.4853515625\n",
      "768/3000 train_loss: 161.45013427734375 test_loss:201.2170867919922\n",
      "769/3000 train_loss: 131.9548797607422 test_loss:206.65139770507812\n",
      "770/3000 train_loss: 157.9261016845703 test_loss:205.4568634033203\n",
      "771/3000 train_loss: 150.66648864746094 test_loss:208.40248107910156\n",
      "772/3000 train_loss: 156.41909790039062 test_loss:200.28604125976562\n",
      "773/3000 train_loss: 147.82688903808594 test_loss:204.9104461669922\n",
      "774/3000 train_loss: 140.88626098632812 test_loss:198.12734985351562\n",
      "775/3000 train_loss: 150.10128784179688 test_loss:196.2101287841797\n",
      "776/3000 train_loss: 249.28607177734375 test_loss:216.9022216796875\n",
      "777/3000 train_loss: 161.6459503173828 test_loss:231.3109588623047\n",
      "778/3000 train_loss: 166.70645141601562 test_loss:232.67967224121094\n",
      "779/3000 train_loss: 154.27671813964844 test_loss:216.27432250976562\n",
      "780/3000 train_loss: 140.02194213867188 test_loss:218.1397247314453\n",
      "781/3000 train_loss: 151.3396453857422 test_loss:211.1272430419922\n",
      "782/3000 train_loss: 156.6951904296875 test_loss:203.03677368164062\n",
      "783/3000 train_loss: 133.84515380859375 test_loss:202.5972137451172\n",
      "784/3000 train_loss: 131.16554260253906 test_loss:206.63673400878906\n",
      "785/3000 train_loss: 131.68020629882812 test_loss:201.943359375\n",
      "786/3000 train_loss: 156.0756072998047 test_loss:203.66175842285156\n",
      "787/3000 train_loss: 145.09463500976562 test_loss:206.0407257080078\n",
      "788/3000 train_loss: 151.25071716308594 test_loss:200.14254760742188\n",
      "789/3000 train_loss: 138.86183166503906 test_loss:206.9788360595703\n",
      "790/3000 train_loss: 146.01611328125 test_loss:212.9280242919922\n",
      "791/3000 train_loss: 146.72213745117188 test_loss:205.6177520751953\n",
      "792/3000 train_loss: 190.03369140625 test_loss:204.27536010742188\n",
      "793/3000 train_loss: 132.86363220214844 test_loss:202.71531677246094\n",
      "794/3000 train_loss: 129.3507843017578 test_loss:205.94024658203125\n",
      "795/3000 train_loss: 128.12828063964844 test_loss:206.77810668945312\n",
      "796/3000 train_loss: 166.83958435058594 test_loss:209.7062530517578\n",
      "797/3000 train_loss: 137.21865844726562 test_loss:202.18600463867188\n",
      "798/3000 train_loss: 133.28265380859375 test_loss:197.27090454101562\n",
      "799/3000 train_loss: 138.03475952148438 test_loss:198.0670623779297\n",
      "800/3000 train_loss: 138.7650604248047 test_loss:210.61032104492188\n",
      "801/3000 train_loss: 136.6564178466797 test_loss:197.79339599609375\n",
      "802/3000 train_loss: 126.79476928710938 test_loss:199.74777221679688\n",
      "803/3000 train_loss: 144.4287872314453 test_loss:202.55897521972656\n",
      "804/3000 train_loss: 164.92138671875 test_loss:200.1558837890625\n",
      "805/3000 train_loss: 131.95046997070312 test_loss:201.02764892578125\n",
      "806/3000 train_loss: 152.39022827148438 test_loss:198.1591796875\n",
      "807/3000 train_loss: 148.09776306152344 test_loss:206.40423583984375\n",
      "808/3000 train_loss: 158.74374389648438 test_loss:206.59780883789062\n",
      "809/3000 train_loss: 143.9319305419922 test_loss:201.2909698486328\n",
      "810/3000 train_loss: 131.46376037597656 test_loss:201.56396484375\n",
      "811/3000 train_loss: 131.8286895751953 test_loss:193.23541259765625\n",
      "812/3000 train_loss: 129.39242553710938 test_loss:191.8970184326172\n",
      "813/3000 train_loss: 138.04306030273438 test_loss:197.4681396484375\n",
      "814/3000 train_loss: 128.12911987304688 test_loss:189.62307739257812\n",
      "815/3000 train_loss: 130.46815490722656 test_loss:195.09310913085938\n",
      "816/3000 train_loss: 184.4925537109375 test_loss:191.10482788085938\n",
      "817/3000 train_loss: 134.5824432373047 test_loss:201.98257446289062\n",
      "818/3000 train_loss: 141.93417358398438 test_loss:197.4327392578125\n",
      "819/3000 train_loss: 140.1269073486328 test_loss:199.7928466796875\n",
      "820/3000 train_loss: 136.2406463623047 test_loss:193.79364013671875\n",
      "821/3000 train_loss: 135.14895629882812 test_loss:198.39834594726562\n",
      "822/3000 train_loss: 139.08319091796875 test_loss:192.35830688476562\n",
      "823/3000 train_loss: 128.4279022216797 test_loss:194.90907287597656\n",
      "824/3000 train_loss: 126.74088287353516 test_loss:199.01377868652344\n",
      "825/3000 train_loss: 146.6038360595703 test_loss:189.57102966308594\n",
      "826/3000 train_loss: 152.05641174316406 test_loss:198.19601440429688\n",
      "827/3000 train_loss: 137.22166442871094 test_loss:197.8516845703125\n",
      "828/3000 train_loss: 136.46839904785156 test_loss:200.65249633789062\n",
      "829/3000 train_loss: 132.30709838867188 test_loss:194.63970947265625\n",
      "830/3000 train_loss: 133.15289306640625 test_loss:194.79725646972656\n",
      "831/3000 train_loss: 146.06295776367188 test_loss:195.6398468017578\n",
      "832/3000 train_loss: 133.12588500976562 test_loss:190.6376190185547\n",
      "833/3000 train_loss: 156.40744018554688 test_loss:193.47042846679688\n",
      "834/3000 train_loss: 155.30870056152344 test_loss:196.36300659179688\n",
      "835/3000 train_loss: 147.23692321777344 test_loss:209.2677459716797\n",
      "836/3000 train_loss: 147.08436584472656 test_loss:198.13238525390625\n",
      "837/3000 train_loss: 156.3048095703125 test_loss:191.98956298828125\n",
      "838/3000 train_loss: 148.55284118652344 test_loss:187.92587280273438\n",
      "839/3000 train_loss: 142.7557373046875 test_loss:190.9063720703125\n",
      "840/3000 train_loss: 133.7333526611328 test_loss:199.4866943359375\n",
      "841/3000 train_loss: 151.0169219970703 test_loss:191.53759765625\n",
      "842/3000 train_loss: 136.51519775390625 test_loss:186.20721435546875\n",
      "843/3000 train_loss: 122.14781951904297 test_loss:186.4844512939453\n",
      "844/3000 train_loss: 135.09878540039062 test_loss:183.07083129882812\n",
      "845/3000 train_loss: 120.0925064086914 test_loss:187.80494689941406\n",
      "846/3000 train_loss: 140.50177001953125 test_loss:198.166748046875\n",
      "847/3000 train_loss: 128.8300018310547 test_loss:184.21905517578125\n",
      "848/3000 train_loss: 135.87567138671875 test_loss:180.42843627929688\n",
      "849/3000 train_loss: 131.34628295898438 test_loss:187.45323181152344\n",
      "850/3000 train_loss: 141.7752685546875 test_loss:190.53915405273438\n",
      "851/3000 train_loss: 133.27056884765625 test_loss:184.37100219726562\n",
      "852/3000 train_loss: 125.75308227539062 test_loss:183.5870361328125\n",
      "853/3000 train_loss: 123.61125183105469 test_loss:185.6728973388672\n",
      "854/3000 train_loss: 126.37307739257812 test_loss:184.42604064941406\n",
      "855/3000 train_loss: 119.11003112792969 test_loss:182.7186737060547\n",
      "856/3000 train_loss: 130.37112426757812 test_loss:184.61497497558594\n",
      "857/3000 train_loss: 136.251953125 test_loss:179.8097381591797\n",
      "858/3000 train_loss: 131.53903198242188 test_loss:186.78829956054688\n",
      "859/3000 train_loss: 127.58024597167969 test_loss:185.76742553710938\n",
      "860/3000 train_loss: 121.47679901123047 test_loss:188.4876251220703\n",
      "861/3000 train_loss: 130.541748046875 test_loss:185.5959930419922\n",
      "862/3000 train_loss: 126.59054565429688 test_loss:179.576416015625\n",
      "863/3000 train_loss: 137.8948516845703 test_loss:178.51905822753906\n",
      "864/3000 train_loss: 127.16797637939453 test_loss:175.83302307128906\n",
      "865/3000 train_loss: 131.48394775390625 test_loss:189.2005615234375\n",
      "866/3000 train_loss: 123.10269165039062 test_loss:189.93536376953125\n",
      "867/3000 train_loss: 135.31890869140625 test_loss:182.34829711914062\n",
      "868/3000 train_loss: 124.51222229003906 test_loss:180.81500244140625\n",
      "869/3000 train_loss: 126.82624053955078 test_loss:177.70509338378906\n",
      "870/3000 train_loss: 127.79428100585938 test_loss:181.80453491210938\n",
      "871/3000 train_loss: 120.08659362792969 test_loss:181.99354553222656\n",
      "872/3000 train_loss: 130.93341064453125 test_loss:177.65481567382812\n",
      "873/3000 train_loss: 129.5102081298828 test_loss:183.22714233398438\n",
      "874/3000 train_loss: 138.65577697753906 test_loss:178.31324768066406\n",
      "875/3000 train_loss: 130.05654907226562 test_loss:185.92462158203125\n",
      "876/3000 train_loss: 127.57978820800781 test_loss:177.7769775390625\n",
      "877/3000 train_loss: 125.89356231689453 test_loss:181.12368774414062\n",
      "878/3000 train_loss: 133.77728271484375 test_loss:179.6034393310547\n",
      "879/3000 train_loss: 132.0873565673828 test_loss:176.05130004882812\n",
      "880/3000 train_loss: 128.5821990966797 test_loss:187.08517456054688\n",
      "881/3000 train_loss: 135.57652282714844 test_loss:180.87168884277344\n",
      "882/3000 train_loss: 125.68804931640625 test_loss:176.8300018310547\n",
      "883/3000 train_loss: 126.21810150146484 test_loss:177.66664123535156\n",
      "884/3000 train_loss: 116.84980010986328 test_loss:178.12107849121094\n",
      "885/3000 train_loss: 130.43563842773438 test_loss:179.34042358398438\n",
      "886/3000 train_loss: 125.33613586425781 test_loss:182.27047729492188\n",
      "887/3000 train_loss: 131.1979217529297 test_loss:183.13430786132812\n",
      "888/3000 train_loss: 134.49502563476562 test_loss:186.42010498046875\n",
      "889/3000 train_loss: 135.9661865234375 test_loss:179.52108764648438\n",
      "890/3000 train_loss: 124.67660522460938 test_loss:183.07664489746094\n",
      "891/3000 train_loss: 131.2154541015625 test_loss:177.49508666992188\n",
      "892/3000 train_loss: 132.9014434814453 test_loss:189.70591735839844\n",
      "893/3000 train_loss: 140.31309509277344 test_loss:182.66189575195312\n",
      "894/3000 train_loss: 131.25680541992188 test_loss:180.66836547851562\n",
      "895/3000 train_loss: 150.14715576171875 test_loss:172.11093139648438\n",
      "896/3000 train_loss: 122.362548828125 test_loss:179.394287109375\n",
      "897/3000 train_loss: 127.43769836425781 test_loss:181.25933837890625\n",
      "898/3000 train_loss: 111.26101684570312 test_loss:176.88656616210938\n",
      "899/3000 train_loss: 110.9322280883789 test_loss:176.82032775878906\n",
      "900/3000 train_loss: 140.9682159423828 test_loss:181.46156311035156\n",
      "901/3000 train_loss: 114.15454864501953 test_loss:173.6519775390625\n",
      "902/3000 train_loss: 108.8357925415039 test_loss:173.08782958984375\n",
      "903/3000 train_loss: 122.84938049316406 test_loss:169.164306640625\n",
      "904/3000 train_loss: 114.96568298339844 test_loss:172.92611694335938\n",
      "905/3000 train_loss: 110.5324478149414 test_loss:170.23036193847656\n",
      "906/3000 train_loss: 112.81230163574219 test_loss:174.62965393066406\n",
      "907/3000 train_loss: 120.09295654296875 test_loss:175.03097534179688\n",
      "908/3000 train_loss: 110.74149322509766 test_loss:179.86618041992188\n",
      "909/3000 train_loss: 136.189453125 test_loss:176.068115234375\n",
      "910/3000 train_loss: 118.12786102294922 test_loss:185.6880645751953\n",
      "911/3000 train_loss: 134.98526000976562 test_loss:170.35812377929688\n",
      "912/3000 train_loss: 121.12294006347656 test_loss:168.9546356201172\n",
      "913/3000 train_loss: 128.4595184326172 test_loss:173.26954650878906\n",
      "914/3000 train_loss: 126.65288543701172 test_loss:169.39718627929688\n",
      "915/3000 train_loss: 112.39749908447266 test_loss:169.5401153564453\n",
      "916/3000 train_loss: 107.65182495117188 test_loss:178.9378662109375\n",
      "917/3000 train_loss: 108.95164489746094 test_loss:174.49237060546875\n",
      "918/3000 train_loss: 134.7196502685547 test_loss:172.56552124023438\n",
      "919/3000 train_loss: 114.859375 test_loss:171.31509399414062\n",
      "920/3000 train_loss: 116.78091430664062 test_loss:178.72201538085938\n",
      "921/3000 train_loss: 106.39791107177734 test_loss:174.97918701171875\n",
      "922/3000 train_loss: 123.8914794921875 test_loss:172.69488525390625\n",
      "923/3000 train_loss: 130.831298828125 test_loss:173.5990447998047\n",
      "924/3000 train_loss: 114.39228057861328 test_loss:179.63673400878906\n",
      "925/3000 train_loss: 112.76332092285156 test_loss:171.9530487060547\n",
      "926/3000 train_loss: 133.07696533203125 test_loss:167.5703582763672\n",
      "927/3000 train_loss: 117.12655639648438 test_loss:170.01815795898438\n",
      "928/3000 train_loss: 113.92742156982422 test_loss:170.2340545654297\n",
      "929/3000 train_loss: 114.95187377929688 test_loss:175.32611083984375\n",
      "930/3000 train_loss: 118.12568664550781 test_loss:169.69552612304688\n",
      "931/3000 train_loss: 112.65288543701172 test_loss:171.43045043945312\n",
      "932/3000 train_loss: 113.41722106933594 test_loss:172.89173889160156\n",
      "933/3000 train_loss: 110.74447631835938 test_loss:166.02540588378906\n",
      "934/3000 train_loss: 123.09588623046875 test_loss:168.25582885742188\n",
      "935/3000 train_loss: 116.9407958984375 test_loss:171.0950469970703\n",
      "936/3000 train_loss: 123.16736602783203 test_loss:172.22531127929688\n",
      "937/3000 train_loss: 116.5521011352539 test_loss:170.8118896484375\n",
      "938/3000 train_loss: 116.38438415527344 test_loss:169.1770782470703\n",
      "939/3000 train_loss: 115.05335998535156 test_loss:172.01849365234375\n",
      "940/3000 train_loss: 119.35330963134766 test_loss:168.45822143554688\n",
      "941/3000 train_loss: 126.60733795166016 test_loss:169.40530395507812\n",
      "942/3000 train_loss: 109.93915557861328 test_loss:167.57447814941406\n",
      "943/3000 train_loss: 114.78617095947266 test_loss:167.19308471679688\n",
      "944/3000 train_loss: 121.46009063720703 test_loss:167.7777557373047\n",
      "945/3000 train_loss: 111.39232635498047 test_loss:169.90530395507812\n",
      "946/3000 train_loss: 111.38273620605469 test_loss:171.2635498046875\n",
      "947/3000 train_loss: 112.41556549072266 test_loss:171.578125\n",
      "948/3000 train_loss: 96.44720458984375 test_loss:164.40386962890625\n",
      "949/3000 train_loss: 120.68824005126953 test_loss:165.7830810546875\n",
      "950/3000 train_loss: 142.25933837890625 test_loss:174.59844970703125\n",
      "951/3000 train_loss: 104.02388000488281 test_loss:173.81130981445312\n",
      "952/3000 train_loss: 108.4518051147461 test_loss:161.67022705078125\n",
      "953/3000 train_loss: 113.07392883300781 test_loss:163.4969024658203\n",
      "954/3000 train_loss: 111.34516143798828 test_loss:161.89369201660156\n",
      "955/3000 train_loss: 113.47138977050781 test_loss:175.76025390625\n",
      "956/3000 train_loss: 113.20638275146484 test_loss:167.50030517578125\n",
      "957/3000 train_loss: 114.47601318359375 test_loss:163.67884826660156\n",
      "958/3000 train_loss: 111.47068786621094 test_loss:165.27798461914062\n",
      "959/3000 train_loss: 124.06901550292969 test_loss:164.1358184814453\n",
      "960/3000 train_loss: 129.96548461914062 test_loss:165.6946258544922\n",
      "961/3000 train_loss: 112.86054992675781 test_loss:162.98435974121094\n",
      "962/3000 train_loss: 106.09188079833984 test_loss:164.10023498535156\n",
      "963/3000 train_loss: 120.61902618408203 test_loss:160.20404052734375\n",
      "964/3000 train_loss: 106.29906463623047 test_loss:160.6663818359375\n",
      "965/3000 train_loss: 119.30081939697266 test_loss:164.5863037109375\n",
      "966/3000 train_loss: 107.93293762207031 test_loss:164.7775115966797\n",
      "967/3000 train_loss: 119.80233764648438 test_loss:167.62889099121094\n",
      "968/3000 train_loss: 122.95063781738281 test_loss:166.60008239746094\n",
      "969/3000 train_loss: 108.22884368896484 test_loss:160.84725952148438\n",
      "970/3000 train_loss: 124.13386535644531 test_loss:164.72894287109375\n",
      "971/3000 train_loss: 108.8799819946289 test_loss:158.60958862304688\n",
      "972/3000 train_loss: 120.50009155273438 test_loss:163.126220703125\n",
      "973/3000 train_loss: 108.93248748779297 test_loss:159.04042053222656\n",
      "974/3000 train_loss: 117.35295867919922 test_loss:161.44332885742188\n",
      "975/3000 train_loss: 102.71179962158203 test_loss:158.23927307128906\n",
      "976/3000 train_loss: 104.17780303955078 test_loss:162.46298217773438\n",
      "977/3000 train_loss: 119.13570404052734 test_loss:158.70643615722656\n",
      "978/3000 train_loss: 105.0506591796875 test_loss:157.0526580810547\n",
      "979/3000 train_loss: 104.62549591064453 test_loss:156.41351318359375\n",
      "980/3000 train_loss: 104.72465515136719 test_loss:155.80691528320312\n",
      "981/3000 train_loss: 106.29000091552734 test_loss:160.5367431640625\n",
      "982/3000 train_loss: 124.9813003540039 test_loss:159.9418487548828\n",
      "983/3000 train_loss: 116.26602172851562 test_loss:163.58457946777344\n",
      "984/3000 train_loss: 105.40081787109375 test_loss:156.2855224609375\n",
      "985/3000 train_loss: 115.77666473388672 test_loss:164.9911651611328\n",
      "986/3000 train_loss: 117.57368469238281 test_loss:163.56842041015625\n",
      "987/3000 train_loss: 116.2139663696289 test_loss:156.3846435546875\n",
      "988/3000 train_loss: 106.34567260742188 test_loss:159.8452606201172\n",
      "989/3000 train_loss: 122.81077575683594 test_loss:158.1036376953125\n",
      "990/3000 train_loss: 98.70555877685547 test_loss:162.42282104492188\n",
      "991/3000 train_loss: 98.1264877319336 test_loss:159.3223114013672\n",
      "992/3000 train_loss: 103.27849578857422 test_loss:154.86984252929688\n",
      "993/3000 train_loss: 106.30239868164062 test_loss:156.80355834960938\n",
      "994/3000 train_loss: 104.47700500488281 test_loss:154.27418518066406\n",
      "995/3000 train_loss: 106.6448974609375 test_loss:154.9913330078125\n",
      "996/3000 train_loss: 99.99304962158203 test_loss:154.33236694335938\n",
      "997/3000 train_loss: 98.99016571044922 test_loss:155.6710662841797\n",
      "998/3000 train_loss: 104.33644104003906 test_loss:157.88206481933594\n",
      "999/3000 train_loss: 125.06877899169922 test_loss:154.92279052734375\n",
      "1000/3000 train_loss: 116.58633422851562 test_loss:157.35423278808594\n",
      "1001/3000 train_loss: 123.15751647949219 test_loss:159.5706787109375\n",
      "1002/3000 train_loss: 121.47988891601562 test_loss:155.604736328125\n",
      "1003/3000 train_loss: 105.66867065429688 test_loss:158.15682983398438\n",
      "1004/3000 train_loss: 111.15692138671875 test_loss:157.9066162109375\n",
      "1005/3000 train_loss: 93.23434448242188 test_loss:153.38525390625\n",
      "1006/3000 train_loss: 108.90988159179688 test_loss:154.9132843017578\n",
      "1007/3000 train_loss: 106.32542419433594 test_loss:154.5265655517578\n",
      "1008/3000 train_loss: 105.41271209716797 test_loss:155.7891082763672\n",
      "1009/3000 train_loss: 101.2636489868164 test_loss:157.8938751220703\n",
      "1010/3000 train_loss: 125.59032440185547 test_loss:151.24533081054688\n",
      "1011/3000 train_loss: 102.66459655761719 test_loss:160.49925231933594\n",
      "1012/3000 train_loss: 112.35914611816406 test_loss:152.84701538085938\n",
      "1013/3000 train_loss: 104.2061538696289 test_loss:152.8175048828125\n",
      "1014/3000 train_loss: 103.45014953613281 test_loss:150.23562622070312\n",
      "1015/3000 train_loss: 108.84274291992188 test_loss:158.15318298339844\n",
      "1016/3000 train_loss: 146.29360961914062 test_loss:159.6750946044922\n",
      "1017/3000 train_loss: 109.16677856445312 test_loss:155.38099670410156\n",
      "1018/3000 train_loss: 107.74211883544922 test_loss:152.0358123779297\n",
      "1019/3000 train_loss: 103.6588134765625 test_loss:151.80038452148438\n",
      "1020/3000 train_loss: 95.4832534790039 test_loss:155.36618041992188\n",
      "1021/3000 train_loss: 106.53172302246094 test_loss:157.28390502929688\n",
      "1022/3000 train_loss: 91.62024688720703 test_loss:148.3009033203125\n",
      "1023/3000 train_loss: 104.53617858886719 test_loss:150.49171447753906\n",
      "1024/3000 train_loss: 113.69318389892578 test_loss:151.588134765625\n",
      "1025/3000 train_loss: 99.83515167236328 test_loss:155.576904296875\n",
      "1026/3000 train_loss: 98.9599838256836 test_loss:151.89613342285156\n",
      "1027/3000 train_loss: 100.02674102783203 test_loss:150.67831420898438\n",
      "1028/3000 train_loss: 93.23690032958984 test_loss:157.7744598388672\n",
      "1029/3000 train_loss: 98.95638275146484 test_loss:148.69552612304688\n",
      "1030/3000 train_loss: 96.67171478271484 test_loss:148.92808532714844\n",
      "1031/3000 train_loss: 105.42455291748047 test_loss:154.84637451171875\n",
      "1032/3000 train_loss: 98.92728424072266 test_loss:151.8074493408203\n",
      "1033/3000 train_loss: 119.68455505371094 test_loss:155.30535888671875\n",
      "1034/3000 train_loss: 117.12552642822266 test_loss:147.9102020263672\n",
      "1035/3000 train_loss: 92.65668487548828 test_loss:151.3375701904297\n",
      "1036/3000 train_loss: 100.77246856689453 test_loss:151.73260498046875\n",
      "1037/3000 train_loss: 99.77576446533203 test_loss:147.64706420898438\n",
      "1038/3000 train_loss: 91.14262390136719 test_loss:150.02719116210938\n",
      "1039/3000 train_loss: 96.5586166381836 test_loss:146.88394165039062\n",
      "1040/3000 train_loss: 92.66703033447266 test_loss:146.00840759277344\n",
      "1041/3000 train_loss: 102.92809295654297 test_loss:156.3394775390625\n",
      "1042/3000 train_loss: 109.68216705322266 test_loss:144.5698699951172\n",
      "1043/3000 train_loss: 100.10164642333984 test_loss:152.71922302246094\n",
      "1044/3000 train_loss: 91.1814193725586 test_loss:147.36683654785156\n",
      "1045/3000 train_loss: 106.09120178222656 test_loss:146.3863067626953\n",
      "1046/3000 train_loss: 107.79731750488281 test_loss:148.76217651367188\n",
      "1047/3000 train_loss: 103.23301696777344 test_loss:147.4724884033203\n",
      "1048/3000 train_loss: 102.35680389404297 test_loss:144.93698120117188\n",
      "1049/3000 train_loss: 104.2105941772461 test_loss:147.13449096679688\n",
      "1050/3000 train_loss: 113.5411376953125 test_loss:144.15859985351562\n",
      "1051/3000 train_loss: 119.64250183105469 test_loss:147.08718872070312\n",
      "1052/3000 train_loss: 105.18148040771484 test_loss:142.26943969726562\n",
      "1053/3000 train_loss: 95.4332504272461 test_loss:140.72271728515625\n",
      "1054/3000 train_loss: 100.35786437988281 test_loss:143.51678466796875\n",
      "1055/3000 train_loss: 108.71759033203125 test_loss:146.31040954589844\n",
      "1056/3000 train_loss: 107.45475006103516 test_loss:141.7928466796875\n",
      "1057/3000 train_loss: 96.16614532470703 test_loss:145.84725952148438\n",
      "1058/3000 train_loss: 86.39341735839844 test_loss:143.22488403320312\n",
      "1059/3000 train_loss: 96.43228149414062 test_loss:145.3060302734375\n",
      "1060/3000 train_loss: 97.66732025146484 test_loss:143.36585998535156\n",
      "1061/3000 train_loss: 119.65068817138672 test_loss:146.13739013671875\n",
      "1062/3000 train_loss: 89.074462890625 test_loss:140.67929077148438\n",
      "1063/3000 train_loss: 97.8516616821289 test_loss:139.093505859375\n",
      "1064/3000 train_loss: 101.36931610107422 test_loss:161.54534912109375\n",
      "1065/3000 train_loss: 97.74421691894531 test_loss:141.0801544189453\n",
      "1066/3000 train_loss: 100.2071762084961 test_loss:138.8980255126953\n",
      "1067/3000 train_loss: 108.21737670898438 test_loss:144.45587158203125\n",
      "1068/3000 train_loss: 99.83124542236328 test_loss:140.7340087890625\n",
      "1069/3000 train_loss: 91.82075500488281 test_loss:142.39291381835938\n",
      "1070/3000 train_loss: 87.92073822021484 test_loss:145.77365112304688\n",
      "1071/3000 train_loss: 95.47586059570312 test_loss:138.61814880371094\n",
      "1072/3000 train_loss: 131.16897583007812 test_loss:144.97366333007812\n",
      "1073/3000 train_loss: 100.56135559082031 test_loss:141.23159790039062\n",
      "1074/3000 train_loss: 99.96376037597656 test_loss:157.50108337402344\n",
      "1075/3000 train_loss: 113.90055847167969 test_loss:142.78134155273438\n",
      "1076/3000 train_loss: 92.96623229980469 test_loss:145.156005859375\n",
      "1077/3000 train_loss: 98.15569305419922 test_loss:139.76336669921875\n",
      "1078/3000 train_loss: 104.38738250732422 test_loss:141.47567749023438\n",
      "1079/3000 train_loss: 98.13157653808594 test_loss:161.96707153320312\n",
      "1080/3000 train_loss: 123.85630798339844 test_loss:142.01132202148438\n",
      "1081/3000 train_loss: 89.92352294921875 test_loss:147.6667022705078\n",
      "1082/3000 train_loss: 100.42557525634766 test_loss:142.03611755371094\n",
      "1083/3000 train_loss: 96.94566345214844 test_loss:145.99887084960938\n",
      "1084/3000 train_loss: 107.94292449951172 test_loss:147.82762145996094\n",
      "1085/3000 train_loss: 85.48197937011719 test_loss:137.12294006347656\n",
      "1086/3000 train_loss: 95.43633270263672 test_loss:141.8617706298828\n",
      "1087/3000 train_loss: 94.51251983642578 test_loss:139.0874481201172\n",
      "1088/3000 train_loss: 90.78034973144531 test_loss:136.29566955566406\n",
      "1089/3000 train_loss: 92.070068359375 test_loss:133.7640380859375\n",
      "1090/3000 train_loss: 101.03240966796875 test_loss:135.3140411376953\n",
      "1091/3000 train_loss: 93.36273956298828 test_loss:138.35365295410156\n",
      "1092/3000 train_loss: 88.79138946533203 test_loss:143.81312561035156\n",
      "1093/3000 train_loss: 91.11708068847656 test_loss:138.66078186035156\n",
      "1094/3000 train_loss: 93.5723876953125 test_loss:140.01148986816406\n",
      "1095/3000 train_loss: 91.84712219238281 test_loss:135.69671630859375\n",
      "1096/3000 train_loss: 92.57066345214844 test_loss:135.95785522460938\n",
      "1097/3000 train_loss: 94.51541137695312 test_loss:141.93528747558594\n",
      "1098/3000 train_loss: 86.55104064941406 test_loss:133.65863037109375\n",
      "1099/3000 train_loss: 82.0180435180664 test_loss:134.38238525390625\n",
      "1100/3000 train_loss: 90.07611083984375 test_loss:139.22579956054688\n",
      "1101/3000 train_loss: 95.02629089355469 test_loss:134.36947631835938\n",
      "1102/3000 train_loss: 108.06159973144531 test_loss:140.71702575683594\n",
      "1103/3000 train_loss: 90.41178131103516 test_loss:141.58984375\n",
      "1104/3000 train_loss: 99.15057373046875 test_loss:137.57650756835938\n",
      "1105/3000 train_loss: 87.45454406738281 test_loss:136.60104370117188\n",
      "1106/3000 train_loss: 121.289794921875 test_loss:137.82839965820312\n",
      "1107/3000 train_loss: 88.34693908691406 test_loss:143.614990234375\n",
      "1108/3000 train_loss: 91.912353515625 test_loss:138.87806701660156\n",
      "1109/3000 train_loss: 95.4382553100586 test_loss:138.8305206298828\n",
      "1110/3000 train_loss: 83.26847076416016 test_loss:137.3421630859375\n",
      "1111/3000 train_loss: 92.83274841308594 test_loss:132.83108520507812\n",
      "1112/3000 train_loss: 94.24392700195312 test_loss:137.3719482421875\n",
      "1113/3000 train_loss: 88.56238555908203 test_loss:136.08065795898438\n",
      "1114/3000 train_loss: 86.16254425048828 test_loss:133.3916015625\n",
      "1115/3000 train_loss: 78.71014404296875 test_loss:136.81214904785156\n",
      "1116/3000 train_loss: 94.45606994628906 test_loss:139.5524444580078\n",
      "1117/3000 train_loss: 84.22891998291016 test_loss:135.70005798339844\n",
      "1118/3000 train_loss: 84.60724639892578 test_loss:133.08358764648438\n",
      "1119/3000 train_loss: 85.92694854736328 test_loss:136.52716064453125\n",
      "1120/3000 train_loss: 90.80159759521484 test_loss:133.3132781982422\n",
      "1121/3000 train_loss: 80.0854263305664 test_loss:132.99696350097656\n",
      "1122/3000 train_loss: 89.20687103271484 test_loss:136.2788543701172\n",
      "1123/3000 train_loss: 103.49020385742188 test_loss:130.02780151367188\n",
      "1124/3000 train_loss: 90.04129028320312 test_loss:136.68441772460938\n",
      "1125/3000 train_loss: 89.52069091796875 test_loss:138.28924560546875\n",
      "1126/3000 train_loss: 103.17584228515625 test_loss:129.87950134277344\n",
      "1127/3000 train_loss: 91.78642272949219 test_loss:132.5144805908203\n",
      "1128/3000 train_loss: 85.52397155761719 test_loss:132.051513671875\n",
      "1129/3000 train_loss: 95.2502670288086 test_loss:146.31480407714844\n",
      "1130/3000 train_loss: 106.51627349853516 test_loss:131.739501953125\n",
      "1131/3000 train_loss: 100.30268096923828 test_loss:137.73095703125\n",
      "1132/3000 train_loss: 94.49651336669922 test_loss:133.62570190429688\n",
      "1133/3000 train_loss: 78.75756072998047 test_loss:144.8629608154297\n",
      "1134/3000 train_loss: 93.94685363769531 test_loss:136.08120727539062\n",
      "1135/3000 train_loss: 92.43693542480469 test_loss:137.29640197753906\n",
      "1136/3000 train_loss: 94.9962387084961 test_loss:133.1118927001953\n",
      "1137/3000 train_loss: 78.57764434814453 test_loss:129.0799560546875\n",
      "1138/3000 train_loss: 79.1784439086914 test_loss:139.2018280029297\n",
      "1139/3000 train_loss: 91.62983703613281 test_loss:146.74728393554688\n",
      "1140/3000 train_loss: 92.83705139160156 test_loss:136.2197723388672\n",
      "1141/3000 train_loss: 92.0243911743164 test_loss:130.1769256591797\n",
      "1142/3000 train_loss: 98.87666320800781 test_loss:134.67715454101562\n",
      "1143/3000 train_loss: 98.88069915771484 test_loss:133.62403869628906\n",
      "1144/3000 train_loss: 83.22106170654297 test_loss:137.49395751953125\n",
      "1145/3000 train_loss: 78.5509033203125 test_loss:135.2472686767578\n",
      "1146/3000 train_loss: 80.24534606933594 test_loss:131.86839294433594\n",
      "1147/3000 train_loss: 90.36322784423828 test_loss:130.08663940429688\n",
      "1148/3000 train_loss: 94.02033996582031 test_loss:131.9346466064453\n",
      "1149/3000 train_loss: 112.97976684570312 test_loss:138.5817108154297\n",
      "1150/3000 train_loss: 84.62805938720703 test_loss:131.90553283691406\n",
      "1151/3000 train_loss: 102.60208892822266 test_loss:128.01312255859375\n",
      "1152/3000 train_loss: 91.06800079345703 test_loss:133.91409301757812\n",
      "1153/3000 train_loss: 91.43010711669922 test_loss:131.08323669433594\n",
      "1154/3000 train_loss: 87.82247161865234 test_loss:132.16293334960938\n",
      "1155/3000 train_loss: 79.84539031982422 test_loss:127.33185577392578\n",
      "1156/3000 train_loss: 79.60395812988281 test_loss:129.36424255371094\n",
      "1157/3000 train_loss: 88.77792358398438 test_loss:144.39407348632812\n",
      "1158/3000 train_loss: 90.1945571899414 test_loss:129.06121826171875\n",
      "1159/3000 train_loss: 98.35401153564453 test_loss:133.8228759765625\n",
      "1160/3000 train_loss: 93.83843994140625 test_loss:130.64944458007812\n",
      "1161/3000 train_loss: 84.3344955444336 test_loss:128.59832763671875\n",
      "1162/3000 train_loss: 84.09671783447266 test_loss:133.66941833496094\n",
      "1163/3000 train_loss: 92.87401580810547 test_loss:130.3966827392578\n",
      "1164/3000 train_loss: 94.55195617675781 test_loss:138.78594970703125\n",
      "1165/3000 train_loss: 99.22572326660156 test_loss:132.01187133789062\n",
      "1166/3000 train_loss: 89.5779800415039 test_loss:128.26678466796875\n",
      "1167/3000 train_loss: 85.86030578613281 test_loss:131.23658752441406\n",
      "1168/3000 train_loss: 82.22712707519531 test_loss:128.53085327148438\n",
      "1169/3000 train_loss: 72.76524353027344 test_loss:129.42852783203125\n",
      "1170/3000 train_loss: 85.73078155517578 test_loss:127.57002258300781\n",
      "1171/3000 train_loss: 78.44482421875 test_loss:130.4845733642578\n",
      "1172/3000 train_loss: 88.13053131103516 test_loss:128.0498046875\n",
      "1173/3000 train_loss: 93.56146240234375 test_loss:142.2827606201172\n",
      "1174/3000 train_loss: 80.94691467285156 test_loss:128.80494689941406\n",
      "1175/3000 train_loss: 81.1590805053711 test_loss:131.94561767578125\n",
      "1176/3000 train_loss: 83.75809478759766 test_loss:137.3253936767578\n",
      "1177/3000 train_loss: 78.8467025756836 test_loss:129.0030059814453\n",
      "1178/3000 train_loss: 81.47492218017578 test_loss:130.6855926513672\n",
      "1179/3000 train_loss: 87.36732482910156 test_loss:133.96051025390625\n",
      "1180/3000 train_loss: 101.86630249023438 test_loss:139.0971221923828\n",
      "1181/3000 train_loss: 74.11154174804688 test_loss:127.4338607788086\n",
      "1182/3000 train_loss: 95.55160522460938 test_loss:137.22779846191406\n",
      "1183/3000 train_loss: 82.9609375 test_loss:134.1458740234375\n",
      "1184/3000 train_loss: 91.4122314453125 test_loss:127.68364715576172\n",
      "1185/3000 train_loss: 84.91185760498047 test_loss:133.16738891601562\n",
      "1186/3000 train_loss: 105.4588394165039 test_loss:138.204345703125\n",
      "1187/3000 train_loss: 83.943115234375 test_loss:136.45220947265625\n",
      "1188/3000 train_loss: 76.31884765625 test_loss:128.47779846191406\n",
      "1189/3000 train_loss: 77.18716430664062 test_loss:128.30575561523438\n",
      "1190/3000 train_loss: 91.76801300048828 test_loss:134.34547424316406\n",
      "1191/3000 train_loss: 89.24456024169922 test_loss:131.04563903808594\n",
      "1192/3000 train_loss: 87.5836181640625 test_loss:128.25314331054688\n",
      "1193/3000 train_loss: 72.42378997802734 test_loss:130.6760711669922\n",
      "1194/3000 train_loss: 79.31580352783203 test_loss:128.1158447265625\n",
      "1195/3000 train_loss: 93.13818359375 test_loss:135.37384033203125\n",
      "1196/3000 train_loss: 88.91075134277344 test_loss:132.51895141601562\n",
      "1197/3000 train_loss: 82.88545227050781 test_loss:133.31317138671875\n",
      "1198/3000 train_loss: 92.34626770019531 test_loss:135.95367431640625\n",
      "1199/3000 train_loss: 98.72112274169922 test_loss:133.42486572265625\n",
      "1200/3000 train_loss: 81.41242980957031 test_loss:131.8502960205078\n",
      "1201/3000 train_loss: 91.51689910888672 test_loss:136.56178283691406\n",
      "1202/3000 train_loss: 85.31683349609375 test_loss:130.70130920410156\n",
      "1203/3000 train_loss: 89.70842742919922 test_loss:129.55099487304688\n",
      "1204/3000 train_loss: 77.81242370605469 test_loss:132.35060119628906\n",
      "1205/3000 train_loss: 93.34134674072266 test_loss:141.93206787109375\n",
      "1206/3000 train_loss: 99.51604461669922 test_loss:124.72291564941406\n",
      "1207/3000 train_loss: 83.25271606445312 test_loss:126.95304870605469\n",
      "1208/3000 train_loss: 87.48844909667969 test_loss:130.88259887695312\n",
      "1209/3000 train_loss: 88.04035949707031 test_loss:130.17196655273438\n",
      "1210/3000 train_loss: 81.0664291381836 test_loss:126.21280670166016\n",
      "1211/3000 train_loss: 84.12283325195312 test_loss:127.39779663085938\n",
      "1212/3000 train_loss: 94.90853881835938 test_loss:124.48042297363281\n",
      "1213/3000 train_loss: 86.98779296875 test_loss:123.50717163085938\n",
      "1214/3000 train_loss: 73.96112823486328 test_loss:120.31871795654297\n",
      "1215/3000 train_loss: 80.44595336914062 test_loss:129.0330810546875\n",
      "1216/3000 train_loss: 86.14043426513672 test_loss:127.55255889892578\n",
      "1217/3000 train_loss: 90.72976684570312 test_loss:124.26972961425781\n",
      "1218/3000 train_loss: 77.69508361816406 test_loss:123.29354858398438\n",
      "1219/3000 train_loss: 80.62226867675781 test_loss:124.73550415039062\n",
      "1220/3000 train_loss: 102.38085174560547 test_loss:120.39949035644531\n",
      "1221/3000 train_loss: 79.4316635131836 test_loss:120.40312957763672\n",
      "1222/3000 train_loss: 80.35248565673828 test_loss:123.0014877319336\n",
      "1223/3000 train_loss: 87.18350219726562 test_loss:122.82915496826172\n",
      "1224/3000 train_loss: 82.14221954345703 test_loss:120.50367736816406\n",
      "1225/3000 train_loss: 77.3581314086914 test_loss:123.45069885253906\n",
      "1226/3000 train_loss: 87.70744323730469 test_loss:131.4268035888672\n",
      "1227/3000 train_loss: 88.94300842285156 test_loss:121.67835998535156\n",
      "1228/3000 train_loss: 89.85038757324219 test_loss:125.961181640625\n",
      "1229/3000 train_loss: 73.66240692138672 test_loss:120.13536071777344\n",
      "1230/3000 train_loss: 69.78887939453125 test_loss:117.7845458984375\n",
      "1231/3000 train_loss: 84.55791473388672 test_loss:133.75413513183594\n",
      "1232/3000 train_loss: 91.84461975097656 test_loss:123.92037963867188\n",
      "1233/3000 train_loss: 73.44020080566406 test_loss:128.6306610107422\n",
      "1234/3000 train_loss: 100.35430908203125 test_loss:126.37205505371094\n",
      "1235/3000 train_loss: 74.41552734375 test_loss:130.02145385742188\n",
      "1236/3000 train_loss: 95.40975952148438 test_loss:133.17591857910156\n",
      "1237/3000 train_loss: 88.61271667480469 test_loss:124.73265075683594\n",
      "1238/3000 train_loss: 90.66973876953125 test_loss:126.44552612304688\n",
      "1239/3000 train_loss: 95.6690673828125 test_loss:126.00884246826172\n",
      "1240/3000 train_loss: 77.58309936523438 test_loss:126.67947387695312\n",
      "1241/3000 train_loss: 77.99687957763672 test_loss:126.07334899902344\n",
      "1242/3000 train_loss: 79.72225189208984 test_loss:119.42666625976562\n",
      "1243/3000 train_loss: 74.47273254394531 test_loss:127.92787170410156\n",
      "1244/3000 train_loss: 76.92050170898438 test_loss:125.10261535644531\n",
      "1245/3000 train_loss: 85.02749633789062 test_loss:123.48904418945312\n",
      "1246/3000 train_loss: 99.82678985595703 test_loss:130.5825958251953\n",
      "1247/3000 train_loss: 81.99069213867188 test_loss:125.23311614990234\n",
      "1248/3000 train_loss: 74.00433349609375 test_loss:121.97018432617188\n",
      "1249/3000 train_loss: 85.79312896728516 test_loss:124.78073120117188\n",
      "1250/3000 train_loss: 84.67363739013672 test_loss:127.41905212402344\n",
      "1251/3000 train_loss: 74.3923568725586 test_loss:129.35235595703125\n",
      "1252/3000 train_loss: 86.5781478881836 test_loss:129.29161071777344\n",
      "1253/3000 train_loss: 79.8575210571289 test_loss:129.95162963867188\n",
      "1254/3000 train_loss: 83.55851745605469 test_loss:126.56219482421875\n",
      "1255/3000 train_loss: 83.36212921142578 test_loss:124.50346374511719\n",
      "1256/3000 train_loss: 83.81490325927734 test_loss:133.17039489746094\n",
      "1257/3000 train_loss: 79.08279418945312 test_loss:125.09774780273438\n",
      "1258/3000 train_loss: 95.78121948242188 test_loss:124.95285034179688\n",
      "1259/3000 train_loss: 81.87939453125 test_loss:121.000732421875\n",
      "1260/3000 train_loss: 75.47697448730469 test_loss:120.6724853515625\n",
      "1261/3000 train_loss: 79.9931869506836 test_loss:123.18814086914062\n",
      "1262/3000 train_loss: 88.09432983398438 test_loss:121.89035034179688\n",
      "1263/3000 train_loss: 70.69784545898438 test_loss:124.02592468261719\n",
      "1264/3000 train_loss: 76.60124206542969 test_loss:119.68731689453125\n",
      "1265/3000 train_loss: 80.07278442382812 test_loss:120.22213745117188\n",
      "1266/3000 train_loss: 84.34832000732422 test_loss:125.6122055053711\n",
      "1267/3000 train_loss: 85.96966552734375 test_loss:118.520751953125\n",
      "1268/3000 train_loss: 73.14713287353516 test_loss:119.42694091796875\n",
      "1269/3000 train_loss: 82.29168701171875 test_loss:122.64557647705078\n",
      "1270/3000 train_loss: 75.82379913330078 test_loss:121.26624298095703\n",
      "1271/3000 train_loss: 74.47821044921875 test_loss:125.94544219970703\n",
      "1272/3000 train_loss: 93.36673736572266 test_loss:132.11016845703125\n",
      "1273/3000 train_loss: 86.75157165527344 test_loss:132.58526611328125\n",
      "1274/3000 train_loss: 95.7616195678711 test_loss:124.57745361328125\n",
      "1275/3000 train_loss: 77.9515151977539 test_loss:125.05736541748047\n",
      "1276/3000 train_loss: 71.22160339355469 test_loss:123.96540832519531\n",
      "1277/3000 train_loss: 68.84122467041016 test_loss:119.52401733398438\n",
      "1278/3000 train_loss: 69.56805419921875 test_loss:117.38687133789062\n",
      "1279/3000 train_loss: 77.59288024902344 test_loss:119.98886108398438\n",
      "1280/3000 train_loss: 88.22184753417969 test_loss:124.82478332519531\n",
      "1281/3000 train_loss: 86.81073760986328 test_loss:120.96926879882812\n",
      "1282/3000 train_loss: 74.0795669555664 test_loss:128.35472106933594\n",
      "1283/3000 train_loss: 88.47257995605469 test_loss:117.97000122070312\n",
      "1284/3000 train_loss: 88.35296630859375 test_loss:117.98853302001953\n",
      "1285/3000 train_loss: 77.23663330078125 test_loss:119.32521057128906\n",
      "1286/3000 train_loss: 87.18923950195312 test_loss:124.99909973144531\n",
      "1287/3000 train_loss: 79.41431427001953 test_loss:123.60400390625\n",
      "1288/3000 train_loss: 73.37213897705078 test_loss:118.78457641601562\n",
      "1289/3000 train_loss: 91.69136810302734 test_loss:123.3424301147461\n",
      "1290/3000 train_loss: 87.33814239501953 test_loss:119.3603515625\n",
      "1291/3000 train_loss: 78.97552490234375 test_loss:120.07676696777344\n",
      "1292/3000 train_loss: 72.39861297607422 test_loss:115.69815826416016\n",
      "1293/3000 train_loss: 79.25471496582031 test_loss:118.42012023925781\n",
      "1294/3000 train_loss: 69.46890258789062 test_loss:115.82704162597656\n",
      "1295/3000 train_loss: 82.8280258178711 test_loss:115.73002624511719\n",
      "1296/3000 train_loss: 72.13915252685547 test_loss:118.09278869628906\n",
      "1297/3000 train_loss: 80.09095001220703 test_loss:115.12406921386719\n",
      "1298/3000 train_loss: 93.55256652832031 test_loss:121.57290649414062\n",
      "1299/3000 train_loss: 76.48792266845703 test_loss:115.5678939819336\n",
      "1300/3000 train_loss: 74.46429443359375 test_loss:122.22713470458984\n",
      "1301/3000 train_loss: 81.98948669433594 test_loss:114.10195922851562\n",
      "1302/3000 train_loss: 69.0234603881836 test_loss:120.19891357421875\n",
      "1303/3000 train_loss: 71.29711151123047 test_loss:114.13931274414062\n",
      "1304/3000 train_loss: 82.51451110839844 test_loss:115.91104125976562\n",
      "1305/3000 train_loss: 76.1533203125 test_loss:116.65005493164062\n",
      "1306/3000 train_loss: 73.20589447021484 test_loss:111.89152526855469\n",
      "1307/3000 train_loss: 86.59233093261719 test_loss:115.746826171875\n",
      "1308/3000 train_loss: 85.22411346435547 test_loss:117.70294189453125\n",
      "1309/3000 train_loss: 78.22382354736328 test_loss:114.58308410644531\n",
      "1310/3000 train_loss: 67.7001724243164 test_loss:120.22016906738281\n",
      "1311/3000 train_loss: 79.90967559814453 test_loss:116.71574401855469\n",
      "1312/3000 train_loss: 75.48700714111328 test_loss:121.62236022949219\n",
      "1313/3000 train_loss: 75.9742660522461 test_loss:115.69598388671875\n",
      "1314/3000 train_loss: 71.7135009765625 test_loss:111.53421020507812\n",
      "1315/3000 train_loss: 79.86380767822266 test_loss:118.6448974609375\n",
      "1316/3000 train_loss: 74.98233032226562 test_loss:118.53599548339844\n",
      "1317/3000 train_loss: 85.97124481201172 test_loss:115.54783630371094\n",
      "1318/3000 train_loss: 77.90635681152344 test_loss:116.45399475097656\n",
      "1319/3000 train_loss: 78.48339080810547 test_loss:112.60073852539062\n",
      "1320/3000 train_loss: 77.67914581298828 test_loss:115.904052734375\n",
      "1321/3000 train_loss: 76.5250015258789 test_loss:119.85160064697266\n",
      "1322/3000 train_loss: 74.7286376953125 test_loss:114.74243927001953\n",
      "1323/3000 train_loss: 71.69596099853516 test_loss:126.82612609863281\n",
      "1324/3000 train_loss: 84.92130279541016 test_loss:125.40739440917969\n",
      "1325/3000 train_loss: 80.5949478149414 test_loss:117.27254486083984\n",
      "1326/3000 train_loss: 75.83972930908203 test_loss:116.06011962890625\n",
      "1327/3000 train_loss: 86.66683197021484 test_loss:116.68213653564453\n",
      "1328/3000 train_loss: 79.3786849975586 test_loss:117.80481719970703\n",
      "1329/3000 train_loss: 73.76268005371094 test_loss:119.02134704589844\n",
      "1330/3000 train_loss: 76.42584991455078 test_loss:117.83493041992188\n",
      "1331/3000 train_loss: 76.8317642211914 test_loss:115.81705474853516\n",
      "1332/3000 train_loss: 65.61547088623047 test_loss:111.67591857910156\n",
      "1333/3000 train_loss: 85.84336853027344 test_loss:114.43801879882812\n",
      "1334/3000 train_loss: 74.65311431884766 test_loss:112.56053161621094\n",
      "1335/3000 train_loss: 84.37528991699219 test_loss:117.74067687988281\n",
      "1336/3000 train_loss: 72.09405517578125 test_loss:116.48431396484375\n",
      "1337/3000 train_loss: 64.83238220214844 test_loss:113.90150451660156\n",
      "1338/3000 train_loss: 71.83590698242188 test_loss:111.33689880371094\n",
      "1339/3000 train_loss: 77.35081481933594 test_loss:115.13925170898438\n",
      "1340/3000 train_loss: 85.03633117675781 test_loss:119.80816650390625\n",
      "1341/3000 train_loss: 76.67149353027344 test_loss:113.90858459472656\n",
      "1342/3000 train_loss: 98.19689178466797 test_loss:136.6063232421875\n",
      "1343/3000 train_loss: 75.47232055664062 test_loss:124.33979034423828\n",
      "1344/3000 train_loss: 81.93704223632812 test_loss:115.94500732421875\n",
      "1345/3000 train_loss: 76.55768585205078 test_loss:115.40225219726562\n",
      "1346/3000 train_loss: 85.20049285888672 test_loss:116.84954833984375\n",
      "1347/3000 train_loss: 73.72473907470703 test_loss:118.39825439453125\n",
      "1348/3000 train_loss: 65.49897003173828 test_loss:115.22522735595703\n",
      "1349/3000 train_loss: 75.02214050292969 test_loss:116.99556732177734\n",
      "1350/3000 train_loss: 75.75926971435547 test_loss:118.58009338378906\n",
      "1351/3000 train_loss: 90.30906677246094 test_loss:111.62474060058594\n",
      "1352/3000 train_loss: 67.75521850585938 test_loss:114.63996887207031\n",
      "1353/3000 train_loss: 88.41375732421875 test_loss:118.91473388671875\n",
      "1354/3000 train_loss: 80.53247833251953 test_loss:128.41746520996094\n",
      "1355/3000 train_loss: 91.83808135986328 test_loss:127.90599822998047\n",
      "1356/3000 train_loss: 73.44417572021484 test_loss:118.88276672363281\n",
      "1357/3000 train_loss: 76.89337921142578 test_loss:117.91474914550781\n",
      "1358/3000 train_loss: 70.61988830566406 test_loss:121.61961364746094\n",
      "1359/3000 train_loss: 74.64646911621094 test_loss:121.15110778808594\n",
      "1360/3000 train_loss: 77.62049865722656 test_loss:116.59426879882812\n",
      "1361/3000 train_loss: 82.02062225341797 test_loss:114.30419921875\n",
      "1362/3000 train_loss: 67.02033233642578 test_loss:120.81585693359375\n",
      "1363/3000 train_loss: 66.24788665771484 test_loss:117.99838256835938\n",
      "1364/3000 train_loss: 75.92364501953125 test_loss:118.26508331298828\n",
      "1365/3000 train_loss: 78.19815063476562 test_loss:116.17414855957031\n",
      "1366/3000 train_loss: 74.49361419677734 test_loss:112.87604522705078\n",
      "1367/3000 train_loss: 72.85536193847656 test_loss:110.69813537597656\n",
      "1368/3000 train_loss: 69.59080505371094 test_loss:118.8626480102539\n",
      "1369/3000 train_loss: 75.83175659179688 test_loss:116.32411193847656\n",
      "1370/3000 train_loss: 74.06024169921875 test_loss:117.75581359863281\n",
      "1371/3000 train_loss: 82.70384216308594 test_loss:114.32527160644531\n",
      "1372/3000 train_loss: 92.79592895507812 test_loss:110.01971435546875\n",
      "1373/3000 train_loss: 82.94591522216797 test_loss:116.20722961425781\n",
      "1374/3000 train_loss: 75.09065246582031 test_loss:109.11865997314453\n",
      "1375/3000 train_loss: 73.21961975097656 test_loss:124.10527038574219\n",
      "1376/3000 train_loss: 103.4482421875 test_loss:114.147216796875\n",
      "1377/3000 train_loss: 80.05210876464844 test_loss:112.31315612792969\n",
      "1378/3000 train_loss: 93.29707336425781 test_loss:111.57620239257812\n",
      "1379/3000 train_loss: 78.68870544433594 test_loss:110.6792984008789\n",
      "1380/3000 train_loss: 66.50824737548828 test_loss:111.24222564697266\n",
      "1381/3000 train_loss: 73.78093719482422 test_loss:123.85913848876953\n",
      "1382/3000 train_loss: 69.8773193359375 test_loss:111.11161804199219\n",
      "1383/3000 train_loss: 77.46653747558594 test_loss:115.18199157714844\n",
      "1384/3000 train_loss: 73.32462310791016 test_loss:110.46333312988281\n",
      "1385/3000 train_loss: 77.33316802978516 test_loss:110.69452667236328\n",
      "1386/3000 train_loss: 74.12728118896484 test_loss:114.93467712402344\n",
      "1387/3000 train_loss: 68.38771057128906 test_loss:111.95348358154297\n",
      "1388/3000 train_loss: 72.79341125488281 test_loss:107.46739196777344\n",
      "1389/3000 train_loss: 68.39064025878906 test_loss:111.62394714355469\n",
      "1390/3000 train_loss: 68.90203857421875 test_loss:107.93912506103516\n",
      "1391/3000 train_loss: 80.84339904785156 test_loss:113.43179321289062\n",
      "1392/3000 train_loss: 73.97362518310547 test_loss:114.7534408569336\n",
      "1393/3000 train_loss: 68.5826416015625 test_loss:114.84163665771484\n",
      "1394/3000 train_loss: 66.84613037109375 test_loss:110.07706451416016\n",
      "1395/3000 train_loss: 65.91256713867188 test_loss:115.48125457763672\n",
      "1396/3000 train_loss: 76.89383697509766 test_loss:108.8793716430664\n",
      "1397/3000 train_loss: 74.93238067626953 test_loss:112.35399627685547\n",
      "1398/3000 train_loss: 70.84988403320312 test_loss:116.65243530273438\n",
      "1399/3000 train_loss: 77.20614624023438 test_loss:115.91984558105469\n",
      "1400/3000 train_loss: 68.68733978271484 test_loss:115.4966812133789\n",
      "1401/3000 train_loss: 78.03770446777344 test_loss:114.4505386352539\n",
      "1402/3000 train_loss: 67.78999328613281 test_loss:114.24917602539062\n",
      "1403/3000 train_loss: 72.63027954101562 test_loss:111.98934936523438\n",
      "1404/3000 train_loss: 69.40741729736328 test_loss:109.65339660644531\n",
      "1405/3000 train_loss: 72.303466796875 test_loss:122.29988098144531\n",
      "1406/3000 train_loss: 72.98472595214844 test_loss:116.62389373779297\n",
      "1407/3000 train_loss: 89.4896469116211 test_loss:110.2378158569336\n",
      "1408/3000 train_loss: 68.70085906982422 test_loss:114.1580810546875\n",
      "1409/3000 train_loss: 76.35921478271484 test_loss:110.0396728515625\n",
      "1410/3000 train_loss: 64.88681030273438 test_loss:109.23014831542969\n",
      "1411/3000 train_loss: 69.17858123779297 test_loss:106.95176696777344\n",
      "1412/3000 train_loss: 70.80938720703125 test_loss:108.8140869140625\n",
      "1413/3000 train_loss: 68.27729034423828 test_loss:108.71223449707031\n",
      "1414/3000 train_loss: 61.93727111816406 test_loss:112.41634368896484\n",
      "1415/3000 train_loss: 77.14881896972656 test_loss:114.1922607421875\n",
      "1416/3000 train_loss: 67.8659896850586 test_loss:112.7700424194336\n",
      "1417/3000 train_loss: 69.35132598876953 test_loss:108.6317138671875\n",
      "1418/3000 train_loss: 77.01766204833984 test_loss:120.08882141113281\n",
      "1419/3000 train_loss: 77.7921371459961 test_loss:112.23493957519531\n",
      "1420/3000 train_loss: 80.89532470703125 test_loss:107.77154541015625\n",
      "1421/3000 train_loss: 74.99642944335938 test_loss:114.31779479980469\n",
      "1422/3000 train_loss: 84.2131576538086 test_loss:113.33406829833984\n",
      "1423/3000 train_loss: 66.17854309082031 test_loss:109.82295227050781\n",
      "1424/3000 train_loss: 64.03317260742188 test_loss:111.04429626464844\n",
      "1425/3000 train_loss: 73.91175842285156 test_loss:109.86460876464844\n",
      "1426/3000 train_loss: 61.6725959777832 test_loss:108.5728988647461\n",
      "1427/3000 train_loss: 89.99900817871094 test_loss:108.30986022949219\n",
      "1428/3000 train_loss: 66.08345031738281 test_loss:111.07545471191406\n",
      "1429/3000 train_loss: 80.10697937011719 test_loss:114.7308578491211\n",
      "1430/3000 train_loss: 71.72642517089844 test_loss:111.28669738769531\n",
      "1431/3000 train_loss: 63.910037994384766 test_loss:112.82705688476562\n",
      "1432/3000 train_loss: 64.95885467529297 test_loss:115.1166763305664\n",
      "1433/3000 train_loss: 71.94928741455078 test_loss:105.81161499023438\n",
      "1434/3000 train_loss: 85.7308578491211 test_loss:118.76443481445312\n",
      "1435/3000 train_loss: 73.69244384765625 test_loss:107.65625762939453\n",
      "1436/3000 train_loss: 67.38188934326172 test_loss:110.35577392578125\n",
      "1437/3000 train_loss: 63.903297424316406 test_loss:107.4957275390625\n",
      "1438/3000 train_loss: 67.64851379394531 test_loss:109.61946105957031\n",
      "1439/3000 train_loss: 78.78971862792969 test_loss:109.8178482055664\n",
      "1440/3000 train_loss: 65.42912292480469 test_loss:112.69287109375\n",
      "1441/3000 train_loss: 66.11084747314453 test_loss:107.20270538330078\n",
      "1442/3000 train_loss: 70.5497055053711 test_loss:107.57135009765625\n",
      "1443/3000 train_loss: 63.7117805480957 test_loss:107.33680725097656\n",
      "1444/3000 train_loss: 70.91148376464844 test_loss:111.70622253417969\n",
      "1445/3000 train_loss: 73.51055908203125 test_loss:111.96279907226562\n",
      "1446/3000 train_loss: 75.71155548095703 test_loss:113.2598876953125\n",
      "1447/3000 train_loss: 70.3254165649414 test_loss:113.05589294433594\n",
      "1448/3000 train_loss: 84.4815902709961 test_loss:108.01982116699219\n",
      "1449/3000 train_loss: 64.49124908447266 test_loss:112.68220520019531\n",
      "1450/3000 train_loss: 71.21990966796875 test_loss:107.99745178222656\n",
      "1451/3000 train_loss: 71.04104614257812 test_loss:110.36898040771484\n",
      "1452/3000 train_loss: 61.276100158691406 test_loss:107.55564880371094\n",
      "1453/3000 train_loss: 76.14967346191406 test_loss:111.75669860839844\n",
      "1454/3000 train_loss: 85.58023071289062 test_loss:108.72162628173828\n",
      "1455/3000 train_loss: 67.33058166503906 test_loss:114.00308990478516\n",
      "1456/3000 train_loss: 72.8772964477539 test_loss:115.53873443603516\n",
      "1457/3000 train_loss: 64.93529510498047 test_loss:115.11237335205078\n",
      "1458/3000 train_loss: 69.66381072998047 test_loss:113.51597595214844\n",
      "1459/3000 train_loss: 72.9983901977539 test_loss:113.931640625\n",
      "1460/3000 train_loss: 66.9344711303711 test_loss:113.48143005371094\n",
      "1461/3000 train_loss: 60.15674591064453 test_loss:109.7461929321289\n",
      "1462/3000 train_loss: 78.51830291748047 test_loss:105.6540756225586\n",
      "1463/3000 train_loss: 61.91001510620117 test_loss:110.3094482421875\n",
      "1464/3000 train_loss: 74.23402404785156 test_loss:109.70614624023438\n",
      "1465/3000 train_loss: 71.79087829589844 test_loss:108.22096252441406\n",
      "1466/3000 train_loss: 61.79875946044922 test_loss:107.12257385253906\n",
      "1467/3000 train_loss: 65.45938110351562 test_loss:113.68836975097656\n",
      "1468/3000 train_loss: 76.08004760742188 test_loss:115.88313293457031\n",
      "1469/3000 train_loss: 77.84706115722656 test_loss:117.89447021484375\n",
      "1470/3000 train_loss: 68.13160705566406 test_loss:109.00064086914062\n",
      "1471/3000 train_loss: 84.40042114257812 test_loss:112.78939819335938\n",
      "1472/3000 train_loss: 71.60785675048828 test_loss:112.6058349609375\n",
      "1473/3000 train_loss: 78.3480224609375 test_loss:120.79694366455078\n",
      "1474/3000 train_loss: 76.75888061523438 test_loss:113.44895935058594\n",
      "1475/3000 train_loss: 71.43936157226562 test_loss:114.66133117675781\n",
      "1476/3000 train_loss: 67.66576385498047 test_loss:115.34172058105469\n",
      "1477/3000 train_loss: 65.90435791015625 test_loss:108.2928695678711\n",
      "1478/3000 train_loss: 65.58695220947266 test_loss:117.14762878417969\n",
      "1479/3000 train_loss: 61.11605453491211 test_loss:118.50043487548828\n",
      "1480/3000 train_loss: 75.33098602294922 test_loss:111.53335571289062\n",
      "1481/3000 train_loss: 70.51044464111328 test_loss:118.99808502197266\n",
      "1482/3000 train_loss: 75.51923370361328 test_loss:111.05109405517578\n",
      "1483/3000 train_loss: 65.58544921875 test_loss:116.80029296875\n",
      "1484/3000 train_loss: 61.50635528564453 test_loss:111.51826477050781\n",
      "1485/3000 train_loss: 63.091819763183594 test_loss:110.67460632324219\n",
      "1486/3000 train_loss: 64.38994598388672 test_loss:115.97254943847656\n",
      "1487/3000 train_loss: 62.695106506347656 test_loss:112.4315185546875\n",
      "1488/3000 train_loss: 74.42266082763672 test_loss:113.2725601196289\n",
      "1489/3000 train_loss: 65.75798797607422 test_loss:109.6356201171875\n",
      "1490/3000 train_loss: 60.590919494628906 test_loss:109.09443664550781\n",
      "1491/3000 train_loss: 72.4461441040039 test_loss:114.21109771728516\n",
      "1492/3000 train_loss: 67.10603332519531 test_loss:111.07730102539062\n",
      "1493/3000 train_loss: 74.15595245361328 test_loss:111.23211669921875\n",
      "1494/3000 train_loss: 62.80361557006836 test_loss:111.60627746582031\n",
      "1495/3000 train_loss: 67.50111389160156 test_loss:109.14350128173828\n",
      "1496/3000 train_loss: 77.3681411743164 test_loss:117.53732299804688\n",
      "1497/3000 train_loss: 63.258575439453125 test_loss:112.9369888305664\n",
      "1498/3000 train_loss: 72.38917541503906 test_loss:113.02752685546875\n",
      "1499/3000 train_loss: 86.91986846923828 test_loss:106.6737060546875\n",
      "1500/3000 train_loss: 64.07725524902344 test_loss:108.35391235351562\n",
      "1501/3000 train_loss: 63.62668991088867 test_loss:105.42515563964844\n",
      "1502/3000 train_loss: 59.9525146484375 test_loss:106.13819122314453\n",
      "1503/3000 train_loss: 65.69461059570312 test_loss:101.71810913085938\n",
      "1504/3000 train_loss: 69.53707122802734 test_loss:104.4156265258789\n",
      "1505/3000 train_loss: 68.6964111328125 test_loss:105.12826538085938\n",
      "1506/3000 train_loss: 74.5219497680664 test_loss:106.43038177490234\n",
      "1507/3000 train_loss: 59.296714782714844 test_loss:105.78617095947266\n",
      "1508/3000 train_loss: 66.24674224853516 test_loss:110.51470947265625\n",
      "1509/3000 train_loss: 66.5804214477539 test_loss:106.30543518066406\n",
      "1510/3000 train_loss: 67.78680419921875 test_loss:111.32814025878906\n",
      "1511/3000 train_loss: 61.25918197631836 test_loss:104.81153869628906\n",
      "1512/3000 train_loss: 54.208702087402344 test_loss:103.93000793457031\n",
      "1513/3000 train_loss: 58.37660217285156 test_loss:103.83016204833984\n",
      "1514/3000 train_loss: 66.68014526367188 test_loss:105.15296936035156\n",
      "1515/3000 train_loss: 65.17108917236328 test_loss:101.4716796875\n",
      "1516/3000 train_loss: 73.04991912841797 test_loss:104.32852935791016\n",
      "1517/3000 train_loss: 60.89824295043945 test_loss:106.88284301757812\n",
      "1518/3000 train_loss: 68.21805572509766 test_loss:107.37263488769531\n",
      "1519/3000 train_loss: 65.9273681640625 test_loss:108.82273864746094\n",
      "1520/3000 train_loss: 57.082374572753906 test_loss:113.46875\n",
      "1521/3000 train_loss: 69.84831237792969 test_loss:106.28822326660156\n",
      "1522/3000 train_loss: 61.179443359375 test_loss:105.12199401855469\n",
      "1523/3000 train_loss: 60.392295837402344 test_loss:105.06067657470703\n",
      "1524/3000 train_loss: 73.6411361694336 test_loss:107.33894348144531\n",
      "1525/3000 train_loss: 69.5318374633789 test_loss:105.36054229736328\n",
      "1526/3000 train_loss: 72.63326263427734 test_loss:102.55351257324219\n",
      "1527/3000 train_loss: 77.48538208007812 test_loss:103.76802062988281\n",
      "1528/3000 train_loss: 60.2772216796875 test_loss:107.47901916503906\n",
      "1529/3000 train_loss: 72.4367446899414 test_loss:103.33787536621094\n",
      "1530/3000 train_loss: 66.55924987792969 test_loss:107.89128112792969\n",
      "1531/3000 train_loss: 77.96125030517578 test_loss:98.0064697265625\n",
      "1532/3000 train_loss: 63.54110336303711 test_loss:102.93365478515625\n",
      "1533/3000 train_loss: 59.87565231323242 test_loss:101.41085815429688\n",
      "1534/3000 train_loss: 58.255828857421875 test_loss:101.22264862060547\n",
      "1535/3000 train_loss: 66.12639617919922 test_loss:100.84385681152344\n",
      "1536/3000 train_loss: 63.806941986083984 test_loss:103.86248779296875\n",
      "1537/3000 train_loss: 63.90119171142578 test_loss:107.60986328125\n",
      "1538/3000 train_loss: 66.62894439697266 test_loss:105.19427490234375\n",
      "1539/3000 train_loss: 65.46600341796875 test_loss:105.01715087890625\n",
      "1540/3000 train_loss: 65.25611114501953 test_loss:105.7739486694336\n",
      "1541/3000 train_loss: 64.01927947998047 test_loss:99.62136840820312\n",
      "1542/3000 train_loss: 62.300994873046875 test_loss:101.22429656982422\n",
      "1543/3000 train_loss: 55.301902770996094 test_loss:105.109619140625\n",
      "1544/3000 train_loss: 72.70223236083984 test_loss:100.03605651855469\n",
      "1545/3000 train_loss: 61.666465759277344 test_loss:102.26701354980469\n",
      "1546/3000 train_loss: 67.85416412353516 test_loss:100.65613555908203\n",
      "1547/3000 train_loss: 65.18020629882812 test_loss:103.1801986694336\n",
      "1548/3000 train_loss: 64.12267303466797 test_loss:104.80979919433594\n",
      "1549/3000 train_loss: 62.609004974365234 test_loss:104.16893005371094\n",
      "1550/3000 train_loss: 59.305633544921875 test_loss:101.97586059570312\n",
      "1551/3000 train_loss: 62.46299743652344 test_loss:99.81678771972656\n",
      "1552/3000 train_loss: 59.53913116455078 test_loss:102.54684448242188\n",
      "1553/3000 train_loss: 71.67402648925781 test_loss:100.52981567382812\n",
      "1554/3000 train_loss: 60.43129348754883 test_loss:108.15000915527344\n",
      "1555/3000 train_loss: 60.297210693359375 test_loss:104.26148986816406\n",
      "1556/3000 train_loss: 58.67462921142578 test_loss:100.19187927246094\n",
      "1557/3000 train_loss: 60.81306838989258 test_loss:104.5738525390625\n",
      "1558/3000 train_loss: 64.53361511230469 test_loss:101.95845031738281\n",
      "1559/3000 train_loss: 54.620079040527344 test_loss:101.16375732421875\n",
      "1560/3000 train_loss: 64.20626831054688 test_loss:102.46719360351562\n",
      "1561/3000 train_loss: 73.52886962890625 test_loss:111.20769500732422\n",
      "1562/3000 train_loss: 64.83181762695312 test_loss:104.7209701538086\n",
      "1563/3000 train_loss: 63.34382247924805 test_loss:102.10505676269531\n",
      "1564/3000 train_loss: 56.54840087890625 test_loss:103.35433959960938\n",
      "1565/3000 train_loss: 74.16284942626953 test_loss:103.28895568847656\n",
      "1566/3000 train_loss: 63.40099334716797 test_loss:101.82717895507812\n",
      "1567/3000 train_loss: 63.026851654052734 test_loss:100.03602600097656\n",
      "1568/3000 train_loss: 55.551795959472656 test_loss:98.84060668945312\n",
      "1569/3000 train_loss: 70.96317291259766 test_loss:102.99577331542969\n",
      "1570/3000 train_loss: 69.08787536621094 test_loss:105.54011535644531\n",
      "1571/3000 train_loss: 71.45406341552734 test_loss:101.34423828125\n",
      "1572/3000 train_loss: 58.412315368652344 test_loss:111.06857299804688\n",
      "1573/3000 train_loss: 58.665409088134766 test_loss:103.02883911132812\n",
      "1574/3000 train_loss: 68.8309097290039 test_loss:109.56086730957031\n",
      "1575/3000 train_loss: 68.4461669921875 test_loss:96.19956970214844\n",
      "1576/3000 train_loss: 80.42469787597656 test_loss:101.50484466552734\n",
      "1577/3000 train_loss: 66.49690246582031 test_loss:99.19563293457031\n",
      "1578/3000 train_loss: 77.19258880615234 test_loss:105.16372680664062\n",
      "1579/3000 train_loss: 68.73661804199219 test_loss:106.94672393798828\n",
      "1580/3000 train_loss: 58.644798278808594 test_loss:98.89753723144531\n",
      "1581/3000 train_loss: 63.47356414794922 test_loss:100.35598754882812\n",
      "1582/3000 train_loss: 65.77220916748047 test_loss:104.1298828125\n",
      "1583/3000 train_loss: 56.743900299072266 test_loss:108.325439453125\n",
      "1584/3000 train_loss: 53.060176849365234 test_loss:104.86421203613281\n",
      "1585/3000 train_loss: 57.553768157958984 test_loss:103.07530212402344\n",
      "1586/3000 train_loss: 77.8220443725586 test_loss:103.37699890136719\n",
      "1587/3000 train_loss: 74.3940658569336 test_loss:104.5309066772461\n",
      "1588/3000 train_loss: 59.85415267944336 test_loss:102.01822662353516\n",
      "1589/3000 train_loss: 55.72306442260742 test_loss:98.78257751464844\n",
      "1590/3000 train_loss: 62.837642669677734 test_loss:106.57685852050781\n",
      "1591/3000 train_loss: 71.17914581298828 test_loss:103.8118896484375\n",
      "1592/3000 train_loss: 58.93184280395508 test_loss:100.35442352294922\n",
      "1593/3000 train_loss: 61.42536926269531 test_loss:105.96394348144531\n",
      "1594/3000 train_loss: 58.768959045410156 test_loss:103.94657897949219\n",
      "1595/3000 train_loss: 67.634033203125 test_loss:102.04769897460938\n",
      "1596/3000 train_loss: 66.75727844238281 test_loss:103.22024536132812\n",
      "1597/3000 train_loss: 59.66244888305664 test_loss:101.69490051269531\n",
      "1598/3000 train_loss: 64.65473175048828 test_loss:100.13521575927734\n",
      "1599/3000 train_loss: 57.946510314941406 test_loss:101.19459533691406\n",
      "1600/3000 train_loss: 59.04020309448242 test_loss:101.62948608398438\n",
      "1601/3000 train_loss: 51.4435920715332 test_loss:96.79408264160156\n",
      "1602/3000 train_loss: 68.41748809814453 test_loss:95.54083251953125\n",
      "1603/3000 train_loss: 69.49327087402344 test_loss:100.5751724243164\n",
      "1604/3000 train_loss: 55.724327087402344 test_loss:95.74771881103516\n",
      "1605/3000 train_loss: 63.06760787963867 test_loss:97.4350357055664\n",
      "1606/3000 train_loss: 58.593360900878906 test_loss:94.79058837890625\n",
      "1607/3000 train_loss: 82.77825164794922 test_loss:92.66447448730469\n",
      "1608/3000 train_loss: 64.98054504394531 test_loss:93.96932983398438\n",
      "1609/3000 train_loss: 68.06019592285156 test_loss:99.97526550292969\n",
      "1610/3000 train_loss: 73.36479187011719 test_loss:104.64683532714844\n",
      "1611/3000 train_loss: 55.57110595703125 test_loss:104.6040267944336\n",
      "1612/3000 train_loss: 58.703857421875 test_loss:98.71310424804688\n",
      "1613/3000 train_loss: 58.463592529296875 test_loss:103.59842681884766\n",
      "1614/3000 train_loss: 64.96751403808594 test_loss:102.46585083007812\n",
      "1615/3000 train_loss: 59.826534271240234 test_loss:105.52068328857422\n",
      "1616/3000 train_loss: 69.42437744140625 test_loss:99.54833221435547\n",
      "1617/3000 train_loss: 63.87293243408203 test_loss:103.19001770019531\n",
      "1618/3000 train_loss: 75.70185089111328 test_loss:105.1570816040039\n",
      "1619/3000 train_loss: 71.5525131225586 test_loss:102.82401275634766\n",
      "1620/3000 train_loss: 62.6099739074707 test_loss:102.37153625488281\n",
      "1621/3000 train_loss: 65.3813247680664 test_loss:103.85382080078125\n",
      "1622/3000 train_loss: 59.75861740112305 test_loss:102.2442626953125\n",
      "1623/3000 train_loss: 59.7325439453125 test_loss:105.30284118652344\n",
      "1624/3000 train_loss: 60.395145416259766 test_loss:101.8609848022461\n",
      "1625/3000 train_loss: 55.17193603515625 test_loss:104.16947937011719\n",
      "1626/3000 train_loss: 52.98038101196289 test_loss:102.87493896484375\n",
      "1627/3000 train_loss: 64.08757781982422 test_loss:105.78907775878906\n",
      "1628/3000 train_loss: 67.26837921142578 test_loss:112.7591552734375\n",
      "1629/3000 train_loss: 52.198822021484375 test_loss:97.12265014648438\n",
      "1630/3000 train_loss: 53.589683532714844 test_loss:99.94660186767578\n",
      "1631/3000 train_loss: 61.46125030517578 test_loss:103.0065689086914\n",
      "1632/3000 train_loss: 73.7271728515625 test_loss:106.15084838867188\n",
      "1633/3000 train_loss: 63.05095672607422 test_loss:107.88142395019531\n",
      "1634/3000 train_loss: 55.770694732666016 test_loss:104.78353118896484\n",
      "1635/3000 train_loss: 61.896400451660156 test_loss:106.20845031738281\n",
      "1636/3000 train_loss: 58.62242889404297 test_loss:101.98272705078125\n",
      "1637/3000 train_loss: 53.173038482666016 test_loss:100.07382202148438\n",
      "1638/3000 train_loss: 55.80234909057617 test_loss:100.27253723144531\n",
      "1639/3000 train_loss: 50.38975143432617 test_loss:101.95478057861328\n",
      "1640/3000 train_loss: 55.45454025268555 test_loss:108.44222259521484\n",
      "1641/3000 train_loss: 86.15916442871094 test_loss:100.42282104492188\n",
      "1642/3000 train_loss: 58.430599212646484 test_loss:114.2978286743164\n",
      "1643/3000 train_loss: 58.569698333740234 test_loss:107.1876449584961\n",
      "1644/3000 train_loss: 64.68648529052734 test_loss:105.22671508789062\n",
      "1645/3000 train_loss: 66.04841613769531 test_loss:102.58293151855469\n",
      "1646/3000 train_loss: 57.56787109375 test_loss:105.89900207519531\n",
      "1647/3000 train_loss: 59.18817901611328 test_loss:98.57098388671875\n",
      "1648/3000 train_loss: 58.88455581665039 test_loss:101.8975830078125\n",
      "1649/3000 train_loss: 63.59346008300781 test_loss:105.41032409667969\n",
      "1650/3000 train_loss: 65.86722564697266 test_loss:110.41714477539062\n",
      "1651/3000 train_loss: 61.99515914916992 test_loss:107.77165985107422\n",
      "1652/3000 train_loss: 55.84058380126953 test_loss:100.37184143066406\n",
      "1653/3000 train_loss: 61.93525695800781 test_loss:104.8689956665039\n",
      "1654/3000 train_loss: 76.83822631835938 test_loss:103.77062225341797\n",
      "1655/3000 train_loss: 60.597110748291016 test_loss:100.60171508789062\n",
      "1656/3000 train_loss: 70.48371124267578 test_loss:103.70858764648438\n",
      "1657/3000 train_loss: 60.255340576171875 test_loss:102.80125427246094\n",
      "1658/3000 train_loss: 68.97837829589844 test_loss:122.40501403808594\n",
      "1659/3000 train_loss: 56.29304504394531 test_loss:101.7105484008789\n",
      "1660/3000 train_loss: 51.46436309814453 test_loss:105.58700561523438\n",
      "1661/3000 train_loss: 62.75753402709961 test_loss:103.06369018554688\n",
      "1662/3000 train_loss: 69.59296417236328 test_loss:104.34909057617188\n",
      "1663/3000 train_loss: 59.598846435546875 test_loss:103.28700256347656\n",
      "1664/3000 train_loss: 51.038543701171875 test_loss:95.9693374633789\n",
      "1665/3000 train_loss: 57.260860443115234 test_loss:95.87178039550781\n",
      "1666/3000 train_loss: 63.17926788330078 test_loss:99.42774200439453\n",
      "1667/3000 train_loss: 55.32090377807617 test_loss:98.79605865478516\n",
      "1668/3000 train_loss: 59.47884750366211 test_loss:94.68116760253906\n",
      "1669/3000 train_loss: 55.51895523071289 test_loss:107.19029235839844\n",
      "1670/3000 train_loss: 59.29453659057617 test_loss:99.92242431640625\n",
      "1671/3000 train_loss: 53.1663703918457 test_loss:96.45610046386719\n",
      "1672/3000 train_loss: 61.143802642822266 test_loss:94.22151947021484\n",
      "1673/3000 train_loss: 58.55492401123047 test_loss:95.37801361083984\n",
      "1674/3000 train_loss: 56.70240020751953 test_loss:99.9329605102539\n",
      "1675/3000 train_loss: 58.57603454589844 test_loss:95.80740356445312\n",
      "1676/3000 train_loss: 68.3471908569336 test_loss:96.17463684082031\n",
      "1677/3000 train_loss: 55.686439514160156 test_loss:97.34663391113281\n",
      "1678/3000 train_loss: 67.0000228881836 test_loss:96.34748077392578\n",
      "1679/3000 train_loss: 78.02922058105469 test_loss:100.04512023925781\n",
      "1680/3000 train_loss: 56.15549087524414 test_loss:98.642822265625\n",
      "1681/3000 train_loss: 56.414432525634766 test_loss:97.27407836914062\n",
      "1682/3000 train_loss: 57.965782165527344 test_loss:98.01008605957031\n",
      "1683/3000 train_loss: 55.48054504394531 test_loss:106.63487243652344\n",
      "1684/3000 train_loss: 66.34685516357422 test_loss:98.73434448242188\n",
      "1685/3000 train_loss: 61.25508117675781 test_loss:99.72322845458984\n",
      "1686/3000 train_loss: 58.89210510253906 test_loss:94.14364624023438\n",
      "1687/3000 train_loss: 60.5225830078125 test_loss:97.84512329101562\n",
      "1688/3000 train_loss: 49.625 test_loss:95.96810913085938\n",
      "1689/3000 train_loss: 63.36186981201172 test_loss:99.93959045410156\n",
      "1690/3000 train_loss: 56.96883773803711 test_loss:101.20558166503906\n",
      "1691/3000 train_loss: 57.57143783569336 test_loss:97.72129821777344\n",
      "1692/3000 train_loss: 57.44557189941406 test_loss:102.8756332397461\n",
      "1693/3000 train_loss: 58.03228759765625 test_loss:99.29529571533203\n",
      "1694/3000 train_loss: 61.93570327758789 test_loss:95.79008483886719\n",
      "1695/3000 train_loss: 69.04356384277344 test_loss:94.26570129394531\n",
      "1696/3000 train_loss: 62.08750915527344 test_loss:97.94401550292969\n",
      "1697/3000 train_loss: 62.16264343261719 test_loss:95.6705093383789\n",
      "1698/3000 train_loss: 82.32781982421875 test_loss:97.92080688476562\n",
      "1699/3000 train_loss: 55.416603088378906 test_loss:94.75611114501953\n",
      "1700/3000 train_loss: 57.28441619873047 test_loss:95.72456359863281\n",
      "1701/3000 train_loss: 56.48847198486328 test_loss:106.53507995605469\n",
      "1702/3000 train_loss: 54.033531188964844 test_loss:96.35608673095703\n",
      "1703/3000 train_loss: 60.29634475708008 test_loss:100.6749038696289\n",
      "1704/3000 train_loss: 52.5319709777832 test_loss:93.4031753540039\n",
      "1705/3000 train_loss: 52.52405548095703 test_loss:94.67530822753906\n",
      "1706/3000 train_loss: 62.084495544433594 test_loss:95.77499389648438\n",
      "1707/3000 train_loss: 70.5038070678711 test_loss:98.20361328125\n",
      "1708/3000 train_loss: 62.84324645996094 test_loss:103.9261474609375\n",
      "1709/3000 train_loss: 57.75893020629883 test_loss:99.85508728027344\n",
      "1710/3000 train_loss: 57.44650650024414 test_loss:96.1805419921875\n",
      "1711/3000 train_loss: 57.22550582885742 test_loss:101.68142700195312\n",
      "1712/3000 train_loss: 56.015960693359375 test_loss:98.22552490234375\n",
      "1713/3000 train_loss: 58.326255798339844 test_loss:94.83641815185547\n",
      "1714/3000 train_loss: 49.90389633178711 test_loss:97.53485107421875\n",
      "1715/3000 train_loss: 51.627376556396484 test_loss:100.02919006347656\n",
      "1716/3000 train_loss: 60.105621337890625 test_loss:96.16790008544922\n",
      "1717/3000 train_loss: 69.51901245117188 test_loss:94.74356079101562\n",
      "1718/3000 train_loss: 53.64257049560547 test_loss:99.03767395019531\n",
      "1719/3000 train_loss: 53.43848419189453 test_loss:94.31758117675781\n",
      "1720/3000 train_loss: 67.19952392578125 test_loss:101.65097045898438\n",
      "1721/3000 train_loss: 59.30826950073242 test_loss:99.05538940429688\n",
      "1722/3000 train_loss: 62.06016159057617 test_loss:96.30319213867188\n",
      "1723/3000 train_loss: 61.39885711669922 test_loss:98.1287841796875\n",
      "1724/3000 train_loss: 71.5377197265625 test_loss:101.30015563964844\n",
      "1725/3000 train_loss: 57.04090118408203 test_loss:97.75480651855469\n",
      "1726/3000 train_loss: 60.59270477294922 test_loss:97.9667739868164\n",
      "1727/3000 train_loss: 61.94744873046875 test_loss:95.95713806152344\n",
      "1728/3000 train_loss: 54.90637969970703 test_loss:99.29235076904297\n",
      "1729/3000 train_loss: 52.83043670654297 test_loss:95.5067138671875\n",
      "1730/3000 train_loss: 49.45381546020508 test_loss:98.24053955078125\n",
      "1731/3000 train_loss: 50.455745697021484 test_loss:99.07772064208984\n",
      "1732/3000 train_loss: 55.65040588378906 test_loss:97.12602996826172\n",
      "1733/3000 train_loss: 65.83004760742188 test_loss:99.58769226074219\n",
      "1734/3000 train_loss: 70.82398986816406 test_loss:96.46932983398438\n",
      "1735/3000 train_loss: 56.729522705078125 test_loss:98.76724243164062\n",
      "1736/3000 train_loss: 54.2801628112793 test_loss:92.93145751953125\n",
      "1737/3000 train_loss: 58.880760192871094 test_loss:95.73290252685547\n",
      "1738/3000 train_loss: 51.955623626708984 test_loss:90.96328735351562\n",
      "1739/3000 train_loss: 55.018280029296875 test_loss:94.40435028076172\n",
      "1740/3000 train_loss: 74.26470947265625 test_loss:96.32661437988281\n",
      "1741/3000 train_loss: 54.85032653808594 test_loss:95.23185729980469\n",
      "1742/3000 train_loss: 53.71128845214844 test_loss:91.99376678466797\n",
      "1743/3000 train_loss: 67.87194061279297 test_loss:92.90348815917969\n",
      "1744/3000 train_loss: 59.054080963134766 test_loss:90.9687728881836\n",
      "1745/3000 train_loss: 51.251976013183594 test_loss:96.39923858642578\n",
      "1746/3000 train_loss: 54.781707763671875 test_loss:94.9559097290039\n",
      "1747/3000 train_loss: 60.02479553222656 test_loss:100.69937896728516\n",
      "1748/3000 train_loss: 63.58475112915039 test_loss:89.7066421508789\n",
      "1749/3000 train_loss: 58.82455825805664 test_loss:94.42660522460938\n",
      "1750/3000 train_loss: 56.423988342285156 test_loss:92.69395446777344\n",
      "1751/3000 train_loss: 59.889381408691406 test_loss:90.54915618896484\n",
      "1752/3000 train_loss: 65.12983703613281 test_loss:98.77384948730469\n",
      "1753/3000 train_loss: 54.834007263183594 test_loss:91.69515228271484\n",
      "1754/3000 train_loss: 49.01085662841797 test_loss:91.90434265136719\n",
      "1755/3000 train_loss: 59.33692932128906 test_loss:89.7681884765625\n",
      "1756/3000 train_loss: 77.39364624023438 test_loss:92.63005828857422\n",
      "1757/3000 train_loss: 74.73357391357422 test_loss:95.95897674560547\n",
      "1758/3000 train_loss: 65.1775131225586 test_loss:101.33036804199219\n",
      "1759/3000 train_loss: 61.31806945800781 test_loss:104.56027221679688\n",
      "1760/3000 train_loss: 66.84408569335938 test_loss:92.32219696044922\n",
      "1761/3000 train_loss: 60.89922332763672 test_loss:92.65757751464844\n",
      "1762/3000 train_loss: 65.03657531738281 test_loss:90.9908447265625\n",
      "1763/3000 train_loss: 57.64971923828125 test_loss:94.32820129394531\n",
      "1764/3000 train_loss: 58.10026550292969 test_loss:94.85778045654297\n",
      "1765/3000 train_loss: 52.86207962036133 test_loss:94.25210571289062\n",
      "1766/3000 train_loss: 51.59105682373047 test_loss:96.07931518554688\n",
      "1767/3000 train_loss: 49.203529357910156 test_loss:94.67927551269531\n",
      "1768/3000 train_loss: 54.823028564453125 test_loss:97.0905532836914\n",
      "1769/3000 train_loss: 57.119590759277344 test_loss:94.96934509277344\n",
      "1770/3000 train_loss: 53.15317916870117 test_loss:94.64996337890625\n",
      "1771/3000 train_loss: 53.824737548828125 test_loss:93.5541763305664\n",
      "1772/3000 train_loss: 64.2259292602539 test_loss:95.57310485839844\n",
      "1773/3000 train_loss: 65.3312759399414 test_loss:98.56587219238281\n",
      "1774/3000 train_loss: 50.16577911376953 test_loss:93.22248077392578\n",
      "1775/3000 train_loss: 57.01231384277344 test_loss:93.84738159179688\n",
      "1776/3000 train_loss: 61.40651321411133 test_loss:93.60271453857422\n",
      "1777/3000 train_loss: 64.66195678710938 test_loss:96.16436767578125\n",
      "1778/3000 train_loss: 63.216087341308594 test_loss:96.33621978759766\n",
      "1779/3000 train_loss: 57.012779235839844 test_loss:103.27473449707031\n",
      "1780/3000 train_loss: 54.212650299072266 test_loss:98.8079833984375\n",
      "1781/3000 train_loss: 52.037837982177734 test_loss:100.79147338867188\n",
      "1782/3000 train_loss: 51.805179595947266 test_loss:100.28315734863281\n",
      "1783/3000 train_loss: 57.274147033691406 test_loss:95.18408966064453\n",
      "1784/3000 train_loss: 61.68058395385742 test_loss:93.98155975341797\n",
      "1785/3000 train_loss: 48.5821418762207 test_loss:95.14874267578125\n",
      "1786/3000 train_loss: 51.0530891418457 test_loss:91.64887237548828\n",
      "1787/3000 train_loss: 58.68142318725586 test_loss:91.54644012451172\n",
      "1788/3000 train_loss: 53.636932373046875 test_loss:91.10059356689453\n",
      "1789/3000 train_loss: 57.936161041259766 test_loss:93.46247863769531\n",
      "1790/3000 train_loss: 59.271366119384766 test_loss:90.69905090332031\n",
      "1791/3000 train_loss: 52.342288970947266 test_loss:90.4671401977539\n",
      "1792/3000 train_loss: 52.63958740234375 test_loss:93.53495788574219\n",
      "1793/3000 train_loss: 62.31342315673828 test_loss:89.45491027832031\n",
      "1794/3000 train_loss: 50.609859466552734 test_loss:93.65467834472656\n",
      "1795/3000 train_loss: 56.59979248046875 test_loss:89.5031967163086\n",
      "1796/3000 train_loss: 49.65367126464844 test_loss:91.33804321289062\n",
      "1797/3000 train_loss: 58.792301177978516 test_loss:96.16553497314453\n",
      "1798/3000 train_loss: 57.64402770996094 test_loss:91.04033660888672\n",
      "1799/3000 train_loss: 47.160888671875 test_loss:95.3555679321289\n",
      "1800/3000 train_loss: 60.628875732421875 test_loss:93.751220703125\n",
      "1801/3000 train_loss: 51.910484313964844 test_loss:93.64462280273438\n",
      "1802/3000 train_loss: 52.34796142578125 test_loss:96.65065002441406\n",
      "1803/3000 train_loss: 55.57559585571289 test_loss:98.98686218261719\n",
      "1804/3000 train_loss: 66.5794448852539 test_loss:97.93687438964844\n",
      "1805/3000 train_loss: 52.65315246582031 test_loss:95.2569580078125\n",
      "1806/3000 train_loss: 55.44678497314453 test_loss:95.68687438964844\n",
      "1807/3000 train_loss: 48.565677642822266 test_loss:99.00364685058594\n",
      "1808/3000 train_loss: 54.856048583984375 test_loss:91.78280639648438\n",
      "1809/3000 train_loss: 60.99740219116211 test_loss:97.30180358886719\n",
      "1810/3000 train_loss: 61.728267669677734 test_loss:95.1448974609375\n",
      "1811/3000 train_loss: 55.562034606933594 test_loss:100.7522201538086\n",
      "1812/3000 train_loss: 66.36770629882812 test_loss:95.30656433105469\n",
      "1813/3000 train_loss: 58.466861724853516 test_loss:89.74495697021484\n",
      "1814/3000 train_loss: 62.27210998535156 test_loss:101.26681518554688\n",
      "1815/3000 train_loss: 57.37139892578125 test_loss:91.49020385742188\n",
      "1816/3000 train_loss: 55.40235137939453 test_loss:88.75599670410156\n",
      "1817/3000 train_loss: 51.098052978515625 test_loss:92.71340942382812\n",
      "1818/3000 train_loss: 49.73566436767578 test_loss:96.58464050292969\n",
      "1819/3000 train_loss: 49.99972152709961 test_loss:90.66606140136719\n",
      "1820/3000 train_loss: 49.96125030517578 test_loss:95.25242614746094\n",
      "1821/3000 train_loss: 51.301353454589844 test_loss:96.41461944580078\n",
      "1822/3000 train_loss: 47.84534454345703 test_loss:91.37621307373047\n",
      "1823/3000 train_loss: 54.39784240722656 test_loss:96.22411346435547\n",
      "1824/3000 train_loss: 50.739559173583984 test_loss:91.1915054321289\n",
      "1825/3000 train_loss: 51.72438049316406 test_loss:91.40045928955078\n",
      "1826/3000 train_loss: 52.24034118652344 test_loss:93.71754455566406\n",
      "1827/3000 train_loss: 49.47963333129883 test_loss:95.85645294189453\n",
      "1828/3000 train_loss: 57.65970230102539 test_loss:93.1800308227539\n",
      "1829/3000 train_loss: 50.45330810546875 test_loss:90.06613159179688\n",
      "1830/3000 train_loss: 57.61076354980469 test_loss:97.14064025878906\n",
      "1831/3000 train_loss: 48.283531188964844 test_loss:93.40103149414062\n",
      "1832/3000 train_loss: 58.80521774291992 test_loss:98.03870391845703\n",
      "1833/3000 train_loss: 52.97468948364258 test_loss:94.00862121582031\n",
      "1834/3000 train_loss: 52.21017837524414 test_loss:91.86581420898438\n",
      "1835/3000 train_loss: 63.74797439575195 test_loss:88.647705078125\n",
      "1836/3000 train_loss: 46.42887878417969 test_loss:92.49622344970703\n",
      "1837/3000 train_loss: 67.22794342041016 test_loss:88.10343933105469\n",
      "1838/3000 train_loss: 56.28181076049805 test_loss:103.62785339355469\n",
      "1839/3000 train_loss: 58.237857818603516 test_loss:95.67823791503906\n",
      "1840/3000 train_loss: 53.854129791259766 test_loss:93.58430480957031\n",
      "1841/3000 train_loss: 47.586551666259766 test_loss:98.07221984863281\n",
      "1842/3000 train_loss: 51.429561614990234 test_loss:90.26390838623047\n",
      "1843/3000 train_loss: 49.14675521850586 test_loss:89.4580307006836\n",
      "1844/3000 train_loss: 51.28838348388672 test_loss:98.64116668701172\n",
      "1845/3000 train_loss: 51.04011154174805 test_loss:90.09941864013672\n",
      "1846/3000 train_loss: 52.50343704223633 test_loss:91.30062103271484\n",
      "1847/3000 train_loss: 70.94056701660156 test_loss:96.02323150634766\n",
      "1848/3000 train_loss: 72.3457260131836 test_loss:97.11358642578125\n",
      "1849/3000 train_loss: 52.20665740966797 test_loss:99.81873321533203\n",
      "1850/3000 train_loss: 67.492919921875 test_loss:96.45809936523438\n",
      "1851/3000 train_loss: 65.25249481201172 test_loss:96.53402709960938\n",
      "1852/3000 train_loss: 57.911293029785156 test_loss:94.50503540039062\n",
      "1853/3000 train_loss: 58.806396484375 test_loss:93.17901611328125\n",
      "1854/3000 train_loss: 54.04694366455078 test_loss:100.16566467285156\n",
      "1855/3000 train_loss: 53.66494369506836 test_loss:95.21742248535156\n",
      "1856/3000 train_loss: 54.0627326965332 test_loss:91.09416961669922\n",
      "1857/3000 train_loss: 52.11396789550781 test_loss:86.59564208984375\n",
      "1858/3000 train_loss: 64.1967544555664 test_loss:91.93191528320312\n",
      "1859/3000 train_loss: 61.62547302246094 test_loss:92.59009552001953\n",
      "1860/3000 train_loss: 45.37268829345703 test_loss:91.80757904052734\n",
      "1861/3000 train_loss: 59.70643615722656 test_loss:96.37999725341797\n",
      "1862/3000 train_loss: 62.30131530761719 test_loss:99.65988159179688\n",
      "1863/3000 train_loss: 54.02933883666992 test_loss:96.04270935058594\n",
      "1864/3000 train_loss: 49.114707946777344 test_loss:91.64036560058594\n",
      "1865/3000 train_loss: 52.08033752441406 test_loss:89.88211059570312\n",
      "1866/3000 train_loss: 51.638946533203125 test_loss:90.80767059326172\n",
      "1867/3000 train_loss: 71.54862976074219 test_loss:100.12117004394531\n",
      "1868/3000 train_loss: 46.439476013183594 test_loss:89.60628509521484\n",
      "1869/3000 train_loss: 57.61000442504883 test_loss:89.30851745605469\n",
      "1870/3000 train_loss: 48.27294921875 test_loss:91.54560852050781\n",
      "1871/3000 train_loss: 54.38934326171875 test_loss:85.42188262939453\n",
      "1872/3000 train_loss: 52.71511459350586 test_loss:85.45272064208984\n",
      "1873/3000 train_loss: 62.252967834472656 test_loss:89.11343383789062\n",
      "1874/3000 train_loss: 53.135292053222656 test_loss:86.73724365234375\n",
      "1875/3000 train_loss: 69.95482635498047 test_loss:84.54216003417969\n",
      "1876/3000 train_loss: 46.814414978027344 test_loss:90.07009887695312\n",
      "1877/3000 train_loss: 73.23600006103516 test_loss:95.08592224121094\n",
      "1878/3000 train_loss: 54.00898742675781 test_loss:96.85211181640625\n",
      "1879/3000 train_loss: 59.26856994628906 test_loss:90.49595642089844\n",
      "1880/3000 train_loss: 65.23662567138672 test_loss:94.36869812011719\n",
      "1881/3000 train_loss: 66.1917724609375 test_loss:94.91581726074219\n",
      "1882/3000 train_loss: 55.25367736816406 test_loss:92.83304595947266\n",
      "1883/3000 train_loss: 54.16508483886719 test_loss:95.67449951171875\n",
      "1884/3000 train_loss: 59.71757507324219 test_loss:88.2771224975586\n",
      "1885/3000 train_loss: 46.48406982421875 test_loss:84.32673645019531\n",
      "1886/3000 train_loss: 55.171287536621094 test_loss:87.65534210205078\n",
      "1887/3000 train_loss: 61.84439468383789 test_loss:93.29618835449219\n",
      "1888/3000 train_loss: 53.20662307739258 test_loss:90.38525390625\n",
      "1889/3000 train_loss: 70.29147338867188 test_loss:95.08816528320312\n",
      "1890/3000 train_loss: 54.568363189697266 test_loss:91.61228942871094\n",
      "1891/3000 train_loss: 50.45560073852539 test_loss:96.29801940917969\n",
      "1892/3000 train_loss: 55.63251876831055 test_loss:95.3837890625\n",
      "1893/3000 train_loss: 60.98506164550781 test_loss:87.4967041015625\n",
      "1894/3000 train_loss: 47.29423904418945 test_loss:95.67573547363281\n",
      "1895/3000 train_loss: 65.449951171875 test_loss:88.75848388671875\n",
      "1896/3000 train_loss: 54.28898620605469 test_loss:87.3316879272461\n",
      "1897/3000 train_loss: 58.693904876708984 test_loss:88.40486907958984\n",
      "1898/3000 train_loss: 50.94804382324219 test_loss:92.80591583251953\n",
      "1899/3000 train_loss: 55.79398727416992 test_loss:93.0394515991211\n",
      "1900/3000 train_loss: 50.12757110595703 test_loss:88.39349365234375\n",
      "1901/3000 train_loss: 48.846317291259766 test_loss:91.66366577148438\n",
      "1902/3000 train_loss: 54.66389083862305 test_loss:96.51486206054688\n",
      "1903/3000 train_loss: 69.36901092529297 test_loss:96.05844116210938\n",
      "1904/3000 train_loss: 54.99459457397461 test_loss:94.06537628173828\n",
      "1905/3000 train_loss: 47.36398696899414 test_loss:95.30393981933594\n",
      "1906/3000 train_loss: 53.49287414550781 test_loss:90.70106506347656\n",
      "1907/3000 train_loss: 58.836029052734375 test_loss:89.90800476074219\n",
      "1908/3000 train_loss: 54.55408477783203 test_loss:90.28169250488281\n",
      "1909/3000 train_loss: 51.90460205078125 test_loss:89.80594635009766\n",
      "1910/3000 train_loss: 50.2584114074707 test_loss:90.86222839355469\n",
      "1911/3000 train_loss: 47.912254333496094 test_loss:89.97622680664062\n",
      "1912/3000 train_loss: 49.88556671142578 test_loss:91.79023742675781\n",
      "1913/3000 train_loss: 53.926841735839844 test_loss:92.73207092285156\n",
      "1914/3000 train_loss: 53.038509368896484 test_loss:85.34013366699219\n",
      "1915/3000 train_loss: 49.86921691894531 test_loss:86.65541076660156\n",
      "1916/3000 train_loss: 44.7502326965332 test_loss:88.75784301757812\n",
      "1917/3000 train_loss: 50.85209655761719 test_loss:86.50049591064453\n",
      "1918/3000 train_loss: 50.22377014160156 test_loss:90.20538330078125\n",
      "1919/3000 train_loss: 51.352561950683594 test_loss:89.32138061523438\n",
      "1920/3000 train_loss: 43.31581497192383 test_loss:89.46922302246094\n",
      "1921/3000 train_loss: 58.45165252685547 test_loss:89.24794006347656\n",
      "1922/3000 train_loss: 44.165008544921875 test_loss:92.14332580566406\n",
      "1923/3000 train_loss: 56.172428131103516 test_loss:87.35830688476562\n",
      "1924/3000 train_loss: 45.574623107910156 test_loss:101.35244750976562\n",
      "1925/3000 train_loss: 54.50043487548828 test_loss:88.41671752929688\n",
      "1926/3000 train_loss: 59.85525131225586 test_loss:89.53954315185547\n",
      "1927/3000 train_loss: 59.34359359741211 test_loss:92.55084228515625\n",
      "1928/3000 train_loss: 54.979736328125 test_loss:88.92031860351562\n",
      "1929/3000 train_loss: 49.697853088378906 test_loss:88.55897521972656\n",
      "1930/3000 train_loss: 41.94377517700195 test_loss:87.37480163574219\n",
      "1931/3000 train_loss: 46.78086471557617 test_loss:88.57281494140625\n",
      "1932/3000 train_loss: 48.04155349731445 test_loss:88.5826644897461\n",
      "1933/3000 train_loss: 48.86479187011719 test_loss:87.12689208984375\n",
      "1934/3000 train_loss: 51.69574737548828 test_loss:86.10371398925781\n",
      "1935/3000 train_loss: 51.44404220581055 test_loss:86.22340393066406\n",
      "1936/3000 train_loss: 48.07669448852539 test_loss:86.61674499511719\n",
      "1937/3000 train_loss: 62.55597686767578 test_loss:89.87654113769531\n",
      "1938/3000 train_loss: 52.7461051940918 test_loss:85.14224243164062\n",
      "1939/3000 train_loss: 47.14177322387695 test_loss:87.59941101074219\n",
      "1940/3000 train_loss: 52.67875671386719 test_loss:89.76631927490234\n",
      "1941/3000 train_loss: 55.22431564331055 test_loss:86.6219482421875\n",
      "1942/3000 train_loss: 48.79218673706055 test_loss:89.78277587890625\n",
      "1943/3000 train_loss: 57.12955093383789 test_loss:84.1333236694336\n",
      "1944/3000 train_loss: 47.93937683105469 test_loss:84.93270111083984\n",
      "1945/3000 train_loss: 49.00161361694336 test_loss:88.3390121459961\n",
      "1946/3000 train_loss: 50.78416442871094 test_loss:86.99070739746094\n",
      "1947/3000 train_loss: 43.39766311645508 test_loss:86.73336791992188\n",
      "1948/3000 train_loss: 53.308753967285156 test_loss:89.30583190917969\n",
      "1949/3000 train_loss: 58.85625076293945 test_loss:90.19904327392578\n",
      "1950/3000 train_loss: 49.103477478027344 test_loss:87.48436737060547\n",
      "1951/3000 train_loss: 60.5237922668457 test_loss:87.92231750488281\n",
      "1952/3000 train_loss: 50.67325210571289 test_loss:99.34681701660156\n",
      "1953/3000 train_loss: 50.826805114746094 test_loss:91.16622161865234\n",
      "1954/3000 train_loss: 48.98768615722656 test_loss:92.97122192382812\n",
      "1955/3000 train_loss: 48.2292366027832 test_loss:90.77285766601562\n",
      "1956/3000 train_loss: 55.167362213134766 test_loss:86.72991180419922\n",
      "1957/3000 train_loss: 52.683509826660156 test_loss:89.48648071289062\n",
      "1958/3000 train_loss: 49.037410736083984 test_loss:90.15574645996094\n",
      "1959/3000 train_loss: 50.06890869140625 test_loss:85.48530578613281\n",
      "1960/3000 train_loss: 44.61104965209961 test_loss:90.41719055175781\n",
      "1961/3000 train_loss: 43.077857971191406 test_loss:88.75265502929688\n",
      "1962/3000 train_loss: 44.91011047363281 test_loss:88.40255737304688\n",
      "1963/3000 train_loss: 52.54875946044922 test_loss:88.58367919921875\n",
      "1964/3000 train_loss: 42.30402755737305 test_loss:90.03108215332031\n",
      "1965/3000 train_loss: 67.7498779296875 test_loss:90.03951263427734\n",
      "1966/3000 train_loss: 43.1816520690918 test_loss:87.38062286376953\n",
      "1967/3000 train_loss: 51.980995178222656 test_loss:93.37033081054688\n",
      "1968/3000 train_loss: 48.443729400634766 test_loss:90.52932739257812\n",
      "1969/3000 train_loss: 56.147071838378906 test_loss:88.42387390136719\n",
      "1970/3000 train_loss: 52.35618591308594 test_loss:91.00459289550781\n",
      "1971/3000 train_loss: 53.310752868652344 test_loss:89.24264526367188\n",
      "1972/3000 train_loss: 53.44086456298828 test_loss:90.6489028930664\n",
      "1973/3000 train_loss: 50.234092712402344 test_loss:93.64163970947266\n",
      "1974/3000 train_loss: 49.77765655517578 test_loss:91.84886169433594\n",
      "1975/3000 train_loss: 47.38105392456055 test_loss:91.2881851196289\n",
      "1976/3000 train_loss: 53.914154052734375 test_loss:88.95970153808594\n",
      "1977/3000 train_loss: 46.830047607421875 test_loss:89.49191284179688\n",
      "1978/3000 train_loss: 51.556880950927734 test_loss:88.79632568359375\n",
      "1979/3000 train_loss: 47.06721878051758 test_loss:92.04219055175781\n",
      "1980/3000 train_loss: 56.52986145019531 test_loss:86.68806457519531\n",
      "1981/3000 train_loss: 54.48995590209961 test_loss:88.63627624511719\n",
      "1982/3000 train_loss: 49.38758087158203 test_loss:94.73612976074219\n",
      "1983/3000 train_loss: 52.53611373901367 test_loss:92.8807373046875\n",
      "1984/3000 train_loss: 54.531982421875 test_loss:89.39967346191406\n",
      "1985/3000 train_loss: 49.96726608276367 test_loss:87.03304290771484\n",
      "1986/3000 train_loss: 51.9055290222168 test_loss:88.57072448730469\n",
      "1987/3000 train_loss: 53.617916107177734 test_loss:88.10348510742188\n",
      "1988/3000 train_loss: 60.820377349853516 test_loss:91.97803497314453\n",
      "1989/3000 train_loss: 48.07652282714844 test_loss:89.88631439208984\n",
      "1990/3000 train_loss: 52.41195297241211 test_loss:92.08318328857422\n",
      "1991/3000 train_loss: 57.002532958984375 test_loss:91.2843017578125\n",
      "1992/3000 train_loss: 46.2035026550293 test_loss:92.19440460205078\n",
      "1993/3000 train_loss: 54.8643798828125 test_loss:90.83767700195312\n",
      "1994/3000 train_loss: 52.937931060791016 test_loss:90.91141510009766\n",
      "1995/3000 train_loss: 54.8812255859375 test_loss:88.36769104003906\n",
      "1996/3000 train_loss: 48.230690002441406 test_loss:91.48538208007812\n",
      "1997/3000 train_loss: 52.45337677001953 test_loss:86.02999877929688\n",
      "1998/3000 train_loss: 64.21941375732422 test_loss:91.1804428100586\n",
      "1999/3000 train_loss: 46.822242736816406 test_loss:87.8789291381836\n",
      "2000/3000 train_loss: 42.41103744506836 test_loss:88.62358093261719\n",
      "2001/3000 train_loss: 50.34760665893555 test_loss:90.23041534423828\n",
      "2002/3000 train_loss: 49.83433151245117 test_loss:91.21157836914062\n",
      "2003/3000 train_loss: 51.084068298339844 test_loss:88.84000396728516\n",
      "2004/3000 train_loss: 48.22078323364258 test_loss:90.55677032470703\n",
      "2005/3000 train_loss: 59.63195037841797 test_loss:89.89942169189453\n",
      "2006/3000 train_loss: 51.011966705322266 test_loss:90.8974609375\n",
      "2007/3000 train_loss: 53.04399871826172 test_loss:86.62760925292969\n",
      "2008/3000 train_loss: 44.06001663208008 test_loss:90.1054458618164\n",
      "2009/3000 train_loss: 51.910335540771484 test_loss:90.20230102539062\n",
      "2010/3000 train_loss: 49.377017974853516 test_loss:85.86250305175781\n",
      "2011/3000 train_loss: 53.43857955932617 test_loss:88.20413970947266\n",
      "2012/3000 train_loss: 53.42802810668945 test_loss:85.9211196899414\n",
      "2013/3000 train_loss: 48.72565460205078 test_loss:85.7960433959961\n",
      "2014/3000 train_loss: 52.01565933227539 test_loss:89.98212432861328\n",
      "2015/3000 train_loss: 42.64312744140625 test_loss:87.31065368652344\n",
      "2016/3000 train_loss: 42.2640266418457 test_loss:92.3025131225586\n",
      "2017/3000 train_loss: 44.267250061035156 test_loss:88.84365844726562\n",
      "2018/3000 train_loss: 45.79548263549805 test_loss:86.59978485107422\n",
      "2019/3000 train_loss: 45.792842864990234 test_loss:89.4549331665039\n",
      "2020/3000 train_loss: 47.37340545654297 test_loss:89.75941467285156\n",
      "2021/3000 train_loss: 66.86839294433594 test_loss:92.92521667480469\n",
      "2022/3000 train_loss: 53.67109680175781 test_loss:94.99876403808594\n",
      "2023/3000 train_loss: 44.4525260925293 test_loss:95.38589477539062\n",
      "2024/3000 train_loss: 51.87760925292969 test_loss:92.9216079711914\n",
      "2025/3000 train_loss: 47.65367889404297 test_loss:95.40576171875\n",
      "2026/3000 train_loss: 47.50250244140625 test_loss:87.73528289794922\n",
      "2027/3000 train_loss: 48.50687026977539 test_loss:89.4969253540039\n",
      "2028/3000 train_loss: 48.4823112487793 test_loss:87.80789947509766\n",
      "2029/3000 train_loss: 51.39726257324219 test_loss:90.08773040771484\n",
      "2030/3000 train_loss: 47.08555603027344 test_loss:93.01902770996094\n",
      "2031/3000 train_loss: 47.71302795410156 test_loss:85.6785888671875\n",
      "2032/3000 train_loss: 49.53362274169922 test_loss:85.83369445800781\n",
      "2033/3000 train_loss: 47.542449951171875 test_loss:86.35233306884766\n",
      "2034/3000 train_loss: 46.008453369140625 test_loss:82.65921783447266\n",
      "2035/3000 train_loss: 48.583702087402344 test_loss:86.31608581542969\n",
      "2036/3000 train_loss: 50.12909698486328 test_loss:81.37820434570312\n",
      "2037/3000 train_loss: 46.326576232910156 test_loss:81.84745025634766\n",
      "2038/3000 train_loss: 40.17458724975586 test_loss:86.80908203125\n",
      "2039/3000 train_loss: 43.71406555175781 test_loss:85.11016845703125\n",
      "2040/3000 train_loss: 52.05397415161133 test_loss:85.81134033203125\n",
      "2041/3000 train_loss: 51.558868408203125 test_loss:99.06707763671875\n",
      "2042/3000 train_loss: 49.02985382080078 test_loss:90.26222229003906\n",
      "2043/3000 train_loss: 53.37068176269531 test_loss:86.01508331298828\n",
      "2044/3000 train_loss: 45.819950103759766 test_loss:97.1331787109375\n",
      "2045/3000 train_loss: 52.43034362792969 test_loss:97.05265808105469\n",
      "2046/3000 train_loss: 56.83707809448242 test_loss:86.71231079101562\n",
      "2047/3000 train_loss: 48.72595977783203 test_loss:88.72923278808594\n",
      "2048/3000 train_loss: 51.724945068359375 test_loss:82.37720489501953\n",
      "2049/3000 train_loss: 46.37757110595703 test_loss:83.83487701416016\n",
      "2050/3000 train_loss: 52.91346740722656 test_loss:86.21009826660156\n",
      "2051/3000 train_loss: 48.68073272705078 test_loss:85.80282592773438\n",
      "2052/3000 train_loss: 59.98224639892578 test_loss:91.54196166992188\n",
      "2053/3000 train_loss: 47.08849334716797 test_loss:83.14694213867188\n",
      "2054/3000 train_loss: 50.95989990234375 test_loss:87.10816955566406\n",
      "2055/3000 train_loss: 55.99509048461914 test_loss:89.08332824707031\n",
      "2056/3000 train_loss: 41.86971664428711 test_loss:88.73323822021484\n",
      "2057/3000 train_loss: 50.18272399902344 test_loss:85.12200927734375\n",
      "2058/3000 train_loss: 54.56825637817383 test_loss:85.79631805419922\n",
      "2059/3000 train_loss: 47.78373718261719 test_loss:86.63386535644531\n",
      "2060/3000 train_loss: 45.03228759765625 test_loss:87.87088012695312\n",
      "2061/3000 train_loss: 41.53862762451172 test_loss:86.06028747558594\n",
      "2062/3000 train_loss: 51.6953125 test_loss:85.37538146972656\n",
      "2063/3000 train_loss: 51.68549728393555 test_loss:83.0402603149414\n",
      "2064/3000 train_loss: 46.05060577392578 test_loss:85.25181579589844\n",
      "2065/3000 train_loss: 42.095947265625 test_loss:91.60645294189453\n",
      "2066/3000 train_loss: 48.73595428466797 test_loss:89.0512924194336\n",
      "2067/3000 train_loss: 48.33570098876953 test_loss:87.08290100097656\n",
      "2068/3000 train_loss: 49.867210388183594 test_loss:83.25188446044922\n",
      "2069/3000 train_loss: 44.067501068115234 test_loss:86.94821166992188\n",
      "2070/3000 train_loss: 58.84452819824219 test_loss:92.17633056640625\n",
      "2071/3000 train_loss: 49.68906784057617 test_loss:87.14500427246094\n",
      "2072/3000 train_loss: 50.7913818359375 test_loss:87.1966781616211\n",
      "2073/3000 train_loss: 46.79356002807617 test_loss:87.87873840332031\n",
      "2074/3000 train_loss: 52.658546447753906 test_loss:86.26752471923828\n",
      "2075/3000 train_loss: 46.653411865234375 test_loss:86.65666961669922\n",
      "2076/3000 train_loss: 52.86064529418945 test_loss:89.55452728271484\n",
      "2077/3000 train_loss: 52.3271484375 test_loss:94.59895324707031\n",
      "2078/3000 train_loss: 60.34367370605469 test_loss:92.0589828491211\n",
      "2079/3000 train_loss: 43.75315856933594 test_loss:83.00401306152344\n",
      "2080/3000 train_loss: 54.89412307739258 test_loss:92.08683776855469\n",
      "2081/3000 train_loss: 50.9334602355957 test_loss:90.455078125\n",
      "2082/3000 train_loss: 48.34310531616211 test_loss:90.54623413085938\n",
      "2083/3000 train_loss: 54.20476531982422 test_loss:84.9687728881836\n",
      "2084/3000 train_loss: 43.438453674316406 test_loss:87.4635238647461\n",
      "2085/3000 train_loss: 48.16633224487305 test_loss:83.69503784179688\n",
      "2086/3000 train_loss: 43.92100524902344 test_loss:80.49685668945312\n",
      "2087/3000 train_loss: 57.970577239990234 test_loss:86.90736389160156\n",
      "2088/3000 train_loss: 52.04283905029297 test_loss:85.91239929199219\n",
      "2089/3000 train_loss: 48.608009338378906 test_loss:82.64424133300781\n",
      "2090/3000 train_loss: 50.13398742675781 test_loss:90.06060791015625\n",
      "2091/3000 train_loss: 47.92790603637695 test_loss:86.16810607910156\n",
      "2092/3000 train_loss: 53.79801940917969 test_loss:87.22500610351562\n",
      "2093/3000 train_loss: 48.072940826416016 test_loss:83.02339172363281\n",
      "2094/3000 train_loss: 57.57335662841797 test_loss:87.6670913696289\n",
      "2095/3000 train_loss: 56.59899139404297 test_loss:90.08341979980469\n",
      "2096/3000 train_loss: 57.45741271972656 test_loss:94.08735656738281\n",
      "2097/3000 train_loss: 57.31574249267578 test_loss:86.6843490600586\n",
      "2098/3000 train_loss: 52.66233825683594 test_loss:85.61534118652344\n",
      "2099/3000 train_loss: 54.89482116699219 test_loss:87.76481628417969\n",
      "2100/3000 train_loss: 53.278995513916016 test_loss:86.7041244506836\n",
      "2101/3000 train_loss: 48.83830261230469 test_loss:85.85166931152344\n",
      "2102/3000 train_loss: 56.78858947753906 test_loss:85.39839935302734\n",
      "2103/3000 train_loss: 40.457759857177734 test_loss:79.55305480957031\n",
      "2104/3000 train_loss: 44.66499328613281 test_loss:78.80718994140625\n",
      "2105/3000 train_loss: 46.50493621826172 test_loss:80.53620910644531\n",
      "2106/3000 train_loss: 52.42176818847656 test_loss:77.41057586669922\n",
      "2107/3000 train_loss: 49.57328796386719 test_loss:82.06998443603516\n",
      "2108/3000 train_loss: 43.90105438232422 test_loss:82.10338592529297\n",
      "2109/3000 train_loss: 59.214378356933594 test_loss:84.98070526123047\n",
      "2110/3000 train_loss: 55.23868942260742 test_loss:78.68313598632812\n",
      "2111/3000 train_loss: 49.74557113647461 test_loss:78.47065734863281\n",
      "2112/3000 train_loss: 51.643089294433594 test_loss:82.38725280761719\n",
      "2113/3000 train_loss: 44.58070373535156 test_loss:80.76344299316406\n",
      "2114/3000 train_loss: 43.31220245361328 test_loss:80.64663696289062\n",
      "2115/3000 train_loss: 38.67210388183594 test_loss:82.49264526367188\n",
      "2116/3000 train_loss: 49.60548400878906 test_loss:82.51472473144531\n",
      "2117/3000 train_loss: 49.772972106933594 test_loss:93.47583770751953\n",
      "2118/3000 train_loss: 44.44512939453125 test_loss:83.53453063964844\n",
      "2119/3000 train_loss: 46.000396728515625 test_loss:82.16445922851562\n",
      "2120/3000 train_loss: 46.855873107910156 test_loss:84.38877868652344\n",
      "2121/3000 train_loss: 51.39179229736328 test_loss:84.24494171142578\n",
      "2122/3000 train_loss: 46.290809631347656 test_loss:82.44561767578125\n",
      "2123/3000 train_loss: 49.29942321777344 test_loss:83.0271224975586\n",
      "2124/3000 train_loss: 47.84344482421875 test_loss:83.1238784790039\n",
      "2125/3000 train_loss: 49.94866180419922 test_loss:86.62981414794922\n",
      "2126/3000 train_loss: 44.323448181152344 test_loss:83.79141998291016\n",
      "2127/3000 train_loss: 47.15845489501953 test_loss:83.14180755615234\n",
      "2128/3000 train_loss: 53.34950637817383 test_loss:83.27936553955078\n",
      "2129/3000 train_loss: 48.73929214477539 test_loss:85.427490234375\n",
      "2130/3000 train_loss: 52.118629455566406 test_loss:84.09968566894531\n",
      "2131/3000 train_loss: 44.183902740478516 test_loss:84.63098907470703\n",
      "2132/3000 train_loss: 48.36814880371094 test_loss:83.11177825927734\n",
      "2133/3000 train_loss: 44.16017150878906 test_loss:92.43431854248047\n",
      "2134/3000 train_loss: 46.73857879638672 test_loss:84.44053649902344\n",
      "2135/3000 train_loss: 45.233192443847656 test_loss:81.54898071289062\n",
      "2136/3000 train_loss: 45.73974609375 test_loss:83.0347671508789\n",
      "2137/3000 train_loss: 45.71091842651367 test_loss:89.17948913574219\n",
      "2138/3000 train_loss: 44.207454681396484 test_loss:88.926513671875\n",
      "2139/3000 train_loss: 47.119102478027344 test_loss:91.9439926147461\n",
      "2140/3000 train_loss: 50.27460479736328 test_loss:89.9498291015625\n",
      "2141/3000 train_loss: 44.7071647644043 test_loss:91.20668029785156\n",
      "2142/3000 train_loss: 45.496726989746094 test_loss:92.08197021484375\n",
      "2143/3000 train_loss: 57.69389343261719 test_loss:86.86558532714844\n",
      "2144/3000 train_loss: 46.277259826660156 test_loss:85.35067749023438\n",
      "2145/3000 train_loss: 81.31086730957031 test_loss:93.99658203125\n",
      "2146/3000 train_loss: 54.224395751953125 test_loss:92.76799011230469\n",
      "2147/3000 train_loss: 48.53179168701172 test_loss:87.60395812988281\n",
      "2148/3000 train_loss: 51.867332458496094 test_loss:86.00446319580078\n",
      "2149/3000 train_loss: 47.79835510253906 test_loss:87.05116271972656\n",
      "2150/3000 train_loss: 48.55526351928711 test_loss:88.32107543945312\n",
      "2151/3000 train_loss: 42.91996383666992 test_loss:88.0150146484375\n",
      "2152/3000 train_loss: 43.81571960449219 test_loss:88.84075927734375\n",
      "2153/3000 train_loss: 39.44514465332031 test_loss:84.89814758300781\n",
      "2154/3000 train_loss: 45.99599075317383 test_loss:90.56784057617188\n",
      "2155/3000 train_loss: 41.608131408691406 test_loss:82.29078674316406\n",
      "2156/3000 train_loss: 47.76324462890625 test_loss:83.78480529785156\n",
      "2157/3000 train_loss: 45.27687454223633 test_loss:87.13077545166016\n",
      "2158/3000 train_loss: 57.413902282714844 test_loss:83.22547149658203\n",
      "2159/3000 train_loss: 50.74076461791992 test_loss:90.83582305908203\n",
      "2160/3000 train_loss: 55.192466735839844 test_loss:85.99009704589844\n",
      "2161/3000 train_loss: 55.188961029052734 test_loss:82.30534362792969\n",
      "2162/3000 train_loss: 47.16135025024414 test_loss:87.29202270507812\n",
      "2163/3000 train_loss: 47.450408935546875 test_loss:86.82091522216797\n",
      "2164/3000 train_loss: 44.28762435913086 test_loss:82.63652801513672\n",
      "2165/3000 train_loss: 44.498355865478516 test_loss:88.54248809814453\n",
      "2166/3000 train_loss: 46.29890441894531 test_loss:87.84310913085938\n",
      "2167/3000 train_loss: 41.65454864501953 test_loss:88.59807586669922\n",
      "2168/3000 train_loss: 48.70079040527344 test_loss:91.85627746582031\n",
      "2169/3000 train_loss: 51.00420379638672 test_loss:88.58138275146484\n",
      "2170/3000 train_loss: 52.13729476928711 test_loss:90.46566772460938\n",
      "2171/3000 train_loss: 40.39940643310547 test_loss:92.5436782836914\n",
      "2172/3000 train_loss: 49.17472457885742 test_loss:89.98328399658203\n",
      "2173/3000 train_loss: 48.88713073730469 test_loss:86.3114013671875\n",
      "2174/3000 train_loss: 52.35007858276367 test_loss:90.36653137207031\n",
      "2175/3000 train_loss: 44.031803131103516 test_loss:84.85065460205078\n",
      "2176/3000 train_loss: 56.79290771484375 test_loss:92.80470275878906\n",
      "2177/3000 train_loss: 45.548160552978516 test_loss:86.81033325195312\n",
      "2178/3000 train_loss: 61.966087341308594 test_loss:82.0146484375\n",
      "2179/3000 train_loss: 73.25777435302734 test_loss:92.87065124511719\n",
      "2180/3000 train_loss: 51.16661071777344 test_loss:82.75407409667969\n",
      "2181/3000 train_loss: 48.985992431640625 test_loss:83.85423278808594\n",
      "2182/3000 train_loss: 52.612030029296875 test_loss:81.87078857421875\n",
      "2183/3000 train_loss: 48.162193298339844 test_loss:82.59070587158203\n",
      "2184/3000 train_loss: 43.71924591064453 test_loss:91.35218811035156\n",
      "2185/3000 train_loss: 55.70250701904297 test_loss:85.95907592773438\n",
      "2186/3000 train_loss: 41.0262336730957 test_loss:83.53562927246094\n",
      "2187/3000 train_loss: 47.78089904785156 test_loss:83.33457946777344\n",
      "2188/3000 train_loss: 46.47715377807617 test_loss:85.092529296875\n",
      "2189/3000 train_loss: 50.15137481689453 test_loss:83.84169006347656\n",
      "2190/3000 train_loss: 41.78208923339844 test_loss:77.16450500488281\n",
      "2191/3000 train_loss: 40.6131477355957 test_loss:81.72322845458984\n",
      "2192/3000 train_loss: 54.82855987548828 test_loss:81.92064666748047\n",
      "2193/3000 train_loss: 46.59299850463867 test_loss:80.77143859863281\n",
      "2194/3000 train_loss: 38.730804443359375 test_loss:83.13868713378906\n",
      "2195/3000 train_loss: 46.38043975830078 test_loss:84.62620544433594\n",
      "2196/3000 train_loss: 49.16266632080078 test_loss:79.37406921386719\n",
      "2197/3000 train_loss: 42.495994567871094 test_loss:81.66092681884766\n",
      "2198/3000 train_loss: 53.05575180053711 test_loss:80.55648803710938\n",
      "2199/3000 train_loss: 53.82181930541992 test_loss:83.05706787109375\n",
      "2200/3000 train_loss: 49.06478500366211 test_loss:80.28532409667969\n",
      "2201/3000 train_loss: 46.46540069580078 test_loss:79.49054718017578\n",
      "2202/3000 train_loss: 42.54348373413086 test_loss:85.0661849975586\n",
      "2203/3000 train_loss: 48.559181213378906 test_loss:78.83714294433594\n",
      "2204/3000 train_loss: 46.08122253417969 test_loss:80.01992797851562\n",
      "2205/3000 train_loss: 48.13360595703125 test_loss:83.20355987548828\n",
      "2206/3000 train_loss: 63.094764709472656 test_loss:81.349365234375\n",
      "2207/3000 train_loss: 43.337467193603516 test_loss:85.08920288085938\n",
      "2208/3000 train_loss: 39.66086196899414 test_loss:83.1220474243164\n",
      "2209/3000 train_loss: 41.85968017578125 test_loss:87.05160522460938\n",
      "2210/3000 train_loss: 53.034324645996094 test_loss:86.36509704589844\n",
      "2211/3000 train_loss: 38.31968688964844 test_loss:85.14871978759766\n",
      "2212/3000 train_loss: 43.80191421508789 test_loss:89.87898254394531\n",
      "2213/3000 train_loss: 48.71059036254883 test_loss:84.41094970703125\n",
      "2214/3000 train_loss: 57.521446228027344 test_loss:93.31071472167969\n",
      "2215/3000 train_loss: 46.44809341430664 test_loss:85.57945251464844\n",
      "2216/3000 train_loss: 43.83721160888672 test_loss:88.36748504638672\n",
      "2217/3000 train_loss: 42.223167419433594 test_loss:83.73808288574219\n",
      "2218/3000 train_loss: 39.76144027709961 test_loss:85.55558013916016\n",
      "2219/3000 train_loss: 50.46113204956055 test_loss:84.1471939086914\n",
      "2220/3000 train_loss: 44.43876647949219 test_loss:81.53142547607422\n",
      "2221/3000 train_loss: 51.079959869384766 test_loss:81.17233276367188\n",
      "2222/3000 train_loss: 54.399559020996094 test_loss:82.85401916503906\n",
      "2223/3000 train_loss: 42.65740203857422 test_loss:79.89351654052734\n",
      "2224/3000 train_loss: 49.97990036010742 test_loss:79.58088684082031\n",
      "2225/3000 train_loss: 44.83346939086914 test_loss:81.09834289550781\n",
      "2226/3000 train_loss: 44.33812713623047 test_loss:83.42012023925781\n",
      "2227/3000 train_loss: 41.709171295166016 test_loss:87.24076843261719\n",
      "2228/3000 train_loss: 43.77187728881836 test_loss:82.23319244384766\n",
      "2229/3000 train_loss: 39.2465934753418 test_loss:84.54769134521484\n",
      "2230/3000 train_loss: 45.37504577636719 test_loss:79.48867797851562\n",
      "2231/3000 train_loss: 45.92129898071289 test_loss:84.45849609375\n",
      "2232/3000 train_loss: 38.05706787109375 test_loss:80.20401763916016\n",
      "2233/3000 train_loss: 39.3846549987793 test_loss:79.63699340820312\n",
      "2234/3000 train_loss: 42.541603088378906 test_loss:84.68917846679688\n",
      "2235/3000 train_loss: 48.647438049316406 test_loss:79.1131362915039\n",
      "2236/3000 train_loss: 46.90458679199219 test_loss:80.1314926147461\n",
      "2237/3000 train_loss: 41.76025390625 test_loss:80.51174926757812\n",
      "2238/3000 train_loss: 44.68732833862305 test_loss:85.53656005859375\n",
      "2239/3000 train_loss: 48.131439208984375 test_loss:79.0965805053711\n",
      "2240/3000 train_loss: 39.59486389160156 test_loss:82.0707015991211\n",
      "2241/3000 train_loss: 43.56924819946289 test_loss:81.28453063964844\n",
      "2242/3000 train_loss: 45.371055603027344 test_loss:80.81333923339844\n",
      "2243/3000 train_loss: 47.41560363769531 test_loss:80.21733093261719\n",
      "2244/3000 train_loss: 42.497440338134766 test_loss:82.22491455078125\n",
      "2245/3000 train_loss: 38.10029983520508 test_loss:78.5545654296875\n",
      "2246/3000 train_loss: 52.4265251159668 test_loss:78.75338745117188\n",
      "2247/3000 train_loss: 41.09254837036133 test_loss:82.70149230957031\n",
      "2248/3000 train_loss: 56.81842041015625 test_loss:77.42274475097656\n",
      "2249/3000 train_loss: 42.71320724487305 test_loss:78.93843841552734\n",
      "2250/3000 train_loss: 42.942962646484375 test_loss:80.52527618408203\n",
      "2251/3000 train_loss: 44.755977630615234 test_loss:78.58240509033203\n",
      "2252/3000 train_loss: 52.73426818847656 test_loss:89.38032531738281\n",
      "2253/3000 train_loss: 52.79154968261719 test_loss:74.64073181152344\n",
      "2254/3000 train_loss: 48.37732696533203 test_loss:80.39015197753906\n",
      "2255/3000 train_loss: 44.764869689941406 test_loss:76.81055450439453\n",
      "2256/3000 train_loss: 39.81727600097656 test_loss:82.40054321289062\n",
      "2257/3000 train_loss: 48.22039031982422 test_loss:80.76510620117188\n",
      "2258/3000 train_loss: 41.106971740722656 test_loss:84.54449462890625\n",
      "2259/3000 train_loss: 52.65141296386719 test_loss:79.68827819824219\n",
      "2260/3000 train_loss: 46.884708404541016 test_loss:83.03189849853516\n",
      "2261/3000 train_loss: 44.98936462402344 test_loss:82.32563781738281\n",
      "2262/3000 train_loss: 55.31721115112305 test_loss:81.82256317138672\n",
      "2263/3000 train_loss: 45.21132278442383 test_loss:76.5986557006836\n",
      "2264/3000 train_loss: 41.616004943847656 test_loss:80.78948974609375\n",
      "2265/3000 train_loss: 35.76079559326172 test_loss:77.77259826660156\n",
      "2266/3000 train_loss: 41.17835998535156 test_loss:78.53974914550781\n",
      "2267/3000 train_loss: 42.70817184448242 test_loss:77.35968017578125\n",
      "2268/3000 train_loss: 38.34668731689453 test_loss:80.43244171142578\n",
      "2269/3000 train_loss: 36.4589729309082 test_loss:80.59934997558594\n",
      "2270/3000 train_loss: 41.23249816894531 test_loss:77.78177642822266\n",
      "2271/3000 train_loss: 40.61103820800781 test_loss:78.39125061035156\n",
      "2272/3000 train_loss: 35.749942779541016 test_loss:78.53170013427734\n",
      "2273/3000 train_loss: 50.472557067871094 test_loss:76.07331848144531\n",
      "2274/3000 train_loss: 62.528297424316406 test_loss:80.89705657958984\n",
      "2275/3000 train_loss: 57.001163482666016 test_loss:78.8222885131836\n",
      "2276/3000 train_loss: 49.807926177978516 test_loss:79.2932357788086\n",
      "2277/3000 train_loss: 47.2044677734375 test_loss:78.95809936523438\n",
      "2278/3000 train_loss: 47.719337463378906 test_loss:76.68666076660156\n",
      "2279/3000 train_loss: 43.90594482421875 test_loss:77.76765441894531\n",
      "2280/3000 train_loss: 41.686126708984375 test_loss:77.7181396484375\n",
      "2281/3000 train_loss: 45.67448425292969 test_loss:79.82508850097656\n",
      "2282/3000 train_loss: 58.96540451049805 test_loss:82.3028335571289\n",
      "2283/3000 train_loss: 49.280635833740234 test_loss:80.22769927978516\n",
      "2284/3000 train_loss: 46.95650100708008 test_loss:77.75627136230469\n",
      "2285/3000 train_loss: 42.846221923828125 test_loss:80.88721466064453\n",
      "2286/3000 train_loss: 51.819454193115234 test_loss:79.66122436523438\n",
      "2287/3000 train_loss: 42.846824645996094 test_loss:82.11873626708984\n",
      "2288/3000 train_loss: 47.05324172973633 test_loss:79.97607421875\n",
      "2289/3000 train_loss: 59.01001739501953 test_loss:78.26250457763672\n",
      "2290/3000 train_loss: 41.77915573120117 test_loss:82.13666534423828\n",
      "2291/3000 train_loss: 46.718666076660156 test_loss:83.5814208984375\n",
      "2292/3000 train_loss: 44.772071838378906 test_loss:79.47090148925781\n",
      "2293/3000 train_loss: 45.94650650024414 test_loss:78.47802734375\n",
      "2294/3000 train_loss: 45.81904983520508 test_loss:78.23989868164062\n",
      "2295/3000 train_loss: 36.05794143676758 test_loss:81.21110534667969\n",
      "2296/3000 train_loss: 42.093414306640625 test_loss:80.73686218261719\n",
      "2297/3000 train_loss: 48.82392120361328 test_loss:81.23408508300781\n",
      "2298/3000 train_loss: 50.291717529296875 test_loss:77.60074615478516\n",
      "2299/3000 train_loss: 51.71564865112305 test_loss:81.50399780273438\n",
      "2300/3000 train_loss: 44.458946228027344 test_loss:77.5500717163086\n",
      "2301/3000 train_loss: 47.02725601196289 test_loss:81.95509338378906\n",
      "2302/3000 train_loss: 45.35798263549805 test_loss:81.03416442871094\n",
      "2303/3000 train_loss: 47.341453552246094 test_loss:83.39675903320312\n",
      "2304/3000 train_loss: 42.5399284362793 test_loss:84.76565551757812\n",
      "2305/3000 train_loss: 43.40332794189453 test_loss:83.49492645263672\n",
      "2306/3000 train_loss: 39.41610336303711 test_loss:80.50054931640625\n",
      "2307/3000 train_loss: 46.4193229675293 test_loss:83.09437561035156\n",
      "2308/3000 train_loss: 40.43497085571289 test_loss:86.55482482910156\n",
      "2309/3000 train_loss: 39.6201171875 test_loss:82.48005676269531\n",
      "2310/3000 train_loss: 100.39009094238281 test_loss:92.30722045898438\n",
      "2311/3000 train_loss: 68.36492919921875 test_loss:97.16110229492188\n",
      "2312/3000 train_loss: 61.85694122314453 test_loss:107.37054443359375\n",
      "2313/3000 train_loss: 53.24787139892578 test_loss:87.11549377441406\n",
      "2314/3000 train_loss: 55.18822479248047 test_loss:89.53893280029297\n",
      "2315/3000 train_loss: 52.051170349121094 test_loss:93.32963562011719\n",
      "2316/3000 train_loss: 44.24342727661133 test_loss:85.38269805908203\n",
      "2317/3000 train_loss: 43.707000732421875 test_loss:82.84713745117188\n",
      "2318/3000 train_loss: 56.71394729614258 test_loss:87.92340087890625\n",
      "2319/3000 train_loss: 49.98282241821289 test_loss:88.32318115234375\n",
      "2320/3000 train_loss: 40.487342834472656 test_loss:83.42867279052734\n",
      "2321/3000 train_loss: 46.388160705566406 test_loss:83.58576965332031\n",
      "2322/3000 train_loss: 42.667755126953125 test_loss:78.44530487060547\n",
      "2323/3000 train_loss: 47.34382629394531 test_loss:80.82133483886719\n",
      "2324/3000 train_loss: 38.19481658935547 test_loss:81.30921936035156\n",
      "2325/3000 train_loss: 41.814823150634766 test_loss:85.88179016113281\n",
      "2326/3000 train_loss: 43.11741256713867 test_loss:79.3218765258789\n",
      "2327/3000 train_loss: 47.61227035522461 test_loss:80.7988510131836\n",
      "2328/3000 train_loss: 45.00718688964844 test_loss:75.92406463623047\n",
      "2329/3000 train_loss: 41.59563446044922 test_loss:76.04339599609375\n",
      "2330/3000 train_loss: 46.22682571411133 test_loss:79.60736846923828\n",
      "2331/3000 train_loss: 40.38982391357422 test_loss:78.23898315429688\n",
      "2332/3000 train_loss: 43.91855239868164 test_loss:85.92153930664062\n",
      "2333/3000 train_loss: 46.03328323364258 test_loss:76.27062225341797\n",
      "2334/3000 train_loss: 54.39763641357422 test_loss:80.11090087890625\n",
      "2335/3000 train_loss: 36.58118438720703 test_loss:80.75103759765625\n",
      "2336/3000 train_loss: 41.851348876953125 test_loss:85.1810302734375\n",
      "2337/3000 train_loss: 45.01329803466797 test_loss:75.6692886352539\n",
      "2338/3000 train_loss: 44.860572814941406 test_loss:80.40522766113281\n",
      "2339/3000 train_loss: 47.52251052856445 test_loss:74.79510498046875\n",
      "2340/3000 train_loss: 39.39761734008789 test_loss:77.71255493164062\n",
      "2341/3000 train_loss: 40.85063934326172 test_loss:79.55622100830078\n",
      "2342/3000 train_loss: 38.19664764404297 test_loss:74.9037094116211\n",
      "2343/3000 train_loss: 42.55601119995117 test_loss:77.26701354980469\n",
      "2344/3000 train_loss: 39.55036544799805 test_loss:84.43696594238281\n",
      "2345/3000 train_loss: 42.62061309814453 test_loss:80.6036376953125\n",
      "2346/3000 train_loss: 55.365074157714844 test_loss:73.89089965820312\n",
      "2347/3000 train_loss: 49.63482666015625 test_loss:75.51870727539062\n",
      "2348/3000 train_loss: 37.624393463134766 test_loss:76.11679077148438\n",
      "2349/3000 train_loss: 42.78208923339844 test_loss:82.1453628540039\n",
      "2350/3000 train_loss: 53.681339263916016 test_loss:73.53118896484375\n",
      "2351/3000 train_loss: 44.30051803588867 test_loss:78.3211441040039\n",
      "2352/3000 train_loss: 50.91975021362305 test_loss:83.97274780273438\n",
      "2353/3000 train_loss: 40.48130798339844 test_loss:79.25146484375\n",
      "2354/3000 train_loss: 62.50939178466797 test_loss:86.60511779785156\n",
      "2355/3000 train_loss: 48.840721130371094 test_loss:77.19837188720703\n",
      "2356/3000 train_loss: 56.83201599121094 test_loss:81.03448486328125\n",
      "2357/3000 train_loss: 48.808963775634766 test_loss:95.79256439208984\n",
      "2358/3000 train_loss: 42.927772521972656 test_loss:84.01376342773438\n",
      "2359/3000 train_loss: 82.36315155029297 test_loss:81.23011779785156\n",
      "2360/3000 train_loss: 60.647064208984375 test_loss:85.94380950927734\n",
      "2361/3000 train_loss: 49.574771881103516 test_loss:83.84882354736328\n",
      "2362/3000 train_loss: 57.920013427734375 test_loss:77.76272583007812\n",
      "2363/3000 train_loss: 59.40721893310547 test_loss:82.04464721679688\n",
      "2364/3000 train_loss: 60.429386138916016 test_loss:84.71843719482422\n",
      "2365/3000 train_loss: 45.16409683227539 test_loss:80.54277038574219\n",
      "2366/3000 train_loss: 49.75328826904297 test_loss:86.73861694335938\n",
      "2367/3000 train_loss: 45.378318786621094 test_loss:86.5289306640625\n",
      "2368/3000 train_loss: 42.28648376464844 test_loss:85.27093505859375\n",
      "2369/3000 train_loss: 42.795265197753906 test_loss:79.241943359375\n",
      "2370/3000 train_loss: 51.77210235595703 test_loss:81.50299835205078\n",
      "2371/3000 train_loss: 45.69731521606445 test_loss:74.91337585449219\n",
      "2372/3000 train_loss: 41.991119384765625 test_loss:77.95036315917969\n",
      "2373/3000 train_loss: 42.48830795288086 test_loss:77.8960952758789\n",
      "2374/3000 train_loss: 40.46708297729492 test_loss:79.2265396118164\n",
      "2375/3000 train_loss: 50.89658737182617 test_loss:77.94424438476562\n",
      "2376/3000 train_loss: 47.298038482666016 test_loss:81.37889099121094\n",
      "2377/3000 train_loss: 48.36425018310547 test_loss:76.61762237548828\n",
      "2378/3000 train_loss: 50.828189849853516 test_loss:86.21443939208984\n",
      "2379/3000 train_loss: 39.190250396728516 test_loss:77.06658935546875\n",
      "2380/3000 train_loss: 61.270362854003906 test_loss:82.33132934570312\n",
      "2381/3000 train_loss: 47.665977478027344 test_loss:76.28578186035156\n",
      "2382/3000 train_loss: 49.57882308959961 test_loss:80.54119873046875\n",
      "2383/3000 train_loss: 42.16025924682617 test_loss:79.5875244140625\n",
      "2384/3000 train_loss: 52.14521789550781 test_loss:75.32959747314453\n",
      "2385/3000 train_loss: 40.34360885620117 test_loss:76.448486328125\n",
      "2386/3000 train_loss: 42.64031982421875 test_loss:75.5351333618164\n",
      "2387/3000 train_loss: 53.869163513183594 test_loss:76.89595031738281\n",
      "2388/3000 train_loss: 40.16001892089844 test_loss:75.59549713134766\n",
      "2389/3000 train_loss: 42.125770568847656 test_loss:80.46973419189453\n",
      "2390/3000 train_loss: 44.90705108642578 test_loss:79.43567657470703\n",
      "2391/3000 train_loss: 46.081443786621094 test_loss:74.6201171875\n",
      "2392/3000 train_loss: 40.73081970214844 test_loss:79.27285766601562\n",
      "2393/3000 train_loss: 44.13410949707031 test_loss:71.76319885253906\n",
      "2394/3000 train_loss: 48.822750091552734 test_loss:77.79067993164062\n",
      "2395/3000 train_loss: 38.14969253540039 test_loss:74.21485137939453\n",
      "2396/3000 train_loss: 56.01469421386719 test_loss:75.26211547851562\n",
      "2397/3000 train_loss: 41.9950065612793 test_loss:75.28372192382812\n",
      "2398/3000 train_loss: 45.84700393676758 test_loss:73.70340728759766\n",
      "2399/3000 train_loss: 39.41060256958008 test_loss:75.79752349853516\n",
      "2400/3000 train_loss: 46.04560089111328 test_loss:78.5479965209961\n",
      "2401/3000 train_loss: 42.894588470458984 test_loss:81.67206573486328\n",
      "2402/3000 train_loss: 55.14872360229492 test_loss:75.46458435058594\n",
      "2403/3000 train_loss: 39.894493103027344 test_loss:76.35401916503906\n",
      "2404/3000 train_loss: 43.826515197753906 test_loss:75.94503784179688\n",
      "2405/3000 train_loss: 47.45726776123047 test_loss:72.54356384277344\n",
      "2406/3000 train_loss: 40.930870056152344 test_loss:79.83338165283203\n",
      "2407/3000 train_loss: 41.630645751953125 test_loss:75.27530670166016\n",
      "2408/3000 train_loss: 43.7287483215332 test_loss:72.134521484375\n",
      "2409/3000 train_loss: 44.699241638183594 test_loss:76.02814483642578\n",
      "2410/3000 train_loss: 47.799652099609375 test_loss:79.71945190429688\n",
      "2411/3000 train_loss: 43.18168640136719 test_loss:79.83202362060547\n",
      "2412/3000 train_loss: 60.45358657836914 test_loss:81.62596130371094\n",
      "2413/3000 train_loss: 43.01442337036133 test_loss:80.35326385498047\n",
      "2414/3000 train_loss: 41.567203521728516 test_loss:78.18623352050781\n",
      "2415/3000 train_loss: 43.798545837402344 test_loss:75.95188903808594\n",
      "2416/3000 train_loss: 42.34605026245117 test_loss:75.13462829589844\n",
      "2417/3000 train_loss: 46.86629104614258 test_loss:71.03607940673828\n",
      "2418/3000 train_loss: 46.234832763671875 test_loss:80.87370300292969\n",
      "2419/3000 train_loss: 40.15153503417969 test_loss:73.61767578125\n",
      "2420/3000 train_loss: 44.929283142089844 test_loss:76.38709259033203\n",
      "2421/3000 train_loss: 39.972049713134766 test_loss:74.29779052734375\n",
      "2422/3000 train_loss: 37.78939437866211 test_loss:74.18392944335938\n",
      "2423/3000 train_loss: 43.25399398803711 test_loss:75.26383972167969\n",
      "2424/3000 train_loss: 39.18288803100586 test_loss:78.06312561035156\n",
      "2425/3000 train_loss: 39.608062744140625 test_loss:78.32339477539062\n",
      "2426/3000 train_loss: 51.22100830078125 test_loss:74.78506469726562\n",
      "2427/3000 train_loss: 40.35693359375 test_loss:73.49435424804688\n",
      "2428/3000 train_loss: 41.69712448120117 test_loss:72.71214294433594\n",
      "2429/3000 train_loss: 41.48641586303711 test_loss:74.81633758544922\n",
      "2430/3000 train_loss: 47.471038818359375 test_loss:75.2686996459961\n",
      "2431/3000 train_loss: 48.78392791748047 test_loss:75.8241958618164\n",
      "2432/3000 train_loss: 40.41679763793945 test_loss:78.99932098388672\n",
      "2433/3000 train_loss: 37.05387496948242 test_loss:80.54652404785156\n",
      "2434/3000 train_loss: 45.3287353515625 test_loss:82.16275024414062\n",
      "2435/3000 train_loss: 40.80350875854492 test_loss:75.27951049804688\n",
      "2436/3000 train_loss: 41.4560432434082 test_loss:81.28472900390625\n",
      "2437/3000 train_loss: 39.80262756347656 test_loss:77.79422760009766\n",
      "2438/3000 train_loss: 44.987388610839844 test_loss:77.95352172851562\n",
      "2439/3000 train_loss: 42.550758361816406 test_loss:77.74363708496094\n",
      "2440/3000 train_loss: 46.0325813293457 test_loss:75.6390151977539\n",
      "2441/3000 train_loss: 60.68433380126953 test_loss:82.94825744628906\n",
      "2442/3000 train_loss: 42.681026458740234 test_loss:79.3633041381836\n",
      "2443/3000 train_loss: 58.82815170288086 test_loss:81.70478820800781\n",
      "2444/3000 train_loss: 42.39099884033203 test_loss:82.52409362792969\n",
      "2445/3000 train_loss: 42.996612548828125 test_loss:81.06031799316406\n",
      "2446/3000 train_loss: 51.0492057800293 test_loss:79.49227905273438\n",
      "2447/3000 train_loss: 55.73872375488281 test_loss:79.58248901367188\n",
      "2448/3000 train_loss: 43.012725830078125 test_loss:81.93193817138672\n",
      "2449/3000 train_loss: 54.53858947753906 test_loss:79.03474426269531\n",
      "2450/3000 train_loss: 41.757293701171875 test_loss:83.14329528808594\n",
      "2451/3000 train_loss: 41.544586181640625 test_loss:78.17858123779297\n",
      "2452/3000 train_loss: 41.91353225708008 test_loss:80.18029022216797\n",
      "2453/3000 train_loss: 39.83441925048828 test_loss:78.41128540039062\n",
      "2454/3000 train_loss: 44.980560302734375 test_loss:77.08679962158203\n",
      "2455/3000 train_loss: 41.93873596191406 test_loss:82.98661804199219\n",
      "2456/3000 train_loss: 37.95627975463867 test_loss:83.22872161865234\n",
      "2457/3000 train_loss: 40.47071838378906 test_loss:89.36054992675781\n",
      "2458/3000 train_loss: 43.67679977416992 test_loss:80.63387298583984\n",
      "2459/3000 train_loss: 56.23997497558594 test_loss:83.80783081054688\n",
      "2460/3000 train_loss: 60.275516510009766 test_loss:77.51237487792969\n",
      "2461/3000 train_loss: 49.212974548339844 test_loss:83.07599639892578\n",
      "2462/3000 train_loss: 40.80095291137695 test_loss:76.78782653808594\n",
      "2463/3000 train_loss: 39.242408752441406 test_loss:75.33983612060547\n",
      "2464/3000 train_loss: 43.10786819458008 test_loss:82.46188354492188\n",
      "2465/3000 train_loss: 48.82173538208008 test_loss:75.95074462890625\n",
      "2466/3000 train_loss: 43.61213684082031 test_loss:78.97521209716797\n",
      "2467/3000 train_loss: 43.16939926147461 test_loss:74.96722412109375\n",
      "2468/3000 train_loss: 45.265533447265625 test_loss:75.83759307861328\n",
      "2469/3000 train_loss: 52.501808166503906 test_loss:77.25788116455078\n",
      "2470/3000 train_loss: 37.96405029296875 test_loss:76.71397399902344\n",
      "2471/3000 train_loss: 49.85969161987305 test_loss:77.06970977783203\n",
      "2472/3000 train_loss: 49.711944580078125 test_loss:82.06107330322266\n",
      "2473/3000 train_loss: 51.22756576538086 test_loss:88.63955688476562\n",
      "2474/3000 train_loss: 46.208248138427734 test_loss:80.96607208251953\n",
      "2475/3000 train_loss: 45.16196823120117 test_loss:76.3700942993164\n",
      "2476/3000 train_loss: 50.82279586791992 test_loss:76.38431549072266\n",
      "2477/3000 train_loss: 46.56989669799805 test_loss:78.41539001464844\n",
      "2478/3000 train_loss: 43.878543853759766 test_loss:76.44210052490234\n",
      "2479/3000 train_loss: 55.65681076049805 test_loss:90.87520599365234\n",
      "2480/3000 train_loss: 42.55473327636719 test_loss:76.5414047241211\n",
      "2481/3000 train_loss: 46.1667594909668 test_loss:79.47196197509766\n",
      "2482/3000 train_loss: 37.463165283203125 test_loss:76.99964141845703\n",
      "2483/3000 train_loss: 40.23114776611328 test_loss:80.3577651977539\n",
      "2484/3000 train_loss: 40.12739562988281 test_loss:80.93608093261719\n",
      "2485/3000 train_loss: 55.28230667114258 test_loss:82.28173065185547\n",
      "2486/3000 train_loss: 39.57736587524414 test_loss:76.27774810791016\n",
      "2487/3000 train_loss: 38.434120178222656 test_loss:82.7900390625\n",
      "2488/3000 train_loss: 39.115760803222656 test_loss:75.32209777832031\n",
      "2489/3000 train_loss: 39.798614501953125 test_loss:80.55587005615234\n",
      "2490/3000 train_loss: 38.88824462890625 test_loss:79.55622100830078\n",
      "2491/3000 train_loss: 49.98893356323242 test_loss:80.69195556640625\n",
      "2492/3000 train_loss: 41.75656509399414 test_loss:75.45927429199219\n",
      "2493/3000 train_loss: 41.163902282714844 test_loss:80.5272216796875\n",
      "2494/3000 train_loss: 41.94725036621094 test_loss:77.16273498535156\n",
      "2495/3000 train_loss: 39.40680694580078 test_loss:78.98312377929688\n",
      "2496/3000 train_loss: 39.11940002441406 test_loss:79.73822784423828\n",
      "2497/3000 train_loss: 41.437801361083984 test_loss:78.4059066772461\n",
      "2498/3000 train_loss: 40.14493942260742 test_loss:77.77652740478516\n",
      "2499/3000 train_loss: 42.333770751953125 test_loss:79.04293823242188\n",
      "2500/3000 train_loss: 40.34706497192383 test_loss:79.11859130859375\n",
      "2501/3000 train_loss: 35.823890686035156 test_loss:75.72185516357422\n",
      "2502/3000 train_loss: 39.19224166870117 test_loss:77.79006958007812\n",
      "2503/3000 train_loss: 36.549198150634766 test_loss:74.18475341796875\n",
      "2504/3000 train_loss: 41.55852508544922 test_loss:75.45760345458984\n",
      "2505/3000 train_loss: 41.20188522338867 test_loss:76.11898803710938\n",
      "2506/3000 train_loss: 43.630958557128906 test_loss:76.42535400390625\n",
      "2507/3000 train_loss: 36.66326904296875 test_loss:77.41102600097656\n",
      "2508/3000 train_loss: 37.979610443115234 test_loss:73.86912536621094\n",
      "2509/3000 train_loss: 38.57823944091797 test_loss:73.13679504394531\n",
      "2510/3000 train_loss: 60.97463607788086 test_loss:73.47157287597656\n",
      "2511/3000 train_loss: 39.30005645751953 test_loss:79.45542907714844\n",
      "2512/3000 train_loss: 38.97882080078125 test_loss:78.05980682373047\n",
      "2513/3000 train_loss: 44.753047943115234 test_loss:74.44339752197266\n",
      "2514/3000 train_loss: 44.09024429321289 test_loss:76.57198333740234\n",
      "2515/3000 train_loss: 47.756324768066406 test_loss:71.98649597167969\n",
      "2516/3000 train_loss: 50.23155212402344 test_loss:71.51801300048828\n",
      "2517/3000 train_loss: 47.70487976074219 test_loss:80.20866394042969\n",
      "2518/3000 train_loss: 35.36688232421875 test_loss:74.85095977783203\n",
      "2519/3000 train_loss: 37.25082778930664 test_loss:74.61133575439453\n",
      "2520/3000 train_loss: 40.85048294067383 test_loss:77.83088684082031\n",
      "2521/3000 train_loss: 45.43960189819336 test_loss:76.92975616455078\n",
      "2522/3000 train_loss: 38.437007904052734 test_loss:78.17506408691406\n",
      "2523/3000 train_loss: 49.25993347167969 test_loss:79.43232727050781\n",
      "2524/3000 train_loss: 33.215641021728516 test_loss:79.05279541015625\n",
      "2525/3000 train_loss: 38.38965606689453 test_loss:82.13634490966797\n",
      "2526/3000 train_loss: 40.90414047241211 test_loss:79.86783599853516\n",
      "2527/3000 train_loss: 39.53601837158203 test_loss:77.58023071289062\n",
      "2528/3000 train_loss: 49.27085876464844 test_loss:75.250244140625\n",
      "2529/3000 train_loss: 44.34275817871094 test_loss:78.12892150878906\n",
      "2530/3000 train_loss: 43.8624267578125 test_loss:78.07807922363281\n",
      "2531/3000 train_loss: 35.535892486572266 test_loss:75.92084503173828\n",
      "2532/3000 train_loss: 44.082603454589844 test_loss:76.904296875\n",
      "2533/3000 train_loss: 44.647438049316406 test_loss:78.42457580566406\n",
      "2534/3000 train_loss: 41.98579025268555 test_loss:80.3849105834961\n",
      "2535/3000 train_loss: 37.38340759277344 test_loss:79.17546844482422\n",
      "2536/3000 train_loss: 56.09883117675781 test_loss:78.69122314453125\n",
      "2537/3000 train_loss: 45.860107421875 test_loss:84.18380737304688\n",
      "2538/3000 train_loss: 52.55099105834961 test_loss:77.21644592285156\n",
      "2539/3000 train_loss: 34.949951171875 test_loss:77.02137756347656\n",
      "2540/3000 train_loss: 41.83724594116211 test_loss:83.37175750732422\n",
      "2541/3000 train_loss: 49.181819915771484 test_loss:77.58222961425781\n",
      "2542/3000 train_loss: 36.354888916015625 test_loss:73.80646514892578\n",
      "2543/3000 train_loss: 39.44873809814453 test_loss:78.13709259033203\n",
      "2544/3000 train_loss: 36.23112487792969 test_loss:73.14187622070312\n",
      "2545/3000 train_loss: 49.93113708496094 test_loss:73.78402709960938\n",
      "2546/3000 train_loss: 37.53139114379883 test_loss:73.00924682617188\n",
      "2547/3000 train_loss: 44.75858688354492 test_loss:75.76220703125\n",
      "2548/3000 train_loss: 44.23871994018555 test_loss:72.40922546386719\n",
      "2549/3000 train_loss: 40.20369338989258 test_loss:74.26421356201172\n",
      "2550/3000 train_loss: 49.16178512573242 test_loss:79.13723754882812\n",
      "2551/3000 train_loss: 41.471580505371094 test_loss:81.1466293334961\n",
      "2552/3000 train_loss: 42.788299560546875 test_loss:74.75923919677734\n",
      "2553/3000 train_loss: 51.949405670166016 test_loss:77.08804321289062\n",
      "2554/3000 train_loss: 42.444522857666016 test_loss:75.10783386230469\n",
      "2555/3000 train_loss: 41.90243911743164 test_loss:86.23404693603516\n",
      "2556/3000 train_loss: 42.347808837890625 test_loss:76.43230438232422\n",
      "2557/3000 train_loss: 48.299537658691406 test_loss:80.85773468017578\n",
      "2558/3000 train_loss: 35.79008483886719 test_loss:73.3884048461914\n",
      "2559/3000 train_loss: 47.817447662353516 test_loss:72.06922912597656\n",
      "2560/3000 train_loss: 46.24100875854492 test_loss:73.10281372070312\n",
      "2561/3000 train_loss: 43.51514434814453 test_loss:76.8221206665039\n",
      "2562/3000 train_loss: 41.61150360107422 test_loss:75.11444091796875\n",
      "2563/3000 train_loss: 45.941741943359375 test_loss:81.28115844726562\n",
      "2564/3000 train_loss: 43.797794342041016 test_loss:73.50012969970703\n",
      "2565/3000 train_loss: 45.315208435058594 test_loss:76.17242431640625\n",
      "2566/3000 train_loss: 46.46687698364258 test_loss:74.33573913574219\n",
      "2567/3000 train_loss: 42.088951110839844 test_loss:76.64836883544922\n",
      "2568/3000 train_loss: 48.521827697753906 test_loss:77.0415267944336\n",
      "2569/3000 train_loss: 39.089115142822266 test_loss:73.21559143066406\n",
      "2570/3000 train_loss: 40.78987503051758 test_loss:73.93649291992188\n",
      "2571/3000 train_loss: 37.82749938964844 test_loss:75.35513305664062\n",
      "2572/3000 train_loss: 44.19324493408203 test_loss:78.36335754394531\n",
      "2573/3000 train_loss: 49.020790100097656 test_loss:75.41314697265625\n",
      "2574/3000 train_loss: 41.00753402709961 test_loss:74.77872467041016\n",
      "2575/3000 train_loss: 43.517154693603516 test_loss:83.837646484375\n",
      "2576/3000 train_loss: 40.31616973876953 test_loss:75.6187973022461\n",
      "2577/3000 train_loss: 35.279537200927734 test_loss:77.0464859008789\n",
      "2578/3000 train_loss: 42.60847473144531 test_loss:77.19098663330078\n",
      "2579/3000 train_loss: 50.8305549621582 test_loss:78.24217224121094\n",
      "2580/3000 train_loss: 49.6317024230957 test_loss:80.50613403320312\n",
      "2581/3000 train_loss: 41.36187744140625 test_loss:72.01852416992188\n",
      "2582/3000 train_loss: 38.48086929321289 test_loss:78.11958312988281\n",
      "2583/3000 train_loss: 43.5856819152832 test_loss:75.22953796386719\n",
      "2584/3000 train_loss: 44.7535400390625 test_loss:77.64092254638672\n",
      "2585/3000 train_loss: 46.35795974731445 test_loss:78.68233489990234\n",
      "2586/3000 train_loss: 42.17001724243164 test_loss:77.66049194335938\n",
      "2587/3000 train_loss: 45.558773040771484 test_loss:79.88594055175781\n",
      "2588/3000 train_loss: 42.37456130981445 test_loss:76.25491333007812\n",
      "2589/3000 train_loss: 42.53202819824219 test_loss:79.14351654052734\n",
      "2590/3000 train_loss: 36.75162887573242 test_loss:77.15117645263672\n",
      "2591/3000 train_loss: 43.62403869628906 test_loss:75.60733032226562\n",
      "2592/3000 train_loss: 49.86597442626953 test_loss:77.82307434082031\n",
      "2593/3000 train_loss: 44.17218780517578 test_loss:79.35360717773438\n",
      "2594/3000 train_loss: 45.92073440551758 test_loss:70.6825180053711\n",
      "2595/3000 train_loss: 43.22652053833008 test_loss:74.61884307861328\n",
      "2596/3000 train_loss: 37.85042190551758 test_loss:78.79395294189453\n",
      "2597/3000 train_loss: 42.72298049926758 test_loss:76.3553466796875\n",
      "2598/3000 train_loss: 42.22423553466797 test_loss:77.31277465820312\n",
      "2599/3000 train_loss: 38.955848693847656 test_loss:75.49640655517578\n",
      "2600/3000 train_loss: 43.90056610107422 test_loss:77.5283432006836\n",
      "2601/3000 train_loss: 39.06682205200195 test_loss:70.75123596191406\n",
      "2602/3000 train_loss: 36.69986343383789 test_loss:70.5625\n",
      "2603/3000 train_loss: 32.79847717285156 test_loss:74.76658630371094\n",
      "2604/3000 train_loss: 40.35261535644531 test_loss:72.63764953613281\n",
      "2605/3000 train_loss: 40.77720642089844 test_loss:76.94259643554688\n",
      "2606/3000 train_loss: 42.64523696899414 test_loss:74.36087799072266\n",
      "2607/3000 train_loss: 44.60868453979492 test_loss:73.2900619506836\n",
      "2608/3000 train_loss: 51.400848388671875 test_loss:81.3392333984375\n",
      "2609/3000 train_loss: 43.273475646972656 test_loss:74.65348815917969\n",
      "2610/3000 train_loss: 48.1347770690918 test_loss:82.41877746582031\n",
      "2611/3000 train_loss: 41.44075012207031 test_loss:77.45849609375\n",
      "2612/3000 train_loss: 45.72050476074219 test_loss:75.82823181152344\n",
      "2613/3000 train_loss: 45.76984786987305 test_loss:76.39251708984375\n",
      "2614/3000 train_loss: 38.840087890625 test_loss:77.05782318115234\n",
      "2615/3000 train_loss: 39.95655059814453 test_loss:72.95182037353516\n",
      "2616/3000 train_loss: 46.68773651123047 test_loss:73.53986358642578\n",
      "2617/3000 train_loss: 41.08122253417969 test_loss:73.0784683227539\n",
      "2618/3000 train_loss: 45.19213104248047 test_loss:76.21051025390625\n",
      "2619/3000 train_loss: 42.760284423828125 test_loss:75.12958526611328\n",
      "2620/3000 train_loss: 39.603729248046875 test_loss:75.26300811767578\n",
      "2621/3000 train_loss: 39.6696662902832 test_loss:74.74099731445312\n",
      "2622/3000 train_loss: 46.06283187866211 test_loss:74.3683090209961\n",
      "2623/3000 train_loss: 65.05790710449219 test_loss:74.5394287109375\n",
      "2624/3000 train_loss: 43.91099548339844 test_loss:84.25541687011719\n",
      "2625/3000 train_loss: 43.961181640625 test_loss:78.33414459228516\n",
      "2626/3000 train_loss: 50.88880157470703 test_loss:84.60673522949219\n",
      "2627/3000 train_loss: 48.18373107910156 test_loss:73.3711166381836\n",
      "2628/3000 train_loss: 45.069637298583984 test_loss:78.30398559570312\n",
      "2629/3000 train_loss: 45.269893646240234 test_loss:76.38780975341797\n",
      "2630/3000 train_loss: 38.17572784423828 test_loss:76.40971374511719\n",
      "2631/3000 train_loss: 45.891544342041016 test_loss:71.83013153076172\n",
      "2632/3000 train_loss: 52.142311096191406 test_loss:76.705078125\n",
      "2633/3000 train_loss: 38.404022216796875 test_loss:73.10128021240234\n",
      "2634/3000 train_loss: 43.09505844116211 test_loss:75.73318481445312\n",
      "2635/3000 train_loss: 41.11020278930664 test_loss:72.62064361572266\n",
      "2636/3000 train_loss: 38.25756072998047 test_loss:76.72506713867188\n",
      "2637/3000 train_loss: 37.103424072265625 test_loss:73.15193939208984\n",
      "2638/3000 train_loss: 43.75981140136719 test_loss:75.59294128417969\n",
      "2639/3000 train_loss: 41.09711456298828 test_loss:77.6419677734375\n",
      "2640/3000 train_loss: 67.312255859375 test_loss:75.63156127929688\n",
      "2641/3000 train_loss: 43.336544036865234 test_loss:84.23883819580078\n",
      "2642/3000 train_loss: 40.36985778808594 test_loss:75.04762268066406\n",
      "2643/3000 train_loss: 40.78185272216797 test_loss:79.58647155761719\n",
      "2644/3000 train_loss: 39.845008850097656 test_loss:77.89129638671875\n",
      "2645/3000 train_loss: 41.87016296386719 test_loss:80.4345703125\n",
      "2646/3000 train_loss: 46.259681701660156 test_loss:80.6976318359375\n",
      "2647/3000 train_loss: 34.328704833984375 test_loss:80.65608215332031\n",
      "2648/3000 train_loss: 40.087913513183594 test_loss:77.03921508789062\n",
      "2649/3000 train_loss: 43.88751983642578 test_loss:78.88195037841797\n",
      "2650/3000 train_loss: 40.42958068847656 test_loss:83.47608184814453\n",
      "2651/3000 train_loss: 35.22323226928711 test_loss:79.35264587402344\n",
      "2652/3000 train_loss: 37.377986907958984 test_loss:74.2162857055664\n",
      "2653/3000 train_loss: 36.50088882446289 test_loss:75.24012756347656\n",
      "2654/3000 train_loss: 43.98284912109375 test_loss:74.0102767944336\n",
      "2655/3000 train_loss: 35.393863677978516 test_loss:75.2920913696289\n",
      "2656/3000 train_loss: 37.53215789794922 test_loss:72.06771850585938\n",
      "2657/3000 train_loss: 41.02842712402344 test_loss:75.01547241210938\n",
      "2658/3000 train_loss: 37.646419525146484 test_loss:77.06415557861328\n",
      "2659/3000 train_loss: 44.86875534057617 test_loss:85.16069793701172\n",
      "2660/3000 train_loss: 39.84193420410156 test_loss:76.87712097167969\n",
      "2661/3000 train_loss: 41.054588317871094 test_loss:79.41903686523438\n",
      "2662/3000 train_loss: 45.70934295654297 test_loss:80.98721313476562\n",
      "2663/3000 train_loss: 38.33567428588867 test_loss:75.38220977783203\n",
      "2664/3000 train_loss: 35.74073028564453 test_loss:73.21440124511719\n",
      "2665/3000 train_loss: 51.262935638427734 test_loss:77.19522094726562\n",
      "2666/3000 train_loss: 42.37825393676758 test_loss:72.46305847167969\n",
      "2667/3000 train_loss: 45.44258499145508 test_loss:75.4259262084961\n",
      "2668/3000 train_loss: 36.42618179321289 test_loss:74.89984130859375\n",
      "2669/3000 train_loss: 37.54700469970703 test_loss:75.9131088256836\n",
      "2670/3000 train_loss: 42.109901428222656 test_loss:76.58850860595703\n",
      "2671/3000 train_loss: 34.86802673339844 test_loss:74.49796295166016\n",
      "2672/3000 train_loss: 36.85955047607422 test_loss:77.3001708984375\n",
      "2673/3000 train_loss: 43.503135681152344 test_loss:79.80523681640625\n",
      "2674/3000 train_loss: 42.192832946777344 test_loss:75.64024353027344\n",
      "2675/3000 train_loss: 44.91534423828125 test_loss:79.11944580078125\n",
      "2676/3000 train_loss: 41.22157287597656 test_loss:79.32228088378906\n",
      "2677/3000 train_loss: 42.067543029785156 test_loss:76.3414535522461\n",
      "2678/3000 train_loss: 39.61963653564453 test_loss:72.20109558105469\n",
      "2679/3000 train_loss: 38.804935455322266 test_loss:72.90144348144531\n",
      "2680/3000 train_loss: 43.31707763671875 test_loss:77.39472198486328\n",
      "2681/3000 train_loss: 40.09111785888672 test_loss:73.02857208251953\n",
      "2682/3000 train_loss: 46.76001739501953 test_loss:71.73131561279297\n",
      "2683/3000 train_loss: 47.21861267089844 test_loss:79.15380859375\n",
      "2684/3000 train_loss: 36.05704116821289 test_loss:73.46451568603516\n",
      "2685/3000 train_loss: 41.009403228759766 test_loss:73.0761947631836\n",
      "2686/3000 train_loss: 37.104400634765625 test_loss:73.17678833007812\n",
      "2687/3000 train_loss: 40.71588134765625 test_loss:69.04653930664062\n",
      "2688/3000 train_loss: 44.50624084472656 test_loss:68.82652282714844\n",
      "2689/3000 train_loss: 42.31905746459961 test_loss:71.62335205078125\n",
      "2690/3000 train_loss: 37.54490280151367 test_loss:71.24803161621094\n",
      "2691/3000 train_loss: 35.302207946777344 test_loss:70.71774291992188\n",
      "2692/3000 train_loss: 39.079898834228516 test_loss:71.57996368408203\n",
      "2693/3000 train_loss: 52.48211669921875 test_loss:74.7134017944336\n",
      "2694/3000 train_loss: 37.033546447753906 test_loss:76.46199035644531\n",
      "2695/3000 train_loss: 42.79917907714844 test_loss:72.70346069335938\n",
      "2696/3000 train_loss: 38.16908645629883 test_loss:71.92809295654297\n",
      "2697/3000 train_loss: 39.920631408691406 test_loss:74.14315032958984\n",
      "2698/3000 train_loss: 36.89352798461914 test_loss:72.06491088867188\n",
      "2699/3000 train_loss: 39.374961853027344 test_loss:71.60676574707031\n",
      "2700/3000 train_loss: 40.61346435546875 test_loss:69.30437469482422\n",
      "2701/3000 train_loss: 39.851409912109375 test_loss:75.7432632446289\n",
      "2702/3000 train_loss: 48.68955612182617 test_loss:76.15914916992188\n",
      "2703/3000 train_loss: 41.333641052246094 test_loss:76.04037475585938\n",
      "2704/3000 train_loss: 32.42051315307617 test_loss:73.88560485839844\n",
      "2705/3000 train_loss: 33.5940055847168 test_loss:73.69803619384766\n",
      "2706/3000 train_loss: 32.50149154663086 test_loss:74.33375549316406\n",
      "2707/3000 train_loss: 40.530086517333984 test_loss:74.58010864257812\n",
      "2708/3000 train_loss: 35.820945739746094 test_loss:70.09066009521484\n",
      "2709/3000 train_loss: 36.31585693359375 test_loss:72.70677185058594\n",
      "2710/3000 train_loss: 42.34563064575195 test_loss:76.42426300048828\n",
      "2711/3000 train_loss: 41.20399856567383 test_loss:73.37983703613281\n",
      "2712/3000 train_loss: 43.02281951904297 test_loss:73.28475952148438\n",
      "2713/3000 train_loss: 44.75242614746094 test_loss:71.8713607788086\n",
      "2714/3000 train_loss: 36.84498596191406 test_loss:76.3420639038086\n",
      "2715/3000 train_loss: 41.82707214355469 test_loss:75.70021057128906\n",
      "2716/3000 train_loss: 45.9264030456543 test_loss:75.91629791259766\n",
      "2717/3000 train_loss: 52.91645812988281 test_loss:77.58597564697266\n",
      "2718/3000 train_loss: 49.219486236572266 test_loss:78.64338684082031\n",
      "2719/3000 train_loss: 46.11077880859375 test_loss:75.6124267578125\n",
      "2720/3000 train_loss: 40.02749252319336 test_loss:72.08958435058594\n",
      "2721/3000 train_loss: 37.461570739746094 test_loss:77.95124816894531\n",
      "2722/3000 train_loss: 49.323036193847656 test_loss:70.54122924804688\n",
      "2723/3000 train_loss: 35.43482208251953 test_loss:71.56417846679688\n",
      "2724/3000 train_loss: 50.03889465332031 test_loss:78.26748657226562\n",
      "2725/3000 train_loss: 34.29569625854492 test_loss:69.98959350585938\n",
      "2726/3000 train_loss: 29.264631271362305 test_loss:71.17622375488281\n",
      "2727/3000 train_loss: 36.37207794189453 test_loss:72.91316223144531\n",
      "2728/3000 train_loss: 35.52849197387695 test_loss:70.67448425292969\n",
      "2729/3000 train_loss: 40.573760986328125 test_loss:73.0436019897461\n",
      "2730/3000 train_loss: 36.70280456542969 test_loss:72.3868637084961\n",
      "2731/3000 train_loss: 41.19420623779297 test_loss:67.59455871582031\n",
      "2732/3000 train_loss: 46.833953857421875 test_loss:73.61666107177734\n",
      "2733/3000 train_loss: 36.763370513916016 test_loss:77.96286010742188\n",
      "2734/3000 train_loss: 34.912109375 test_loss:71.95856475830078\n",
      "2735/3000 train_loss: 42.976078033447266 test_loss:75.98824310302734\n",
      "2736/3000 train_loss: 43.8179817199707 test_loss:74.88798522949219\n",
      "2737/3000 train_loss: 33.8919563293457 test_loss:80.22860717773438\n",
      "2738/3000 train_loss: 36.74623107910156 test_loss:73.2369384765625\n",
      "2739/3000 train_loss: 35.09113693237305 test_loss:69.63884735107422\n",
      "2740/3000 train_loss: 43.16276931762695 test_loss:77.92825317382812\n",
      "2741/3000 train_loss: 45.38216781616211 test_loss:72.7381591796875\n",
      "2742/3000 train_loss: 38.7467041015625 test_loss:78.13545227050781\n",
      "2743/3000 train_loss: 38.6296501159668 test_loss:77.732421875\n",
      "2744/3000 train_loss: 49.74956512451172 test_loss:77.70292663574219\n",
      "2745/3000 train_loss: 39.5732307434082 test_loss:77.26799011230469\n",
      "2746/3000 train_loss: 34.01642608642578 test_loss:69.30609130859375\n",
      "2747/3000 train_loss: 35.40981674194336 test_loss:68.0556640625\n",
      "2748/3000 train_loss: 37.00774383544922 test_loss:69.8523941040039\n",
      "2749/3000 train_loss: 47.336483001708984 test_loss:72.70761108398438\n",
      "2750/3000 train_loss: 48.71735382080078 test_loss:73.37905883789062\n",
      "2751/3000 train_loss: 47.45429992675781 test_loss:76.34507751464844\n",
      "2752/3000 train_loss: 41.02454376220703 test_loss:73.37123107910156\n",
      "2753/3000 train_loss: 43.186363220214844 test_loss:67.76427459716797\n",
      "2754/3000 train_loss: 42.365440368652344 test_loss:73.93283081054688\n",
      "2755/3000 train_loss: 42.505855560302734 test_loss:66.76380920410156\n",
      "2756/3000 train_loss: 38.93415832519531 test_loss:78.85498046875\n",
      "2757/3000 train_loss: 42.20891189575195 test_loss:75.53714752197266\n",
      "2758/3000 train_loss: 39.487674713134766 test_loss:75.183349609375\n",
      "2759/3000 train_loss: 47.23679733276367 test_loss:76.52685546875\n",
      "2760/3000 train_loss: 45.71641540527344 test_loss:74.58383178710938\n",
      "2761/3000 train_loss: 44.50898742675781 test_loss:75.25062561035156\n",
      "2762/3000 train_loss: 40.072669982910156 test_loss:74.09050750732422\n",
      "2763/3000 train_loss: 44.578243255615234 test_loss:75.06056213378906\n",
      "2764/3000 train_loss: 41.217350006103516 test_loss:74.3392333984375\n",
      "2765/3000 train_loss: 35.448020935058594 test_loss:72.52357482910156\n",
      "2766/3000 train_loss: 38.852108001708984 test_loss:69.38257598876953\n",
      "2767/3000 train_loss: 36.34455490112305 test_loss:68.5828857421875\n",
      "2768/3000 train_loss: 41.248374938964844 test_loss:69.42794799804688\n",
      "2769/3000 train_loss: 37.4596061706543 test_loss:69.49935913085938\n",
      "2770/3000 train_loss: 47.47941970825195 test_loss:68.08428955078125\n",
      "2771/3000 train_loss: 52.08070755004883 test_loss:67.20601654052734\n",
      "2772/3000 train_loss: 51.58431625366211 test_loss:67.38665008544922\n",
      "2773/3000 train_loss: 47.13825988769531 test_loss:74.1584243774414\n",
      "2774/3000 train_loss: 39.54103088378906 test_loss:67.42475891113281\n",
      "2775/3000 train_loss: 45.30209732055664 test_loss:70.16876220703125\n",
      "2776/3000 train_loss: 45.19617462158203 test_loss:68.17200469970703\n",
      "2777/3000 train_loss: 34.90871047973633 test_loss:73.00509643554688\n",
      "2778/3000 train_loss: 35.894561767578125 test_loss:69.33775329589844\n",
      "2779/3000 train_loss: 37.81547546386719 test_loss:74.16000366210938\n",
      "2780/3000 train_loss: 38.615692138671875 test_loss:70.81924438476562\n",
      "2781/3000 train_loss: 41.715362548828125 test_loss:68.94474029541016\n",
      "2782/3000 train_loss: 38.69124984741211 test_loss:70.32586669921875\n",
      "2783/3000 train_loss: 47.37432861328125 test_loss:72.4059066772461\n",
      "2784/3000 train_loss: 46.610801696777344 test_loss:72.51506042480469\n",
      "2785/3000 train_loss: 47.48863983154297 test_loss:72.18197631835938\n",
      "2786/3000 train_loss: 44.046905517578125 test_loss:68.61003112792969\n",
      "2787/3000 train_loss: 37.70132064819336 test_loss:67.38761901855469\n",
      "2788/3000 train_loss: 34.194488525390625 test_loss:70.39525604248047\n",
      "2789/3000 train_loss: 32.55665588378906 test_loss:68.60671997070312\n",
      "2790/3000 train_loss: 38.816673278808594 test_loss:67.13851928710938\n",
      "2791/3000 train_loss: 38.17656326293945 test_loss:68.53887939453125\n",
      "2792/3000 train_loss: 28.92864990234375 test_loss:66.35994720458984\n",
      "2793/3000 train_loss: 33.71381378173828 test_loss:69.7545166015625\n",
      "2794/3000 train_loss: 37.3206787109375 test_loss:68.39111328125\n",
      "2795/3000 train_loss: 74.0362777709961 test_loss:77.1946029663086\n",
      "2796/3000 train_loss: 72.3875961303711 test_loss:92.33088684082031\n",
      "2797/3000 train_loss: 50.807228088378906 test_loss:88.40936279296875\n",
      "2798/3000 train_loss: 51.17962646484375 test_loss:76.009765625\n",
      "2799/3000 train_loss: 46.72829055786133 test_loss:80.90287780761719\n",
      "2800/3000 train_loss: 39.88642883300781 test_loss:73.23779296875\n",
      "2801/3000 train_loss: 39.185054779052734 test_loss:75.276611328125\n",
      "2802/3000 train_loss: 37.46166229248047 test_loss:79.19621276855469\n",
      "2803/3000 train_loss: 48.851966857910156 test_loss:76.25146484375\n",
      "2804/3000 train_loss: 36.0922737121582 test_loss:73.82394409179688\n",
      "2805/3000 train_loss: 44.68555450439453 test_loss:77.63845825195312\n",
      "2806/3000 train_loss: 37.31460189819336 test_loss:71.66984558105469\n",
      "2807/3000 train_loss: 38.375762939453125 test_loss:77.4434585571289\n",
      "2808/3000 train_loss: 36.434120178222656 test_loss:69.96086883544922\n",
      "2809/3000 train_loss: 45.470970153808594 test_loss:76.87449645996094\n",
      "2810/3000 train_loss: 41.55234909057617 test_loss:70.39990997314453\n",
      "2811/3000 train_loss: 34.52106475830078 test_loss:70.00515747070312\n",
      "2812/3000 train_loss: 38.07328796386719 test_loss:73.94347381591797\n",
      "2813/3000 train_loss: 46.892578125 test_loss:68.67359924316406\n",
      "2814/3000 train_loss: 42.170475006103516 test_loss:72.67113494873047\n",
      "2815/3000 train_loss: 31.723926544189453 test_loss:71.82733154296875\n",
      "2816/3000 train_loss: 42.52423858642578 test_loss:71.44068908691406\n",
      "2817/3000 train_loss: 40.96931838989258 test_loss:70.16259002685547\n",
      "2818/3000 train_loss: 36.26709747314453 test_loss:70.33250427246094\n",
      "2819/3000 train_loss: 40.43025207519531 test_loss:73.54679107666016\n",
      "2820/3000 train_loss: 43.339542388916016 test_loss:69.44438171386719\n",
      "2821/3000 train_loss: 37.96733474731445 test_loss:73.90148162841797\n",
      "2822/3000 train_loss: 41.725059509277344 test_loss:71.99703216552734\n",
      "2823/3000 train_loss: 34.2484130859375 test_loss:72.42444610595703\n",
      "2824/3000 train_loss: 36.932769775390625 test_loss:69.31098937988281\n",
      "2825/3000 train_loss: 41.37459182739258 test_loss:69.90219116210938\n",
      "2826/3000 train_loss: 40.43592071533203 test_loss:71.10903930664062\n",
      "2827/3000 train_loss: 39.92156982421875 test_loss:75.27297973632812\n",
      "2828/3000 train_loss: 39.8840446472168 test_loss:76.50373840332031\n",
      "2829/3000 train_loss: 42.36034393310547 test_loss:72.87953186035156\n",
      "2830/3000 train_loss: 32.20216751098633 test_loss:73.70557403564453\n",
      "2831/3000 train_loss: 38.76339340209961 test_loss:75.1128158569336\n",
      "2832/3000 train_loss: 35.73287582397461 test_loss:69.96505737304688\n",
      "2833/3000 train_loss: 46.68642807006836 test_loss:72.64219665527344\n",
      "2834/3000 train_loss: 33.64448547363281 test_loss:70.56730651855469\n",
      "2835/3000 train_loss: 42.63593673706055 test_loss:78.0205078125\n",
      "2836/3000 train_loss: 35.635501861572266 test_loss:71.82040405273438\n",
      "2837/3000 train_loss: 33.709442138671875 test_loss:73.80711364746094\n",
      "2838/3000 train_loss: 38.97332000732422 test_loss:74.02789306640625\n",
      "2839/3000 train_loss: 40.380619049072266 test_loss:76.13067626953125\n",
      "2840/3000 train_loss: 40.79042053222656 test_loss:74.07347869873047\n",
      "2841/3000 train_loss: 47.56267166137695 test_loss:74.86416625976562\n",
      "2842/3000 train_loss: 45.89100646972656 test_loss:82.37220764160156\n",
      "2843/3000 train_loss: 47.7304573059082 test_loss:71.44071197509766\n",
      "2844/3000 train_loss: 32.542110443115234 test_loss:69.20539855957031\n",
      "2845/3000 train_loss: 35.76275634765625 test_loss:73.92572021484375\n",
      "2846/3000 train_loss: 37.35757827758789 test_loss:70.22718811035156\n",
      "2847/3000 train_loss: 35.13751220703125 test_loss:70.107666015625\n",
      "2848/3000 train_loss: 44.39481735229492 test_loss:70.3746109008789\n",
      "2849/3000 train_loss: 30.362491607666016 test_loss:68.7043228149414\n",
      "2850/3000 train_loss: 38.8145637512207 test_loss:69.47625732421875\n",
      "2851/3000 train_loss: 40.171016693115234 test_loss:70.40177154541016\n",
      "2852/3000 train_loss: 43.15592956542969 test_loss:69.66593933105469\n",
      "2853/3000 train_loss: 41.516475677490234 test_loss:68.86564636230469\n",
      "2854/3000 train_loss: 46.21895980834961 test_loss:70.97640991210938\n",
      "2855/3000 train_loss: 41.02111053466797 test_loss:74.11636352539062\n",
      "2856/3000 train_loss: 39.09019088745117 test_loss:70.0008544921875\n",
      "2857/3000 train_loss: 38.828033447265625 test_loss:75.40766906738281\n",
      "2858/3000 train_loss: 33.66440963745117 test_loss:74.64717102050781\n",
      "2859/3000 train_loss: 36.95858383178711 test_loss:77.10272216796875\n",
      "2860/3000 train_loss: 35.97096252441406 test_loss:75.94879913330078\n",
      "2861/3000 train_loss: 42.567710876464844 test_loss:72.66324615478516\n",
      "2862/3000 train_loss: 40.67552185058594 test_loss:76.09513854980469\n",
      "2863/3000 train_loss: 37.30091857910156 test_loss:73.56929016113281\n",
      "2864/3000 train_loss: 40.600345611572266 test_loss:73.6131591796875\n",
      "2865/3000 train_loss: 38.76167297363281 test_loss:69.85824584960938\n",
      "2866/3000 train_loss: 44.59757995605469 test_loss:73.4230728149414\n",
      "2867/3000 train_loss: 39.64958190917969 test_loss:73.66770935058594\n",
      "2868/3000 train_loss: 44.4235954284668 test_loss:69.55998992919922\n",
      "2869/3000 train_loss: 38.53105545043945 test_loss:77.10844421386719\n",
      "2870/3000 train_loss: 39.85301208496094 test_loss:71.2568359375\n",
      "2871/3000 train_loss: 37.644535064697266 test_loss:73.10791778564453\n",
      "2872/3000 train_loss: 50.09501647949219 test_loss:72.47815704345703\n",
      "2873/3000 train_loss: 39.84494400024414 test_loss:74.70573425292969\n",
      "2874/3000 train_loss: 36.68369674682617 test_loss:69.8730239868164\n",
      "2875/3000 train_loss: 39.21613693237305 test_loss:71.09481811523438\n",
      "2876/3000 train_loss: 33.20539093017578 test_loss:70.41619873046875\n",
      "2877/3000 train_loss: 43.534732818603516 test_loss:66.99087524414062\n",
      "2878/3000 train_loss: 52.18010711669922 test_loss:77.22134399414062\n",
      "2879/3000 train_loss: 42.389549255371094 test_loss:73.26591491699219\n",
      "2880/3000 train_loss: 38.32352066040039 test_loss:74.6777572631836\n",
      "2881/3000 train_loss: 40.39802551269531 test_loss:71.78346252441406\n",
      "2882/3000 train_loss: 40.69669723510742 test_loss:73.20861053466797\n",
      "2883/3000 train_loss: 39.07262420654297 test_loss:71.96207427978516\n",
      "2884/3000 train_loss: 38.26224136352539 test_loss:71.48068237304688\n",
      "2885/3000 train_loss: 46.16880416870117 test_loss:72.12262725830078\n",
      "2886/3000 train_loss: 44.832969665527344 test_loss:74.88597106933594\n",
      "2887/3000 train_loss: 39.05873489379883 test_loss:68.33847045898438\n",
      "2888/3000 train_loss: 33.69884490966797 test_loss:74.63665008544922\n",
      "2889/3000 train_loss: 39.584896087646484 test_loss:71.98039245605469\n",
      "2890/3000 train_loss: 34.92112350463867 test_loss:74.64718627929688\n",
      "2891/3000 train_loss: 41.324371337890625 test_loss:67.40695190429688\n",
      "2892/3000 train_loss: 38.10054016113281 test_loss:71.52262115478516\n",
      "2893/3000 train_loss: 41.5263786315918 test_loss:68.63934326171875\n",
      "2894/3000 train_loss: 37.59664535522461 test_loss:71.4852294921875\n",
      "2895/3000 train_loss: 40.98127365112305 test_loss:72.12174987792969\n",
      "2896/3000 train_loss: 46.818824768066406 test_loss:72.79817199707031\n",
      "2897/3000 train_loss: 42.89932632446289 test_loss:73.9661636352539\n",
      "2898/3000 train_loss: 39.30117416381836 test_loss:75.32614135742188\n",
      "2899/3000 train_loss: 35.331756591796875 test_loss:72.35240173339844\n",
      "2900/3000 train_loss: 40.406673431396484 test_loss:70.22664642333984\n",
      "2901/3000 train_loss: 32.69403076171875 test_loss:71.54049682617188\n",
      "2902/3000 train_loss: 36.963043212890625 test_loss:69.60832214355469\n",
      "2903/3000 train_loss: 41.92704772949219 test_loss:74.81683349609375\n",
      "2904/3000 train_loss: 49.87649154663086 test_loss:74.2817611694336\n",
      "2905/3000 train_loss: 35.93987274169922 test_loss:74.3111801147461\n",
      "2906/3000 train_loss: 35.12633514404297 test_loss:72.75857543945312\n",
      "2907/3000 train_loss: 46.51874542236328 test_loss:71.61137390136719\n",
      "2908/3000 train_loss: 38.99449157714844 test_loss:75.38398742675781\n",
      "2909/3000 train_loss: 35.38840866088867 test_loss:70.23629760742188\n",
      "2910/3000 train_loss: 41.230506896972656 test_loss:69.02094268798828\n",
      "2911/3000 train_loss: 43.02130126953125 test_loss:74.49569702148438\n",
      "2912/3000 train_loss: 34.93610382080078 test_loss:72.156494140625\n",
      "2913/3000 train_loss: 35.43524932861328 test_loss:74.8824462890625\n",
      "2914/3000 train_loss: 42.244632720947266 test_loss:73.45683288574219\n",
      "2915/3000 train_loss: 42.439517974853516 test_loss:73.37165069580078\n",
      "2916/3000 train_loss: 46.46688461303711 test_loss:71.35830688476562\n",
      "2917/3000 train_loss: 37.068321228027344 test_loss:68.5372314453125\n",
      "2918/3000 train_loss: 34.048343658447266 test_loss:68.41239166259766\n",
      "2919/3000 train_loss: 39.47193908691406 test_loss:75.01295471191406\n",
      "2920/3000 train_loss: 35.81072235107422 test_loss:72.0531005859375\n",
      "2921/3000 train_loss: 36.439918518066406 test_loss:72.65519714355469\n",
      "2922/3000 train_loss: 41.56203842163086 test_loss:69.98348236083984\n",
      "2923/3000 train_loss: 30.93987274169922 test_loss:72.28981018066406\n",
      "2924/3000 train_loss: 36.48916244506836 test_loss:68.65443420410156\n",
      "2925/3000 train_loss: 33.63237380981445 test_loss:70.80989074707031\n",
      "2926/3000 train_loss: 46.53720474243164 test_loss:69.81685638427734\n",
      "2927/3000 train_loss: 39.145751953125 test_loss:69.99687194824219\n",
      "2928/3000 train_loss: 41.51756286621094 test_loss:69.52119445800781\n",
      "2929/3000 train_loss: 35.647220611572266 test_loss:68.81534576416016\n",
      "2930/3000 train_loss: 36.88200378417969 test_loss:69.70851135253906\n",
      "2931/3000 train_loss: 36.95362091064453 test_loss:71.35986328125\n",
      "2932/3000 train_loss: 42.99110412597656 test_loss:71.09251403808594\n",
      "2933/3000 train_loss: 31.232196807861328 test_loss:67.46995544433594\n",
      "2934/3000 train_loss: 40.486202239990234 test_loss:74.67842102050781\n",
      "2935/3000 train_loss: 40.830322265625 test_loss:70.54509735107422\n",
      "2936/3000 train_loss: 44.033042907714844 test_loss:73.39623260498047\n",
      "2937/3000 train_loss: 36.38167953491211 test_loss:74.89957427978516\n",
      "2938/3000 train_loss: 46.94154357910156 test_loss:79.05406188964844\n",
      "2939/3000 train_loss: 36.83850860595703 test_loss:82.12516784667969\n",
      "2940/3000 train_loss: 32.92665100097656 test_loss:77.96324157714844\n",
      "2941/3000 train_loss: 40.43913650512695 test_loss:75.376220703125\n",
      "2942/3000 train_loss: 37.919822692871094 test_loss:72.89742279052734\n",
      "2943/3000 train_loss: 37.224605560302734 test_loss:71.4010009765625\n",
      "2944/3000 train_loss: 43.910587310791016 test_loss:71.86219787597656\n",
      "2945/3000 train_loss: 44.19856262207031 test_loss:73.74823760986328\n",
      "2946/3000 train_loss: 41.20183563232422 test_loss:71.94624328613281\n",
      "2947/3000 train_loss: 45.07166290283203 test_loss:73.38310241699219\n",
      "2948/3000 train_loss: 44.05120849609375 test_loss:76.4012451171875\n",
      "2949/3000 train_loss: 39.87520217895508 test_loss:67.84881591796875\n",
      "2950/3000 train_loss: 37.68402099609375 test_loss:71.2396469116211\n",
      "2951/3000 train_loss: 37.44221115112305 test_loss:67.548095703125\n",
      "2952/3000 train_loss: 38.80104064941406 test_loss:68.8586654663086\n",
      "2953/3000 train_loss: 33.490848541259766 test_loss:68.5267562866211\n",
      "2954/3000 train_loss: 34.75291442871094 test_loss:68.88861846923828\n",
      "2955/3000 train_loss: 36.2045783996582 test_loss:67.12922668457031\n",
      "2956/3000 train_loss: 41.63762664794922 test_loss:72.00594329833984\n",
      "2957/3000 train_loss: 39.78000259399414 test_loss:69.05880737304688\n",
      "2958/3000 train_loss: 31.708024978637695 test_loss:72.4746322631836\n",
      "2959/3000 train_loss: 36.28746795654297 test_loss:68.3909912109375\n",
      "2960/3000 train_loss: 33.42814636230469 test_loss:67.75721740722656\n",
      "2961/3000 train_loss: 34.297637939453125 test_loss:70.01829528808594\n",
      "2962/3000 train_loss: 41.7689208984375 test_loss:71.91062927246094\n",
      "2963/3000 train_loss: 33.63668441772461 test_loss:70.546630859375\n",
      "2964/3000 train_loss: 39.371490478515625 test_loss:70.67568969726562\n",
      "2965/3000 train_loss: 35.129417419433594 test_loss:70.22901916503906\n",
      "2966/3000 train_loss: 37.1602783203125 test_loss:68.88726806640625\n",
      "2967/3000 train_loss: 46.26806640625 test_loss:71.48197937011719\n",
      "2968/3000 train_loss: 38.73564529418945 test_loss:71.17716217041016\n",
      "2969/3000 train_loss: 36.914634704589844 test_loss:69.24259185791016\n",
      "2970/3000 train_loss: 40.047855377197266 test_loss:73.43067932128906\n",
      "2971/3000 train_loss: 39.523033142089844 test_loss:68.10629272460938\n",
      "2972/3000 train_loss: 29.18266487121582 test_loss:75.3133544921875\n",
      "2973/3000 train_loss: 33.53814697265625 test_loss:75.44480895996094\n",
      "2974/3000 train_loss: 43.033592224121094 test_loss:74.96101379394531\n",
      "2975/3000 train_loss: 40.3533935546875 test_loss:74.60186767578125\n",
      "2976/3000 train_loss: 38.34849548339844 test_loss:75.61144256591797\n",
      "2977/3000 train_loss: 39.90802001953125 test_loss:70.92579650878906\n",
      "2978/3000 train_loss: 42.53141784667969 test_loss:72.93656921386719\n",
      "2979/3000 train_loss: 51.22402572631836 test_loss:79.86534881591797\n",
      "2980/3000 train_loss: 40.7624626159668 test_loss:71.91549682617188\n",
      "2981/3000 train_loss: 39.1325798034668 test_loss:73.63459777832031\n",
      "2982/3000 train_loss: 36.340633392333984 test_loss:69.0313491821289\n",
      "2983/3000 train_loss: 33.531002044677734 test_loss:68.60128021240234\n",
      "2984/3000 train_loss: 36.30263137817383 test_loss:71.45692443847656\n",
      "2985/3000 train_loss: 42.39352035522461 test_loss:65.0986099243164\n",
      "2986/3000 train_loss: 41.48163604736328 test_loss:72.73281860351562\n",
      "2987/3000 train_loss: 37.2287483215332 test_loss:69.09857177734375\n",
      "2988/3000 train_loss: 36.52608871459961 test_loss:77.354248046875\n",
      "2989/3000 train_loss: 47.88921356201172 test_loss:69.96794128417969\n",
      "2990/3000 train_loss: 39.904544830322266 test_loss:71.30303192138672\n",
      "2991/3000 train_loss: 34.66563415527344 test_loss:70.57630920410156\n",
      "2992/3000 train_loss: 32.03082275390625 test_loss:70.77218627929688\n",
      "2993/3000 train_loss: 49.16423416137695 test_loss:81.01959228515625\n",
      "2994/3000 train_loss: 37.34131622314453 test_loss:72.20703887939453\n",
      "2995/3000 train_loss: 36.05872344970703 test_loss:69.20741271972656\n",
      "2996/3000 train_loss: 36.705299377441406 test_loss:77.80757904052734\n",
      "2997/3000 train_loss: 38.76350021362305 test_loss:72.49488830566406\n",
      "2998/3000 train_loss: 35.90852737426758 test_loss:75.50190734863281\n",
      "2999/3000 train_loss: 35.29459762573242 test_loss:77.14798736572266\n",
      "3000/3000 train_loss: 36.38621520996094 test_loss:71.02485656738281\n"
     ]
    }
   ],
   "source": [
    "losses = train(model = unet, optimizer = optimizer, criterion=criterion, data_tr=train_data,\n",
    "               data_val = test_data, scheduler = scheduler,device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "id": "6Ew7_F0-q7aL",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "d34fe676-d064-4662-eea0-824ea9c88fe5"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(71.0249)"
      ]
     },
     "metadata": {},
     "execution_count": 181
    }
   ],
   "source": [
    "avg_loss = 0\n",
    "# unet.eval()\n",
    "# unet.train()\n",
    "preds = []\n",
    "i = 0\n",
    "test_anomaly_losses = []\n",
    "test_normal_losses = []\n",
    "test_losses = []\n",
    "test_real = y_test.tolist()\n",
    "for batch in test_data:\n",
    "  with torch.no_grad():\n",
    "    # unet.train()\n",
    "    predictions = unet(batch.to(device)).cpu()\n",
    "    preds.append(predictions)\n",
    "  loss = criterion(predictions, batch.cpu())\n",
    "  for j in range(len(predictions)):\n",
    "    if int(y_test[i]) == 1:\n",
    "      test_anomaly_losses.append(float(criterion(predictions[j], batch[j])))\n",
    "    else:\n",
    "      test_normal_losses.append(float(criterion(predictions[j], batch[j])))\n",
    "    i += 1\n",
    "    test_losses.append(criterion(predictions[j], batch[j]))\n",
    "  # print(loss)\n",
    "  # print(loss)\n",
    "  avg_loss += loss / len(test_data)\n",
    "# avg_loss\n",
    "\n",
    "avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "id": "VpDKorrRso9o",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "1d202271-5204-4759-f0fe-75f8af20dce5"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(91.17505803108216, 43.8880546951294)"
      ]
     },
     "metadata": {},
     "execution_count": 182
    }
   ],
   "source": [
    "sum(test_anomaly_losses)/len(test_anomaly_losses) , sum(test_normal_losses)/len(test_normal_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5LEvbZKYuh7J",
    "outputId": "4a1a51a7-a310-43dc-ffcf-ca0f3ed037d4"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.8049999999999999\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "vals = np.arange(20, 100, 0.5).tolist()\n",
    "for threshold in vals:\n",
    "  preds = []\n",
    "  for j in range(len(test_losses)):\n",
    "    if test_losses[j] > threshold:\n",
    "      preds.append(1)\n",
    "    else:\n",
    "      preds.append(0)\n",
    "  \n",
    "  results.append(roc_auc_score(test_real,preds))\n",
    "\n",
    "print(max(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QaSSqG8SbAw2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
