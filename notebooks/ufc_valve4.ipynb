{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9SStKf4G0V5H"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim import Adam\n",
        "import torchaudio\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from skimage.util import img_as_ubyte\n",
        "import pandas as pd\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import librosa\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import io\n",
        "from sklearn.metrics import roc_auc_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XtxbKLZq5KX",
        "outputId": "0c6de58c-f736-440b-85ce-a6ad2ae77c29"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYxHegIM0Z4i",
        "outputId": "8b35e1ba-3d36-4aca-c569-e94fbad47a14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "h9DATQwS0ivD"
      },
      "outputs": [],
      "source": [
        "class MimiiDataset(Dataset):\n",
        "    def __init__(self,audio_dir, n_fft = 1024, win_length = 1024,\n",
        "                 hop_length = 512,power = 2,n_mels = 128,pad_mode = 'reflect',\n",
        "                 sr = 16000,center = True,norm = None):\n",
        "      \n",
        "        super(MimiiDataset, self).__init__()\n",
        "        self.audio_dir = audio_dir\n",
        "        self.n_mels = n_mels\n",
        "        self.n_fft = n_fft\n",
        "        self.win_length = win_length\n",
        "        self.hop_length = hop_length\n",
        "        self.power = power\n",
        "        self.pad_mode = pad_mode\n",
        "        self.sr = sr\n",
        "        self.center = center\n",
        "        self.norm = norm\n",
        "\n",
        "    def get_files(self):\n",
        "       return self.train_files, self.test_files\n",
        "    \n",
        "    def get_data(self,device, id):\n",
        "        \n",
        "        self.train_files, self.train_labels = self._train_file_list(device, id)\n",
        "        self.test_files, self.test_labels = self._test_file_list(device, id)\n",
        "        \n",
        "        self.train_data = self.get_audios(self.train_files)\n",
        "        self.test_data = self.get_audios(self.test_files)\n",
        "        \n",
        "        return self.train_data, self.test_data, self.train_labels, self.test_labels\n",
        "    \n",
        "    def _train_file_list(self, device, id):\n",
        "        query = os.path.abspath(\n",
        "            f\"{self.audio_dir}/{device}/train/normal_id_0{id}*.wav\"\n",
        "        )\n",
        "        train_normal_files = sorted(glob.glob(query))\n",
        "        train_normal_labels = np.zeros(len(train_normal_files))\n",
        "        \n",
        "        query = os.path.abspath(\n",
        "                f\"{self.audio_dir}/{device}/train/anomaly_id_0{id}*.wav\"\n",
        "            )\n",
        "        train_anomaly_files = sorted(glob.glob(query))\n",
        "        train_anomaly_labels = np.ones(len(train_anomaly_files))\n",
        "        \n",
        "        train_file_list = np.concatenate((train_normal_files, train_anomaly_files), axis=0)\n",
        "        train_labels = np.concatenate((train_normal_labels, train_anomaly_labels), axis=0)\n",
        "        \n",
        "        return train_file_list, train_labels\n",
        "    \n",
        "    def _test_file_list(self, device, id):     \n",
        "        query = os.path.abspath(\n",
        "            f\"{self.audio_dir}/{device}/test/normal_id_0{id}*.wav\"\n",
        "            )\n",
        "        test_normal_files = sorted(glob.glob(query))\n",
        "        test_normal_labels = np.zeros(len(test_normal_files))\n",
        "        \n",
        "        query = os.path.abspath(\n",
        "            f\"{self.audio_dir}/{device}/test/anomaly_id_0{id}*.wav\"\n",
        "            )\n",
        "        test_anomaly_files = sorted(glob.glob(query))\n",
        "        test_anomaly_labels = np.ones(len(test_anomaly_files))\n",
        "        \n",
        "        test_file_list = np.concatenate((test_normal_files, \n",
        "                                          test_anomaly_files), axis=0)\n",
        "        test_labels = np.concatenate((test_normal_labels,\n",
        "                                      test_anomaly_labels), axis=0)\n",
        "          \n",
        "        return test_file_list, test_labels\n",
        "\n",
        "    def normalize(self,tensor):\n",
        "        tensor_minusmean = tensor - tensor.mean()\n",
        "        return tensor_minusmean/np.absolute(tensor_minusmean).max()\n",
        "\n",
        "    def make0min(self,tensornd):\n",
        "        tensor = tensornd.numpy()\n",
        "        res = np.where(tensor == 0, 1E-19 , tensor)\n",
        "        return torch.from_numpy(res)\n",
        "\n",
        "    def spectrogrameToImage(self,specgram):\n",
        "        # specgram = torchaudio.transforms.MelSpectrogram(n_fft=1024, win_length=1024, \n",
        "        #                                                 hop_length=512, power=2, \n",
        "        #                                                 normalized=True, n_mels=128)(waveform )\n",
        "        specgram= self.make0min(specgram)\n",
        "        specgram = specgram.log2()[0,:,:].numpy()\n",
        "        \n",
        "        tr2image = transforms.Compose([transforms.ToPILImage()])\n",
        "\n",
        "        specgram= self.normalize(specgram)\n",
        "        # specgram = img_as_ubyte(specgram)\n",
        "        specgramImage = tr2image(specgram)\n",
        "        return specgramImage\n",
        "\n",
        "    def get_logmelspectrogram(self, waveform):\n",
        "        melspec = librosa.feature.melspectrogram(\n",
        "          n_fft=self.n_fft, win_length=self.win_length, hop_length=self.hop_length,\n",
        "          power=self.power,n_mels=self.n_mels,pad_mode=self.pad_mode,sr=self.sr,\n",
        "          center=self.center,norm=self.norm,htk=True,\n",
        "          y = waveform.numpy()\n",
        "        )\n",
        "\n",
        "        logmelspec = librosa.power_to_db(melspec)\n",
        "\n",
        "        return logmelspec\n",
        "\n",
        "    def get_melspectrogram(self,waveform):\n",
        "        melspec = librosa.feature.melspectrogram(\n",
        "            n_fft=self.n_fft, win_length=self.win_length, hop_length=self.hop_length,\n",
        "            power=self.power,n_mels=self.n_mels,pad_mode=self.pad_mode,sr=self.sr,\n",
        "            center=self.center,norm=self.norm,htk=True,\n",
        "            y = waveform.numpy()\n",
        "        )\n",
        "\n",
        "        return melspec\n",
        "    \n",
        "    def get_mfcc(self,waveform):\n",
        "        mfcc = librosa.feature.mfcc(    \n",
        "            n_fft=self.n_fft, win_length=self.win_length, \n",
        "            hop_length=self.hop_length,pad_mode=self.pad_mode,sr=self.sr,\n",
        "            center=self.center,norm=self.norm,n_mfcc=40,\n",
        "            y = waveform.numpy()\n",
        "        )\n",
        "\n",
        "        return mfcc\n",
        "\n",
        "    def get_chroma_stft(self,waveform):\n",
        "        stft = librosa.feature.chroma_stft(\n",
        "            n_fft=self.n_fft, win_length=self.win_length, \n",
        "            hop_length=self.hop_length,pad_mode=self.pad_mode,sr=self.sr,\n",
        "            center=self.center,norm=self.norm,n_chroma=12,\n",
        "            y=waveform.numpy()\n",
        "        )\n",
        "\n",
        "        return stft\n",
        "\n",
        "    def get_spectral_contrast(self,waveform):\n",
        "        spec_contrast = librosa.feature.spectral_contrast(    \n",
        "            n_fft=self.n_fft, win_length=self.win_length,center=self.center,\n",
        "            hop_length=self.hop_length,pad_mode=self.pad_mode,sr=self.sr,\n",
        "            y = waveform.numpy()\n",
        "        )\n",
        "\n",
        "        return spec_contrast\n",
        "    \n",
        "    def get_tonnetz(self,waveform):\n",
        "        harmonic = librosa.effects.harmonic(waveform.numpy())\n",
        "        tonnetz = librosa.feature.tonnetz(y=harmonic,sr=self.sr)\n",
        "\n",
        "        return tonnetz\n",
        "\n",
        "    def get_audios(self, file_list):\n",
        "        data = []\n",
        "        for i in range(len(file_list)):\n",
        "          y, sr = torchaudio.load(file_list[i])  \n",
        "          data.append(y)\n",
        "\n",
        "        return data\n",
        "    def _derive_data(self, file_list):\n",
        "        train_data = []\n",
        "        test_data = []\n",
        "        train_mode = True\n",
        "        for file_list in [self.train_files, self.test_files]:\n",
        "          tr2tensor = transforms.Compose([transforms.PILToTensor()])\n",
        "          data = []\n",
        "          for j in range(len(file_list)):\n",
        "            y, sr = torchaudio.load(file_list[j])  \n",
        "            spec = self.get_melspectrogram(y)\n",
        "            spec = self.spectrogrameToImage(spec)\n",
        "            spec = spec.convert('RGB')\n",
        "            vectors = tr2tensor(spec)\n",
        "            if train_mode:     \n",
        "              train_data.append(vectors)\n",
        "            else:\n",
        "              test_data.append(vectors)\n",
        "            \n",
        "          train_mode = False\n",
        "                \n",
        "        return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "S96soeIc0o13"
      },
      "outputs": [],
      "source": [
        "dataset = MimiiDataset('/content/drive/MyDrive/mimii')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {
        "id": "Gn2zdn92doi1"
      },
      "outputs": [],
      "source": [
        "_, _, y_train, y_test = dataset.get_data('valve', 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {
        "id": "SgjpeWy_RV1C"
      },
      "outputs": [],
      "source": [
        "train_mixed_f = torch.load('/content/drive/MyDrive/mixed_features/train_mf_valve4.pt')\n",
        "test_mixed_f = torch.load('/content/drive/MyDrive/mixed_features/test_mf_valve4.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEl3qOh-mZVK",
        "outputId": "22c2bbfd-96f3-4105-8448-2af5a4a78fae"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([900, 193])"
            ]
          },
          "metadata": {},
          "execution_count": 188
        }
      ],
      "source": [
        "train_mixed_f.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {
        "id": "jWMPVGu1qiEq"
      },
      "outputs": [],
      "source": [
        "train_data = DataLoader(train_mixed_f, batch_size=64, shuffle = True)\n",
        "test_data = DataLoader(test_mixed_f, batch_size = 64, shuffle= False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {
        "id": "vNTBTRe6qnBq"
      },
      "outputs": [],
      "source": [
        "class UNet_FC(nn.Module):\n",
        "\n",
        "  def __init__(self, in_features):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.bn = nn.BatchNorm1d(128)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "    self.fc0 = nn.Linear(in_features=in_features,out_features=in_features)\n",
        "\n",
        "    # encoder\n",
        "    self.fc1 = nn.Linear(in_features=in_features, out_features=128)\n",
        "    self.fc2 = nn.Linear(in_features=128,out_features=128)\n",
        "    self.fc3 = nn.Linear(in_features=128, out_features=128)\n",
        "    self.fc4 = nn.Linear(in_features=128, out_features=128)\n",
        "    self.fc5 = nn.Linear(in_features=128, out_features=8)\n",
        "\n",
        "    # decoder\n",
        "    self.fc6 = nn.Linear(in_features=8, out_features=128)\n",
        "    self.fc7 = nn.Linear(in_features=128*2, out_features=128)\n",
        "    self.fc8 = nn.Linear(in_features=128*2, out_features=128)\n",
        "    self.fc9 = nn.Linear(in_features=128*2, out_features=128)\n",
        "\n",
        "    self.out = nn.Linear(in_features=128*2, out_features=in_features)\n",
        "\n",
        "  def encoder(self, x):\n",
        "    input = self.fc0(x)\n",
        "\n",
        "    x1 = self.relu(self.bn(self.fc1(input)))\n",
        "    x2 = self.relu(self.bn(self.fc2(x1)))\n",
        "    x3 = self.relu(self.bn(self.fc3(x2)))\n",
        "    x4 = self.relu(self.bn(self.fc4(x3)))\n",
        "    x5 = self.relu(self.fc5(x4))\n",
        "\n",
        "    return [x5, x4, x3, x2, x1]\n",
        "\n",
        "  def decoder(self, x):\n",
        "    x6 = self.relu(self.fc6(x[0]))\n",
        "    con1 = torch.cat((x6,x[1]), 1) \n",
        "    x7 = self.relu(self.bn(self.fc7(con1)))\n",
        "    con2 = torch.cat((x7,x[2]), 1)\n",
        "    x8 = self.relu(self.bn(self.fc8(con2)))\n",
        "    con3 = torch.cat((x8,x[3]), 1)\n",
        "    x9 = self.relu(self.bn(self.fc9(con3)))\n",
        "    con4 = torch.cat((x9,x[4]), 1)\n",
        "\n",
        "    x10 = self.out(con4)\n",
        "\n",
        "    return x10\n",
        "  \n",
        "  def forward(self, x):\n",
        "    # encoded = self.encoder(x)\n",
        "\n",
        "    # decoded = self.decoder(encoded)\n",
        "    input = self.fc0(x)\n",
        "\n",
        "    x1 = self.relu(self.bn(self.fc1(input)))\n",
        "    x2 = self.relu(self.bn(self.fc2(x1)))\n",
        "    x3 = self.relu(self.bn(self.fc3(x2)))\n",
        "    x4 = self.relu(self.bn(self.fc4(x3)))\n",
        "    x5 = self.relu(self.fc5(x4))\n",
        "\n",
        "    xy = [x5, x4, x3, x2, x1]\n",
        "\n",
        "    x6 = self.relu(self.fc6(xy[0]))\n",
        "    con1 = torch.cat((x6,xy[1]), 1) \n",
        "    x7 = self.relu(self.bn(self.fc7(con1)))\n",
        "    con2 = torch.cat((x7,xy[2]), 1)\n",
        "    x8 = self.relu(self.bn(self.fc8(con2)))\n",
        "    con3 = torch.cat((x8,xy[3]), 1)\n",
        "    x9 = self.relu(self.bn(self.fc9(con3)))\n",
        "    con4 = torch.cat((x9,xy[4]), 1)\n",
        "\n",
        "    x10 = self.out(con4)\n",
        "\n",
        "    # return decoded\n",
        "    return x10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "metadata": {
        "id": "ZfgcBtQ3qn5l"
      },
      "outputs": [],
      "source": [
        "def train(model, optimizer, criterion, data_tr, data_val, scheduler = None,\n",
        "          epochs = 3000, device = 'cpu'):\n",
        "    # X_val, Y_val = next(iter(data_val))\n",
        "    losses = []\n",
        "    prev_avg_loss = 100000\n",
        "    for epoch in range(epochs):\n",
        "        train_avg_loss = 0\n",
        "        test_avg_loss = 0\n",
        "        # model.train()  # train mode\n",
        "        for batch in data_tr:\n",
        "          # data to device\n",
        "          batch = batch.to(device)\n",
        "          # set parameter gradients to zero\n",
        "          optimizer.zero_grad()\n",
        "          # forward\n",
        "          # print(Y_batch.shape)\n",
        "          predictions = model(batch)\n",
        "          loss = criterion(predictions, batch)\n",
        "          loss.backward() # backward-pass\n",
        "          optimizer.step()  # update weights\n",
        "          # calculate loss to show the user\n",
        "          if scheduler:\n",
        "            scheduler.step(loss)\n",
        "          train_avg_loss += loss / len(data_tr)\n",
        "\n",
        "        # model.eval()\n",
        "        for batch in data_val:\n",
        "          with torch.no_grad():\n",
        "            preds = model(batch.to(device)).cpu()\n",
        "            loss = criterion(preds,batch)\n",
        "            test_avg_loss += loss / len(data_val)\n",
        "                    \n",
        "        losses.append(train_avg_loss.item())\n",
        "        # if (epoch+1)%50 == 0:\n",
        "        print(\"{}/{} train_loss: {} test_loss:{}\".format(epoch+1, epochs, train_avg_loss, test_avg_loss))\n",
        "        # if test_avg_loss < 70:\n",
        "        #   break\n",
        "    return losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {
        "id": "ptkVTF55quOL"
      },
      "outputs": [],
      "source": [
        "unet = UNet_FC(in_features=193).to(device)\n",
        "optimizer = Adam(params = unet.parameters(), lr = 10e-3)\n",
        "# optimizer = Adam(params = unet.parameters())\n",
        "criterion = nn.MSELoss()\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, \n",
        "                                                       min_lr=10e-4, mode = 'min',\n",
        "                                                       patience = 30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkfmYl9oXhcB",
        "outputId": "24f6cd84-c9ca-4ec7-9c2e-0bfa7bb57ccc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/3000 train_loss: 331715.03125 test_loss:328063.375\n",
            "2/3000 train_loss: 327553.15625 test_loss:324151.625\n",
            "3/3000 train_loss: 324931.0 test_loss:318297.5\n",
            "4/3000 train_loss: 315856.84375 test_loss:310205.0625\n",
            "5/3000 train_loss: 305383.4375 test_loss:299828.75\n",
            "6/3000 train_loss: 297341.0 test_loss:286903.0\n",
            "7/3000 train_loss: 281738.90625 test_loss:272268.90625\n",
            "8/3000 train_loss: 270253.3125 test_loss:263113.75\n",
            "9/3000 train_loss: 261371.390625 test_loss:255008.046875\n",
            "10/3000 train_loss: 257265.484375 test_loss:246971.0625\n",
            "11/3000 train_loss: 246561.90625 test_loss:238340.21875\n",
            "12/3000 train_loss: 235894.765625 test_loss:230327.375\n",
            "13/3000 train_loss: 225922.046875 test_loss:220773.21875\n",
            "14/3000 train_loss: 218491.671875 test_loss:212619.90625\n",
            "15/3000 train_loss: 208071.953125 test_loss:203494.546875\n",
            "16/3000 train_loss: 201032.15625 test_loss:194557.3125\n",
            "17/3000 train_loss: 190842.84375 test_loss:185880.03125\n",
            "18/3000 train_loss: 182881.953125 test_loss:176884.125\n",
            "19/3000 train_loss: 172665.890625 test_loss:167920.1875\n",
            "20/3000 train_loss: 164773.75 test_loss:158695.09375\n",
            "21/3000 train_loss: 154826.0 test_loss:150663.09375\n",
            "22/3000 train_loss: 147158.5 test_loss:141451.640625\n",
            "23/3000 train_loss: 138157.109375 test_loss:131986.59375\n",
            "24/3000 train_loss: 128744.234375 test_loss:124079.515625\n",
            "25/3000 train_loss: 120643.234375 test_loss:115288.0625\n",
            "26/3000 train_loss: 113355.3359375 test_loss:107233.9921875\n",
            "27/3000 train_loss: 106500.171875 test_loss:99609.8125\n",
            "28/3000 train_loss: 97514.609375 test_loss:92282.515625\n",
            "29/3000 train_loss: 88710.5625 test_loss:84514.6171875\n",
            "30/3000 train_loss: 80598.7578125 test_loss:77279.1640625\n",
            "31/3000 train_loss: 74695.6953125 test_loss:70481.4921875\n",
            "32/3000 train_loss: 68740.640625 test_loss:65046.9140625\n",
            "33/3000 train_loss: 65160.87890625 test_loss:61470.47265625\n",
            "34/3000 train_loss: 61234.6171875 test_loss:58387.1875\n",
            "35/3000 train_loss: 58346.46484375 test_loss:55111.3984375\n",
            "36/3000 train_loss: 54350.5625 test_loss:52327.484375\n",
            "37/3000 train_loss: 51332.43359375 test_loss:49391.34375\n",
            "38/3000 train_loss: 49564.57421875 test_loss:47109.125\n",
            "39/3000 train_loss: 46118.640625 test_loss:44228.43359375\n",
            "40/3000 train_loss: 44342.46875 test_loss:41795.41015625\n",
            "41/3000 train_loss: 41123.45703125 test_loss:39924.52734375\n",
            "42/3000 train_loss: 39858.76953125 test_loss:37376.29296875\n",
            "43/3000 train_loss: 37002.0 test_loss:35125.9921875\n",
            "44/3000 train_loss: 34741.82421875 test_loss:33102.02734375\n",
            "45/3000 train_loss: 32332.103515625 test_loss:31060.8828125\n",
            "46/3000 train_loss: 30988.751953125 test_loss:29092.814453125\n",
            "47/3000 train_loss: 28758.6640625 test_loss:27100.859375\n",
            "48/3000 train_loss: 27706.87109375 test_loss:25875.763671875\n",
            "49/3000 train_loss: 26357.736328125 test_loss:24385.62890625\n",
            "50/3000 train_loss: 25128.9296875 test_loss:22317.92578125\n",
            "51/3000 train_loss: 21944.521484375 test_loss:20503.015625\n",
            "52/3000 train_loss: 21282.86328125 test_loss:19292.845703125\n",
            "53/3000 train_loss: 19054.6953125 test_loss:18082.6328125\n",
            "54/3000 train_loss: 18260.548828125 test_loss:17038.8125\n",
            "55/3000 train_loss: 18042.33203125 test_loss:15511.3037109375\n",
            "56/3000 train_loss: 15195.810546875 test_loss:14342.306640625\n",
            "57/3000 train_loss: 14879.53125 test_loss:13332.6953125\n",
            "58/3000 train_loss: 13279.2333984375 test_loss:13246.3154296875\n",
            "59/3000 train_loss: 13359.306640625 test_loss:12118.767578125\n",
            "60/3000 train_loss: 13172.466796875 test_loss:11152.015625\n",
            "61/3000 train_loss: 11117.0302734375 test_loss:10749.11328125\n",
            "62/3000 train_loss: 11402.83984375 test_loss:10200.4853515625\n",
            "63/3000 train_loss: 11662.6455078125 test_loss:9791.1455078125\n",
            "64/3000 train_loss: 10366.1689453125 test_loss:9555.958984375\n",
            "65/3000 train_loss: 9761.6689453125 test_loss:9162.662109375\n",
            "66/3000 train_loss: 9398.1044921875 test_loss:8833.408203125\n",
            "67/3000 train_loss: 8839.576171875 test_loss:8491.4404296875\n",
            "68/3000 train_loss: 8462.5439453125 test_loss:8252.4091796875\n",
            "69/3000 train_loss: 8410.3896484375 test_loss:7882.93505859375\n",
            "70/3000 train_loss: 8639.88671875 test_loss:7670.2685546875\n",
            "71/3000 train_loss: 7983.09765625 test_loss:7617.119140625\n",
            "72/3000 train_loss: 8093.91552734375 test_loss:7509.9462890625\n",
            "73/3000 train_loss: 7967.42919921875 test_loss:7141.994140625\n",
            "74/3000 train_loss: 7507.1982421875 test_loss:6840.2412109375\n",
            "75/3000 train_loss: 7466.634765625 test_loss:6638.9228515625\n",
            "76/3000 train_loss: 6730.193359375 test_loss:6331.943359375\n",
            "77/3000 train_loss: 6304.041015625 test_loss:6093.04736328125\n",
            "78/3000 train_loss: 5954.16796875 test_loss:5806.81884765625\n",
            "79/3000 train_loss: 5882.6884765625 test_loss:5564.64013671875\n",
            "80/3000 train_loss: 6063.27783203125 test_loss:5405.93798828125\n",
            "81/3000 train_loss: 5488.296875 test_loss:5188.07421875\n",
            "82/3000 train_loss: 5612.267578125 test_loss:5059.6064453125\n",
            "83/3000 train_loss: 5361.87158203125 test_loss:4793.1767578125\n",
            "84/3000 train_loss: 4752.966796875 test_loss:4602.26220703125\n",
            "85/3000 train_loss: 4914.7421875 test_loss:4472.5380859375\n",
            "86/3000 train_loss: 5564.66796875 test_loss:4254.4951171875\n",
            "87/3000 train_loss: 4877.60498046875 test_loss:4131.79248046875\n",
            "88/3000 train_loss: 4179.533203125 test_loss:3948.45654296875\n",
            "89/3000 train_loss: 4106.5078125 test_loss:3767.108154296875\n",
            "90/3000 train_loss: 4372.439453125 test_loss:3690.7109375\n",
            "91/3000 train_loss: 3626.702880859375 test_loss:3547.930419921875\n",
            "92/3000 train_loss: 3893.848388671875 test_loss:3500.90673828125\n",
            "93/3000 train_loss: 3497.74755859375 test_loss:3418.17919921875\n",
            "94/3000 train_loss: 3447.957275390625 test_loss:3244.79833984375\n",
            "95/3000 train_loss: 3693.619384765625 test_loss:3174.776611328125\n",
            "96/3000 train_loss: 3337.689453125 test_loss:3099.943359375\n",
            "97/3000 train_loss: 3052.000244140625 test_loss:2962.084228515625\n",
            "98/3000 train_loss: 2953.9326171875 test_loss:2798.7138671875\n",
            "99/3000 train_loss: 2701.77734375 test_loss:2735.621337890625\n",
            "100/3000 train_loss: 3006.74072265625 test_loss:2590.904296875\n",
            "101/3000 train_loss: 2531.77392578125 test_loss:2485.81494140625\n",
            "102/3000 train_loss: 2649.354736328125 test_loss:2392.633056640625\n",
            "103/3000 train_loss: 2734.19189453125 test_loss:2297.6435546875\n",
            "104/3000 train_loss: 2280.546630859375 test_loss:2232.23681640625\n",
            "105/3000 train_loss: 2172.06005859375 test_loss:2141.557861328125\n",
            "106/3000 train_loss: 2364.959716796875 test_loss:2057.3984375\n",
            "107/3000 train_loss: 2026.2310791015625 test_loss:2001.872314453125\n",
            "108/3000 train_loss: 1950.326416015625 test_loss:1960.4083251953125\n",
            "109/3000 train_loss: 2101.67236328125 test_loss:1846.679443359375\n",
            "110/3000 train_loss: 2047.482421875 test_loss:1801.961669921875\n",
            "111/3000 train_loss: 1753.6854248046875 test_loss:1727.3094482421875\n",
            "112/3000 train_loss: 1724.984375 test_loss:1649.7386474609375\n",
            "113/3000 train_loss: 1728.708251953125 test_loss:1634.50146484375\n",
            "114/3000 train_loss: 1718.7427978515625 test_loss:1573.9154052734375\n",
            "115/3000 train_loss: 1587.5711669921875 test_loss:1576.956787109375\n",
            "116/3000 train_loss: 1514.6240234375 test_loss:1484.3067626953125\n",
            "117/3000 train_loss: 1440.7879638671875 test_loss:1418.9334716796875\n",
            "118/3000 train_loss: 1507.8701171875 test_loss:1384.6278076171875\n",
            "119/3000 train_loss: 1326.2989501953125 test_loss:1308.1890869140625\n",
            "120/3000 train_loss: 1376.285888671875 test_loss:1334.231201171875\n",
            "121/3000 train_loss: 1418.16015625 test_loss:1252.146484375\n",
            "122/3000 train_loss: 1345.783935546875 test_loss:1307.0400390625\n",
            "123/3000 train_loss: 1226.361572265625 test_loss:1198.993896484375\n",
            "124/3000 train_loss: 1247.4913330078125 test_loss:1190.9140625\n",
            "125/3000 train_loss: 1151.674560546875 test_loss:1099.4598388671875\n",
            "126/3000 train_loss: 1373.2886962890625 test_loss:1093.4661865234375\n",
            "127/3000 train_loss: 1027.06494140625 test_loss:1079.680419921875\n",
            "128/3000 train_loss: 1043.2471923828125 test_loss:1059.9892578125\n",
            "129/3000 train_loss: 1019.578857421875 test_loss:1038.5374755859375\n",
            "130/3000 train_loss: 1087.501708984375 test_loss:988.9141845703125\n",
            "131/3000 train_loss: 986.0166015625 test_loss:993.4163818359375\n",
            "132/3000 train_loss: 1097.3580322265625 test_loss:953.7792358398438\n",
            "133/3000 train_loss: 1128.48486328125 test_loss:970.1852416992188\n",
            "134/3000 train_loss: 844.47509765625 test_loss:933.3514404296875\n",
            "135/3000 train_loss: 1276.77685546875 test_loss:984.212646484375\n",
            "136/3000 train_loss: 1087.2376708984375 test_loss:1217.869384765625\n",
            "137/3000 train_loss: 1392.9403076171875 test_loss:1327.1356201171875\n",
            "138/3000 train_loss: 1441.745361328125 test_loss:1208.781005859375\n",
            "139/3000 train_loss: 1191.272705078125 test_loss:1112.964599609375\n",
            "140/3000 train_loss: 1199.037109375 test_loss:1054.7371826171875\n",
            "141/3000 train_loss: 1052.4278564453125 test_loss:1078.541015625\n",
            "142/3000 train_loss: 954.101806640625 test_loss:1037.404541015625\n",
            "143/3000 train_loss: 932.9866333007812 test_loss:978.6591796875\n",
            "144/3000 train_loss: 1101.658935546875 test_loss:954.208984375\n",
            "145/3000 train_loss: 882.0038452148438 test_loss:887.8389892578125\n",
            "146/3000 train_loss: 998.7833251953125 test_loss:897.548095703125\n",
            "147/3000 train_loss: 890.9844970703125 test_loss:923.398193359375\n",
            "148/3000 train_loss: 757.8192749023438 test_loss:837.919921875\n",
            "149/3000 train_loss: 773.8001098632812 test_loss:849.1765747070312\n",
            "150/3000 train_loss: 748.9189453125 test_loss:806.1104736328125\n",
            "151/3000 train_loss: 794.46142578125 test_loss:801.5792846679688\n",
            "152/3000 train_loss: 796.7699584960938 test_loss:772.3373413085938\n",
            "153/3000 train_loss: 798.6353149414062 test_loss:759.01513671875\n",
            "154/3000 train_loss: 770.78173828125 test_loss:786.245849609375\n",
            "155/3000 train_loss: 713.0037841796875 test_loss:752.3560791015625\n",
            "156/3000 train_loss: 698.5885009765625 test_loss:757.4115600585938\n",
            "157/3000 train_loss: 1115.0992431640625 test_loss:742.2526245117188\n",
            "158/3000 train_loss: 1108.243408203125 test_loss:763.304931640625\n",
            "159/3000 train_loss: 887.07177734375 test_loss:808.0765380859375\n",
            "160/3000 train_loss: 1129.000732421875 test_loss:797.28369140625\n",
            "161/3000 train_loss: 654.0882568359375 test_loss:706.1180419921875\n",
            "162/3000 train_loss: 957.802734375 test_loss:748.1468505859375\n",
            "163/3000 train_loss: 822.72265625 test_loss:774.5841064453125\n",
            "164/3000 train_loss: 920.313720703125 test_loss:710.4114379882812\n",
            "165/3000 train_loss: 723.5126953125 test_loss:695.115966796875\n",
            "166/3000 train_loss: 1119.349365234375 test_loss:686.6767578125\n",
            "167/3000 train_loss: 679.3565063476562 test_loss:717.7127685546875\n",
            "168/3000 train_loss: 946.7728271484375 test_loss:683.0635375976562\n",
            "169/3000 train_loss: 637.5968627929688 test_loss:688.9749755859375\n",
            "170/3000 train_loss: 718.755859375 test_loss:680.6050415039062\n",
            "171/3000 train_loss: 694.0897827148438 test_loss:678.3604736328125\n",
            "172/3000 train_loss: 927.6290283203125 test_loss:683.9421997070312\n",
            "173/3000 train_loss: 880.0519409179688 test_loss:857.4191284179688\n",
            "174/3000 train_loss: 970.3226318359375 test_loss:761.545166015625\n",
            "175/3000 train_loss: 888.7266845703125 test_loss:811.3072509765625\n",
            "176/3000 train_loss: 702.2832641601562 test_loss:725.8740844726562\n",
            "177/3000 train_loss: 896.8759765625 test_loss:741.9728393554688\n",
            "178/3000 train_loss: 751.977783203125 test_loss:705.0791625976562\n",
            "179/3000 train_loss: 684.708251953125 test_loss:697.77978515625\n",
            "180/3000 train_loss: 662.5634765625 test_loss:722.3564453125\n",
            "181/3000 train_loss: 667.4881591796875 test_loss:717.3961181640625\n",
            "182/3000 train_loss: 698.074462890625 test_loss:711.9337768554688\n",
            "183/3000 train_loss: 903.0794677734375 test_loss:738.312255859375\n",
            "184/3000 train_loss: 665.6702880859375 test_loss:727.0482177734375\n",
            "185/3000 train_loss: 1000.677734375 test_loss:736.4030151367188\n",
            "186/3000 train_loss: 753.151611328125 test_loss:739.3788452148438\n",
            "187/3000 train_loss: 821.7823486328125 test_loss:695.5255126953125\n",
            "188/3000 train_loss: 673.8305053710938 test_loss:743.2022094726562\n",
            "189/3000 train_loss: 907.5709228515625 test_loss:701.3673095703125\n",
            "190/3000 train_loss: 646.1723022460938 test_loss:699.2077026367188\n",
            "191/3000 train_loss: 702.3294677734375 test_loss:690.530029296875\n",
            "192/3000 train_loss: 673.1331787109375 test_loss:711.9697875976562\n",
            "193/3000 train_loss: 541.5658569335938 test_loss:661.377685546875\n",
            "194/3000 train_loss: 739.834716796875 test_loss:687.9293823242188\n",
            "195/3000 train_loss: 624.0304565429688 test_loss:712.0222778320312\n",
            "196/3000 train_loss: 640.3673095703125 test_loss:702.7245483398438\n",
            "197/3000 train_loss: 621.4901123046875 test_loss:711.812744140625\n",
            "198/3000 train_loss: 577.2371826171875 test_loss:697.1063232421875\n",
            "199/3000 train_loss: 659.341552734375 test_loss:693.0853271484375\n",
            "200/3000 train_loss: 746.2850341796875 test_loss:671.3992309570312\n",
            "201/3000 train_loss: 762.7607421875 test_loss:721.3865356445312\n",
            "202/3000 train_loss: 1361.3809814453125 test_loss:728.2852172851562\n",
            "203/3000 train_loss: 708.3489379882812 test_loss:688.863525390625\n",
            "204/3000 train_loss: 687.579833984375 test_loss:631.8431396484375\n",
            "205/3000 train_loss: 657.361572265625 test_loss:634.9402465820312\n",
            "206/3000 train_loss: 667.6272583007812 test_loss:673.054931640625\n",
            "207/3000 train_loss: 596.5338134765625 test_loss:631.8558349609375\n",
            "208/3000 train_loss: 733.3441162109375 test_loss:651.5865478515625\n",
            "209/3000 train_loss: 644.2763061523438 test_loss:611.7220458984375\n",
            "210/3000 train_loss: 646.3529663085938 test_loss:614.70703125\n",
            "211/3000 train_loss: 700.21728515625 test_loss:635.5570678710938\n",
            "212/3000 train_loss: 620.49853515625 test_loss:632.6582641601562\n",
            "213/3000 train_loss: 939.3235473632812 test_loss:625.4284057617188\n",
            "214/3000 train_loss: 1012.200927734375 test_loss:610.7183227539062\n",
            "215/3000 train_loss: 644.56591796875 test_loss:635.13037109375\n",
            "216/3000 train_loss: 586.1898803710938 test_loss:614.1757202148438\n",
            "217/3000 train_loss: 584.4017333984375 test_loss:603.5208129882812\n",
            "218/3000 train_loss: 536.1965942382812 test_loss:599.4680786132812\n",
            "219/3000 train_loss: 558.6671752929688 test_loss:605.9959106445312\n",
            "220/3000 train_loss: 507.230712890625 test_loss:593.9437866210938\n",
            "221/3000 train_loss: 551.4954223632812 test_loss:592.0887451171875\n",
            "222/3000 train_loss: 680.05615234375 test_loss:605.5750732421875\n",
            "223/3000 train_loss: 534.24560546875 test_loss:591.6082153320312\n",
            "224/3000 train_loss: 574.4224853515625 test_loss:592.7644653320312\n",
            "225/3000 train_loss: 521.0525512695312 test_loss:586.9653930664062\n",
            "226/3000 train_loss: 570.8265380859375 test_loss:582.8043212890625\n",
            "227/3000 train_loss: 669.9371337890625 test_loss:588.8491821289062\n",
            "228/3000 train_loss: 556.7485961914062 test_loss:595.082275390625\n",
            "229/3000 train_loss: 573.50390625 test_loss:598.1040649414062\n",
            "230/3000 train_loss: 527.2227783203125 test_loss:593.057861328125\n",
            "231/3000 train_loss: 646.567626953125 test_loss:587.328125\n",
            "232/3000 train_loss: 855.9439697265625 test_loss:595.6981201171875\n",
            "233/3000 train_loss: 503.789306640625 test_loss:584.7561645507812\n",
            "234/3000 train_loss: 531.015625 test_loss:577.9427490234375\n",
            "235/3000 train_loss: 514.8945922851562 test_loss:579.44140625\n",
            "236/3000 train_loss: 531.2037353515625 test_loss:576.2850952148438\n",
            "237/3000 train_loss: 612.7256469726562 test_loss:600.1681518554688\n",
            "238/3000 train_loss: 581.0343017578125 test_loss:574.337890625\n",
            "239/3000 train_loss: 540.3163452148438 test_loss:579.8265380859375\n",
            "240/3000 train_loss: 548.0541381835938 test_loss:587.0719604492188\n",
            "241/3000 train_loss: 527.3748168945312 test_loss:567.509033203125\n",
            "242/3000 train_loss: 517.4661254882812 test_loss:565.741943359375\n",
            "243/3000 train_loss: 670.208740234375 test_loss:571.4820556640625\n",
            "244/3000 train_loss: 491.0837707519531 test_loss:568.7743530273438\n",
            "245/3000 train_loss: 512.5855102539062 test_loss:565.9894409179688\n",
            "246/3000 train_loss: 617.605224609375 test_loss:558.847900390625\n",
            "247/3000 train_loss: 720.3133544921875 test_loss:565.087890625\n",
            "248/3000 train_loss: 487.0234069824219 test_loss:567.9691162109375\n",
            "249/3000 train_loss: 836.27587890625 test_loss:567.1158447265625\n",
            "250/3000 train_loss: 593.1084594726562 test_loss:600.4580078125\n",
            "251/3000 train_loss: 625.3577880859375 test_loss:579.9056396484375\n",
            "252/3000 train_loss: 526.6964111328125 test_loss:586.21875\n",
            "253/3000 train_loss: 563.6314086914062 test_loss:560.7999877929688\n",
            "254/3000 train_loss: 962.6486206054688 test_loss:552.7451171875\n",
            "255/3000 train_loss: 662.7999267578125 test_loss:553.2577514648438\n",
            "256/3000 train_loss: 883.7738037109375 test_loss:559.1536865234375\n",
            "257/3000 train_loss: 617.967041015625 test_loss:591.7245483398438\n",
            "258/3000 train_loss: 579.1431884765625 test_loss:600.732666015625\n",
            "259/3000 train_loss: 735.1012573242188 test_loss:590.3251953125\n",
            "260/3000 train_loss: 552.75 test_loss:581.4822998046875\n",
            "261/3000 train_loss: 520.6519165039062 test_loss:574.2130126953125\n",
            "262/3000 train_loss: 576.4004516601562 test_loss:560.6382446289062\n",
            "263/3000 train_loss: 564.0687255859375 test_loss:568.066650390625\n",
            "264/3000 train_loss: 683.435546875 test_loss:558.7381591796875\n",
            "265/3000 train_loss: 506.2304382324219 test_loss:581.0588989257812\n",
            "266/3000 train_loss: 768.734130859375 test_loss:571.1741943359375\n",
            "267/3000 train_loss: 665.7758178710938 test_loss:581.1124877929688\n",
            "268/3000 train_loss: 649.5200805664062 test_loss:604.2770385742188\n",
            "269/3000 train_loss: 524.3873291015625 test_loss:566.484375\n",
            "270/3000 train_loss: 900.9307861328125 test_loss:570.5382690429688\n",
            "271/3000 train_loss: 899.5017700195312 test_loss:627.59619140625\n",
            "272/3000 train_loss: 617.1021118164062 test_loss:598.2595825195312\n",
            "273/3000 train_loss: 518.07421875 test_loss:579.8618774414062\n",
            "274/3000 train_loss: 506.7359924316406 test_loss:566.7722778320312\n",
            "275/3000 train_loss: 521.15380859375 test_loss:566.6522216796875\n",
            "276/3000 train_loss: 584.3453979492188 test_loss:565.8012084960938\n",
            "277/3000 train_loss: 617.288818359375 test_loss:570.1883544921875\n",
            "278/3000 train_loss: 818.0556640625 test_loss:572.1239624023438\n",
            "279/3000 train_loss: 591.0157470703125 test_loss:571.899658203125\n",
            "280/3000 train_loss: 549.8867797851562 test_loss:562.8519897460938\n",
            "281/3000 train_loss: 540.86376953125 test_loss:557.5927734375\n",
            "282/3000 train_loss: 507.0008544921875 test_loss:555.982421875\n",
            "283/3000 train_loss: 542.0824584960938 test_loss:553.319580078125\n",
            "284/3000 train_loss: 559.1936645507812 test_loss:546.0555419921875\n",
            "285/3000 train_loss: 493.6794128417969 test_loss:563.3687744140625\n",
            "286/3000 train_loss: 518.5414428710938 test_loss:549.396240234375\n",
            "287/3000 train_loss: 539.98876953125 test_loss:557.7342529296875\n",
            "288/3000 train_loss: 510.3688659667969 test_loss:550.7696533203125\n",
            "289/3000 train_loss: 563.4835205078125 test_loss:551.0437622070312\n",
            "290/3000 train_loss: 529.8572998046875 test_loss:557.26806640625\n",
            "291/3000 train_loss: 539.6491088867188 test_loss:544.8674926757812\n",
            "292/3000 train_loss: 670.1583251953125 test_loss:548.651123046875\n",
            "293/3000 train_loss: 659.0379638671875 test_loss:571.0855712890625\n",
            "294/3000 train_loss: 501.9125671386719 test_loss:554.1995849609375\n",
            "295/3000 train_loss: 758.0115356445312 test_loss:556.3023071289062\n",
            "296/3000 train_loss: 563.7643432617188 test_loss:561.8839111328125\n",
            "297/3000 train_loss: 527.9127197265625 test_loss:558.9672241210938\n",
            "298/3000 train_loss: 509.0789489746094 test_loss:553.1964111328125\n",
            "299/3000 train_loss: 710.0276489257812 test_loss:544.9708251953125\n",
            "300/3000 train_loss: 530.7725219726562 test_loss:614.93115234375\n",
            "301/3000 train_loss: 576.6610107421875 test_loss:566.470458984375\n",
            "302/3000 train_loss: 503.42315673828125 test_loss:575.9509887695312\n",
            "303/3000 train_loss: 740.5330810546875 test_loss:547.0979614257812\n",
            "304/3000 train_loss: 515.5048828125 test_loss:578.4119873046875\n",
            "305/3000 train_loss: 606.9076538085938 test_loss:551.2073364257812\n",
            "306/3000 train_loss: 644.29736328125 test_loss:567.96240234375\n",
            "307/3000 train_loss: 565.640380859375 test_loss:567.6000366210938\n",
            "308/3000 train_loss: 469.2039794921875 test_loss:575.54541015625\n",
            "309/3000 train_loss: 510.8431396484375 test_loss:559.5635375976562\n",
            "310/3000 train_loss: 776.939697265625 test_loss:547.697021484375\n",
            "311/3000 train_loss: 542.6884155273438 test_loss:584.266357421875\n",
            "312/3000 train_loss: 531.6199340820312 test_loss:543.6231689453125\n",
            "313/3000 train_loss: 468.9664611816406 test_loss:539.0119018554688\n",
            "314/3000 train_loss: 827.25439453125 test_loss:557.1103515625\n",
            "315/3000 train_loss: 503.8768615722656 test_loss:549.989501953125\n",
            "316/3000 train_loss: 565.3671875 test_loss:543.5606689453125\n",
            "317/3000 train_loss: 503.0369873046875 test_loss:566.6256103515625\n",
            "318/3000 train_loss: 550.2216186523438 test_loss:556.5952758789062\n",
            "319/3000 train_loss: 512.1345825195312 test_loss:559.0841674804688\n",
            "320/3000 train_loss: 461.5096740722656 test_loss:550.9093017578125\n",
            "321/3000 train_loss: 629.9072875976562 test_loss:535.75\n",
            "322/3000 train_loss: 490.037841796875 test_loss:542.0889282226562\n",
            "323/3000 train_loss: 643.3978881835938 test_loss:539.8939208984375\n",
            "324/3000 train_loss: 470.43157958984375 test_loss:537.4873046875\n",
            "325/3000 train_loss: 508.0887451171875 test_loss:534.12060546875\n",
            "326/3000 train_loss: 506.94390869140625 test_loss:542.9899291992188\n",
            "327/3000 train_loss: 487.2891540527344 test_loss:533.23876953125\n",
            "328/3000 train_loss: 489.49432373046875 test_loss:524.236083984375\n",
            "329/3000 train_loss: 519.2427368164062 test_loss:520.78955078125\n",
            "330/3000 train_loss: 457.1501770019531 test_loss:523.8414916992188\n",
            "331/3000 train_loss: 465.2994689941406 test_loss:512.91064453125\n",
            "332/3000 train_loss: 506.9596862792969 test_loss:524.7106323242188\n",
            "333/3000 train_loss: 441.4312438964844 test_loss:516.078125\n",
            "334/3000 train_loss: 701.2689208984375 test_loss:525.39013671875\n",
            "335/3000 train_loss: 954.30615234375 test_loss:526.3246459960938\n",
            "336/3000 train_loss: 557.1620483398438 test_loss:566.71142578125\n",
            "337/3000 train_loss: 505.4017333984375 test_loss:528.518310546875\n",
            "338/3000 train_loss: 484.0863037109375 test_loss:522.8519287109375\n",
            "339/3000 train_loss: 448.83709716796875 test_loss:525.5314331054688\n",
            "340/3000 train_loss: 1289.1256103515625 test_loss:521.8906860351562\n",
            "341/3000 train_loss: 548.547607421875 test_loss:526.4122314453125\n",
            "342/3000 train_loss: 519.1979370117188 test_loss:536.9207763671875\n",
            "343/3000 train_loss: 517.9646606445312 test_loss:517.8599853515625\n",
            "344/3000 train_loss: 1096.448974609375 test_loss:520.2905883789062\n",
            "345/3000 train_loss: 476.9787902832031 test_loss:524.7733154296875\n",
            "346/3000 train_loss: 441.5006408691406 test_loss:524.5377197265625\n",
            "347/3000 train_loss: 612.8187255859375 test_loss:528.8848876953125\n",
            "348/3000 train_loss: 611.88525390625 test_loss:525.311767578125\n",
            "349/3000 train_loss: 508.0400390625 test_loss:533.0003051757812\n",
            "350/3000 train_loss: 495.8518371582031 test_loss:518.412841796875\n",
            "351/3000 train_loss: 588.4330444335938 test_loss:515.9404296875\n",
            "352/3000 train_loss: 453.01666259765625 test_loss:531.1318969726562\n",
            "353/3000 train_loss: 429.56329345703125 test_loss:506.4901123046875\n",
            "354/3000 train_loss: 579.8071899414062 test_loss:532.3050537109375\n",
            "355/3000 train_loss: 623.9136962890625 test_loss:513.9398193359375\n",
            "356/3000 train_loss: 621.2564697265625 test_loss:514.2044677734375\n",
            "357/3000 train_loss: 757.586669921875 test_loss:530.560546875\n",
            "358/3000 train_loss: 510.8189392089844 test_loss:548.7864379882812\n",
            "359/3000 train_loss: 531.7364501953125 test_loss:514.4223022460938\n",
            "360/3000 train_loss: 485.46575927734375 test_loss:508.4382019042969\n",
            "361/3000 train_loss: 459.79217529296875 test_loss:516.863037109375\n",
            "362/3000 train_loss: 563.779052734375 test_loss:504.2001953125\n",
            "363/3000 train_loss: 439.0841064453125 test_loss:510.866943359375\n",
            "364/3000 train_loss: 932.185791015625 test_loss:505.2777099609375\n",
            "365/3000 train_loss: 539.9070434570312 test_loss:515.9721069335938\n",
            "366/3000 train_loss: 589.46875 test_loss:524.6361694335938\n",
            "367/3000 train_loss: 458.81134033203125 test_loss:509.5102233886719\n",
            "368/3000 train_loss: 542.1512451171875 test_loss:507.2790832519531\n",
            "369/3000 train_loss: 434.2220153808594 test_loss:501.68695068359375\n",
            "370/3000 train_loss: 438.7635803222656 test_loss:501.4047546386719\n",
            "371/3000 train_loss: 671.4256591796875 test_loss:501.4223327636719\n",
            "372/3000 train_loss: 482.3224792480469 test_loss:505.9732971191406\n",
            "373/3000 train_loss: 479.7091369628906 test_loss:508.0346374511719\n",
            "374/3000 train_loss: 515.9452514648438 test_loss:502.706787109375\n",
            "375/3000 train_loss: 464.7231140136719 test_loss:512.7095947265625\n",
            "376/3000 train_loss: 569.9813232421875 test_loss:500.791259765625\n",
            "377/3000 train_loss: 445.8340148925781 test_loss:499.9993896484375\n",
            "378/3000 train_loss: 448.792724609375 test_loss:496.04150390625\n",
            "379/3000 train_loss: 468.0401306152344 test_loss:496.1526794433594\n",
            "380/3000 train_loss: 433.2356872558594 test_loss:493.87103271484375\n",
            "381/3000 train_loss: 531.4541625976562 test_loss:494.8898620605469\n",
            "382/3000 train_loss: 543.0887451171875 test_loss:500.94012451171875\n",
            "383/3000 train_loss: 473.49786376953125 test_loss:489.70611572265625\n",
            "384/3000 train_loss: 434.6605529785156 test_loss:490.3132019042969\n",
            "385/3000 train_loss: 468.6249694824219 test_loss:486.39404296875\n",
            "386/3000 train_loss: 518.196533203125 test_loss:507.9201965332031\n",
            "387/3000 train_loss: 575.5142211914062 test_loss:508.60577392578125\n",
            "388/3000 train_loss: 443.27313232421875 test_loss:483.2646789550781\n",
            "389/3000 train_loss: 448.1888732910156 test_loss:474.58001708984375\n",
            "390/3000 train_loss: 479.3375244140625 test_loss:474.1435241699219\n",
            "391/3000 train_loss: 482.1722412109375 test_loss:475.72235107421875\n",
            "392/3000 train_loss: 512.94189453125 test_loss:476.1409912109375\n",
            "393/3000 train_loss: 471.3453063964844 test_loss:473.42547607421875\n",
            "394/3000 train_loss: 472.0286560058594 test_loss:486.80059814453125\n",
            "395/3000 train_loss: 490.4090881347656 test_loss:474.98150634765625\n",
            "396/3000 train_loss: 434.32025146484375 test_loss:472.51123046875\n",
            "397/3000 train_loss: 451.8756103515625 test_loss:467.0754699707031\n",
            "398/3000 train_loss: 446.54345703125 test_loss:474.23431396484375\n",
            "399/3000 train_loss: 437.1392822265625 test_loss:475.34698486328125\n",
            "400/3000 train_loss: 434.8824462890625 test_loss:467.61993408203125\n",
            "401/3000 train_loss: 559.779052734375 test_loss:468.74273681640625\n",
            "402/3000 train_loss: 508.26580810546875 test_loss:481.8832702636719\n",
            "403/3000 train_loss: 463.414794921875 test_loss:463.5287780761719\n",
            "404/3000 train_loss: 502.94171142578125 test_loss:470.6342468261719\n",
            "405/3000 train_loss: 517.228515625 test_loss:471.845458984375\n",
            "406/3000 train_loss: 452.16998291015625 test_loss:463.268798828125\n",
            "407/3000 train_loss: 569.32177734375 test_loss:472.53350830078125\n",
            "408/3000 train_loss: 483.5486145019531 test_loss:463.8006591796875\n",
            "409/3000 train_loss: 461.20892333984375 test_loss:493.2242431640625\n",
            "410/3000 train_loss: 485.526123046875 test_loss:457.168701171875\n",
            "411/3000 train_loss: 428.9617004394531 test_loss:454.913818359375\n",
            "412/3000 train_loss: 454.19732666015625 test_loss:462.0899658203125\n",
            "413/3000 train_loss: 455.46368408203125 test_loss:471.7060241699219\n",
            "414/3000 train_loss: 435.4361572265625 test_loss:463.7027893066406\n",
            "415/3000 train_loss: 711.70263671875 test_loss:463.539306640625\n",
            "416/3000 train_loss: 486.5047912597656 test_loss:478.1473388671875\n",
            "417/3000 train_loss: 491.082275390625 test_loss:517.9527587890625\n",
            "418/3000 train_loss: 609.8441162109375 test_loss:473.36260986328125\n",
            "419/3000 train_loss: 467.8935241699219 test_loss:517.8088989257812\n",
            "420/3000 train_loss: 459.01361083984375 test_loss:507.0439758300781\n",
            "421/3000 train_loss: 483.4421691894531 test_loss:532.7462768554688\n",
            "422/3000 train_loss: 499.61077880859375 test_loss:508.7508239746094\n",
            "423/3000 train_loss: 562.30419921875 test_loss:488.13836669921875\n",
            "424/3000 train_loss: 519.1539916992188 test_loss:507.9718017578125\n",
            "425/3000 train_loss: 613.5091552734375 test_loss:495.81884765625\n",
            "426/3000 train_loss: 473.76007080078125 test_loss:479.41790771484375\n",
            "427/3000 train_loss: 418.0135803222656 test_loss:470.7283630371094\n",
            "428/3000 train_loss: 424.2139892578125 test_loss:473.237548828125\n",
            "429/3000 train_loss: 413.7047119140625 test_loss:485.7271423339844\n",
            "430/3000 train_loss: 588.1007080078125 test_loss:486.1603698730469\n",
            "431/3000 train_loss: 425.7772521972656 test_loss:482.6318359375\n",
            "432/3000 train_loss: 408.4400634765625 test_loss:454.29547119140625\n",
            "433/3000 train_loss: 443.0146484375 test_loss:464.7685546875\n",
            "434/3000 train_loss: 403.0078125 test_loss:451.345947265625\n",
            "435/3000 train_loss: 379.1058349609375 test_loss:446.9825439453125\n",
            "436/3000 train_loss: 437.09405517578125 test_loss:452.561767578125\n",
            "437/3000 train_loss: 972.768310546875 test_loss:467.8388671875\n",
            "438/3000 train_loss: 478.6033935546875 test_loss:467.37261962890625\n",
            "439/3000 train_loss: 472.81744384765625 test_loss:460.10430908203125\n",
            "440/3000 train_loss: 451.5621337890625 test_loss:448.0477600097656\n",
            "441/3000 train_loss: 580.831787109375 test_loss:461.9695739746094\n",
            "442/3000 train_loss: 573.271240234375 test_loss:456.6988220214844\n",
            "443/3000 train_loss: 424.8193359375 test_loss:456.6790466308594\n",
            "444/3000 train_loss: 397.7294921875 test_loss:450.0294189453125\n",
            "445/3000 train_loss: 573.2160034179688 test_loss:458.19610595703125\n",
            "446/3000 train_loss: 682.9942016601562 test_loss:446.72515869140625\n",
            "447/3000 train_loss: 483.56890869140625 test_loss:448.7285461425781\n",
            "448/3000 train_loss: 409.3916931152344 test_loss:451.41259765625\n",
            "449/3000 train_loss: 470.6745300292969 test_loss:448.6180419921875\n",
            "450/3000 train_loss: 506.61859130859375 test_loss:445.63446044921875\n",
            "451/3000 train_loss: 774.6865234375 test_loss:445.0046081542969\n",
            "452/3000 train_loss: 441.11724853515625 test_loss:436.13336181640625\n",
            "453/3000 train_loss: 752.7552490234375 test_loss:438.325439453125\n",
            "454/3000 train_loss: 425.159912109375 test_loss:442.24786376953125\n",
            "455/3000 train_loss: 456.27252197265625 test_loss:442.9824523925781\n",
            "456/3000 train_loss: 401.3852844238281 test_loss:435.52252197265625\n",
            "457/3000 train_loss: 390.4342041015625 test_loss:425.4198303222656\n",
            "458/3000 train_loss: 456.01806640625 test_loss:421.02984619140625\n",
            "459/3000 train_loss: 407.8725280761719 test_loss:431.52587890625\n",
            "460/3000 train_loss: 467.04241943359375 test_loss:423.1234130859375\n",
            "461/3000 train_loss: 517.6219482421875 test_loss:423.8930358886719\n",
            "462/3000 train_loss: 461.929443359375 test_loss:433.24627685546875\n",
            "463/3000 train_loss: 375.9856262207031 test_loss:421.55029296875\n",
            "464/3000 train_loss: 367.23291015625 test_loss:422.5623779296875\n",
            "465/3000 train_loss: 399.58154296875 test_loss:414.42266845703125\n",
            "466/3000 train_loss: 479.2862548828125 test_loss:414.8847351074219\n",
            "467/3000 train_loss: 473.02850341796875 test_loss:419.3552551269531\n",
            "468/3000 train_loss: 402.62322998046875 test_loss:429.4308166503906\n",
            "469/3000 train_loss: 392.6273193359375 test_loss:418.8853759765625\n",
            "470/3000 train_loss: 418.1478576660156 test_loss:419.67181396484375\n",
            "471/3000 train_loss: 381.80853271484375 test_loss:420.030029296875\n",
            "472/3000 train_loss: 392.2701416015625 test_loss:417.11688232421875\n",
            "473/3000 train_loss: 462.28399658203125 test_loss:420.1121520996094\n",
            "474/3000 train_loss: 402.24285888671875 test_loss:443.44305419921875\n",
            "475/3000 train_loss: 447.9375 test_loss:434.5977783203125\n",
            "476/3000 train_loss: 989.4581909179688 test_loss:439.0059814453125\n",
            "477/3000 train_loss: 562.87890625 test_loss:519.0427856445312\n",
            "478/3000 train_loss: 565.09619140625 test_loss:468.0114440917969\n",
            "479/3000 train_loss: 437.4107666015625 test_loss:481.8642578125\n",
            "480/3000 train_loss: 413.8690490722656 test_loss:441.72760009765625\n",
            "481/3000 train_loss: 412.33123779296875 test_loss:431.48797607421875\n",
            "482/3000 train_loss: 897.5819091796875 test_loss:464.3134765625\n",
            "483/3000 train_loss: 516.7330322265625 test_loss:443.4017639160156\n",
            "484/3000 train_loss: 508.30474853515625 test_loss:446.2123107910156\n",
            "485/3000 train_loss: 478.6760559082031 test_loss:492.349365234375\n",
            "486/3000 train_loss: 407.4044189453125 test_loss:441.9715576171875\n",
            "487/3000 train_loss: 443.5073547363281 test_loss:435.600341796875\n",
            "488/3000 train_loss: 469.12060546875 test_loss:475.9476623535156\n",
            "489/3000 train_loss: 437.1897888183594 test_loss:441.5047302246094\n",
            "490/3000 train_loss: 408.4031066894531 test_loss:439.681396484375\n",
            "491/3000 train_loss: 421.58636474609375 test_loss:428.2041015625\n",
            "492/3000 train_loss: 634.9058837890625 test_loss:429.45684814453125\n",
            "493/3000 train_loss: 452.3509826660156 test_loss:456.6375732421875\n",
            "494/3000 train_loss: 401.4216613769531 test_loss:426.69659423828125\n",
            "495/3000 train_loss: 392.89886474609375 test_loss:424.47479248046875\n",
            "496/3000 train_loss: 428.8477478027344 test_loss:456.7667541503906\n",
            "497/3000 train_loss: 493.69146728515625 test_loss:417.7650146484375\n",
            "498/3000 train_loss: 443.3934020996094 test_loss:443.4144287109375\n",
            "499/3000 train_loss: 445.6304016113281 test_loss:425.1921081542969\n",
            "500/3000 train_loss: 373.6295471191406 test_loss:425.0274658203125\n",
            "501/3000 train_loss: 397.90948486328125 test_loss:417.77783203125\n",
            "502/3000 train_loss: 401.81689453125 test_loss:419.57000732421875\n",
            "503/3000 train_loss: 387.3760986328125 test_loss:412.61865234375\n",
            "504/3000 train_loss: 652.3927612304688 test_loss:408.81475830078125\n",
            "505/3000 train_loss: 404.0378112792969 test_loss:413.82110595703125\n",
            "506/3000 train_loss: 469.2587890625 test_loss:418.73309326171875\n",
            "507/3000 train_loss: 425.86358642578125 test_loss:414.034912109375\n",
            "508/3000 train_loss: 465.34698486328125 test_loss:408.61328125\n",
            "509/3000 train_loss: 404.2498474121094 test_loss:407.21783447265625\n",
            "510/3000 train_loss: 362.68121337890625 test_loss:412.6044921875\n",
            "511/3000 train_loss: 351.8219909667969 test_loss:403.24169921875\n",
            "512/3000 train_loss: 346.36578369140625 test_loss:412.03094482421875\n",
            "513/3000 train_loss: 411.083984375 test_loss:412.0841979980469\n",
            "514/3000 train_loss: 385.079833984375 test_loss:410.19537353515625\n",
            "515/3000 train_loss: 608.5506591796875 test_loss:402.9205322265625\n",
            "516/3000 train_loss: 437.7792053222656 test_loss:413.948974609375\n",
            "517/3000 train_loss: 380.5445556640625 test_loss:416.22711181640625\n",
            "518/3000 train_loss: 383.1006164550781 test_loss:424.63836669921875\n",
            "519/3000 train_loss: 360.521728515625 test_loss:434.10223388671875\n",
            "520/3000 train_loss: 420.9582214355469 test_loss:405.284912109375\n",
            "521/3000 train_loss: 396.510986328125 test_loss:409.5863952636719\n",
            "522/3000 train_loss: 906.4232788085938 test_loss:409.36920166015625\n",
            "523/3000 train_loss: 372.0498352050781 test_loss:409.41571044921875\n",
            "524/3000 train_loss: 367.4367980957031 test_loss:429.7355041503906\n",
            "525/3000 train_loss: 363.01458740234375 test_loss:406.4609375\n",
            "526/3000 train_loss: 445.20147705078125 test_loss:403.63043212890625\n",
            "527/3000 train_loss: 386.2142639160156 test_loss:403.7721862792969\n",
            "528/3000 train_loss: 418.7001953125 test_loss:417.46124267578125\n",
            "529/3000 train_loss: 356.3785400390625 test_loss:416.4517517089844\n",
            "530/3000 train_loss: 357.76995849609375 test_loss:413.73858642578125\n",
            "531/3000 train_loss: 397.2164306640625 test_loss:404.7805480957031\n",
            "532/3000 train_loss: 374.1810302734375 test_loss:413.89178466796875\n",
            "533/3000 train_loss: 364.627685546875 test_loss:396.34375\n",
            "534/3000 train_loss: 710.5679931640625 test_loss:409.1382141113281\n",
            "535/3000 train_loss: 379.3426818847656 test_loss:404.6061706542969\n",
            "536/3000 train_loss: 455.0044860839844 test_loss:404.6786193847656\n",
            "537/3000 train_loss: 451.36065673828125 test_loss:421.83782958984375\n",
            "538/3000 train_loss: 442.44610595703125 test_loss:413.47412109375\n",
            "539/3000 train_loss: 382.192626953125 test_loss:401.31903076171875\n",
            "540/3000 train_loss: 400.8648986816406 test_loss:405.2065124511719\n",
            "541/3000 train_loss: 661.7487182617188 test_loss:430.63592529296875\n",
            "542/3000 train_loss: 510.1737976074219 test_loss:423.12078857421875\n",
            "543/3000 train_loss: 538.0037841796875 test_loss:442.2048034667969\n",
            "544/3000 train_loss: 428.7129211425781 test_loss:449.9458312988281\n",
            "545/3000 train_loss: 360.8116149902344 test_loss:444.7847900390625\n",
            "546/3000 train_loss: 404.74078369140625 test_loss:447.271728515625\n",
            "547/3000 train_loss: 361.67742919921875 test_loss:456.173095703125\n",
            "548/3000 train_loss: 382.69976806640625 test_loss:431.735107421875\n",
            "549/3000 train_loss: 405.91693115234375 test_loss:429.1619873046875\n",
            "550/3000 train_loss: 553.787353515625 test_loss:431.32568359375\n",
            "551/3000 train_loss: 410.5389404296875 test_loss:429.23004150390625\n",
            "552/3000 train_loss: 375.3641052246094 test_loss:415.41510009765625\n",
            "553/3000 train_loss: 414.5337829589844 test_loss:414.34552001953125\n",
            "554/3000 train_loss: 843.7796630859375 test_loss:416.4216613769531\n",
            "555/3000 train_loss: 360.77227783203125 test_loss:432.647216796875\n",
            "556/3000 train_loss: 580.811279296875 test_loss:424.88037109375\n",
            "557/3000 train_loss: 410.58721923828125 test_loss:449.23614501953125\n",
            "558/3000 train_loss: 367.4272766113281 test_loss:444.5598449707031\n",
            "559/3000 train_loss: 383.3653564453125 test_loss:456.84417724609375\n",
            "560/3000 train_loss: 386.1101379394531 test_loss:425.42138671875\n",
            "561/3000 train_loss: 408.8402099609375 test_loss:419.32366943359375\n",
            "562/3000 train_loss: 425.63818359375 test_loss:421.88275146484375\n",
            "563/3000 train_loss: 387.551025390625 test_loss:418.8328857421875\n",
            "564/3000 train_loss: 466.5953674316406 test_loss:424.1807861328125\n",
            "565/3000 train_loss: 629.6825561523438 test_loss:416.74127197265625\n",
            "566/3000 train_loss: 358.9106140136719 test_loss:415.3736877441406\n",
            "567/3000 train_loss: 412.16400146484375 test_loss:416.2257385253906\n",
            "568/3000 train_loss: 408.0986022949219 test_loss:415.5539855957031\n",
            "569/3000 train_loss: 350.0781555175781 test_loss:420.1798095703125\n",
            "570/3000 train_loss: 377.1025390625 test_loss:410.3338317871094\n",
            "571/3000 train_loss: 459.66510009765625 test_loss:411.3517150878906\n",
            "572/3000 train_loss: 460.7557678222656 test_loss:416.63818359375\n",
            "573/3000 train_loss: 357.3294982910156 test_loss:413.733642578125\n",
            "574/3000 train_loss: 347.0151062011719 test_loss:413.2373046875\n",
            "575/3000 train_loss: 525.7365112304688 test_loss:404.861328125\n",
            "576/3000 train_loss: 363.2866516113281 test_loss:430.89093017578125\n",
            "577/3000 train_loss: 460.69384765625 test_loss:406.13751220703125\n",
            "578/3000 train_loss: 373.8181457519531 test_loss:421.24700927734375\n",
            "579/3000 train_loss: 364.0959777832031 test_loss:411.0233459472656\n",
            "580/3000 train_loss: 344.695068359375 test_loss:402.9210510253906\n",
            "581/3000 train_loss: 394.0968322753906 test_loss:404.7452087402344\n",
            "582/3000 train_loss: 475.5765380859375 test_loss:408.27655029296875\n",
            "583/3000 train_loss: 429.32147216796875 test_loss:407.07293701171875\n",
            "584/3000 train_loss: 334.89349365234375 test_loss:412.2945556640625\n",
            "585/3000 train_loss: 333.1206359863281 test_loss:400.4917297363281\n",
            "586/3000 train_loss: 373.9448547363281 test_loss:402.1595764160156\n",
            "587/3000 train_loss: 355.5190124511719 test_loss:389.9565734863281\n",
            "588/3000 train_loss: 380.4250183105469 test_loss:387.8876037597656\n",
            "589/3000 train_loss: 359.27606201171875 test_loss:396.4002990722656\n",
            "590/3000 train_loss: 355.2974548339844 test_loss:389.89947509765625\n",
            "591/3000 train_loss: 326.3106994628906 test_loss:388.2839660644531\n",
            "592/3000 train_loss: 335.357177734375 test_loss:385.62677001953125\n",
            "593/3000 train_loss: 386.45843505859375 test_loss:395.004638671875\n",
            "594/3000 train_loss: 364.2447814941406 test_loss:385.9257507324219\n",
            "595/3000 train_loss: 403.908935546875 test_loss:382.9286193847656\n",
            "596/3000 train_loss: 341.4810791015625 test_loss:391.27947998046875\n",
            "597/3000 train_loss: 355.986083984375 test_loss:392.97601318359375\n",
            "598/3000 train_loss: 342.7297668457031 test_loss:384.856201171875\n",
            "599/3000 train_loss: 348.2072448730469 test_loss:388.25701904296875\n",
            "600/3000 train_loss: 351.5107421875 test_loss:377.88323974609375\n",
            "601/3000 train_loss: 352.4105224609375 test_loss:383.3792724609375\n",
            "602/3000 train_loss: 362.13922119140625 test_loss:380.10064697265625\n",
            "603/3000 train_loss: 344.5869445800781 test_loss:380.85797119140625\n",
            "604/3000 train_loss: 363.66485595703125 test_loss:386.2983703613281\n",
            "605/3000 train_loss: 327.5501403808594 test_loss:386.9606018066406\n",
            "606/3000 train_loss: 352.9464416503906 test_loss:378.5403137207031\n",
            "607/3000 train_loss: 321.8160095214844 test_loss:383.1141357421875\n",
            "608/3000 train_loss: 384.0503845214844 test_loss:375.657470703125\n",
            "609/3000 train_loss: 304.9781494140625 test_loss:384.6392822265625\n",
            "610/3000 train_loss: 345.0948791503906 test_loss:379.4725341796875\n",
            "611/3000 train_loss: 321.7955322265625 test_loss:386.294921875\n",
            "612/3000 train_loss: 327.77105712890625 test_loss:377.2835693359375\n",
            "613/3000 train_loss: 323.3663024902344 test_loss:386.1219482421875\n",
            "614/3000 train_loss: 312.9906005859375 test_loss:376.20562744140625\n",
            "615/3000 train_loss: 328.46063232421875 test_loss:381.440185546875\n",
            "616/3000 train_loss: 343.64068603515625 test_loss:376.0135192871094\n",
            "617/3000 train_loss: 463.4722595214844 test_loss:380.9405517578125\n",
            "618/3000 train_loss: 437.0020751953125 test_loss:388.9976806640625\n",
            "619/3000 train_loss: 373.6110534667969 test_loss:398.7132263183594\n",
            "620/3000 train_loss: 360.66796875 test_loss:396.5445251464844\n",
            "621/3000 train_loss: 369.1481018066406 test_loss:383.1942138671875\n",
            "622/3000 train_loss: 347.76409912109375 test_loss:390.1884460449219\n",
            "623/3000 train_loss: 336.4776306152344 test_loss:387.150390625\n",
            "624/3000 train_loss: 354.7960205078125 test_loss:394.70208740234375\n",
            "625/3000 train_loss: 330.3461608886719 test_loss:382.9583435058594\n",
            "626/3000 train_loss: 297.9515686035156 test_loss:377.3084411621094\n",
            "627/3000 train_loss: 303.7880554199219 test_loss:377.5284729003906\n",
            "628/3000 train_loss: 379.52899169921875 test_loss:374.146240234375\n",
            "629/3000 train_loss: 290.5254211425781 test_loss:370.5308837890625\n",
            "630/3000 train_loss: 317.630126953125 test_loss:372.61077880859375\n",
            "631/3000 train_loss: 340.4888610839844 test_loss:372.50848388671875\n",
            "632/3000 train_loss: 616.8704833984375 test_loss:392.42938232421875\n",
            "633/3000 train_loss: 327.0798645019531 test_loss:399.44866943359375\n",
            "634/3000 train_loss: 321.5841064453125 test_loss:386.4710998535156\n",
            "635/3000 train_loss: 330.7073974609375 test_loss:382.35284423828125\n",
            "636/3000 train_loss: 612.0313720703125 test_loss:417.7154846191406\n",
            "637/3000 train_loss: 363.1340026855469 test_loss:384.4306640625\n",
            "638/3000 train_loss: 403.02459716796875 test_loss:373.95953369140625\n",
            "639/3000 train_loss: 439.91949462890625 test_loss:368.97021484375\n",
            "640/3000 train_loss: 359.7620544433594 test_loss:375.8095703125\n",
            "641/3000 train_loss: 329.19036865234375 test_loss:374.1947937011719\n",
            "642/3000 train_loss: 327.9349060058594 test_loss:374.506103515625\n",
            "643/3000 train_loss: 338.58233642578125 test_loss:360.4498291015625\n",
            "644/3000 train_loss: 346.0968017578125 test_loss:361.7008361816406\n",
            "645/3000 train_loss: 345.9207763671875 test_loss:364.30108642578125\n",
            "646/3000 train_loss: 384.06982421875 test_loss:372.52496337890625\n",
            "647/3000 train_loss: 497.09228515625 test_loss:370.99981689453125\n",
            "648/3000 train_loss: 563.7739868164062 test_loss:384.1419677734375\n",
            "649/3000 train_loss: 310.9621887207031 test_loss:372.57720947265625\n",
            "650/3000 train_loss: 398.83148193359375 test_loss:369.8938293457031\n",
            "651/3000 train_loss: 449.1881103515625 test_loss:354.7148742675781\n",
            "652/3000 train_loss: 315.8526611328125 test_loss:362.118896484375\n",
            "653/3000 train_loss: 352.87725830078125 test_loss:357.3273010253906\n",
            "654/3000 train_loss: 341.771484375 test_loss:363.7518005371094\n",
            "655/3000 train_loss: 326.1284484863281 test_loss:355.583984375\n",
            "656/3000 train_loss: 392.3007507324219 test_loss:359.4498291015625\n",
            "657/3000 train_loss: 299.0001220703125 test_loss:354.43609619140625\n",
            "658/3000 train_loss: 315.2998962402344 test_loss:353.02252197265625\n",
            "659/3000 train_loss: 369.72088623046875 test_loss:354.88885498046875\n",
            "660/3000 train_loss: 318.95269775390625 test_loss:351.2929992675781\n",
            "661/3000 train_loss: 370.1389465332031 test_loss:361.14532470703125\n",
            "662/3000 train_loss: 367.7789611816406 test_loss:361.5228271484375\n",
            "663/3000 train_loss: 339.0062255859375 test_loss:348.41876220703125\n",
            "664/3000 train_loss: 330.997802734375 test_loss:346.2364196777344\n",
            "665/3000 train_loss: 383.88134765625 test_loss:354.4839782714844\n",
            "666/3000 train_loss: 309.9856262207031 test_loss:356.156494140625\n",
            "667/3000 train_loss: 295.87396240234375 test_loss:358.1747741699219\n",
            "668/3000 train_loss: 420.478271484375 test_loss:348.0973815917969\n",
            "669/3000 train_loss: 305.09259033203125 test_loss:352.6611022949219\n",
            "670/3000 train_loss: 346.3046875 test_loss:355.2873229980469\n",
            "671/3000 train_loss: 317.0740051269531 test_loss:353.2991027832031\n",
            "672/3000 train_loss: 301.0247802734375 test_loss:347.2276611328125\n",
            "673/3000 train_loss: 316.8908996582031 test_loss:349.2815246582031\n",
            "674/3000 train_loss: 339.0738525390625 test_loss:348.43804931640625\n",
            "675/3000 train_loss: 534.5986328125 test_loss:351.29119873046875\n",
            "676/3000 train_loss: 316.4166259765625 test_loss:364.2188415527344\n",
            "677/3000 train_loss: 326.662353515625 test_loss:362.37298583984375\n",
            "678/3000 train_loss: 322.2349548339844 test_loss:357.1761169433594\n",
            "679/3000 train_loss: 327.8697814941406 test_loss:355.8177795410156\n",
            "680/3000 train_loss: 310.4342346191406 test_loss:350.83587646484375\n",
            "681/3000 train_loss: 287.3499450683594 test_loss:350.1226501464844\n",
            "682/3000 train_loss: 324.57025146484375 test_loss:347.2217102050781\n",
            "683/3000 train_loss: 324.72271728515625 test_loss:356.6497802734375\n",
            "684/3000 train_loss: 313.4565124511719 test_loss:352.6475524902344\n",
            "685/3000 train_loss: 364.91424560546875 test_loss:345.8458251953125\n",
            "686/3000 train_loss: 328.8651123046875 test_loss:345.5776062011719\n",
            "687/3000 train_loss: 278.82891845703125 test_loss:352.65191650390625\n",
            "688/3000 train_loss: 369.9244384765625 test_loss:350.8698425292969\n",
            "689/3000 train_loss: 344.99041748046875 test_loss:376.58172607421875\n",
            "690/3000 train_loss: 431.50274658203125 test_loss:372.3580627441406\n",
            "691/3000 train_loss: 332.5649719238281 test_loss:355.6911315917969\n",
            "692/3000 train_loss: 383.26385498046875 test_loss:355.8835144042969\n",
            "693/3000 train_loss: 472.743896484375 test_loss:357.76263427734375\n",
            "694/3000 train_loss: 340.03277587890625 test_loss:374.22119140625\n",
            "695/3000 train_loss: 303.27789306640625 test_loss:362.17236328125\n",
            "696/3000 train_loss: 326.4638977050781 test_loss:352.2907409667969\n",
            "697/3000 train_loss: 357.7879333496094 test_loss:349.0413513183594\n",
            "698/3000 train_loss: 384.0589904785156 test_loss:356.66864013671875\n",
            "699/3000 train_loss: 407.69110107421875 test_loss:356.7042541503906\n",
            "700/3000 train_loss: 476.1779479980469 test_loss:350.3297424316406\n",
            "701/3000 train_loss: 299.8019714355469 test_loss:350.1593017578125\n",
            "702/3000 train_loss: 314.3795471191406 test_loss:343.89837646484375\n",
            "703/3000 train_loss: 314.0420227050781 test_loss:342.5960388183594\n",
            "704/3000 train_loss: 322.6944885253906 test_loss:340.94061279296875\n",
            "705/3000 train_loss: 371.3984680175781 test_loss:345.0748291015625\n",
            "706/3000 train_loss: 308.2599182128906 test_loss:340.4002685546875\n",
            "707/3000 train_loss: 293.7396240234375 test_loss:337.90582275390625\n",
            "708/3000 train_loss: 407.0009460449219 test_loss:342.01318359375\n",
            "709/3000 train_loss: 328.53173828125 test_loss:342.4920654296875\n",
            "710/3000 train_loss: 300.03717041015625 test_loss:340.5517272949219\n",
            "711/3000 train_loss: 343.08349609375 test_loss:345.04864501953125\n",
            "712/3000 train_loss: 542.6094970703125 test_loss:333.7743225097656\n",
            "713/3000 train_loss: 395.4278259277344 test_loss:349.1383056640625\n",
            "714/3000 train_loss: 409.3302001953125 test_loss:343.5365905761719\n",
            "715/3000 train_loss: 351.4400939941406 test_loss:351.93768310546875\n",
            "716/3000 train_loss: 341.7287292480469 test_loss:349.2452697753906\n",
            "717/3000 train_loss: 294.6307373046875 test_loss:340.64569091796875\n",
            "718/3000 train_loss: 452.64288330078125 test_loss:335.5294494628906\n",
            "719/3000 train_loss: 414.8804931640625 test_loss:335.42572021484375\n",
            "720/3000 train_loss: 316.54669189453125 test_loss:339.27178955078125\n",
            "721/3000 train_loss: 364.4107666015625 test_loss:328.0426940917969\n",
            "722/3000 train_loss: 328.49652099609375 test_loss:331.7579345703125\n",
            "723/3000 train_loss: 339.582275390625 test_loss:332.7723693847656\n",
            "724/3000 train_loss: 272.2999267578125 test_loss:331.2971496582031\n",
            "725/3000 train_loss: 294.1585998535156 test_loss:327.1963195800781\n",
            "726/3000 train_loss: 373.0872802734375 test_loss:328.8650207519531\n",
            "727/3000 train_loss: 288.6424865722656 test_loss:325.77691650390625\n",
            "728/3000 train_loss: 376.3177490234375 test_loss:322.1404113769531\n",
            "729/3000 train_loss: 331.22515869140625 test_loss:320.9498596191406\n",
            "730/3000 train_loss: 328.2635803222656 test_loss:320.504638671875\n",
            "731/3000 train_loss: 349.1796569824219 test_loss:319.427490234375\n",
            "732/3000 train_loss: 285.562744140625 test_loss:331.99871826171875\n",
            "733/3000 train_loss: 286.3294372558594 test_loss:323.9237976074219\n",
            "734/3000 train_loss: 289.3498840332031 test_loss:323.9342346191406\n",
            "735/3000 train_loss: 326.049072265625 test_loss:327.6907958984375\n",
            "736/3000 train_loss: 396.41802978515625 test_loss:329.4864807128906\n",
            "737/3000 train_loss: 307.3315124511719 test_loss:348.2995910644531\n",
            "738/3000 train_loss: 358.3052978515625 test_loss:327.47705078125\n",
            "739/3000 train_loss: 302.7460021972656 test_loss:318.0126953125\n",
            "740/3000 train_loss: 476.72698974609375 test_loss:316.9228515625\n",
            "741/3000 train_loss: 333.53167724609375 test_loss:323.06085205078125\n",
            "742/3000 train_loss: 400.94769287109375 test_loss:328.9083251953125\n",
            "743/3000 train_loss: 320.2906494140625 test_loss:327.36346435546875\n",
            "744/3000 train_loss: 310.7843322753906 test_loss:329.7667541503906\n",
            "745/3000 train_loss: 356.252685546875 test_loss:327.62322998046875\n",
            "746/3000 train_loss: 339.8628234863281 test_loss:335.92657470703125\n",
            "747/3000 train_loss: 296.03851318359375 test_loss:324.09527587890625\n",
            "748/3000 train_loss: 285.9493408203125 test_loss:320.596923828125\n",
            "749/3000 train_loss: 343.9871826171875 test_loss:313.1651611328125\n",
            "750/3000 train_loss: 277.33172607421875 test_loss:313.1566162109375\n",
            "751/3000 train_loss: 486.08270263671875 test_loss:316.6346435546875\n",
            "752/3000 train_loss: 361.66363525390625 test_loss:332.3288269042969\n",
            "753/3000 train_loss: 305.0296325683594 test_loss:322.4404296875\n",
            "754/3000 train_loss: 294.5199890136719 test_loss:314.65472412109375\n",
            "755/3000 train_loss: 294.6138610839844 test_loss:313.7638854980469\n",
            "756/3000 train_loss: 343.02178955078125 test_loss:312.635498046875\n",
            "757/3000 train_loss: 337.9980163574219 test_loss:317.5981140136719\n",
            "758/3000 train_loss: 266.7206115722656 test_loss:313.017822265625\n",
            "759/3000 train_loss: 299.344482421875 test_loss:312.5500183105469\n",
            "760/3000 train_loss: 357.9525146484375 test_loss:315.40594482421875\n",
            "761/3000 train_loss: 297.4765319824219 test_loss:320.4829406738281\n",
            "762/3000 train_loss: 271.856201171875 test_loss:315.73785400390625\n",
            "763/3000 train_loss: 294.0542907714844 test_loss:321.11260986328125\n",
            "764/3000 train_loss: 340.563720703125 test_loss:306.412353515625\n",
            "765/3000 train_loss: 326.08380126953125 test_loss:305.8022766113281\n",
            "766/3000 train_loss: 261.6971740722656 test_loss:324.6845703125\n",
            "767/3000 train_loss: 293.1954345703125 test_loss:315.6284484863281\n",
            "768/3000 train_loss: 318.7646484375 test_loss:317.5917053222656\n",
            "769/3000 train_loss: 358.4203796386719 test_loss:337.9729919433594\n",
            "770/3000 train_loss: 337.6339416503906 test_loss:361.060302734375\n",
            "771/3000 train_loss: 346.86212158203125 test_loss:350.7008972167969\n",
            "772/3000 train_loss: 424.0760192871094 test_loss:337.156982421875\n",
            "773/3000 train_loss: 321.0169982910156 test_loss:324.8832092285156\n",
            "774/3000 train_loss: 302.18121337890625 test_loss:316.2693786621094\n",
            "775/3000 train_loss: 484.10589599609375 test_loss:322.2215270996094\n",
            "776/3000 train_loss: 309.73944091796875 test_loss:339.31317138671875\n",
            "777/3000 train_loss: 312.8503112792969 test_loss:325.0293884277344\n",
            "778/3000 train_loss: 309.3756103515625 test_loss:319.1321716308594\n",
            "779/3000 train_loss: 285.9643249511719 test_loss:312.73846435546875\n",
            "780/3000 train_loss: 313.1759948730469 test_loss:316.1436767578125\n",
            "781/3000 train_loss: 282.24005126953125 test_loss:307.5104064941406\n",
            "782/3000 train_loss: 279.6578674316406 test_loss:308.1609802246094\n",
            "783/3000 train_loss: 295.4468688964844 test_loss:307.8930358886719\n",
            "784/3000 train_loss: 557.4605712890625 test_loss:319.99981689453125\n",
            "785/3000 train_loss: 294.6407775878906 test_loss:332.91204833984375\n",
            "786/3000 train_loss: 294.2572937011719 test_loss:322.829345703125\n",
            "787/3000 train_loss: 285.18084716796875 test_loss:322.319091796875\n",
            "788/3000 train_loss: 333.31817626953125 test_loss:318.0987243652344\n",
            "789/3000 train_loss: 294.38018798828125 test_loss:318.681884765625\n",
            "790/3000 train_loss: 290.451416015625 test_loss:315.16021728515625\n",
            "791/3000 train_loss: 495.2059326171875 test_loss:319.06463623046875\n",
            "792/3000 train_loss: 328.6084289550781 test_loss:324.4277648925781\n",
            "793/3000 train_loss: 431.2373046875 test_loss:327.3198547363281\n",
            "794/3000 train_loss: 316.86322021484375 test_loss:325.9717712402344\n",
            "795/3000 train_loss: 327.5408935546875 test_loss:308.908935546875\n",
            "796/3000 train_loss: 304.319091796875 test_loss:305.73681640625\n",
            "797/3000 train_loss: 274.65765380859375 test_loss:309.43060302734375\n",
            "798/3000 train_loss: 346.7264404296875 test_loss:306.64031982421875\n",
            "799/3000 train_loss: 310.328125 test_loss:307.4684143066406\n",
            "800/3000 train_loss: 257.3211669921875 test_loss:303.1370849609375\n",
            "801/3000 train_loss: 285.6528015136719 test_loss:305.5014953613281\n",
            "802/3000 train_loss: 304.74993896484375 test_loss:295.21942138671875\n",
            "803/3000 train_loss: 310.979736328125 test_loss:295.0592956542969\n",
            "804/3000 train_loss: 273.79168701171875 test_loss:302.90936279296875\n",
            "805/3000 train_loss: 349.73291015625 test_loss:306.2569580078125\n",
            "806/3000 train_loss: 274.6060791015625 test_loss:311.8933410644531\n",
            "807/3000 train_loss: 287.7265625 test_loss:304.893310546875\n",
            "808/3000 train_loss: 277.27392578125 test_loss:301.48382568359375\n",
            "809/3000 train_loss: 254.0115966796875 test_loss:298.32080078125\n",
            "810/3000 train_loss: 286.312255859375 test_loss:301.8153381347656\n",
            "811/3000 train_loss: 307.8534851074219 test_loss:300.8070373535156\n",
            "812/3000 train_loss: 295.37469482421875 test_loss:300.912841796875\n",
            "813/3000 train_loss: 297.1322021484375 test_loss:306.4738464355469\n",
            "814/3000 train_loss: 275.76922607421875 test_loss:296.3736572265625\n",
            "815/3000 train_loss: 392.0537109375 test_loss:294.056640625\n",
            "816/3000 train_loss: 263.63067626953125 test_loss:294.95721435546875\n",
            "817/3000 train_loss: 318.02276611328125 test_loss:296.1312255859375\n",
            "818/3000 train_loss: 249.9300994873047 test_loss:297.2540283203125\n",
            "819/3000 train_loss: 287.88385009765625 test_loss:290.4447326660156\n",
            "820/3000 train_loss: 271.39703369140625 test_loss:292.55023193359375\n",
            "821/3000 train_loss: 329.395751953125 test_loss:297.7422180175781\n",
            "822/3000 train_loss: 260.32135009765625 test_loss:294.0001525878906\n",
            "823/3000 train_loss: 345.9280700683594 test_loss:295.95849609375\n",
            "824/3000 train_loss: 354.94952392578125 test_loss:307.52166748046875\n",
            "825/3000 train_loss: 293.639892578125 test_loss:297.8761291503906\n",
            "826/3000 train_loss: 293.72723388671875 test_loss:291.868896484375\n",
            "827/3000 train_loss: 271.5162353515625 test_loss:299.03131103515625\n",
            "828/3000 train_loss: 520.5211791992188 test_loss:300.68780517578125\n",
            "829/3000 train_loss: 280.2251281738281 test_loss:298.74462890625\n",
            "830/3000 train_loss: 289.0299987792969 test_loss:300.0198974609375\n",
            "831/3000 train_loss: 291.14874267578125 test_loss:292.659912109375\n",
            "832/3000 train_loss: 265.90716552734375 test_loss:297.2319030761719\n",
            "833/3000 train_loss: 261.0159912109375 test_loss:294.27032470703125\n",
            "834/3000 train_loss: 276.3955383300781 test_loss:291.5933837890625\n",
            "835/3000 train_loss: 236.4842529296875 test_loss:295.62548828125\n",
            "836/3000 train_loss: 268.6693420410156 test_loss:294.7379150390625\n",
            "837/3000 train_loss: 254.58123779296875 test_loss:300.20050048828125\n",
            "838/3000 train_loss: 314.5369567871094 test_loss:290.879150390625\n",
            "839/3000 train_loss: 318.3445129394531 test_loss:295.537109375\n",
            "840/3000 train_loss: 275.989013671875 test_loss:296.20263671875\n",
            "841/3000 train_loss: 357.951171875 test_loss:288.2289733886719\n",
            "842/3000 train_loss: 269.7930908203125 test_loss:293.59539794921875\n",
            "843/3000 train_loss: 265.7666931152344 test_loss:290.8545837402344\n",
            "844/3000 train_loss: 260.61724853515625 test_loss:289.70831298828125\n",
            "845/3000 train_loss: 265.1052551269531 test_loss:283.2814636230469\n",
            "846/3000 train_loss: 275.566162109375 test_loss:285.8482666015625\n",
            "847/3000 train_loss: 291.17889404296875 test_loss:283.7977600097656\n",
            "848/3000 train_loss: 336.9198303222656 test_loss:292.55181884765625\n",
            "849/3000 train_loss: 259.7415466308594 test_loss:290.7004699707031\n",
            "850/3000 train_loss: 330.15460205078125 test_loss:290.6783142089844\n",
            "851/3000 train_loss: 275.3482971191406 test_loss:293.14666748046875\n",
            "852/3000 train_loss: 271.3861999511719 test_loss:285.1628112792969\n",
            "853/3000 train_loss: 240.9396514892578 test_loss:283.0346984863281\n",
            "854/3000 train_loss: 253.28648376464844 test_loss:281.967041015625\n",
            "855/3000 train_loss: 240.1652374267578 test_loss:285.7036437988281\n",
            "856/3000 train_loss: 262.8074035644531 test_loss:286.9444885253906\n",
            "857/3000 train_loss: 553.0632934570312 test_loss:290.02880859375\n",
            "858/3000 train_loss: 296.18096923828125 test_loss:304.3531494140625\n",
            "859/3000 train_loss: 330.5608215332031 test_loss:294.9780578613281\n",
            "860/3000 train_loss: 271.5041809082031 test_loss:297.9061584472656\n",
            "861/3000 train_loss: 335.6259765625 test_loss:285.7351379394531\n",
            "862/3000 train_loss: 278.2957458496094 test_loss:283.1069641113281\n",
            "863/3000 train_loss: 256.0476379394531 test_loss:279.49737548828125\n",
            "864/3000 train_loss: 378.7511291503906 test_loss:279.96820068359375\n",
            "865/3000 train_loss: 239.46243286132812 test_loss:284.11175537109375\n",
            "866/3000 train_loss: 263.3846435546875 test_loss:278.1890869140625\n",
            "867/3000 train_loss: 237.20654296875 test_loss:277.3031005859375\n",
            "868/3000 train_loss: 249.16488647460938 test_loss:275.8251953125\n",
            "869/3000 train_loss: 388.81268310546875 test_loss:281.70355224609375\n",
            "870/3000 train_loss: 312.48785400390625 test_loss:282.9539794921875\n",
            "871/3000 train_loss: 254.2704315185547 test_loss:285.67333984375\n",
            "872/3000 train_loss: 294.1019287109375 test_loss:278.4012451171875\n",
            "873/3000 train_loss: 255.67349243164062 test_loss:274.33819580078125\n",
            "874/3000 train_loss: 265.0772399902344 test_loss:275.4237060546875\n",
            "875/3000 train_loss: 312.386962890625 test_loss:278.2118225097656\n",
            "876/3000 train_loss: 290.2432556152344 test_loss:276.38690185546875\n",
            "877/3000 train_loss: 262.3757019042969 test_loss:277.1918640136719\n",
            "878/3000 train_loss: 263.9981384277344 test_loss:274.0255126953125\n",
            "879/3000 train_loss: 262.63348388671875 test_loss:273.50048828125\n",
            "880/3000 train_loss: 522.7793579101562 test_loss:274.6899719238281\n",
            "881/3000 train_loss: 261.6233825683594 test_loss:275.544677734375\n",
            "882/3000 train_loss: 272.58154296875 test_loss:279.7718200683594\n",
            "883/3000 train_loss: 315.44207763671875 test_loss:282.33197021484375\n",
            "884/3000 train_loss: 308.4335632324219 test_loss:274.7967834472656\n",
            "885/3000 train_loss: 250.20025634765625 test_loss:279.2672119140625\n",
            "886/3000 train_loss: 239.6894989013672 test_loss:275.7313232421875\n",
            "887/3000 train_loss: 295.50372314453125 test_loss:271.44677734375\n",
            "888/3000 train_loss: 247.28887939453125 test_loss:274.48583984375\n",
            "889/3000 train_loss: 339.72021484375 test_loss:270.00299072265625\n",
            "890/3000 train_loss: 308.26531982421875 test_loss:278.80682373046875\n",
            "891/3000 train_loss: 275.4455871582031 test_loss:281.28424072265625\n",
            "892/3000 train_loss: 338.3742980957031 test_loss:277.0390930175781\n",
            "893/3000 train_loss: 266.7956237792969 test_loss:274.42694091796875\n",
            "894/3000 train_loss: 318.60113525390625 test_loss:278.8203430175781\n",
            "895/3000 train_loss: 250.79608154296875 test_loss:278.528076171875\n",
            "896/3000 train_loss: 264.3182373046875 test_loss:291.8702697753906\n",
            "897/3000 train_loss: 253.2427978515625 test_loss:268.3585510253906\n",
            "898/3000 train_loss: 294.2401428222656 test_loss:266.5848388671875\n",
            "899/3000 train_loss: 271.0881042480469 test_loss:267.33526611328125\n",
            "900/3000 train_loss: 244.44796752929688 test_loss:269.4044189453125\n",
            "901/3000 train_loss: 244.70840454101562 test_loss:269.114990234375\n",
            "902/3000 train_loss: 243.84951782226562 test_loss:270.9735107421875\n",
            "903/3000 train_loss: 283.52197265625 test_loss:269.341796875\n",
            "904/3000 train_loss: 246.74349975585938 test_loss:272.72442626953125\n",
            "905/3000 train_loss: 289.9686584472656 test_loss:274.7655029296875\n",
            "906/3000 train_loss: 306.5634765625 test_loss:282.27618408203125\n",
            "907/3000 train_loss: 250.0753173828125 test_loss:276.8426513671875\n",
            "908/3000 train_loss: 258.8364562988281 test_loss:287.6107177734375\n",
            "909/3000 train_loss: 346.5400390625 test_loss:273.73431396484375\n",
            "910/3000 train_loss: 286.37799072265625 test_loss:279.4863586425781\n",
            "911/3000 train_loss: 248.22789001464844 test_loss:285.44842529296875\n",
            "912/3000 train_loss: 272.03033447265625 test_loss:288.4679260253906\n",
            "913/3000 train_loss: 255.03245544433594 test_loss:285.32818603515625\n",
            "914/3000 train_loss: 248.52833557128906 test_loss:283.8277893066406\n",
            "915/3000 train_loss: 278.94683837890625 test_loss:282.4200439453125\n",
            "916/3000 train_loss: 273.787109375 test_loss:282.3452453613281\n",
            "917/3000 train_loss: 269.7850646972656 test_loss:276.18115234375\n",
            "918/3000 train_loss: 241.88766479492188 test_loss:275.5689697265625\n",
            "919/3000 train_loss: 242.4176025390625 test_loss:275.9135437011719\n",
            "920/3000 train_loss: 252.23326110839844 test_loss:279.8065185546875\n",
            "921/3000 train_loss: 240.49989318847656 test_loss:281.47015380859375\n",
            "922/3000 train_loss: 259.18994140625 test_loss:280.2554931640625\n",
            "923/3000 train_loss: 264.84625244140625 test_loss:278.62310791015625\n",
            "924/3000 train_loss: 240.42225646972656 test_loss:278.626220703125\n",
            "925/3000 train_loss: 283.12725830078125 test_loss:281.35601806640625\n",
            "926/3000 train_loss: 266.5377502441406 test_loss:282.7933044433594\n",
            "927/3000 train_loss: 263.27996826171875 test_loss:276.4647216796875\n",
            "928/3000 train_loss: 261.5470886230469 test_loss:278.15057373046875\n",
            "929/3000 train_loss: 220.93618774414062 test_loss:277.23724365234375\n",
            "930/3000 train_loss: 268.5104064941406 test_loss:275.7403564453125\n",
            "931/3000 train_loss: 285.03240966796875 test_loss:279.0467834472656\n",
            "932/3000 train_loss: 235.56239318847656 test_loss:278.2196044921875\n",
            "933/3000 train_loss: 239.72415161132812 test_loss:271.6126708984375\n",
            "934/3000 train_loss: 307.68597412109375 test_loss:273.1392517089844\n",
            "935/3000 train_loss: 247.197998046875 test_loss:274.39471435546875\n",
            "936/3000 train_loss: 253.08607482910156 test_loss:269.2041015625\n",
            "937/3000 train_loss: 318.697998046875 test_loss:274.9861145019531\n",
            "938/3000 train_loss: 264.3494873046875 test_loss:278.92889404296875\n",
            "939/3000 train_loss: 251.21388244628906 test_loss:282.9512939453125\n",
            "940/3000 train_loss: 238.2445831298828 test_loss:273.3002624511719\n",
            "941/3000 train_loss: 260.1034851074219 test_loss:271.47698974609375\n",
            "942/3000 train_loss: 281.3009033203125 test_loss:269.2337951660156\n",
            "943/3000 train_loss: 237.8990020751953 test_loss:263.0754089355469\n",
            "944/3000 train_loss: 283.145263671875 test_loss:261.89556884765625\n",
            "945/3000 train_loss: 295.8985595703125 test_loss:261.8937683105469\n",
            "946/3000 train_loss: 297.5782470703125 test_loss:266.3831787109375\n",
            "947/3000 train_loss: 242.16226196289062 test_loss:263.16680908203125\n",
            "948/3000 train_loss: 226.10165405273438 test_loss:260.93121337890625\n",
            "949/3000 train_loss: 238.67578125 test_loss:255.4151153564453\n",
            "950/3000 train_loss: 276.66656494140625 test_loss:260.10906982421875\n",
            "951/3000 train_loss: 429.5166015625 test_loss:257.7132568359375\n",
            "952/3000 train_loss: 251.06378173828125 test_loss:266.4214782714844\n",
            "953/3000 train_loss: 252.7505340576172 test_loss:274.3614196777344\n",
            "954/3000 train_loss: 318.1955261230469 test_loss:273.0017395019531\n",
            "955/3000 train_loss: 242.28225708007812 test_loss:271.1941223144531\n",
            "956/3000 train_loss: 335.26416015625 test_loss:273.1341857910156\n",
            "957/3000 train_loss: 255.0887908935547 test_loss:289.9771728515625\n",
            "958/3000 train_loss: 261.33795166015625 test_loss:276.1988220214844\n",
            "959/3000 train_loss: 272.1063232421875 test_loss:274.3439636230469\n",
            "960/3000 train_loss: 231.67538452148438 test_loss:263.8606262207031\n",
            "961/3000 train_loss: 269.19696044921875 test_loss:262.978515625\n",
            "962/3000 train_loss: 276.7644348144531 test_loss:268.197509765625\n",
            "963/3000 train_loss: 311.93646240234375 test_loss:264.4244689941406\n",
            "964/3000 train_loss: 301.45721435546875 test_loss:262.72503662109375\n",
            "965/3000 train_loss: 277.128173828125 test_loss:255.88961791992188\n",
            "966/3000 train_loss: 443.70111083984375 test_loss:257.16046142578125\n",
            "967/3000 train_loss: 286.7164001464844 test_loss:280.84820556640625\n",
            "968/3000 train_loss: 256.0464782714844 test_loss:266.5653991699219\n",
            "969/3000 train_loss: 252.26480102539062 test_loss:262.8028564453125\n",
            "970/3000 train_loss: 248.22451782226562 test_loss:259.4317321777344\n",
            "971/3000 train_loss: 236.93072509765625 test_loss:258.5556945800781\n",
            "972/3000 train_loss: 216.83132934570312 test_loss:261.93218994140625\n",
            "973/3000 train_loss: 461.8955993652344 test_loss:265.3558349609375\n",
            "974/3000 train_loss: 296.1062927246094 test_loss:283.197998046875\n",
            "975/3000 train_loss: 238.4163360595703 test_loss:264.4918518066406\n",
            "976/3000 train_loss: 234.09254455566406 test_loss:260.6029052734375\n",
            "977/3000 train_loss: 234.95797729492188 test_loss:256.5711669921875\n",
            "978/3000 train_loss: 234.97427368164062 test_loss:257.7178039550781\n",
            "979/3000 train_loss: 216.1355438232422 test_loss:255.86572265625\n",
            "980/3000 train_loss: 393.17718505859375 test_loss:253.0242156982422\n",
            "981/3000 train_loss: 240.33218383789062 test_loss:293.99951171875\n",
            "982/3000 train_loss: 290.6452331542969 test_loss:295.4586181640625\n",
            "983/3000 train_loss: 360.80865478515625 test_loss:282.88299560546875\n",
            "984/3000 train_loss: 253.70883178710938 test_loss:268.1081848144531\n",
            "985/3000 train_loss: 241.09902954101562 test_loss:262.32843017578125\n",
            "986/3000 train_loss: 277.90509033203125 test_loss:264.9191589355469\n",
            "987/3000 train_loss: 217.3834686279297 test_loss:261.9052734375\n",
            "988/3000 train_loss: 252.4021759033203 test_loss:258.935302734375\n",
            "989/3000 train_loss: 291.16375732421875 test_loss:261.6434326171875\n",
            "990/3000 train_loss: 225.30712890625 test_loss:277.41644287109375\n",
            "991/3000 train_loss: 226.92701721191406 test_loss:258.6653747558594\n",
            "992/3000 train_loss: 237.4915008544922 test_loss:254.76622009277344\n",
            "993/3000 train_loss: 242.50733947753906 test_loss:255.9315948486328\n",
            "994/3000 train_loss: 221.3104248046875 test_loss:260.7122802734375\n",
            "995/3000 train_loss: 282.2872314453125 test_loss:263.756103515625\n",
            "996/3000 train_loss: 230.4888916015625 test_loss:281.39752197265625\n",
            "997/3000 train_loss: 231.35137939453125 test_loss:258.1346435546875\n",
            "998/3000 train_loss: 250.13235473632812 test_loss:255.75230407714844\n",
            "999/3000 train_loss: 232.39675903320312 test_loss:261.55401611328125\n",
            "1000/3000 train_loss: 246.6936492919922 test_loss:252.92730712890625\n",
            "1001/3000 train_loss: 243.1339111328125 test_loss:250.30728149414062\n",
            "1002/3000 train_loss: 227.2775421142578 test_loss:252.1505889892578\n",
            "1003/3000 train_loss: 231.29449462890625 test_loss:253.75701904296875\n",
            "1004/3000 train_loss: 223.58934020996094 test_loss:250.02224731445312\n",
            "1005/3000 train_loss: 252.93829345703125 test_loss:253.10061645507812\n",
            "1006/3000 train_loss: 236.3023223876953 test_loss:258.0255126953125\n",
            "1007/3000 train_loss: 246.3413543701172 test_loss:262.81878662109375\n",
            "1008/3000 train_loss: 292.834716796875 test_loss:255.61526489257812\n",
            "1009/3000 train_loss: 231.0168914794922 test_loss:264.72705078125\n",
            "1010/3000 train_loss: 258.50030517578125 test_loss:257.1380615234375\n",
            "1011/3000 train_loss: 253.2696075439453 test_loss:253.27517700195312\n",
            "1012/3000 train_loss: 230.18389892578125 test_loss:244.14227294921875\n",
            "1013/3000 train_loss: 235.0232391357422 test_loss:247.86590576171875\n",
            "1014/3000 train_loss: 219.8804473876953 test_loss:246.9103240966797\n",
            "1015/3000 train_loss: 257.09564208984375 test_loss:245.7089385986328\n",
            "1016/3000 train_loss: 233.7951202392578 test_loss:249.70071411132812\n",
            "1017/3000 train_loss: 249.437255859375 test_loss:245.42454528808594\n",
            "1018/3000 train_loss: 320.720947265625 test_loss:241.64939880371094\n",
            "1019/3000 train_loss: 231.04339599609375 test_loss:247.17996215820312\n",
            "1020/3000 train_loss: 235.1072540283203 test_loss:245.5803985595703\n",
            "1021/3000 train_loss: 238.40086364746094 test_loss:245.06631469726562\n",
            "1022/3000 train_loss: 210.16656494140625 test_loss:244.8463134765625\n",
            "1023/3000 train_loss: 223.19586181640625 test_loss:246.31317138671875\n",
            "1024/3000 train_loss: 233.78668212890625 test_loss:243.20687866210938\n",
            "1025/3000 train_loss: 270.10009765625 test_loss:247.8228759765625\n",
            "1026/3000 train_loss: 396.9043884277344 test_loss:252.3408203125\n",
            "1027/3000 train_loss: 209.06109619140625 test_loss:255.93545532226562\n",
            "1028/3000 train_loss: 224.11001586914062 test_loss:245.82687377929688\n",
            "1029/3000 train_loss: 270.61480712890625 test_loss:241.75274658203125\n",
            "1030/3000 train_loss: 209.96865844726562 test_loss:243.05154418945312\n",
            "1031/3000 train_loss: 243.44412231445312 test_loss:242.68800354003906\n",
            "1032/3000 train_loss: 298.2472839355469 test_loss:265.8287353515625\n",
            "1033/3000 train_loss: 253.86410522460938 test_loss:253.66403198242188\n",
            "1034/3000 train_loss: 281.48175048828125 test_loss:243.98291015625\n",
            "1035/3000 train_loss: 211.34490966796875 test_loss:253.92555236816406\n",
            "1036/3000 train_loss: 203.96713256835938 test_loss:240.65072631835938\n",
            "1037/3000 train_loss: 249.6796875 test_loss:250.4491424560547\n",
            "1038/3000 train_loss: 290.38555908203125 test_loss:251.84527587890625\n",
            "1039/3000 train_loss: 293.3636779785156 test_loss:251.65814208984375\n",
            "1040/3000 train_loss: 238.90670776367188 test_loss:246.7616729736328\n",
            "1041/3000 train_loss: 224.7955322265625 test_loss:246.55282592773438\n",
            "1042/3000 train_loss: 209.67474365234375 test_loss:242.49623107910156\n",
            "1043/3000 train_loss: 248.57861328125 test_loss:238.69662475585938\n",
            "1044/3000 train_loss: 254.43408203125 test_loss:245.11618041992188\n",
            "1045/3000 train_loss: 250.81369018554688 test_loss:257.115478515625\n",
            "1046/3000 train_loss: 369.0768737792969 test_loss:243.10867309570312\n",
            "1047/3000 train_loss: 225.77919006347656 test_loss:246.72254943847656\n",
            "1048/3000 train_loss: 343.2762451171875 test_loss:244.89122009277344\n",
            "1049/3000 train_loss: 360.83392333984375 test_loss:239.6278076171875\n",
            "1050/3000 train_loss: 225.4677734375 test_loss:262.9454345703125\n",
            "1051/3000 train_loss: 243.97796630859375 test_loss:240.69583129882812\n",
            "1052/3000 train_loss: 207.66278076171875 test_loss:239.88113403320312\n",
            "1053/3000 train_loss: 212.40887451171875 test_loss:240.47161865234375\n",
            "1054/3000 train_loss: 203.16331481933594 test_loss:238.9876708984375\n",
            "1055/3000 train_loss: 356.6301574707031 test_loss:238.48709106445312\n",
            "1056/3000 train_loss: 224.24380493164062 test_loss:237.41793823242188\n",
            "1057/3000 train_loss: 265.79827880859375 test_loss:234.09046936035156\n",
            "1058/3000 train_loss: 279.16497802734375 test_loss:236.33616638183594\n",
            "1059/3000 train_loss: 276.9696044921875 test_loss:245.35342407226562\n",
            "1060/3000 train_loss: 205.6448974609375 test_loss:239.1422576904297\n",
            "1061/3000 train_loss: 221.06747436523438 test_loss:242.6878204345703\n",
            "1062/3000 train_loss: 206.4141845703125 test_loss:233.26690673828125\n",
            "1063/3000 train_loss: 208.4208984375 test_loss:232.9483642578125\n",
            "1064/3000 train_loss: 288.4565734863281 test_loss:253.9495086669922\n",
            "1065/3000 train_loss: 214.33248901367188 test_loss:261.662841796875\n",
            "1066/3000 train_loss: 226.12362670898438 test_loss:251.3074951171875\n",
            "1067/3000 train_loss: 216.69085693359375 test_loss:249.59246826171875\n",
            "1068/3000 train_loss: 211.82321166992188 test_loss:242.3294677734375\n",
            "1069/3000 train_loss: 202.22531127929688 test_loss:237.42218017578125\n",
            "1070/3000 train_loss: 211.45565795898438 test_loss:238.72349548339844\n",
            "1071/3000 train_loss: 226.75376892089844 test_loss:244.52603149414062\n",
            "1072/3000 train_loss: 216.86648559570312 test_loss:244.92984008789062\n",
            "1073/3000 train_loss: 230.31793212890625 test_loss:244.1494903564453\n",
            "1074/3000 train_loss: 427.22265625 test_loss:277.93487548828125\n",
            "1075/3000 train_loss: 208.3636932373047 test_loss:255.68270874023438\n",
            "1076/3000 train_loss: 253.71856689453125 test_loss:239.82952880859375\n",
            "1077/3000 train_loss: 216.14100646972656 test_loss:251.33848571777344\n",
            "1078/3000 train_loss: 285.69512939453125 test_loss:238.66632080078125\n",
            "1079/3000 train_loss: 211.79714965820312 test_loss:252.90744018554688\n",
            "1080/3000 train_loss: 228.95236206054688 test_loss:237.63296508789062\n",
            "1081/3000 train_loss: 251.4620819091797 test_loss:235.39785766601562\n",
            "1082/3000 train_loss: 211.18751525878906 test_loss:233.19720458984375\n",
            "1083/3000 train_loss: 201.01564025878906 test_loss:227.27255249023438\n",
            "1084/3000 train_loss: 188.5726318359375 test_loss:229.45654296875\n",
            "1085/3000 train_loss: 196.415771484375 test_loss:227.72584533691406\n",
            "1086/3000 train_loss: 203.23834228515625 test_loss:230.975830078125\n",
            "1087/3000 train_loss: 185.7230987548828 test_loss:230.69973754882812\n",
            "1088/3000 train_loss: 341.48736572265625 test_loss:233.50033569335938\n",
            "1089/3000 train_loss: 224.9901123046875 test_loss:237.49473571777344\n",
            "1090/3000 train_loss: 313.62451171875 test_loss:222.00271606445312\n",
            "1091/3000 train_loss: 407.8467712402344 test_loss:227.9661865234375\n",
            "1092/3000 train_loss: 220.30087280273438 test_loss:231.03585815429688\n",
            "1093/3000 train_loss: 249.03746032714844 test_loss:233.36610412597656\n",
            "1094/3000 train_loss: 213.10751342773438 test_loss:238.23648071289062\n",
            "1095/3000 train_loss: 303.70037841796875 test_loss:235.376708984375\n",
            "1096/3000 train_loss: 372.0020751953125 test_loss:237.48257446289062\n",
            "1097/3000 train_loss: 269.031005859375 test_loss:268.97113037109375\n",
            "1098/3000 train_loss: 225.54730224609375 test_loss:254.30154418945312\n",
            "1099/3000 train_loss: 216.84222412109375 test_loss:245.46981811523438\n",
            "1100/3000 train_loss: 218.72659301757812 test_loss:243.14942932128906\n",
            "1101/3000 train_loss: 230.16497802734375 test_loss:242.5662841796875\n",
            "1102/3000 train_loss: 258.248779296875 test_loss:240.7407989501953\n",
            "1103/3000 train_loss: 223.09881591796875 test_loss:239.400634765625\n",
            "1104/3000 train_loss: 215.68276977539062 test_loss:237.64028930664062\n",
            "1105/3000 train_loss: 197.47955322265625 test_loss:233.86875915527344\n",
            "1106/3000 train_loss: 507.4708251953125 test_loss:247.01858520507812\n",
            "1107/3000 train_loss: 276.0556335449219 test_loss:256.853271484375\n",
            "1108/3000 train_loss: 212.8359375 test_loss:240.49998474121094\n",
            "1109/3000 train_loss: 221.67782592773438 test_loss:235.30711364746094\n",
            "1110/3000 train_loss: 195.77232360839844 test_loss:233.86065673828125\n",
            "1111/3000 train_loss: 310.1669921875 test_loss:232.69589233398438\n",
            "1112/3000 train_loss: 210.3497314453125 test_loss:234.99952697753906\n",
            "1113/3000 train_loss: 243.1063690185547 test_loss:233.83282470703125\n",
            "1114/3000 train_loss: 211.11273193359375 test_loss:236.8182830810547\n",
            "1115/3000 train_loss: 209.88941955566406 test_loss:237.4023895263672\n",
            "1116/3000 train_loss: 197.3435516357422 test_loss:232.72900390625\n",
            "1117/3000 train_loss: 211.3046417236328 test_loss:227.436767578125\n",
            "1118/3000 train_loss: 304.1024475097656 test_loss:226.80789184570312\n",
            "1119/3000 train_loss: 212.12159729003906 test_loss:223.17759704589844\n",
            "1120/3000 train_loss: 276.2011413574219 test_loss:223.1036376953125\n",
            "1121/3000 train_loss: 312.7497863769531 test_loss:223.44143676757812\n",
            "1122/3000 train_loss: 223.92813110351562 test_loss:228.09286499023438\n",
            "1123/3000 train_loss: 315.9303894042969 test_loss:234.04824829101562\n",
            "1124/3000 train_loss: 267.4854736328125 test_loss:242.44302368164062\n",
            "1125/3000 train_loss: 297.6578369140625 test_loss:240.69886779785156\n",
            "1126/3000 train_loss: 333.853271484375 test_loss:235.1496124267578\n",
            "1127/3000 train_loss: 430.8330383300781 test_loss:251.95709228515625\n",
            "1128/3000 train_loss: 253.2917022705078 test_loss:264.3701171875\n",
            "1129/3000 train_loss: 258.8112487792969 test_loss:242.3042449951172\n",
            "1130/3000 train_loss: 227.34124755859375 test_loss:232.73519897460938\n",
            "1131/3000 train_loss: 203.47398376464844 test_loss:232.63003540039062\n",
            "1132/3000 train_loss: 292.4621276855469 test_loss:225.01063537597656\n",
            "1133/3000 train_loss: 275.64898681640625 test_loss:231.32440185546875\n",
            "1134/3000 train_loss: 322.95806884765625 test_loss:229.30941772460938\n",
            "1135/3000 train_loss: 251.45672607421875 test_loss:234.40440368652344\n",
            "1136/3000 train_loss: 223.20712280273438 test_loss:225.94845581054688\n",
            "1137/3000 train_loss: 231.53652954101562 test_loss:228.83465576171875\n",
            "1138/3000 train_loss: 201.1910400390625 test_loss:226.94210815429688\n",
            "1139/3000 train_loss: 232.14627075195312 test_loss:227.60366821289062\n",
            "1140/3000 train_loss: 187.40736389160156 test_loss:222.3120880126953\n",
            "1141/3000 train_loss: 260.75140380859375 test_loss:224.26446533203125\n",
            "1142/3000 train_loss: 207.89886474609375 test_loss:234.82131958007812\n",
            "1143/3000 train_loss: 210.85986328125 test_loss:229.22256469726562\n",
            "1144/3000 train_loss: 253.62017822265625 test_loss:221.70535278320312\n",
            "1145/3000 train_loss: 241.26931762695312 test_loss:230.68968200683594\n",
            "1146/3000 train_loss: 175.01690673828125 test_loss:224.7779998779297\n",
            "1147/3000 train_loss: 275.5111083984375 test_loss:221.84970092773438\n",
            "1148/3000 train_loss: 261.4876708984375 test_loss:228.93138122558594\n",
            "1149/3000 train_loss: 358.6916198730469 test_loss:247.06436157226562\n",
            "1150/3000 train_loss: 219.04905700683594 test_loss:245.3052978515625\n",
            "1151/3000 train_loss: 208.18081665039062 test_loss:239.49404907226562\n",
            "1152/3000 train_loss: 217.9680633544922 test_loss:234.29415893554688\n",
            "1153/3000 train_loss: 217.26992797851562 test_loss:226.85748291015625\n",
            "1154/3000 train_loss: 282.52496337890625 test_loss:229.56106567382812\n",
            "1155/3000 train_loss: 202.63243103027344 test_loss:232.50067138671875\n",
            "1156/3000 train_loss: 204.06039428710938 test_loss:229.98208618164062\n",
            "1157/3000 train_loss: 213.494873046875 test_loss:225.06121826171875\n",
            "1158/3000 train_loss: 262.601318359375 test_loss:226.99891662597656\n",
            "1159/3000 train_loss: 210.63778686523438 test_loss:236.82177734375\n",
            "1160/3000 train_loss: 211.68540954589844 test_loss:238.23287963867188\n",
            "1161/3000 train_loss: 215.52981567382812 test_loss:232.78115844726562\n",
            "1162/3000 train_loss: 229.92654418945312 test_loss:232.7809600830078\n",
            "1163/3000 train_loss: 210.25775146484375 test_loss:230.35696411132812\n",
            "1164/3000 train_loss: 181.5621337890625 test_loss:226.58505249023438\n",
            "1165/3000 train_loss: 186.13661193847656 test_loss:229.5526123046875\n",
            "1166/3000 train_loss: 190.40545654296875 test_loss:227.0689697265625\n",
            "1167/3000 train_loss: 210.42327880859375 test_loss:225.3282012939453\n",
            "1168/3000 train_loss: 204.97222900390625 test_loss:223.66461181640625\n",
            "1169/3000 train_loss: 290.7238464355469 test_loss:224.9410400390625\n",
            "1170/3000 train_loss: 273.2474365234375 test_loss:230.18771362304688\n",
            "1171/3000 train_loss: 200.12342834472656 test_loss:221.99844360351562\n",
            "1172/3000 train_loss: 206.9493865966797 test_loss:218.81039428710938\n",
            "1173/3000 train_loss: 263.89312744140625 test_loss:220.79266357421875\n",
            "1174/3000 train_loss: 211.2652130126953 test_loss:216.4684295654297\n",
            "1175/3000 train_loss: 187.64801025390625 test_loss:222.42422485351562\n",
            "1176/3000 train_loss: 212.13119506835938 test_loss:227.5140380859375\n",
            "1177/3000 train_loss: 198.51947021484375 test_loss:228.65792846679688\n",
            "1178/3000 train_loss: 182.922119140625 test_loss:228.77281188964844\n",
            "1179/3000 train_loss: 217.1334228515625 test_loss:227.50384521484375\n",
            "1180/3000 train_loss: 207.50160217285156 test_loss:231.20965576171875\n",
            "1181/3000 train_loss: 222.02328491210938 test_loss:230.094970703125\n",
            "1182/3000 train_loss: 252.3459014892578 test_loss:231.0001678466797\n",
            "1183/3000 train_loss: 182.26763916015625 test_loss:227.8489532470703\n",
            "1184/3000 train_loss: 193.32859802246094 test_loss:220.58364868164062\n",
            "1185/3000 train_loss: 341.9686279296875 test_loss:226.6707305908203\n",
            "1186/3000 train_loss: 190.23426818847656 test_loss:232.46514892578125\n",
            "1187/3000 train_loss: 256.6777648925781 test_loss:232.5818634033203\n",
            "1188/3000 train_loss: 241.01113891601562 test_loss:225.5883331298828\n",
            "1189/3000 train_loss: 201.22381591796875 test_loss:224.53079223632812\n",
            "1190/3000 train_loss: 211.57017517089844 test_loss:218.55734252929688\n",
            "1191/3000 train_loss: 287.00128173828125 test_loss:217.42733764648438\n",
            "1192/3000 train_loss: 187.15235900878906 test_loss:231.4876708984375\n",
            "1193/3000 train_loss: 239.77354431152344 test_loss:231.78060913085938\n",
            "1194/3000 train_loss: 193.04733276367188 test_loss:220.10426330566406\n",
            "1195/3000 train_loss: 220.35244750976562 test_loss:220.34368896484375\n",
            "1196/3000 train_loss: 206.71963500976562 test_loss:221.6651153564453\n",
            "1197/3000 train_loss: 253.8632354736328 test_loss:221.83950805664062\n",
            "1198/3000 train_loss: 240.31930541992188 test_loss:219.19027709960938\n",
            "1199/3000 train_loss: 208.92254638671875 test_loss:220.257568359375\n",
            "1200/3000 train_loss: 191.70938110351562 test_loss:219.76889038085938\n",
            "1201/3000 train_loss: 256.3564147949219 test_loss:221.35630798339844\n",
            "1202/3000 train_loss: 214.77261352539062 test_loss:220.996826171875\n",
            "1203/3000 train_loss: 189.6484832763672 test_loss:225.75384521484375\n",
            "1204/3000 train_loss: 195.88027954101562 test_loss:222.48358154296875\n",
            "1205/3000 train_loss: 218.30357360839844 test_loss:216.94097900390625\n",
            "1206/3000 train_loss: 276.30181884765625 test_loss:218.77572631835938\n",
            "1207/3000 train_loss: 214.95840454101562 test_loss:225.2618408203125\n",
            "1208/3000 train_loss: 207.15318298339844 test_loss:222.94773864746094\n",
            "1209/3000 train_loss: 478.61029052734375 test_loss:215.0631103515625\n",
            "1210/3000 train_loss: 191.05780029296875 test_loss:214.605712890625\n",
            "1211/3000 train_loss: 180.74017333984375 test_loss:209.5165252685547\n",
            "1212/3000 train_loss: 209.8788604736328 test_loss:208.6314697265625\n",
            "1213/3000 train_loss: 196.1322479248047 test_loss:214.1912841796875\n",
            "1214/3000 train_loss: 229.10977172851562 test_loss:215.1558074951172\n",
            "1215/3000 train_loss: 194.21690368652344 test_loss:222.43490600585938\n",
            "1216/3000 train_loss: 197.15316772460938 test_loss:210.7618865966797\n",
            "1217/3000 train_loss: 179.2520751953125 test_loss:213.2296905517578\n",
            "1218/3000 train_loss: 203.48892211914062 test_loss:212.31126403808594\n",
            "1219/3000 train_loss: 214.1661376953125 test_loss:210.98196411132812\n",
            "1220/3000 train_loss: 195.94998168945312 test_loss:210.63262939453125\n",
            "1221/3000 train_loss: 469.5783996582031 test_loss:211.87936401367188\n",
            "1222/3000 train_loss: 175.75677490234375 test_loss:218.08914184570312\n",
            "1223/3000 train_loss: 211.4627227783203 test_loss:228.96160888671875\n",
            "1224/3000 train_loss: 257.9382019042969 test_loss:248.01779174804688\n",
            "1225/3000 train_loss: 239.82777404785156 test_loss:237.0399932861328\n",
            "1226/3000 train_loss: 227.9204864501953 test_loss:219.25119018554688\n",
            "1227/3000 train_loss: 199.642578125 test_loss:219.44207763671875\n",
            "1228/3000 train_loss: 261.3666076660156 test_loss:215.8532257080078\n",
            "1229/3000 train_loss: 205.17068481445312 test_loss:213.1266632080078\n",
            "1230/3000 train_loss: 186.78616333007812 test_loss:208.8846435546875\n",
            "1231/3000 train_loss: 241.20681762695312 test_loss:208.33299255371094\n",
            "1232/3000 train_loss: 208.6365966796875 test_loss:220.14430236816406\n",
            "1233/3000 train_loss: 214.1257781982422 test_loss:216.3306121826172\n",
            "1234/3000 train_loss: 206.9320068359375 test_loss:216.75173950195312\n",
            "1235/3000 train_loss: 216.66786193847656 test_loss:220.74134826660156\n",
            "1236/3000 train_loss: 271.18463134765625 test_loss:218.89764404296875\n",
            "1237/3000 train_loss: 291.35302734375 test_loss:219.32398986816406\n",
            "1238/3000 train_loss: 357.9222412109375 test_loss:223.36558532714844\n",
            "1239/3000 train_loss: 195.64137268066406 test_loss:212.07225036621094\n",
            "1240/3000 train_loss: 220.25192260742188 test_loss:209.92575073242188\n",
            "1241/3000 train_loss: 202.93402099609375 test_loss:209.6114044189453\n",
            "1242/3000 train_loss: 479.8397521972656 test_loss:217.23446655273438\n",
            "1243/3000 train_loss: 236.51617431640625 test_loss:229.354736328125\n",
            "1244/3000 train_loss: 210.39920043945312 test_loss:220.51138305664062\n",
            "1245/3000 train_loss: 234.4498291015625 test_loss:216.7670440673828\n",
            "1246/3000 train_loss: 202.40762329101562 test_loss:220.9404754638672\n",
            "1247/3000 train_loss: 217.84698486328125 test_loss:215.4292449951172\n",
            "1248/3000 train_loss: 198.67691040039062 test_loss:212.80545043945312\n",
            "1249/3000 train_loss: 213.5411376953125 test_loss:220.15602111816406\n",
            "1250/3000 train_loss: 190.7481689453125 test_loss:222.8326873779297\n",
            "1251/3000 train_loss: 318.64215087890625 test_loss:227.2040252685547\n",
            "1252/3000 train_loss: 203.9406280517578 test_loss:235.89404296875\n",
            "1253/3000 train_loss: 247.786865234375 test_loss:227.26852416992188\n",
            "1254/3000 train_loss: 211.33084106445312 test_loss:231.56817626953125\n",
            "1255/3000 train_loss: 261.762451171875 test_loss:218.23211669921875\n",
            "1256/3000 train_loss: 213.61260986328125 test_loss:211.03028869628906\n",
            "1257/3000 train_loss: 203.36624145507812 test_loss:211.99786376953125\n",
            "1258/3000 train_loss: 189.51829528808594 test_loss:219.53817749023438\n",
            "1259/3000 train_loss: 226.93624877929688 test_loss:209.7064208984375\n",
            "1260/3000 train_loss: 186.12071228027344 test_loss:204.71522521972656\n",
            "1261/3000 train_loss: 167.37493896484375 test_loss:208.30307006835938\n",
            "1262/3000 train_loss: 204.06988525390625 test_loss:207.58535766601562\n",
            "1263/3000 train_loss: 255.40823364257812 test_loss:211.98495483398438\n",
            "1264/3000 train_loss: 321.1165771484375 test_loss:213.57395935058594\n",
            "1265/3000 train_loss: 184.2904815673828 test_loss:205.26620483398438\n",
            "1266/3000 train_loss: 300.0133972167969 test_loss:204.74632263183594\n",
            "1267/3000 train_loss: 184.37554931640625 test_loss:211.05462646484375\n",
            "1268/3000 train_loss: 186.17007446289062 test_loss:206.9014892578125\n",
            "1269/3000 train_loss: 250.18466186523438 test_loss:206.61810302734375\n",
            "1270/3000 train_loss: 272.94940185546875 test_loss:209.67649841308594\n",
            "1271/3000 train_loss: 205.48483276367188 test_loss:216.36309814453125\n",
            "1272/3000 train_loss: 169.91085815429688 test_loss:205.63507080078125\n",
            "1273/3000 train_loss: 176.9875946044922 test_loss:202.24603271484375\n",
            "1274/3000 train_loss: 236.14450073242188 test_loss:203.75283813476562\n",
            "1275/3000 train_loss: 215.87948608398438 test_loss:204.51980590820312\n",
            "1276/3000 train_loss: 240.7893829345703 test_loss:204.55722045898438\n",
            "1277/3000 train_loss: 193.86767578125 test_loss:205.74388122558594\n",
            "1278/3000 train_loss: 190.07498168945312 test_loss:198.8391876220703\n",
            "1279/3000 train_loss: 176.06675720214844 test_loss:200.070068359375\n",
            "1280/3000 train_loss: 187.82415771484375 test_loss:202.88705444335938\n",
            "1281/3000 train_loss: 251.85708618164062 test_loss:203.28575134277344\n",
            "1282/3000 train_loss: 206.07713317871094 test_loss:200.14707946777344\n",
            "1283/3000 train_loss: 228.86734008789062 test_loss:201.55743408203125\n",
            "1284/3000 train_loss: 283.2236022949219 test_loss:200.0841827392578\n",
            "1285/3000 train_loss: 198.75257873535156 test_loss:206.52301025390625\n",
            "1286/3000 train_loss: 185.06298828125 test_loss:202.758056640625\n",
            "1287/3000 train_loss: 171.82528686523438 test_loss:204.98101806640625\n",
            "1288/3000 train_loss: 212.79266357421875 test_loss:202.66375732421875\n",
            "1289/3000 train_loss: 185.66847229003906 test_loss:213.05416870117188\n",
            "1290/3000 train_loss: 242.251953125 test_loss:200.83653259277344\n",
            "1291/3000 train_loss: 208.51548767089844 test_loss:197.69973754882812\n",
            "1292/3000 train_loss: 189.2730712890625 test_loss:196.02005004882812\n",
            "1293/3000 train_loss: 180.51271057128906 test_loss:198.5392608642578\n",
            "1294/3000 train_loss: 180.82310485839844 test_loss:200.98828125\n",
            "1295/3000 train_loss: 254.0652313232422 test_loss:205.0935821533203\n",
            "1296/3000 train_loss: 208.00155639648438 test_loss:201.13519287109375\n",
            "1297/3000 train_loss: 189.72372436523438 test_loss:199.62704467773438\n",
            "1298/3000 train_loss: 186.05593872070312 test_loss:197.11026000976562\n",
            "1299/3000 train_loss: 188.37750244140625 test_loss:197.45574951171875\n",
            "1300/3000 train_loss: 172.90493774414062 test_loss:201.32388305664062\n",
            "1301/3000 train_loss: 212.98309326171875 test_loss:200.55714416503906\n",
            "1302/3000 train_loss: 172.63710021972656 test_loss:196.29937744140625\n",
            "1303/3000 train_loss: 196.8585968017578 test_loss:203.15406799316406\n",
            "1304/3000 train_loss: 180.36044311523438 test_loss:195.76107788085938\n",
            "1305/3000 train_loss: 206.3297576904297 test_loss:196.99740600585938\n",
            "1306/3000 train_loss: 180.30787658691406 test_loss:199.19180297851562\n",
            "1307/3000 train_loss: 204.1637725830078 test_loss:198.98809814453125\n",
            "1308/3000 train_loss: 226.8114471435547 test_loss:214.4587860107422\n",
            "1309/3000 train_loss: 179.371337890625 test_loss:194.24964904785156\n",
            "1310/3000 train_loss: 190.79000854492188 test_loss:192.1356201171875\n",
            "1311/3000 train_loss: 202.55630493164062 test_loss:193.61972045898438\n",
            "1312/3000 train_loss: 281.8495178222656 test_loss:202.20785522460938\n",
            "1313/3000 train_loss: 187.97349548339844 test_loss:211.39450073242188\n",
            "1314/3000 train_loss: 203.5082244873047 test_loss:201.83248901367188\n",
            "1315/3000 train_loss: 233.48361206054688 test_loss:198.80972290039062\n",
            "1316/3000 train_loss: 173.0425262451172 test_loss:194.54444885253906\n",
            "1317/3000 train_loss: 172.5750274658203 test_loss:196.82794189453125\n",
            "1318/3000 train_loss: 193.77420043945312 test_loss:196.38108825683594\n",
            "1319/3000 train_loss: 290.90545654296875 test_loss:208.9025421142578\n",
            "1320/3000 train_loss: 322.18231201171875 test_loss:199.47763061523438\n",
            "1321/3000 train_loss: 282.73455810546875 test_loss:200.6147003173828\n",
            "1322/3000 train_loss: 180.59498596191406 test_loss:214.2556915283203\n",
            "1323/3000 train_loss: 206.57273864746094 test_loss:204.7482452392578\n",
            "1324/3000 train_loss: 190.39500427246094 test_loss:192.27230834960938\n",
            "1325/3000 train_loss: 176.1197052001953 test_loss:194.20626831054688\n",
            "1326/3000 train_loss: 165.3642120361328 test_loss:194.36306762695312\n",
            "1327/3000 train_loss: 396.0899963378906 test_loss:193.8543701171875\n",
            "1328/3000 train_loss: 202.08407592773438 test_loss:194.09019470214844\n",
            "1329/3000 train_loss: 200.84814453125 test_loss:195.23780822753906\n",
            "1330/3000 train_loss: 308.0489501953125 test_loss:201.2480926513672\n",
            "1331/3000 train_loss: 262.9700927734375 test_loss:243.20166015625\n",
            "1332/3000 train_loss: 231.44952392578125 test_loss:221.50509643554688\n",
            "1333/3000 train_loss: 182.4119110107422 test_loss:202.3660125732422\n",
            "1334/3000 train_loss: 218.4202880859375 test_loss:200.42498779296875\n",
            "1335/3000 train_loss: 171.01443481445312 test_loss:196.84561157226562\n",
            "1336/3000 train_loss: 181.8034210205078 test_loss:196.39889526367188\n",
            "1337/3000 train_loss: 206.48486328125 test_loss:193.4591827392578\n",
            "1338/3000 train_loss: 177.1944580078125 test_loss:191.3885040283203\n",
            "1339/3000 train_loss: 167.9852752685547 test_loss:192.61746215820312\n",
            "1340/3000 train_loss: 455.985595703125 test_loss:204.20208740234375\n",
            "1341/3000 train_loss: 334.16961669921875 test_loss:206.94757080078125\n",
            "1342/3000 train_loss: 280.8430480957031 test_loss:215.4132537841797\n",
            "1343/3000 train_loss: 230.74835205078125 test_loss:219.4419708251953\n",
            "1344/3000 train_loss: 230.05841064453125 test_loss:220.52957153320312\n",
            "1345/3000 train_loss: 177.0823516845703 test_loss:200.85659790039062\n",
            "1346/3000 train_loss: 174.2161407470703 test_loss:199.7240447998047\n",
            "1347/3000 train_loss: 229.77957153320312 test_loss:197.75051879882812\n",
            "1348/3000 train_loss: 159.93785095214844 test_loss:196.2274932861328\n",
            "1349/3000 train_loss: 193.38494873046875 test_loss:195.01629638671875\n",
            "1350/3000 train_loss: 183.96127319335938 test_loss:194.86578369140625\n",
            "1351/3000 train_loss: 171.5061798095703 test_loss:193.05120849609375\n",
            "1352/3000 train_loss: 198.05381774902344 test_loss:189.14508056640625\n",
            "1353/3000 train_loss: 238.9190673828125 test_loss:198.55770874023438\n",
            "1354/3000 train_loss: 185.5489501953125 test_loss:196.87330627441406\n",
            "1355/3000 train_loss: 164.89268493652344 test_loss:190.5635986328125\n",
            "1356/3000 train_loss: 175.7115936279297 test_loss:193.4133758544922\n",
            "1357/3000 train_loss: 236.68466186523438 test_loss:191.95265197753906\n",
            "1358/3000 train_loss: 183.39752197265625 test_loss:188.48416137695312\n",
            "1359/3000 train_loss: 175.76992797851562 test_loss:192.40431213378906\n",
            "1360/3000 train_loss: 158.67169189453125 test_loss:190.9826202392578\n",
            "1361/3000 train_loss: 166.13690185546875 test_loss:189.75772094726562\n",
            "1362/3000 train_loss: 172.4805145263672 test_loss:190.0758056640625\n",
            "1363/3000 train_loss: 191.9832305908203 test_loss:189.22158813476562\n",
            "1364/3000 train_loss: 180.75009155273438 test_loss:199.8189697265625\n",
            "1365/3000 train_loss: 183.54742431640625 test_loss:193.0609893798828\n",
            "1366/3000 train_loss: 204.67962646484375 test_loss:186.86956787109375\n",
            "1367/3000 train_loss: 249.87135314941406 test_loss:190.65533447265625\n",
            "1368/3000 train_loss: 183.5362548828125 test_loss:188.24600219726562\n",
            "1369/3000 train_loss: 169.39584350585938 test_loss:185.46841430664062\n",
            "1370/3000 train_loss: 168.95335388183594 test_loss:187.64434814453125\n",
            "1371/3000 train_loss: 181.48391723632812 test_loss:185.95556640625\n",
            "1372/3000 train_loss: 341.126953125 test_loss:188.99169921875\n",
            "1373/3000 train_loss: 246.61239624023438 test_loss:195.54652404785156\n",
            "1374/3000 train_loss: 253.81973266601562 test_loss:189.10910034179688\n",
            "1375/3000 train_loss: 217.57168579101562 test_loss:187.81503295898438\n",
            "1376/3000 train_loss: 170.00694274902344 test_loss:192.8212127685547\n",
            "1377/3000 train_loss: 221.03707885742188 test_loss:192.10667419433594\n",
            "1378/3000 train_loss: 176.64547729492188 test_loss:196.37179565429688\n",
            "1379/3000 train_loss: 197.64378356933594 test_loss:188.08938598632812\n",
            "1380/3000 train_loss: 188.76168823242188 test_loss:189.748779296875\n",
            "1381/3000 train_loss: 180.86215209960938 test_loss:192.1795654296875\n",
            "1382/3000 train_loss: 180.1615753173828 test_loss:189.32876586914062\n",
            "1383/3000 train_loss: 168.4604034423828 test_loss:189.04067993164062\n",
            "1384/3000 train_loss: 167.32623291015625 test_loss:190.13226318359375\n",
            "1385/3000 train_loss: 189.70050048828125 test_loss:189.23829650878906\n",
            "1386/3000 train_loss: 182.05972290039062 test_loss:190.44998168945312\n",
            "1387/3000 train_loss: 188.20635986328125 test_loss:191.24627685546875\n",
            "1388/3000 train_loss: 384.1148376464844 test_loss:194.86680603027344\n",
            "1389/3000 train_loss: 158.1879425048828 test_loss:197.03860473632812\n",
            "1390/3000 train_loss: 213.2111358642578 test_loss:190.991943359375\n",
            "1391/3000 train_loss: 171.8314971923828 test_loss:192.66477966308594\n",
            "1392/3000 train_loss: 245.83541870117188 test_loss:192.13204956054688\n",
            "1393/3000 train_loss: 228.93072509765625 test_loss:227.2734832763672\n",
            "1394/3000 train_loss: 177.37136840820312 test_loss:196.35899353027344\n",
            "1395/3000 train_loss: 166.76284790039062 test_loss:188.86949157714844\n",
            "1396/3000 train_loss: 192.1606903076172 test_loss:189.3873748779297\n",
            "1397/3000 train_loss: 159.5192413330078 test_loss:193.50314331054688\n",
            "1398/3000 train_loss: 198.13580322265625 test_loss:189.62136840820312\n",
            "1399/3000 train_loss: 183.76239013671875 test_loss:194.81988525390625\n",
            "1400/3000 train_loss: 181.073974609375 test_loss:188.98146057128906\n",
            "1401/3000 train_loss: 166.39483642578125 test_loss:188.24716186523438\n",
            "1402/3000 train_loss: 216.08340454101562 test_loss:186.14427185058594\n",
            "1403/3000 train_loss: 172.01834106445312 test_loss:182.08139038085938\n",
            "1404/3000 train_loss: 174.48851013183594 test_loss:184.4232940673828\n",
            "1405/3000 train_loss: 190.42022705078125 test_loss:184.32177734375\n",
            "1406/3000 train_loss: 178.48580932617188 test_loss:195.82562255859375\n",
            "1407/3000 train_loss: 205.6336669921875 test_loss:190.23170471191406\n",
            "1408/3000 train_loss: 158.53382873535156 test_loss:186.7895050048828\n",
            "1409/3000 train_loss: 176.5863037109375 test_loss:193.90731811523438\n",
            "1410/3000 train_loss: 182.35968017578125 test_loss:185.51803588867188\n",
            "1411/3000 train_loss: 167.0056610107422 test_loss:184.04843139648438\n",
            "1412/3000 train_loss: 251.59422302246094 test_loss:182.96145629882812\n",
            "1413/3000 train_loss: 294.18206787109375 test_loss:187.30783081054688\n",
            "1414/3000 train_loss: 165.94723510742188 test_loss:186.01467895507812\n",
            "1415/3000 train_loss: 175.59872436523438 test_loss:182.58099365234375\n",
            "1416/3000 train_loss: 154.8677978515625 test_loss:182.9109344482422\n",
            "1417/3000 train_loss: 176.5365447998047 test_loss:183.7930450439453\n",
            "1418/3000 train_loss: 159.5348663330078 test_loss:181.13970947265625\n",
            "1419/3000 train_loss: 157.55886840820312 test_loss:183.6865234375\n",
            "1420/3000 train_loss: 150.5164794921875 test_loss:181.2369384765625\n",
            "1421/3000 train_loss: 187.5552978515625 test_loss:183.3802947998047\n",
            "1422/3000 train_loss: 153.61410522460938 test_loss:183.2436065673828\n",
            "1423/3000 train_loss: 221.44216918945312 test_loss:185.46054077148438\n",
            "1424/3000 train_loss: 173.16317749023438 test_loss:180.47364807128906\n",
            "1425/3000 train_loss: 234.98260498046875 test_loss:180.25250244140625\n",
            "1426/3000 train_loss: 207.7104949951172 test_loss:186.4093475341797\n",
            "1427/3000 train_loss: 156.34768676757812 test_loss:191.76040649414062\n",
            "1428/3000 train_loss: 183.27377319335938 test_loss:185.0693817138672\n",
            "1429/3000 train_loss: 159.70130920410156 test_loss:183.3004913330078\n",
            "1430/3000 train_loss: 170.73562622070312 test_loss:183.61483764648438\n",
            "1431/3000 train_loss: 174.50177001953125 test_loss:179.70098876953125\n",
            "1432/3000 train_loss: 174.68359375 test_loss:179.55401611328125\n",
            "1433/3000 train_loss: 213.2393798828125 test_loss:185.94154357910156\n",
            "1434/3000 train_loss: 190.04293823242188 test_loss:197.38255310058594\n",
            "1435/3000 train_loss: 185.44509887695312 test_loss:187.56240844726562\n",
            "1436/3000 train_loss: 171.9560089111328 test_loss:184.81063842773438\n",
            "1437/3000 train_loss: 153.38621520996094 test_loss:194.2054901123047\n",
            "1438/3000 train_loss: 292.75341796875 test_loss:186.3523712158203\n",
            "1439/3000 train_loss: 164.2921600341797 test_loss:197.3201446533203\n",
            "1440/3000 train_loss: 168.34805297851562 test_loss:184.56881713867188\n",
            "1441/3000 train_loss: 176.97421264648438 test_loss:180.04727172851562\n",
            "1442/3000 train_loss: 189.38662719726562 test_loss:179.7708740234375\n",
            "1443/3000 train_loss: 157.40098571777344 test_loss:180.0355224609375\n",
            "1444/3000 train_loss: 268.0545959472656 test_loss:180.55984497070312\n",
            "1445/3000 train_loss: 261.3948059082031 test_loss:182.2350616455078\n",
            "1446/3000 train_loss: 170.042724609375 test_loss:193.60748291015625\n",
            "1447/3000 train_loss: 170.4370574951172 test_loss:179.582763671875\n",
            "1448/3000 train_loss: 176.2209930419922 test_loss:180.8865203857422\n",
            "1449/3000 train_loss: 179.81277465820312 test_loss:188.273193359375\n",
            "1450/3000 train_loss: 162.44229125976562 test_loss:177.4146728515625\n",
            "1451/3000 train_loss: 170.7581024169922 test_loss:177.29539489746094\n",
            "1452/3000 train_loss: 196.43740844726562 test_loss:175.21414184570312\n",
            "1453/3000 train_loss: 213.25990295410156 test_loss:186.10928344726562\n",
            "1454/3000 train_loss: 169.27096557617188 test_loss:178.08078002929688\n",
            "1455/3000 train_loss: 210.95774841308594 test_loss:177.74440002441406\n",
            "1456/3000 train_loss: 220.86038208007812 test_loss:181.4957275390625\n",
            "1457/3000 train_loss: 192.70858764648438 test_loss:179.76226806640625\n",
            "1458/3000 train_loss: 198.63035583496094 test_loss:180.44851684570312\n",
            "1459/3000 train_loss: 234.1253662109375 test_loss:177.1692352294922\n",
            "1460/3000 train_loss: 173.64248657226562 test_loss:183.489013671875\n",
            "1461/3000 train_loss: 160.1514434814453 test_loss:181.0784454345703\n",
            "1462/3000 train_loss: 339.4548034667969 test_loss:186.54946899414062\n",
            "1463/3000 train_loss: 178.12306213378906 test_loss:202.3436279296875\n",
            "1464/3000 train_loss: 194.8317413330078 test_loss:190.30712890625\n",
            "1465/3000 train_loss: 181.22511291503906 test_loss:186.15386962890625\n",
            "1466/3000 train_loss: 182.49139404296875 test_loss:186.83644104003906\n",
            "1467/3000 train_loss: 169.21974182128906 test_loss:188.2741241455078\n",
            "1468/3000 train_loss: 158.67002868652344 test_loss:187.39739990234375\n",
            "1469/3000 train_loss: 155.6533966064453 test_loss:181.9530029296875\n",
            "1470/3000 train_loss: 166.91641235351562 test_loss:183.073486328125\n",
            "1471/3000 train_loss: 162.45098876953125 test_loss:178.85174560546875\n",
            "1472/3000 train_loss: 152.95840454101562 test_loss:179.7880859375\n",
            "1473/3000 train_loss: 204.55335998535156 test_loss:176.5318603515625\n",
            "1474/3000 train_loss: 148.99627685546875 test_loss:176.63418579101562\n",
            "1475/3000 train_loss: 196.15858459472656 test_loss:179.2979736328125\n",
            "1476/3000 train_loss: 150.3759002685547 test_loss:184.8106231689453\n",
            "1477/3000 train_loss: 160.16690063476562 test_loss:176.7302703857422\n",
            "1478/3000 train_loss: 229.77688598632812 test_loss:175.5431365966797\n",
            "1479/3000 train_loss: 151.8389434814453 test_loss:174.36737060546875\n",
            "1480/3000 train_loss: 186.99037170410156 test_loss:181.76116943359375\n",
            "1481/3000 train_loss: 235.30642700195312 test_loss:178.80288696289062\n",
            "1482/3000 train_loss: 243.3058319091797 test_loss:182.87245178222656\n",
            "1483/3000 train_loss: 253.28628540039062 test_loss:181.8964080810547\n",
            "1484/3000 train_loss: 193.5796661376953 test_loss:185.8829345703125\n",
            "1485/3000 train_loss: 175.3231201171875 test_loss:188.17742919921875\n",
            "1486/3000 train_loss: 178.39952087402344 test_loss:181.62066650390625\n",
            "1487/3000 train_loss: 157.9461212158203 test_loss:181.49392700195312\n",
            "1488/3000 train_loss: 179.20858764648438 test_loss:185.5072479248047\n",
            "1489/3000 train_loss: 164.05007934570312 test_loss:177.43453979492188\n",
            "1490/3000 train_loss: 242.92449951171875 test_loss:178.98184204101562\n",
            "1491/3000 train_loss: 200.5460968017578 test_loss:194.48715209960938\n",
            "1492/3000 train_loss: 171.22132873535156 test_loss:180.23513793945312\n",
            "1493/3000 train_loss: 217.7115020751953 test_loss:179.72640991210938\n",
            "1494/3000 train_loss: 266.0447998046875 test_loss:180.03768920898438\n",
            "1495/3000 train_loss: 240.60153198242188 test_loss:175.08241271972656\n",
            "1496/3000 train_loss: 165.77037048339844 test_loss:185.60897827148438\n",
            "1497/3000 train_loss: 158.95623779296875 test_loss:182.8389892578125\n",
            "1498/3000 train_loss: 269.26580810546875 test_loss:188.9349365234375\n",
            "1499/3000 train_loss: 192.48532104492188 test_loss:178.16090393066406\n",
            "1500/3000 train_loss: 181.18263244628906 test_loss:179.15052795410156\n",
            "1501/3000 train_loss: 183.70323181152344 test_loss:180.57810974121094\n",
            "1502/3000 train_loss: 177.09877014160156 test_loss:176.6996612548828\n",
            "1503/3000 train_loss: 160.84742736816406 test_loss:179.16587829589844\n",
            "1504/3000 train_loss: 154.91075134277344 test_loss:180.78359985351562\n",
            "1505/3000 train_loss: 162.88082885742188 test_loss:174.7227783203125\n",
            "1506/3000 train_loss: 214.27191162109375 test_loss:173.07847595214844\n",
            "1507/3000 train_loss: 193.77391052246094 test_loss:173.0652618408203\n",
            "1508/3000 train_loss: 142.1934814453125 test_loss:176.65916442871094\n",
            "1509/3000 train_loss: 267.679931640625 test_loss:178.31838989257812\n",
            "1510/3000 train_loss: 166.85305786132812 test_loss:191.8177490234375\n",
            "1511/3000 train_loss: 149.14100646972656 test_loss:178.23641967773438\n",
            "1512/3000 train_loss: 164.2777862548828 test_loss:189.3925018310547\n",
            "1513/3000 train_loss: 181.86825561523438 test_loss:181.44461059570312\n",
            "1514/3000 train_loss: 170.5987548828125 test_loss:173.76919555664062\n",
            "1515/3000 train_loss: 161.00941467285156 test_loss:175.51840209960938\n",
            "1516/3000 train_loss: 193.17822265625 test_loss:176.41624450683594\n",
            "1517/3000 train_loss: 212.66348266601562 test_loss:172.63116455078125\n",
            "1518/3000 train_loss: 177.79898071289062 test_loss:182.107177734375\n",
            "1519/3000 train_loss: 161.673095703125 test_loss:183.37596130371094\n",
            "1520/3000 train_loss: 219.3267822265625 test_loss:186.4444122314453\n",
            "1521/3000 train_loss: 149.08236694335938 test_loss:184.25489807128906\n",
            "1522/3000 train_loss: 163.47943115234375 test_loss:177.6669921875\n",
            "1523/3000 train_loss: 230.9603271484375 test_loss:173.61630249023438\n",
            "1524/3000 train_loss: 146.5224151611328 test_loss:171.80126953125\n",
            "1525/3000 train_loss: 166.81533813476562 test_loss:173.2476806640625\n",
            "1526/3000 train_loss: 153.14430236816406 test_loss:175.1793212890625\n",
            "1527/3000 train_loss: 202.1992950439453 test_loss:173.60302734375\n",
            "1528/3000 train_loss: 168.23855590820312 test_loss:183.18264770507812\n",
            "1529/3000 train_loss: 169.5547332763672 test_loss:172.965087890625\n",
            "1530/3000 train_loss: 154.13916015625 test_loss:170.0983428955078\n",
            "1531/3000 train_loss: 161.1564178466797 test_loss:169.7010955810547\n",
            "1532/3000 train_loss: 175.5595245361328 test_loss:169.42535400390625\n",
            "1533/3000 train_loss: 163.831298828125 test_loss:176.4625701904297\n",
            "1534/3000 train_loss: 143.2079620361328 test_loss:174.90139770507812\n",
            "1535/3000 train_loss: 155.37869262695312 test_loss:176.80148315429688\n",
            "1536/3000 train_loss: 149.29776000976562 test_loss:173.95501708984375\n",
            "1537/3000 train_loss: 147.2001953125 test_loss:171.95001220703125\n",
            "1538/3000 train_loss: 149.59390258789062 test_loss:168.8733673095703\n",
            "1539/3000 train_loss: 161.29287719726562 test_loss:171.18817138671875\n",
            "1540/3000 train_loss: 156.89529418945312 test_loss:174.93251037597656\n",
            "1541/3000 train_loss: 175.046142578125 test_loss:168.7074432373047\n",
            "1542/3000 train_loss: 159.5171661376953 test_loss:178.19955444335938\n",
            "1543/3000 train_loss: 179.52462768554688 test_loss:172.27874755859375\n",
            "1544/3000 train_loss: 178.7060546875 test_loss:175.4820556640625\n",
            "1545/3000 train_loss: 236.5142822265625 test_loss:167.44960021972656\n",
            "1546/3000 train_loss: 156.2654266357422 test_loss:165.89102172851562\n",
            "1547/3000 train_loss: 174.24517822265625 test_loss:166.81674194335938\n",
            "1548/3000 train_loss: 189.65145874023438 test_loss:172.20872497558594\n",
            "1549/3000 train_loss: 153.25059509277344 test_loss:172.13555908203125\n",
            "1550/3000 train_loss: 155.08038330078125 test_loss:172.03260803222656\n",
            "1551/3000 train_loss: 167.38819885253906 test_loss:168.73451232910156\n",
            "1552/3000 train_loss: 146.2737579345703 test_loss:170.98895263671875\n",
            "1553/3000 train_loss: 232.34335327148438 test_loss:175.216064453125\n",
            "1554/3000 train_loss: 179.75277709960938 test_loss:180.52572631835938\n",
            "1555/3000 train_loss: 153.38824462890625 test_loss:172.3876953125\n",
            "1556/3000 train_loss: 151.851318359375 test_loss:165.47213745117188\n",
            "1557/3000 train_loss: 191.51290893554688 test_loss:166.65206909179688\n",
            "1558/3000 train_loss: 160.48626708984375 test_loss:178.41824340820312\n",
            "1559/3000 train_loss: 185.49453735351562 test_loss:178.1719512939453\n",
            "1560/3000 train_loss: 154.01718139648438 test_loss:178.65377807617188\n",
            "1561/3000 train_loss: 167.97900390625 test_loss:171.28204345703125\n",
            "1562/3000 train_loss: 175.8076171875 test_loss:172.924560546875\n",
            "1563/3000 train_loss: 249.45916748046875 test_loss:172.92869567871094\n",
            "1564/3000 train_loss: 193.28793334960938 test_loss:170.80657958984375\n",
            "1565/3000 train_loss: 149.4002685546875 test_loss:164.87635803222656\n",
            "1566/3000 train_loss: 199.64349365234375 test_loss:167.4415283203125\n",
            "1567/3000 train_loss: 166.33941650390625 test_loss:178.6129913330078\n",
            "1568/3000 train_loss: 156.8556671142578 test_loss:172.9981231689453\n",
            "1569/3000 train_loss: 180.90383911132812 test_loss:175.26344299316406\n",
            "1570/3000 train_loss: 182.1252899169922 test_loss:171.50677490234375\n",
            "1571/3000 train_loss: 181.9239044189453 test_loss:177.05767822265625\n",
            "1572/3000 train_loss: 259.981201171875 test_loss:176.80287170410156\n",
            "1573/3000 train_loss: 175.150390625 test_loss:175.13026428222656\n",
            "1574/3000 train_loss: 161.5710906982422 test_loss:171.04898071289062\n",
            "1575/3000 train_loss: 171.08010864257812 test_loss:168.9918975830078\n",
            "1576/3000 train_loss: 294.6260070800781 test_loss:178.0635986328125\n",
            "1577/3000 train_loss: 234.1246337890625 test_loss:243.7525177001953\n",
            "1578/3000 train_loss: 204.78269958496094 test_loss:189.71160888671875\n",
            "1579/3000 train_loss: 179.57101440429688 test_loss:176.18148803710938\n",
            "1580/3000 train_loss: 170.81643676757812 test_loss:181.81912231445312\n",
            "1581/3000 train_loss: 158.62828063964844 test_loss:175.76443481445312\n",
            "1582/3000 train_loss: 163.47091674804688 test_loss:172.21792602539062\n",
            "1583/3000 train_loss: 159.6780548095703 test_loss:172.46426391601562\n",
            "1584/3000 train_loss: 153.07997131347656 test_loss:173.3037872314453\n",
            "1585/3000 train_loss: 218.6173095703125 test_loss:170.25352478027344\n",
            "1586/3000 train_loss: 161.14105224609375 test_loss:170.32196044921875\n",
            "1587/3000 train_loss: 171.25624084472656 test_loss:169.32347106933594\n",
            "1588/3000 train_loss: 159.6549835205078 test_loss:167.50538635253906\n",
            "1589/3000 train_loss: 147.12655639648438 test_loss:169.81927490234375\n",
            "1590/3000 train_loss: 269.22271728515625 test_loss:169.40261840820312\n",
            "1591/3000 train_loss: 171.0318603515625 test_loss:177.61866760253906\n",
            "1592/3000 train_loss: 162.26235961914062 test_loss:173.5634002685547\n",
            "1593/3000 train_loss: 160.22984313964844 test_loss:176.52706909179688\n",
            "1594/3000 train_loss: 194.9973602294922 test_loss:172.00790405273438\n",
            "1595/3000 train_loss: 186.95223999023438 test_loss:172.18333435058594\n",
            "1596/3000 train_loss: 146.4085693359375 test_loss:170.998046875\n",
            "1597/3000 train_loss: 212.03834533691406 test_loss:168.35140991210938\n",
            "1598/3000 train_loss: 137.2550506591797 test_loss:173.8428955078125\n",
            "1599/3000 train_loss: 165.9627227783203 test_loss:175.19723510742188\n",
            "1600/3000 train_loss: 138.47714233398438 test_loss:170.94320678710938\n",
            "1601/3000 train_loss: 155.45767211914062 test_loss:168.02381896972656\n",
            "1602/3000 train_loss: 165.51412963867188 test_loss:167.2438507080078\n",
            "1603/3000 train_loss: 149.03135681152344 test_loss:166.0472412109375\n",
            "1604/3000 train_loss: 161.705322265625 test_loss:165.08482360839844\n",
            "1605/3000 train_loss: 154.2303009033203 test_loss:165.12899780273438\n",
            "1606/3000 train_loss: 226.92294311523438 test_loss:164.39727783203125\n",
            "1607/3000 train_loss: 149.40975952148438 test_loss:162.67105102539062\n",
            "1608/3000 train_loss: 170.83419799804688 test_loss:161.33856201171875\n",
            "1609/3000 train_loss: 170.30679321289062 test_loss:163.4432373046875\n",
            "1610/3000 train_loss: 142.59849548339844 test_loss:161.2211456298828\n",
            "1611/3000 train_loss: 186.24710083007812 test_loss:167.89218139648438\n",
            "1612/3000 train_loss: 222.74342346191406 test_loss:176.72154235839844\n",
            "1613/3000 train_loss: 241.6580810546875 test_loss:178.6759796142578\n",
            "1614/3000 train_loss: 165.6400604248047 test_loss:158.17694091796875\n",
            "1615/3000 train_loss: 148.68582153320312 test_loss:159.23046875\n",
            "1616/3000 train_loss: 370.24468994140625 test_loss:172.98953247070312\n",
            "1617/3000 train_loss: 163.09136962890625 test_loss:171.71876525878906\n",
            "1618/3000 train_loss: 146.42771911621094 test_loss:162.57138061523438\n",
            "1619/3000 train_loss: 275.8951110839844 test_loss:185.78680419921875\n",
            "1620/3000 train_loss: 202.556884765625 test_loss:203.0250701904297\n",
            "1621/3000 train_loss: 164.3525390625 test_loss:169.11392211914062\n",
            "1622/3000 train_loss: 268.4118957519531 test_loss:173.75433349609375\n",
            "1623/3000 train_loss: 254.76565551757812 test_loss:184.8676300048828\n",
            "1624/3000 train_loss: 158.19398498535156 test_loss:171.25531005859375\n",
            "1625/3000 train_loss: 173.06883239746094 test_loss:165.564208984375\n",
            "1626/3000 train_loss: 181.2191162109375 test_loss:162.73736572265625\n",
            "1627/3000 train_loss: 167.05361938476562 test_loss:164.03170776367188\n",
            "1628/3000 train_loss: 175.0411834716797 test_loss:163.17672729492188\n",
            "1629/3000 train_loss: 231.16156005859375 test_loss:170.5074920654297\n",
            "1630/3000 train_loss: 252.00782775878906 test_loss:174.0265350341797\n",
            "1631/3000 train_loss: 253.5833740234375 test_loss:174.82118225097656\n",
            "1632/3000 train_loss: 167.70814514160156 test_loss:176.94317626953125\n",
            "1633/3000 train_loss: 150.9845428466797 test_loss:168.80271911621094\n",
            "1634/3000 train_loss: 262.08038330078125 test_loss:171.4300537109375\n",
            "1635/3000 train_loss: 173.01649475097656 test_loss:172.1924285888672\n",
            "1636/3000 train_loss: 183.62179565429688 test_loss:166.99765014648438\n",
            "1637/3000 train_loss: 220.75852966308594 test_loss:166.91781616210938\n",
            "1638/3000 train_loss: 159.94728088378906 test_loss:163.06317138671875\n",
            "1639/3000 train_loss: 177.1943359375 test_loss:163.7506866455078\n",
            "1640/3000 train_loss: 163.595703125 test_loss:159.9677276611328\n",
            "1641/3000 train_loss: 148.83860778808594 test_loss:158.11953735351562\n",
            "1642/3000 train_loss: 151.44627380371094 test_loss:159.9635772705078\n",
            "1643/3000 train_loss: 134.97824096679688 test_loss:161.00665283203125\n",
            "1644/3000 train_loss: 128.740234375 test_loss:159.06781005859375\n",
            "1645/3000 train_loss: 330.4691467285156 test_loss:162.12576293945312\n",
            "1646/3000 train_loss: 159.36737060546875 test_loss:157.17404174804688\n",
            "1647/3000 train_loss: 267.58953857421875 test_loss:160.17178344726562\n",
            "1648/3000 train_loss: 263.4488525390625 test_loss:163.07891845703125\n",
            "1649/3000 train_loss: 150.22601318359375 test_loss:161.14749145507812\n",
            "1650/3000 train_loss: 153.3873291015625 test_loss:164.35411071777344\n",
            "1651/3000 train_loss: 154.00247192382812 test_loss:156.78274536132812\n",
            "1652/3000 train_loss: 291.4908752441406 test_loss:157.32229614257812\n",
            "1653/3000 train_loss: 169.26724243164062 test_loss:160.63731384277344\n",
            "1654/3000 train_loss: 137.40072631835938 test_loss:167.2448272705078\n",
            "1655/3000 train_loss: 186.4908447265625 test_loss:160.59371948242188\n",
            "1656/3000 train_loss: 159.62997436523438 test_loss:162.46681213378906\n",
            "1657/3000 train_loss: 151.79605102539062 test_loss:161.91546630859375\n",
            "1658/3000 train_loss: 149.82290649414062 test_loss:156.42958068847656\n",
            "1659/3000 train_loss: 222.02001953125 test_loss:165.2510986328125\n",
            "1660/3000 train_loss: 142.84573364257812 test_loss:169.58389282226562\n",
            "1661/3000 train_loss: 275.93902587890625 test_loss:164.23997497558594\n",
            "1662/3000 train_loss: 130.58734130859375 test_loss:167.85374450683594\n",
            "1663/3000 train_loss: 153.45477294921875 test_loss:164.0376739501953\n",
            "1664/3000 train_loss: 144.5223388671875 test_loss:158.51870727539062\n",
            "1665/3000 train_loss: 141.324462890625 test_loss:163.837646484375\n",
            "1666/3000 train_loss: 139.73779296875 test_loss:159.99815368652344\n",
            "1667/3000 train_loss: 143.39682006835938 test_loss:158.6872100830078\n",
            "1668/3000 train_loss: 137.4300079345703 test_loss:157.82830810546875\n",
            "1669/3000 train_loss: 175.1573028564453 test_loss:156.09423828125\n",
            "1670/3000 train_loss: 170.48773193359375 test_loss:153.183837890625\n",
            "1671/3000 train_loss: 131.46739196777344 test_loss:154.41014099121094\n",
            "1672/3000 train_loss: 150.7682342529297 test_loss:156.06640625\n",
            "1673/3000 train_loss: 185.0997314453125 test_loss:160.15060424804688\n",
            "1674/3000 train_loss: 151.72039794921875 test_loss:158.4046630859375\n",
            "1675/3000 train_loss: 232.48451232910156 test_loss:159.87863159179688\n",
            "1676/3000 train_loss: 144.08380126953125 test_loss:156.7940673828125\n",
            "1677/3000 train_loss: 188.2197265625 test_loss:168.6136474609375\n",
            "1678/3000 train_loss: 176.80369567871094 test_loss:166.96905517578125\n",
            "1679/3000 train_loss: 171.36087036132812 test_loss:166.30337524414062\n",
            "1680/3000 train_loss: 150.37246704101562 test_loss:161.56185913085938\n",
            "1681/3000 train_loss: 149.84805297851562 test_loss:176.24710083007812\n",
            "1682/3000 train_loss: 179.822509765625 test_loss:171.3358612060547\n",
            "1683/3000 train_loss: 167.50111389160156 test_loss:160.8815155029297\n",
            "1684/3000 train_loss: 158.1919403076172 test_loss:162.76400756835938\n",
            "1685/3000 train_loss: 220.14126586914062 test_loss:160.0047149658203\n",
            "1686/3000 train_loss: 179.1094970703125 test_loss:167.43870544433594\n",
            "1687/3000 train_loss: 164.15159606933594 test_loss:162.65188598632812\n",
            "1688/3000 train_loss: 146.47193908691406 test_loss:163.85818481445312\n",
            "1689/3000 train_loss: 198.76075744628906 test_loss:161.10443115234375\n",
            "1690/3000 train_loss: 185.20040893554688 test_loss:156.99522399902344\n",
            "1691/3000 train_loss: 158.22750854492188 test_loss:157.6546173095703\n",
            "1692/3000 train_loss: 366.7317199707031 test_loss:161.57699584960938\n",
            "1693/3000 train_loss: 208.61209106445312 test_loss:171.96768188476562\n",
            "1694/3000 train_loss: 153.5386199951172 test_loss:162.54428100585938\n",
            "1695/3000 train_loss: 155.9074249267578 test_loss:159.0883026123047\n",
            "1696/3000 train_loss: 143.27590942382812 test_loss:159.8027801513672\n",
            "1697/3000 train_loss: 183.0860595703125 test_loss:157.68099975585938\n",
            "1698/3000 train_loss: 145.90740966796875 test_loss:152.9293212890625\n",
            "1699/3000 train_loss: 163.89895629882812 test_loss:155.13742065429688\n",
            "1700/3000 train_loss: 137.80699157714844 test_loss:159.93301391601562\n",
            "1701/3000 train_loss: 181.35711669921875 test_loss:158.94985961914062\n",
            "1702/3000 train_loss: 196.24957275390625 test_loss:160.8565673828125\n",
            "1703/3000 train_loss: 141.3983154296875 test_loss:161.55125427246094\n",
            "1704/3000 train_loss: 153.3950653076172 test_loss:159.99075317382812\n",
            "1705/3000 train_loss: 186.64659118652344 test_loss:160.901123046875\n",
            "1706/3000 train_loss: 145.35316467285156 test_loss:154.1617889404297\n",
            "1707/3000 train_loss: 167.29660034179688 test_loss:153.96591186523438\n",
            "1708/3000 train_loss: 151.71336364746094 test_loss:161.96804809570312\n",
            "1709/3000 train_loss: 137.0764617919922 test_loss:156.82275390625\n",
            "1710/3000 train_loss: 148.9777069091797 test_loss:157.92782592773438\n",
            "1711/3000 train_loss: 131.32339477539062 test_loss:155.7452392578125\n",
            "1712/3000 train_loss: 189.73089599609375 test_loss:155.7706298828125\n",
            "1713/3000 train_loss: 269.9608154296875 test_loss:168.41017150878906\n",
            "1714/3000 train_loss: 219.03794860839844 test_loss:157.0384063720703\n",
            "1715/3000 train_loss: 172.95860290527344 test_loss:152.79283142089844\n",
            "1716/3000 train_loss: 143.21331787109375 test_loss:150.56832885742188\n",
            "1717/3000 train_loss: 156.72569274902344 test_loss:151.22853088378906\n",
            "1718/3000 train_loss: 199.81398010253906 test_loss:152.5956573486328\n",
            "1719/3000 train_loss: 250.44610595703125 test_loss:164.17926025390625\n",
            "1720/3000 train_loss: 153.65335083007812 test_loss:177.84130859375\n",
            "1721/3000 train_loss: 196.39959716796875 test_loss:172.52110290527344\n",
            "1722/3000 train_loss: 168.92031860351562 test_loss:174.28445434570312\n",
            "1723/3000 train_loss: 157.4611053466797 test_loss:162.91322326660156\n",
            "1724/3000 train_loss: 267.713623046875 test_loss:163.5931396484375\n",
            "1725/3000 train_loss: 144.64576721191406 test_loss:165.1986541748047\n",
            "1726/3000 train_loss: 154.1046600341797 test_loss:162.49591064453125\n",
            "1727/3000 train_loss: 145.5622100830078 test_loss:159.3131561279297\n",
            "1728/3000 train_loss: 137.264404296875 test_loss:154.94259643554688\n",
            "1729/3000 train_loss: 130.12985229492188 test_loss:153.17459106445312\n",
            "1730/3000 train_loss: 153.03207397460938 test_loss:152.19366455078125\n",
            "1731/3000 train_loss: 135.1050567626953 test_loss:153.17041015625\n",
            "1732/3000 train_loss: 223.55270385742188 test_loss:152.02528381347656\n",
            "1733/3000 train_loss: 132.3438262939453 test_loss:155.74066162109375\n",
            "1734/3000 train_loss: 348.8714294433594 test_loss:154.23861694335938\n",
            "1735/3000 train_loss: 162.48196411132812 test_loss:161.5714569091797\n",
            "1736/3000 train_loss: 206.25132751464844 test_loss:157.57130432128906\n",
            "1737/3000 train_loss: 308.05413818359375 test_loss:165.26588439941406\n",
            "1738/3000 train_loss: 139.7749481201172 test_loss:170.57833862304688\n",
            "1739/3000 train_loss: 244.88502502441406 test_loss:160.7483673095703\n",
            "1740/3000 train_loss: 150.9010467529297 test_loss:164.58372497558594\n",
            "1741/3000 train_loss: 168.8878936767578 test_loss:158.41952514648438\n",
            "1742/3000 train_loss: 181.48651123046875 test_loss:161.40725708007812\n",
            "1743/3000 train_loss: 131.1774444580078 test_loss:154.6167755126953\n",
            "1744/3000 train_loss: 235.79620361328125 test_loss:151.11184692382812\n",
            "1745/3000 train_loss: 199.01226806640625 test_loss:149.47686767578125\n",
            "1746/3000 train_loss: 165.13055419921875 test_loss:152.1962432861328\n",
            "1747/3000 train_loss: 144.93443298339844 test_loss:157.85992431640625\n",
            "1748/3000 train_loss: 175.1683807373047 test_loss:155.28961181640625\n",
            "1749/3000 train_loss: 166.0660858154297 test_loss:151.9294891357422\n",
            "1750/3000 train_loss: 165.7509307861328 test_loss:150.8775634765625\n",
            "1751/3000 train_loss: 136.026123046875 test_loss:150.3728790283203\n",
            "1752/3000 train_loss: 128.7129669189453 test_loss:149.71839904785156\n",
            "1753/3000 train_loss: 215.49749755859375 test_loss:152.2879638671875\n",
            "1754/3000 train_loss: 136.46556091308594 test_loss:148.07376098632812\n",
            "1755/3000 train_loss: 139.49266052246094 test_loss:146.7506561279297\n",
            "1756/3000 train_loss: 137.66705322265625 test_loss:147.8070068359375\n",
            "1757/3000 train_loss: 126.43919372558594 test_loss:146.9368896484375\n",
            "1758/3000 train_loss: 161.59512329101562 test_loss:146.02500915527344\n",
            "1759/3000 train_loss: 139.01614379882812 test_loss:147.81884765625\n",
            "1760/3000 train_loss: 131.47463989257812 test_loss:145.62655639648438\n",
            "1761/3000 train_loss: 135.86111450195312 test_loss:149.67015075683594\n",
            "1762/3000 train_loss: 141.43319702148438 test_loss:147.96499633789062\n",
            "1763/3000 train_loss: 130.48593139648438 test_loss:147.90182495117188\n",
            "1764/3000 train_loss: 132.1217803955078 test_loss:147.7084197998047\n",
            "1765/3000 train_loss: 172.703125 test_loss:147.5615234375\n",
            "1766/3000 train_loss: 151.27317810058594 test_loss:151.2760009765625\n",
            "1767/3000 train_loss: 148.43923950195312 test_loss:159.47280883789062\n",
            "1768/3000 train_loss: 127.5782470703125 test_loss:151.20822143554688\n",
            "1769/3000 train_loss: 140.25033569335938 test_loss:149.67860412597656\n",
            "1770/3000 train_loss: 230.12380981445312 test_loss:150.27671813964844\n",
            "1771/3000 train_loss: 163.58883666992188 test_loss:155.27859497070312\n",
            "1772/3000 train_loss: 158.8516845703125 test_loss:150.53506469726562\n",
            "1773/3000 train_loss: 137.13400268554688 test_loss:149.61968994140625\n",
            "1774/3000 train_loss: 130.1427764892578 test_loss:146.45262145996094\n",
            "1775/3000 train_loss: 234.98643493652344 test_loss:148.15109252929688\n",
            "1776/3000 train_loss: 143.91920471191406 test_loss:147.72877502441406\n",
            "1777/3000 train_loss: 140.4753875732422 test_loss:146.786865234375\n",
            "1778/3000 train_loss: 164.0933380126953 test_loss:147.61415100097656\n",
            "1779/3000 train_loss: 141.38671875 test_loss:150.58004760742188\n",
            "1780/3000 train_loss: 141.03892517089844 test_loss:150.18942260742188\n",
            "1781/3000 train_loss: 147.40818786621094 test_loss:145.41806030273438\n",
            "1782/3000 train_loss: 148.637939453125 test_loss:143.70474243164062\n",
            "1783/3000 train_loss: 179.06182861328125 test_loss:145.873779296875\n",
            "1784/3000 train_loss: 193.27340698242188 test_loss:145.3636474609375\n",
            "1785/3000 train_loss: 212.27212524414062 test_loss:143.40939331054688\n",
            "1786/3000 train_loss: 161.25662231445312 test_loss:150.2601318359375\n",
            "1787/3000 train_loss: 142.28794860839844 test_loss:147.15811157226562\n",
            "1788/3000 train_loss: 253.74354553222656 test_loss:145.64947509765625\n",
            "1789/3000 train_loss: 122.95623779296875 test_loss:147.25152587890625\n",
            "1790/3000 train_loss: 201.22152709960938 test_loss:143.3114776611328\n",
            "1791/3000 train_loss: 138.6090850830078 test_loss:146.88787841796875\n",
            "1792/3000 train_loss: 158.72836303710938 test_loss:147.65487670898438\n",
            "1793/3000 train_loss: 137.4428253173828 test_loss:150.37322998046875\n",
            "1794/3000 train_loss: 170.34512329101562 test_loss:146.7864227294922\n",
            "1795/3000 train_loss: 159.59698486328125 test_loss:152.76336669921875\n",
            "1796/3000 train_loss: 134.75997924804688 test_loss:154.93228149414062\n",
            "1797/3000 train_loss: 190.45437622070312 test_loss:151.68899536132812\n",
            "1798/3000 train_loss: 138.21945190429688 test_loss:151.50650024414062\n",
            "1799/3000 train_loss: 126.14627075195312 test_loss:148.2801055908203\n",
            "1800/3000 train_loss: 129.7131805419922 test_loss:144.729736328125\n",
            "1801/3000 train_loss: 125.88262176513672 test_loss:149.69639587402344\n",
            "1802/3000 train_loss: 150.4365234375 test_loss:145.86061096191406\n",
            "1803/3000 train_loss: 228.99215698242188 test_loss:146.51600646972656\n",
            "1804/3000 train_loss: 162.69895935058594 test_loss:153.62713623046875\n",
            "1805/3000 train_loss: 126.56150817871094 test_loss:148.26698303222656\n",
            "1806/3000 train_loss: 200.64016723632812 test_loss:145.278564453125\n",
            "1807/3000 train_loss: 174.71951293945312 test_loss:154.82806396484375\n",
            "1808/3000 train_loss: 156.4386749267578 test_loss:148.30780029296875\n",
            "1809/3000 train_loss: 168.33331298828125 test_loss:148.77064514160156\n",
            "1810/3000 train_loss: 166.0321807861328 test_loss:147.72035217285156\n",
            "1811/3000 train_loss: 165.69557189941406 test_loss:153.02699279785156\n",
            "1812/3000 train_loss: 157.13638305664062 test_loss:145.42567443847656\n",
            "1813/3000 train_loss: 136.22315979003906 test_loss:145.87490844726562\n",
            "1814/3000 train_loss: 229.39144897460938 test_loss:145.75363159179688\n",
            "1815/3000 train_loss: 236.47854614257812 test_loss:180.25807189941406\n",
            "1816/3000 train_loss: 183.70042419433594 test_loss:183.17559814453125\n",
            "1817/3000 train_loss: 164.59127807617188 test_loss:167.49867248535156\n",
            "1818/3000 train_loss: 222.4644775390625 test_loss:164.25820922851562\n",
            "1819/3000 train_loss: 158.6850128173828 test_loss:171.77548217773438\n",
            "1820/3000 train_loss: 172.05471801757812 test_loss:188.5533447265625\n",
            "1821/3000 train_loss: 156.83670043945312 test_loss:166.61343383789062\n",
            "1822/3000 train_loss: 235.19683837890625 test_loss:164.8880157470703\n",
            "1823/3000 train_loss: 154.92123413085938 test_loss:160.24716186523438\n",
            "1824/3000 train_loss: 150.13397216796875 test_loss:156.12542724609375\n",
            "1825/3000 train_loss: 254.08349609375 test_loss:163.19845581054688\n",
            "1826/3000 train_loss: 168.73695373535156 test_loss:150.58811950683594\n",
            "1827/3000 train_loss: 150.104248046875 test_loss:146.3338165283203\n",
            "1828/3000 train_loss: 175.45458984375 test_loss:150.3736572265625\n",
            "1829/3000 train_loss: 160.0668182373047 test_loss:150.66436767578125\n",
            "1830/3000 train_loss: 135.639892578125 test_loss:150.68544006347656\n",
            "1831/3000 train_loss: 195.83163452148438 test_loss:153.15338134765625\n",
            "1832/3000 train_loss: 131.88528442382812 test_loss:154.36770629882812\n",
            "1833/3000 train_loss: 201.89505004882812 test_loss:155.635986328125\n",
            "1834/3000 train_loss: 150.19540405273438 test_loss:152.44544982910156\n",
            "1835/3000 train_loss: 123.96651458740234 test_loss:150.4556427001953\n",
            "1836/3000 train_loss: 134.70758056640625 test_loss:147.51368713378906\n",
            "1837/3000 train_loss: 128.2747344970703 test_loss:154.2556610107422\n",
            "1838/3000 train_loss: 124.61210632324219 test_loss:153.48143005371094\n",
            "1839/3000 train_loss: 163.32522583007812 test_loss:155.0947723388672\n",
            "1840/3000 train_loss: 321.8951416015625 test_loss:151.89871215820312\n",
            "1841/3000 train_loss: 173.64703369140625 test_loss:160.36529541015625\n",
            "1842/3000 train_loss: 151.27487182617188 test_loss:170.57591247558594\n",
            "1843/3000 train_loss: 145.83697509765625 test_loss:153.27627563476562\n",
            "1844/3000 train_loss: 222.53982543945312 test_loss:150.02227783203125\n",
            "1845/3000 train_loss: 133.13226318359375 test_loss:150.82752990722656\n",
            "1846/3000 train_loss: 132.87843322753906 test_loss:144.84439086914062\n",
            "1847/3000 train_loss: 116.27946472167969 test_loss:148.57247924804688\n",
            "1848/3000 train_loss: 167.23846435546875 test_loss:144.95730590820312\n",
            "1849/3000 train_loss: 178.547119140625 test_loss:145.80738830566406\n",
            "1850/3000 train_loss: 138.44322204589844 test_loss:148.9351043701172\n",
            "1851/3000 train_loss: 150.62649536132812 test_loss:154.40591430664062\n",
            "1852/3000 train_loss: 228.0107421875 test_loss:149.17555236816406\n",
            "1853/3000 train_loss: 207.65780639648438 test_loss:151.301513671875\n",
            "1854/3000 train_loss: 142.2340545654297 test_loss:145.82733154296875\n",
            "1855/3000 train_loss: 124.84877014160156 test_loss:146.46826171875\n",
            "1856/3000 train_loss: 136.2992706298828 test_loss:147.4512481689453\n",
            "1857/3000 train_loss: 159.26617431640625 test_loss:154.2606201171875\n",
            "1858/3000 train_loss: 136.452880859375 test_loss:154.5276641845703\n",
            "1859/3000 train_loss: 265.0472106933594 test_loss:154.1051025390625\n",
            "1860/3000 train_loss: 262.81463623046875 test_loss:160.8404541015625\n",
            "1861/3000 train_loss: 145.43995666503906 test_loss:164.38912963867188\n",
            "1862/3000 train_loss: 176.84442138671875 test_loss:160.4131622314453\n",
            "1863/3000 train_loss: 125.1595458984375 test_loss:152.6090087890625\n",
            "1864/3000 train_loss: 123.05477142333984 test_loss:144.6888885498047\n",
            "1865/3000 train_loss: 129.7814483642578 test_loss:146.10108947753906\n",
            "1866/3000 train_loss: 119.35689544677734 test_loss:146.285400390625\n",
            "1867/3000 train_loss: 116.34919738769531 test_loss:149.02232360839844\n",
            "1868/3000 train_loss: 136.69674682617188 test_loss:152.47593688964844\n",
            "1869/3000 train_loss: 165.95114135742188 test_loss:149.4266357421875\n",
            "1870/3000 train_loss: 140.75628662109375 test_loss:145.74118041992188\n",
            "1871/3000 train_loss: 141.81744384765625 test_loss:158.70355224609375\n",
            "1872/3000 train_loss: 126.81814575195312 test_loss:157.82864379882812\n",
            "1873/3000 train_loss: 126.87191772460938 test_loss:149.52493286132812\n",
            "1874/3000 train_loss: 172.97958374023438 test_loss:149.7694854736328\n",
            "1875/3000 train_loss: 196.46340942382812 test_loss:150.1519317626953\n",
            "1876/3000 train_loss: 127.8355941772461 test_loss:142.9480743408203\n",
            "1877/3000 train_loss: 181.76718139648438 test_loss:144.8941650390625\n",
            "1878/3000 train_loss: 124.46253967285156 test_loss:148.72222900390625\n",
            "1879/3000 train_loss: 127.32853698730469 test_loss:141.58741760253906\n",
            "1880/3000 train_loss: 109.49453735351562 test_loss:141.5167236328125\n",
            "1881/3000 train_loss: 176.64498901367188 test_loss:144.89842224121094\n",
            "1882/3000 train_loss: 111.74920654296875 test_loss:144.45616149902344\n",
            "1883/3000 train_loss: 288.4538269042969 test_loss:142.09487915039062\n",
            "1884/3000 train_loss: 120.70977783203125 test_loss:138.56532287597656\n",
            "1885/3000 train_loss: 148.20968627929688 test_loss:138.37332153320312\n",
            "1886/3000 train_loss: 135.73866271972656 test_loss:139.1804962158203\n",
            "1887/3000 train_loss: 137.38787841796875 test_loss:141.11305236816406\n",
            "1888/3000 train_loss: 125.31684112548828 test_loss:138.7371368408203\n",
            "1889/3000 train_loss: 117.26033020019531 test_loss:138.66775512695312\n",
            "1890/3000 train_loss: 125.27532196044922 test_loss:136.2453155517578\n",
            "1891/3000 train_loss: 149.9884033203125 test_loss:140.7624969482422\n",
            "1892/3000 train_loss: 121.29944610595703 test_loss:137.56436157226562\n",
            "1893/3000 train_loss: 155.18209838867188 test_loss:139.96510314941406\n",
            "1894/3000 train_loss: 129.63250732421875 test_loss:142.01246643066406\n",
            "1895/3000 train_loss: 182.58506774902344 test_loss:142.6000213623047\n",
            "1896/3000 train_loss: 125.87742614746094 test_loss:145.67315673828125\n",
            "1897/3000 train_loss: 123.62968444824219 test_loss:146.29360961914062\n",
            "1898/3000 train_loss: 246.88909912109375 test_loss:141.67396545410156\n",
            "1899/3000 train_loss: 182.21942138671875 test_loss:149.87969970703125\n",
            "1900/3000 train_loss: 126.33341979980469 test_loss:142.09930419921875\n",
            "1901/3000 train_loss: 143.96791076660156 test_loss:142.8599090576172\n",
            "1902/3000 train_loss: 155.46087646484375 test_loss:144.15814208984375\n",
            "1903/3000 train_loss: 134.74627685546875 test_loss:139.17001342773438\n",
            "1904/3000 train_loss: 149.30389404296875 test_loss:139.59640502929688\n",
            "1905/3000 train_loss: 125.06497192382812 test_loss:138.602294921875\n",
            "1906/3000 train_loss: 144.17388916015625 test_loss:141.92330932617188\n",
            "1907/3000 train_loss: 130.34872436523438 test_loss:139.61715698242188\n",
            "1908/3000 train_loss: 110.94630432128906 test_loss:138.10279846191406\n",
            "1909/3000 train_loss: 135.208984375 test_loss:138.26226806640625\n",
            "1910/3000 train_loss: 128.3992156982422 test_loss:141.31634521484375\n",
            "1911/3000 train_loss: 120.92731475830078 test_loss:140.2631072998047\n",
            "1912/3000 train_loss: 140.42994689941406 test_loss:140.04872131347656\n",
            "1913/3000 train_loss: 149.06874084472656 test_loss:142.17300415039062\n",
            "1914/3000 train_loss: 563.1754150390625 test_loss:139.29820251464844\n",
            "1915/3000 train_loss: 118.46034240722656 test_loss:136.90359497070312\n",
            "1916/3000 train_loss: 124.37642669677734 test_loss:132.69451904296875\n",
            "1917/3000 train_loss: 123.45250701904297 test_loss:136.20465087890625\n",
            "1918/3000 train_loss: 149.77700805664062 test_loss:135.6518096923828\n",
            "1919/3000 train_loss: 132.94107055664062 test_loss:143.36788940429688\n",
            "1920/3000 train_loss: 129.5717315673828 test_loss:140.06883239746094\n",
            "1921/3000 train_loss: 138.95040893554688 test_loss:138.9957733154297\n",
            "1922/3000 train_loss: 140.406982421875 test_loss:136.74964904785156\n",
            "1923/3000 train_loss: 113.31510925292969 test_loss:134.87721252441406\n",
            "1924/3000 train_loss: 123.25831604003906 test_loss:135.49351501464844\n",
            "1925/3000 train_loss: 145.7666473388672 test_loss:135.521728515625\n",
            "1926/3000 train_loss: 116.626220703125 test_loss:137.6704559326172\n",
            "1927/3000 train_loss: 109.00437927246094 test_loss:134.22926330566406\n",
            "1928/3000 train_loss: 126.82815551757812 test_loss:133.2660675048828\n",
            "1929/3000 train_loss: 284.13043212890625 test_loss:138.45863342285156\n",
            "1930/3000 train_loss: 152.7735595703125 test_loss:161.13372802734375\n",
            "1931/3000 train_loss: 143.81887817382812 test_loss:151.23895263671875\n",
            "1932/3000 train_loss: 199.05812072753906 test_loss:135.50100708007812\n",
            "1933/3000 train_loss: 209.71173095703125 test_loss:139.06466674804688\n",
            "1934/3000 train_loss: 175.70431518554688 test_loss:133.18252563476562\n",
            "1935/3000 train_loss: 119.41688537597656 test_loss:135.52566528320312\n",
            "1936/3000 train_loss: 113.12518310546875 test_loss:134.89688110351562\n",
            "1937/3000 train_loss: 142.6170654296875 test_loss:137.86221313476562\n",
            "1938/3000 train_loss: 214.84539794921875 test_loss:132.8200225830078\n",
            "1939/3000 train_loss: 133.41607666015625 test_loss:133.11090087890625\n",
            "1940/3000 train_loss: 153.07412719726562 test_loss:131.5943145751953\n",
            "1941/3000 train_loss: 121.10569763183594 test_loss:132.17166137695312\n",
            "1942/3000 train_loss: 280.508544921875 test_loss:132.97621154785156\n",
            "1943/3000 train_loss: 146.85821533203125 test_loss:138.49298095703125\n",
            "1944/3000 train_loss: 255.6021728515625 test_loss:131.86569213867188\n",
            "1945/3000 train_loss: 192.30079650878906 test_loss:130.5216064453125\n",
            "1946/3000 train_loss: 192.98394775390625 test_loss:149.5001983642578\n",
            "1947/3000 train_loss: 131.1101837158203 test_loss:156.4089813232422\n",
            "1948/3000 train_loss: 135.8770751953125 test_loss:143.85302734375\n",
            "1949/3000 train_loss: 114.79194641113281 test_loss:139.59759521484375\n",
            "1950/3000 train_loss: 127.59378051757812 test_loss:138.1394805908203\n",
            "1951/3000 train_loss: 158.80320739746094 test_loss:136.0371551513672\n",
            "1952/3000 train_loss: 146.8226318359375 test_loss:137.8763885498047\n",
            "1953/3000 train_loss: 104.24757385253906 test_loss:132.6682891845703\n",
            "1954/3000 train_loss: 124.16195678710938 test_loss:132.01841735839844\n",
            "1955/3000 train_loss: 170.9878692626953 test_loss:132.60943603515625\n",
            "1956/3000 train_loss: 126.47335815429688 test_loss:137.16455078125\n",
            "1957/3000 train_loss: 120.2244644165039 test_loss:135.17723083496094\n",
            "1958/3000 train_loss: 155.88014221191406 test_loss:135.52914428710938\n",
            "1959/3000 train_loss: 159.1923828125 test_loss:137.19921875\n",
            "1960/3000 train_loss: 112.3033676147461 test_loss:135.62200927734375\n",
            "1961/3000 train_loss: 133.50331115722656 test_loss:131.52061462402344\n",
            "1962/3000 train_loss: 108.58345031738281 test_loss:130.68014526367188\n",
            "1963/3000 train_loss: 166.60397338867188 test_loss:128.5839080810547\n",
            "1964/3000 train_loss: 150.41073608398438 test_loss:134.36903381347656\n",
            "1965/3000 train_loss: 127.70555114746094 test_loss:132.37208557128906\n",
            "1966/3000 train_loss: 125.26370239257812 test_loss:125.2980728149414\n",
            "1967/3000 train_loss: 125.67071533203125 test_loss:128.5157470703125\n",
            "1968/3000 train_loss: 125.47648620605469 test_loss:129.2932891845703\n",
            "1969/3000 train_loss: 129.9762420654297 test_loss:129.8549041748047\n",
            "1970/3000 train_loss: 149.79391479492188 test_loss:132.95751953125\n",
            "1971/3000 train_loss: 152.99237060546875 test_loss:135.16342163085938\n",
            "1972/3000 train_loss: 128.5633544921875 test_loss:131.13796997070312\n",
            "1973/3000 train_loss: 134.77392578125 test_loss:127.90823364257812\n",
            "1974/3000 train_loss: 109.1417236328125 test_loss:127.48051452636719\n",
            "1975/3000 train_loss: 111.44080352783203 test_loss:129.47369384765625\n",
            "1976/3000 train_loss: 152.29884338378906 test_loss:129.63946533203125\n",
            "1977/3000 train_loss: 135.45814514160156 test_loss:137.187744140625\n",
            "1978/3000 train_loss: 201.78358459472656 test_loss:135.42161560058594\n",
            "1979/3000 train_loss: 165.78512573242188 test_loss:130.60614013671875\n",
            "1980/3000 train_loss: 108.13533020019531 test_loss:135.25633239746094\n",
            "1981/3000 train_loss: 119.05388641357422 test_loss:133.69715881347656\n",
            "1982/3000 train_loss: 139.51683044433594 test_loss:135.35470581054688\n",
            "1983/3000 train_loss: 118.55339050292969 test_loss:127.2188491821289\n",
            "1984/3000 train_loss: 218.24148559570312 test_loss:124.85076904296875\n",
            "1985/3000 train_loss: 103.68310546875 test_loss:132.1970672607422\n",
            "1986/3000 train_loss: 130.95785522460938 test_loss:126.99252319335938\n",
            "1987/3000 train_loss: 154.63726806640625 test_loss:131.52989196777344\n",
            "1988/3000 train_loss: 130.7278594970703 test_loss:126.01565551757812\n",
            "1989/3000 train_loss: 144.3289794921875 test_loss:126.49476623535156\n",
            "1990/3000 train_loss: 124.32454681396484 test_loss:127.09136199951172\n",
            "1991/3000 train_loss: 126.06292724609375 test_loss:126.43907165527344\n",
            "1992/3000 train_loss: 116.29017639160156 test_loss:126.22807312011719\n",
            "1993/3000 train_loss: 152.7564239501953 test_loss:126.02604675292969\n",
            "1994/3000 train_loss: 108.72105407714844 test_loss:133.70147705078125\n",
            "1995/3000 train_loss: 198.29107666015625 test_loss:129.24761962890625\n",
            "1996/3000 train_loss: 129.04312133789062 test_loss:127.90010070800781\n",
            "1997/3000 train_loss: 129.15463256835938 test_loss:128.826171875\n",
            "1998/3000 train_loss: 125.54670715332031 test_loss:126.31571960449219\n",
            "1999/3000 train_loss: 123.4664306640625 test_loss:129.04283142089844\n",
            "2000/3000 train_loss: 116.46424865722656 test_loss:126.8965835571289\n",
            "2001/3000 train_loss: 163.46556091308594 test_loss:125.440673828125\n",
            "2002/3000 train_loss: 177.41543579101562 test_loss:126.50628662109375\n",
            "2003/3000 train_loss: 109.47842407226562 test_loss:129.451171875\n",
            "2004/3000 train_loss: 139.2172088623047 test_loss:123.70809173583984\n",
            "2005/3000 train_loss: 185.86724853515625 test_loss:131.85626220703125\n",
            "2006/3000 train_loss: 131.34271240234375 test_loss:140.4164276123047\n",
            "2007/3000 train_loss: 160.4572296142578 test_loss:133.20306396484375\n",
            "2008/3000 train_loss: 124.71996307373047 test_loss:132.74668884277344\n",
            "2009/3000 train_loss: 129.35084533691406 test_loss:130.33456420898438\n",
            "2010/3000 train_loss: 220.91891479492188 test_loss:124.55921936035156\n",
            "2011/3000 train_loss: 121.91502380371094 test_loss:133.60842895507812\n",
            "2012/3000 train_loss: 163.98040771484375 test_loss:129.0043487548828\n",
            "2013/3000 train_loss: 121.20645904541016 test_loss:123.57209777832031\n",
            "2014/3000 train_loss: 168.7904052734375 test_loss:126.00761413574219\n",
            "2015/3000 train_loss: 138.7119140625 test_loss:125.52755737304688\n",
            "2016/3000 train_loss: 129.5703582763672 test_loss:125.44522094726562\n",
            "2017/3000 train_loss: 127.19547271728516 test_loss:128.7932891845703\n",
            "2018/3000 train_loss: 141.0944061279297 test_loss:125.62356567382812\n",
            "2019/3000 train_loss: 215.59912109375 test_loss:132.5213165283203\n",
            "2020/3000 train_loss: 214.69326782226562 test_loss:130.48397827148438\n",
            "2021/3000 train_loss: 128.10226440429688 test_loss:125.51221466064453\n",
            "2022/3000 train_loss: 123.7452621459961 test_loss:132.95257568359375\n",
            "2023/3000 train_loss: 229.06134033203125 test_loss:126.35237121582031\n",
            "2024/3000 train_loss: 125.49437713623047 test_loss:123.0377197265625\n",
            "2025/3000 train_loss: 99.9542236328125 test_loss:121.34223937988281\n",
            "2026/3000 train_loss: 125.52424621582031 test_loss:119.92033386230469\n",
            "2027/3000 train_loss: 115.71858978271484 test_loss:127.33341217041016\n",
            "2028/3000 train_loss: 120.43523406982422 test_loss:126.18968963623047\n",
            "2029/3000 train_loss: 111.99375915527344 test_loss:123.1158447265625\n",
            "2030/3000 train_loss: 113.72051239013672 test_loss:125.82882690429688\n",
            "2031/3000 train_loss: 273.7127685546875 test_loss:122.49699401855469\n",
            "2032/3000 train_loss: 132.35369873046875 test_loss:128.31971740722656\n",
            "2033/3000 train_loss: 201.8527374267578 test_loss:127.65621948242188\n",
            "2034/3000 train_loss: 108.10646057128906 test_loss:137.79296875\n",
            "2035/3000 train_loss: 114.70431518554688 test_loss:130.4912567138672\n",
            "2036/3000 train_loss: 137.9912872314453 test_loss:130.09852600097656\n",
            "2037/3000 train_loss: 117.42739868164062 test_loss:134.0651397705078\n",
            "2038/3000 train_loss: 103.6319580078125 test_loss:128.5360107421875\n",
            "2039/3000 train_loss: 159.07342529296875 test_loss:126.05476379394531\n",
            "2040/3000 train_loss: 124.19434356689453 test_loss:128.622802734375\n",
            "2041/3000 train_loss: 145.5717010498047 test_loss:125.19178771972656\n",
            "2042/3000 train_loss: 120.59043884277344 test_loss:125.1207046508789\n",
            "2043/3000 train_loss: 150.30899047851562 test_loss:125.88038635253906\n",
            "2044/3000 train_loss: 131.3363037109375 test_loss:125.83261108398438\n",
            "2045/3000 train_loss: 118.08390045166016 test_loss:128.23440551757812\n",
            "2046/3000 train_loss: 105.44770812988281 test_loss:122.70909118652344\n",
            "2047/3000 train_loss: 106.7301025390625 test_loss:122.97834014892578\n",
            "2048/3000 train_loss: 118.80916595458984 test_loss:129.73663330078125\n",
            "2049/3000 train_loss: 115.2509536743164 test_loss:129.54075622558594\n",
            "2050/3000 train_loss: 140.7358856201172 test_loss:129.73817443847656\n",
            "2051/3000 train_loss: 127.14036560058594 test_loss:128.36952209472656\n",
            "2052/3000 train_loss: 127.08843994140625 test_loss:131.26292419433594\n",
            "2053/3000 train_loss: 114.53240203857422 test_loss:128.6827392578125\n",
            "2054/3000 train_loss: 117.88800048828125 test_loss:127.35408782958984\n",
            "2055/3000 train_loss: 106.23934173583984 test_loss:125.17267608642578\n",
            "2056/3000 train_loss: 113.84207916259766 test_loss:122.11405944824219\n",
            "2057/3000 train_loss: 130.76588439941406 test_loss:123.61973571777344\n",
            "2058/3000 train_loss: 107.62593078613281 test_loss:124.4228286743164\n",
            "2059/3000 train_loss: 102.2952880859375 test_loss:128.34791564941406\n",
            "2060/3000 train_loss: 133.72543334960938 test_loss:126.79446411132812\n",
            "2061/3000 train_loss: 134.8078155517578 test_loss:123.49266052246094\n",
            "2062/3000 train_loss: 108.58505249023438 test_loss:119.63328552246094\n",
            "2063/3000 train_loss: 123.74909973144531 test_loss:122.43492126464844\n",
            "2064/3000 train_loss: 159.73696899414062 test_loss:124.57914733886719\n",
            "2065/3000 train_loss: 122.42289733886719 test_loss:128.23941040039062\n",
            "2066/3000 train_loss: 186.677001953125 test_loss:124.85163879394531\n",
            "2067/3000 train_loss: 140.3732452392578 test_loss:131.6350555419922\n",
            "2068/3000 train_loss: 100.46511840820312 test_loss:135.0232391357422\n",
            "2069/3000 train_loss: 240.813232421875 test_loss:128.74266052246094\n",
            "2070/3000 train_loss: 165.51075744628906 test_loss:140.59133911132812\n",
            "2071/3000 train_loss: 166.24017333984375 test_loss:134.5956573486328\n",
            "2072/3000 train_loss: 286.22955322265625 test_loss:134.0063934326172\n",
            "2073/3000 train_loss: 155.65200805664062 test_loss:138.4243621826172\n",
            "2074/3000 train_loss: 120.20732116699219 test_loss:132.04037475585938\n",
            "2075/3000 train_loss: 132.8030242919922 test_loss:125.79835510253906\n",
            "2076/3000 train_loss: 158.14141845703125 test_loss:126.84494018554688\n",
            "2077/3000 train_loss: 126.51641082763672 test_loss:122.29356384277344\n",
            "2078/3000 train_loss: 115.63973999023438 test_loss:123.20106506347656\n",
            "2079/3000 train_loss: 119.70156860351562 test_loss:122.1325454711914\n",
            "2080/3000 train_loss: 153.18649291992188 test_loss:120.4158935546875\n",
            "2081/3000 train_loss: 106.83953857421875 test_loss:124.57875061035156\n",
            "2082/3000 train_loss: 137.5452117919922 test_loss:123.55155944824219\n",
            "2083/3000 train_loss: 108.05264282226562 test_loss:125.81631469726562\n",
            "2084/3000 train_loss: 126.68492126464844 test_loss:125.01213073730469\n",
            "2085/3000 train_loss: 120.29084014892578 test_loss:122.42874145507812\n",
            "2086/3000 train_loss: 106.00871276855469 test_loss:126.43335723876953\n",
            "2087/3000 train_loss: 114.99977111816406 test_loss:122.43861389160156\n",
            "2088/3000 train_loss: 126.24044799804688 test_loss:129.0340118408203\n",
            "2089/3000 train_loss: 122.21065521240234 test_loss:127.47896575927734\n",
            "2090/3000 train_loss: 125.97423553466797 test_loss:134.44662475585938\n",
            "2091/3000 train_loss: 116.96297454833984 test_loss:129.9607696533203\n",
            "2092/3000 train_loss: 107.58809661865234 test_loss:126.23825073242188\n",
            "2093/3000 train_loss: 151.52716064453125 test_loss:126.41242980957031\n",
            "2094/3000 train_loss: 122.5875244140625 test_loss:123.12786865234375\n",
            "2095/3000 train_loss: 124.93145751953125 test_loss:122.40769958496094\n",
            "2096/3000 train_loss: 113.14967346191406 test_loss:125.36383056640625\n",
            "2097/3000 train_loss: 145.80734252929688 test_loss:122.36329650878906\n",
            "2098/3000 train_loss: 125.01051330566406 test_loss:126.04177856445312\n",
            "2099/3000 train_loss: 166.43740844726562 test_loss:125.20918273925781\n",
            "2100/3000 train_loss: 96.24337768554688 test_loss:123.03060913085938\n",
            "2101/3000 train_loss: 135.50534057617188 test_loss:121.938232421875\n",
            "2102/3000 train_loss: 140.91830444335938 test_loss:122.15477752685547\n",
            "2103/3000 train_loss: 102.65129852294922 test_loss:123.5244140625\n",
            "2104/3000 train_loss: 104.6884994506836 test_loss:126.744140625\n",
            "2105/3000 train_loss: 127.40349578857422 test_loss:124.8381118774414\n",
            "2106/3000 train_loss: 132.3894805908203 test_loss:123.3492660522461\n",
            "2107/3000 train_loss: 134.63572692871094 test_loss:124.23414611816406\n",
            "2108/3000 train_loss: 121.10689544677734 test_loss:120.82662963867188\n",
            "2109/3000 train_loss: 139.28216552734375 test_loss:118.60823059082031\n",
            "2110/3000 train_loss: 120.76774597167969 test_loss:123.38894653320312\n",
            "2111/3000 train_loss: 112.1015625 test_loss:120.4925765991211\n",
            "2112/3000 train_loss: 139.5006103515625 test_loss:141.81678771972656\n",
            "2113/3000 train_loss: 258.31878662109375 test_loss:136.2692413330078\n",
            "2114/3000 train_loss: 137.2704620361328 test_loss:150.19036865234375\n",
            "2115/3000 train_loss: 121.08243560791016 test_loss:135.14044189453125\n",
            "2116/3000 train_loss: 180.85618591308594 test_loss:136.62025451660156\n",
            "2117/3000 train_loss: 137.29351806640625 test_loss:161.9857635498047\n",
            "2118/3000 train_loss: 139.05796813964844 test_loss:137.1865692138672\n",
            "2119/3000 train_loss: 121.42404174804688 test_loss:122.79962158203125\n",
            "2120/3000 train_loss: 109.80004119873047 test_loss:124.28091430664062\n",
            "2121/3000 train_loss: 143.14353942871094 test_loss:125.63391876220703\n",
            "2122/3000 train_loss: 104.29885864257812 test_loss:122.08229064941406\n",
            "2123/3000 train_loss: 146.31155395507812 test_loss:119.95181274414062\n",
            "2124/3000 train_loss: 130.38198852539062 test_loss:121.45098876953125\n",
            "2125/3000 train_loss: 241.30889892578125 test_loss:121.13788604736328\n",
            "2126/3000 train_loss: 132.59396362304688 test_loss:130.11859130859375\n",
            "2127/3000 train_loss: 182.635009765625 test_loss:125.49314880371094\n",
            "2128/3000 train_loss: 156.29568481445312 test_loss:121.58642578125\n",
            "2129/3000 train_loss: 132.1759033203125 test_loss:132.12120056152344\n",
            "2130/3000 train_loss: 125.931396484375 test_loss:130.15103149414062\n",
            "2131/3000 train_loss: 109.94359588623047 test_loss:120.63447570800781\n",
            "2132/3000 train_loss: 113.89930725097656 test_loss:116.93096160888672\n",
            "2133/3000 train_loss: 119.96257781982422 test_loss:118.9704818725586\n",
            "2134/3000 train_loss: 128.05406188964844 test_loss:120.81798553466797\n",
            "2135/3000 train_loss: 140.97940063476562 test_loss:121.3145980834961\n",
            "2136/3000 train_loss: 130.4143829345703 test_loss:130.609375\n",
            "2137/3000 train_loss: 157.40054321289062 test_loss:128.66934204101562\n",
            "2138/3000 train_loss: 111.57213592529297 test_loss:120.44029235839844\n",
            "2139/3000 train_loss: 215.36911010742188 test_loss:119.59709167480469\n",
            "2140/3000 train_loss: 114.62823486328125 test_loss:131.68150329589844\n",
            "2141/3000 train_loss: 106.78773498535156 test_loss:128.1490478515625\n",
            "2142/3000 train_loss: 107.27994537353516 test_loss:123.04611206054688\n",
            "2143/3000 train_loss: 153.09310913085938 test_loss:120.35150909423828\n",
            "2144/3000 train_loss: 106.19659423828125 test_loss:117.93396759033203\n",
            "2145/3000 train_loss: 99.77214813232422 test_loss:118.66568756103516\n",
            "2146/3000 train_loss: 151.96417236328125 test_loss:124.03211975097656\n",
            "2147/3000 train_loss: 165.01480102539062 test_loss:124.68840789794922\n",
            "2148/3000 train_loss: 127.02214050292969 test_loss:128.74398803710938\n",
            "2149/3000 train_loss: 102.06112670898438 test_loss:125.0255126953125\n",
            "2150/3000 train_loss: 105.10292053222656 test_loss:122.13284301757812\n",
            "2151/3000 train_loss: 110.3278579711914 test_loss:123.96148681640625\n",
            "2152/3000 train_loss: 125.46945190429688 test_loss:123.95930480957031\n",
            "2153/3000 train_loss: 144.6319122314453 test_loss:123.80731201171875\n",
            "2154/3000 train_loss: 315.8469543457031 test_loss:120.81674194335938\n",
            "2155/3000 train_loss: 130.4967803955078 test_loss:140.40200805664062\n",
            "2156/3000 train_loss: 130.54493713378906 test_loss:137.29562377929688\n",
            "2157/3000 train_loss: 126.0821533203125 test_loss:129.80923461914062\n",
            "2158/3000 train_loss: 123.61907958984375 test_loss:130.18359375\n",
            "2159/3000 train_loss: 128.10325622558594 test_loss:130.35159301757812\n",
            "2160/3000 train_loss: 117.00123596191406 test_loss:129.69976806640625\n",
            "2161/3000 train_loss: 132.66317749023438 test_loss:125.26953125\n",
            "2162/3000 train_loss: 209.03701782226562 test_loss:126.31436157226562\n",
            "2163/3000 train_loss: 127.15168762207031 test_loss:119.70873260498047\n",
            "2164/3000 train_loss: 283.8304138183594 test_loss:123.62750244140625\n",
            "2165/3000 train_loss: 181.83367919921875 test_loss:125.68724060058594\n",
            "2166/3000 train_loss: 172.70065307617188 test_loss:120.39857482910156\n",
            "2167/3000 train_loss: 140.33645629882812 test_loss:148.00253295898438\n",
            "2168/3000 train_loss: 157.310791015625 test_loss:147.5499267578125\n",
            "2169/3000 train_loss: 130.32534790039062 test_loss:146.54747009277344\n",
            "2170/3000 train_loss: 127.02885437011719 test_loss:136.99862670898438\n",
            "2171/3000 train_loss: 132.3397216796875 test_loss:133.3671112060547\n",
            "2172/3000 train_loss: 126.73361206054688 test_loss:137.2703399658203\n",
            "2173/3000 train_loss: 115.21937561035156 test_loss:132.4568328857422\n",
            "2174/3000 train_loss: 127.64045715332031 test_loss:136.53048706054688\n",
            "2175/3000 train_loss: 114.9537582397461 test_loss:132.07962036132812\n",
            "2176/3000 train_loss: 254.99075317382812 test_loss:130.5727081298828\n",
            "2177/3000 train_loss: 116.63341522216797 test_loss:124.29421997070312\n",
            "2178/3000 train_loss: 153.94476318359375 test_loss:128.07843017578125\n",
            "2179/3000 train_loss: 137.49423217773438 test_loss:126.87760162353516\n",
            "2180/3000 train_loss: 154.77719116210938 test_loss:122.48390197753906\n",
            "2181/3000 train_loss: 147.96102905273438 test_loss:123.2227554321289\n",
            "2182/3000 train_loss: 129.2598114013672 test_loss:119.16102600097656\n",
            "2183/3000 train_loss: 118.68646240234375 test_loss:121.90540313720703\n",
            "2184/3000 train_loss: 120.50299835205078 test_loss:120.9970703125\n",
            "2185/3000 train_loss: 176.34873962402344 test_loss:129.0430145263672\n",
            "2186/3000 train_loss: 353.5379333496094 test_loss:126.89666748046875\n",
            "2187/3000 train_loss: 183.37310791015625 test_loss:120.8241958618164\n",
            "2188/3000 train_loss: 121.65767669677734 test_loss:125.7305908203125\n",
            "2189/3000 train_loss: 130.57540893554688 test_loss:121.77363586425781\n",
            "2190/3000 train_loss: 121.82237243652344 test_loss:124.89193725585938\n",
            "2191/3000 train_loss: 100.02040100097656 test_loss:121.74151611328125\n",
            "2192/3000 train_loss: 100.88839721679688 test_loss:122.24046325683594\n",
            "2193/3000 train_loss: 120.00141906738281 test_loss:120.958984375\n",
            "2194/3000 train_loss: 171.13873291015625 test_loss:124.81132507324219\n",
            "2195/3000 train_loss: 148.183349609375 test_loss:127.56341552734375\n",
            "2196/3000 train_loss: 128.1049346923828 test_loss:128.89730834960938\n",
            "2197/3000 train_loss: 138.03616333007812 test_loss:120.46459197998047\n",
            "2198/3000 train_loss: 104.71605682373047 test_loss:120.15011596679688\n",
            "2199/3000 train_loss: 119.86958312988281 test_loss:117.52430725097656\n",
            "2200/3000 train_loss: 119.44512939453125 test_loss:115.5233154296875\n",
            "2201/3000 train_loss: 107.20783996582031 test_loss:117.65399169921875\n",
            "2202/3000 train_loss: 138.19662475585938 test_loss:120.27455139160156\n",
            "2203/3000 train_loss: 168.63650512695312 test_loss:122.30210876464844\n",
            "2204/3000 train_loss: 279.64801025390625 test_loss:130.1077423095703\n",
            "2205/3000 train_loss: 113.79489135742188 test_loss:136.3056640625\n",
            "2206/3000 train_loss: 170.59573364257812 test_loss:144.81495666503906\n",
            "2207/3000 train_loss: 177.89205932617188 test_loss:142.06687927246094\n",
            "2208/3000 train_loss: 152.4570770263672 test_loss:129.65394592285156\n",
            "2209/3000 train_loss: 131.20916748046875 test_loss:132.37680053710938\n",
            "2210/3000 train_loss: 168.06605529785156 test_loss:130.42095947265625\n",
            "2211/3000 train_loss: 149.74978637695312 test_loss:138.72064208984375\n",
            "2212/3000 train_loss: 174.5400390625 test_loss:134.7116241455078\n",
            "2213/3000 train_loss: 113.31818389892578 test_loss:130.22189331054688\n",
            "2214/3000 train_loss: 129.2877960205078 test_loss:124.69430541992188\n",
            "2215/3000 train_loss: 109.57111358642578 test_loss:126.06317901611328\n",
            "2216/3000 train_loss: 125.83708190917969 test_loss:122.86027526855469\n",
            "2217/3000 train_loss: 105.954345703125 test_loss:122.04071807861328\n",
            "2218/3000 train_loss: 123.14312744140625 test_loss:127.69290161132812\n",
            "2219/3000 train_loss: 127.76934051513672 test_loss:136.4203643798828\n",
            "2220/3000 train_loss: 121.41521453857422 test_loss:121.32107543945312\n",
            "2221/3000 train_loss: 115.60658264160156 test_loss:117.04290771484375\n",
            "2222/3000 train_loss: 124.06529235839844 test_loss:117.5150146484375\n",
            "2223/3000 train_loss: 124.09232330322266 test_loss:119.29971313476562\n",
            "2224/3000 train_loss: 150.85598754882812 test_loss:120.85185241699219\n",
            "2225/3000 train_loss: 146.05137634277344 test_loss:124.40647888183594\n",
            "2226/3000 train_loss: 111.48597717285156 test_loss:127.45028686523438\n",
            "2227/3000 train_loss: 106.26072692871094 test_loss:124.9926528930664\n",
            "2228/3000 train_loss: 104.59312438964844 test_loss:124.1292724609375\n",
            "2229/3000 train_loss: 160.79629516601562 test_loss:123.36793518066406\n",
            "2230/3000 train_loss: 105.54163360595703 test_loss:131.4473876953125\n",
            "2231/3000 train_loss: 118.67988586425781 test_loss:126.21244049072266\n",
            "2232/3000 train_loss: 104.96986389160156 test_loss:124.19415283203125\n",
            "2233/3000 train_loss: 114.97364807128906 test_loss:121.27030944824219\n",
            "2234/3000 train_loss: 108.69681549072266 test_loss:122.09153747558594\n",
            "2235/3000 train_loss: 168.7095947265625 test_loss:121.6116943359375\n",
            "2236/3000 train_loss: 300.6055603027344 test_loss:132.9786834716797\n",
            "2237/3000 train_loss: 162.53976440429688 test_loss:136.06021118164062\n",
            "2238/3000 train_loss: 241.26809692382812 test_loss:128.5041046142578\n",
            "2239/3000 train_loss: 136.9375762939453 test_loss:133.103515625\n",
            "2240/3000 train_loss: 212.923095703125 test_loss:128.93875122070312\n",
            "2241/3000 train_loss: 107.91588592529297 test_loss:119.83074188232422\n",
            "2242/3000 train_loss: 291.0551452636719 test_loss:120.89108276367188\n",
            "2243/3000 train_loss: 149.3524169921875 test_loss:119.98912048339844\n",
            "2244/3000 train_loss: 102.42457580566406 test_loss:121.45079803466797\n",
            "2245/3000 train_loss: 137.19651794433594 test_loss:120.67066955566406\n",
            "2246/3000 train_loss: 292.02435302734375 test_loss:118.34251403808594\n",
            "2247/3000 train_loss: 284.18548583984375 test_loss:137.47401428222656\n",
            "2248/3000 train_loss: 121.02442932128906 test_loss:125.01953125\n",
            "2249/3000 train_loss: 105.29258728027344 test_loss:120.8960189819336\n",
            "2250/3000 train_loss: 126.4942398071289 test_loss:123.70465850830078\n",
            "2251/3000 train_loss: 151.35166931152344 test_loss:120.53999328613281\n",
            "2252/3000 train_loss: 140.39816284179688 test_loss:124.01376342773438\n",
            "2253/3000 train_loss: 107.9030990600586 test_loss:133.7025604248047\n",
            "2254/3000 train_loss: 147.9050750732422 test_loss:123.10693359375\n",
            "2255/3000 train_loss: 136.11158752441406 test_loss:120.08258056640625\n",
            "2256/3000 train_loss: 135.1007537841797 test_loss:154.6869354248047\n",
            "2257/3000 train_loss: 130.0078887939453 test_loss:143.69760131835938\n",
            "2258/3000 train_loss: 143.4935302734375 test_loss:135.85618591308594\n",
            "2259/3000 train_loss: 180.77703857421875 test_loss:131.6902618408203\n",
            "2260/3000 train_loss: 165.7095947265625 test_loss:126.73579406738281\n",
            "2261/3000 train_loss: 184.24014282226562 test_loss:132.0479736328125\n",
            "2262/3000 train_loss: 149.94139099121094 test_loss:124.65840148925781\n",
            "2263/3000 train_loss: 134.26702880859375 test_loss:126.67398071289062\n",
            "2264/3000 train_loss: 116.77760314941406 test_loss:126.32708740234375\n",
            "2265/3000 train_loss: 127.25827026367188 test_loss:122.91400146484375\n",
            "2266/3000 train_loss: 109.82073974609375 test_loss:126.54620361328125\n",
            "2267/3000 train_loss: 110.76657104492188 test_loss:125.008544921875\n",
            "2268/3000 train_loss: 114.84623718261719 test_loss:122.38372802734375\n",
            "2269/3000 train_loss: 131.29591369628906 test_loss:125.75923919677734\n",
            "2270/3000 train_loss: 110.70991516113281 test_loss:125.05059051513672\n",
            "2271/3000 train_loss: 102.3740234375 test_loss:122.76312255859375\n",
            "2272/3000 train_loss: 105.16486358642578 test_loss:127.12953186035156\n",
            "2273/3000 train_loss: 129.1779327392578 test_loss:122.80351257324219\n",
            "2274/3000 train_loss: 90.0296630859375 test_loss:121.09970092773438\n",
            "2275/3000 train_loss: 97.98330688476562 test_loss:119.30848693847656\n",
            "2276/3000 train_loss: 107.02014923095703 test_loss:119.1353759765625\n",
            "2277/3000 train_loss: 104.20854187011719 test_loss:121.08131408691406\n",
            "2278/3000 train_loss: 99.760986328125 test_loss:118.02500915527344\n",
            "2279/3000 train_loss: 110.28443908691406 test_loss:120.3736572265625\n",
            "2280/3000 train_loss: 107.84239959716797 test_loss:124.5476303100586\n",
            "2281/3000 train_loss: 104.8303451538086 test_loss:120.33262634277344\n",
            "2282/3000 train_loss: 109.98017883300781 test_loss:119.87049865722656\n",
            "2283/3000 train_loss: 93.97891235351562 test_loss:119.88298034667969\n",
            "2284/3000 train_loss: 102.3372802734375 test_loss:119.54090118408203\n",
            "2285/3000 train_loss: 113.30323791503906 test_loss:117.62427520751953\n",
            "2286/3000 train_loss: 168.65621948242188 test_loss:118.28176879882812\n",
            "2287/3000 train_loss: 154.96115112304688 test_loss:124.95956420898438\n",
            "2288/3000 train_loss: 176.80014038085938 test_loss:123.40553283691406\n",
            "2289/3000 train_loss: 126.53804016113281 test_loss:127.21514129638672\n",
            "2290/3000 train_loss: 116.60256958007812 test_loss:121.3826904296875\n",
            "2291/3000 train_loss: 167.7486572265625 test_loss:118.83607482910156\n",
            "2292/3000 train_loss: 122.36399841308594 test_loss:123.11126708984375\n",
            "2293/3000 train_loss: 97.9201431274414 test_loss:124.95298767089844\n",
            "2294/3000 train_loss: 110.41851806640625 test_loss:124.27597045898438\n",
            "2295/3000 train_loss: 107.40897369384766 test_loss:119.123779296875\n",
            "2296/3000 train_loss: 105.15936279296875 test_loss:120.19776916503906\n",
            "2297/3000 train_loss: 99.26029968261719 test_loss:118.56025695800781\n",
            "2298/3000 train_loss: 103.91271209716797 test_loss:120.16069030761719\n",
            "2299/3000 train_loss: 121.79302215576172 test_loss:122.50419616699219\n",
            "2300/3000 train_loss: 116.85415649414062 test_loss:125.61004638671875\n",
            "2301/3000 train_loss: 110.0012435913086 test_loss:119.95219421386719\n",
            "2302/3000 train_loss: 113.62281799316406 test_loss:118.15132141113281\n",
            "2303/3000 train_loss: 95.17830657958984 test_loss:121.22671508789062\n",
            "2304/3000 train_loss: 113.8887939453125 test_loss:119.82221221923828\n",
            "2305/3000 train_loss: 118.83463287353516 test_loss:120.06509399414062\n",
            "2306/3000 train_loss: 99.39494323730469 test_loss:117.34957885742188\n",
            "2307/3000 train_loss: 97.01298522949219 test_loss:114.9768295288086\n",
            "2308/3000 train_loss: 102.43189239501953 test_loss:115.27569580078125\n",
            "2309/3000 train_loss: 114.572021484375 test_loss:118.75656127929688\n",
            "2310/3000 train_loss: 218.67445373535156 test_loss:122.08103942871094\n",
            "2311/3000 train_loss: 114.84886169433594 test_loss:124.714599609375\n",
            "2312/3000 train_loss: 97.64434814453125 test_loss:119.59303283691406\n",
            "2313/3000 train_loss: 102.62701416015625 test_loss:113.99899291992188\n",
            "2314/3000 train_loss: 191.02005004882812 test_loss:113.00643920898438\n",
            "2315/3000 train_loss: 112.92892456054688 test_loss:113.82359313964844\n",
            "2316/3000 train_loss: 284.77496337890625 test_loss:128.50051879882812\n",
            "2317/3000 train_loss: 135.65855407714844 test_loss:150.23536682128906\n",
            "2318/3000 train_loss: 152.16819763183594 test_loss:133.82049560546875\n",
            "2319/3000 train_loss: 128.16770935058594 test_loss:128.31430053710938\n",
            "2320/3000 train_loss: 191.23922729492188 test_loss:126.68940734863281\n",
            "2321/3000 train_loss: 111.96674346923828 test_loss:124.05323791503906\n",
            "2322/3000 train_loss: 99.96806335449219 test_loss:121.198486328125\n",
            "2323/3000 train_loss: 122.77784729003906 test_loss:119.08634948730469\n",
            "2324/3000 train_loss: 214.03482055664062 test_loss:121.33651733398438\n",
            "2325/3000 train_loss: 127.78611755371094 test_loss:120.00860595703125\n",
            "2326/3000 train_loss: 117.55964660644531 test_loss:116.60073852539062\n",
            "2327/3000 train_loss: 160.2666015625 test_loss:118.90413665771484\n",
            "2328/3000 train_loss: 138.07656860351562 test_loss:123.1158218383789\n",
            "2329/3000 train_loss: 108.73707580566406 test_loss:120.21871948242188\n",
            "2330/3000 train_loss: 186.6748809814453 test_loss:119.28242492675781\n",
            "2331/3000 train_loss: 117.34184265136719 test_loss:117.48258972167969\n",
            "2332/3000 train_loss: 186.98257446289062 test_loss:114.62154388427734\n",
            "2333/3000 train_loss: 111.29606628417969 test_loss:119.88975524902344\n",
            "2334/3000 train_loss: 99.7853775024414 test_loss:119.00050354003906\n",
            "2335/3000 train_loss: 271.75286865234375 test_loss:118.40362548828125\n",
            "2336/3000 train_loss: 114.6778564453125 test_loss:129.3127899169922\n",
            "2337/3000 train_loss: 199.697998046875 test_loss:120.2020263671875\n",
            "2338/3000 train_loss: 99.92979431152344 test_loss:114.40045928955078\n",
            "2339/3000 train_loss: 107.85970306396484 test_loss:113.02734375\n",
            "2340/3000 train_loss: 101.38385772705078 test_loss:115.95114135742188\n",
            "2341/3000 train_loss: 101.21199798583984 test_loss:114.73564147949219\n",
            "2342/3000 train_loss: 104.3541030883789 test_loss:114.0086669921875\n",
            "2343/3000 train_loss: 122.99608612060547 test_loss:115.32632446289062\n",
            "2344/3000 train_loss: 93.22264099121094 test_loss:114.28724670410156\n",
            "2345/3000 train_loss: 127.29939270019531 test_loss:113.08554077148438\n",
            "2346/3000 train_loss: 372.1593322753906 test_loss:118.79850769042969\n",
            "2347/3000 train_loss: 209.29815673828125 test_loss:122.82310485839844\n",
            "2348/3000 train_loss: 155.54551696777344 test_loss:125.21878051757812\n",
            "2349/3000 train_loss: 164.3873291015625 test_loss:121.80752563476562\n",
            "2350/3000 train_loss: 149.04696655273438 test_loss:122.00299835205078\n",
            "2351/3000 train_loss: 209.12814331054688 test_loss:117.29972839355469\n",
            "2352/3000 train_loss: 103.15065002441406 test_loss:112.99360656738281\n",
            "2353/3000 train_loss: 108.51805114746094 test_loss:114.24118041992188\n",
            "2354/3000 train_loss: 99.64232635498047 test_loss:115.30722045898438\n",
            "2355/3000 train_loss: 125.20599365234375 test_loss:118.17994689941406\n",
            "2356/3000 train_loss: 106.861083984375 test_loss:124.06310272216797\n",
            "2357/3000 train_loss: 138.53060913085938 test_loss:117.14599609375\n",
            "2358/3000 train_loss: 249.79156494140625 test_loss:115.77262115478516\n",
            "2359/3000 train_loss: 99.50559997558594 test_loss:117.642578125\n",
            "2360/3000 train_loss: 112.08773803710938 test_loss:116.03678894042969\n",
            "2361/3000 train_loss: 99.04898834228516 test_loss:116.4580307006836\n",
            "2362/3000 train_loss: 103.58049774169922 test_loss:114.59123229980469\n",
            "2363/3000 train_loss: 161.2674560546875 test_loss:114.79188537597656\n",
            "2364/3000 train_loss: 153.76513671875 test_loss:124.29096984863281\n",
            "2365/3000 train_loss: 128.49151611328125 test_loss:120.71450805664062\n",
            "2366/3000 train_loss: 115.37019348144531 test_loss:116.68583679199219\n",
            "2367/3000 train_loss: 174.82350158691406 test_loss:113.52659606933594\n",
            "2368/3000 train_loss: 141.75596618652344 test_loss:122.97955322265625\n",
            "2369/3000 train_loss: 131.86302185058594 test_loss:125.17127990722656\n",
            "2370/3000 train_loss: 208.39901733398438 test_loss:125.47027587890625\n",
            "2371/3000 train_loss: 126.66044616699219 test_loss:124.2021255493164\n",
            "2372/3000 train_loss: 126.17221069335938 test_loss:127.62322235107422\n",
            "2373/3000 train_loss: 145.8789825439453 test_loss:123.56952667236328\n",
            "2374/3000 train_loss: 109.72930908203125 test_loss:116.65164184570312\n",
            "2375/3000 train_loss: 103.98735046386719 test_loss:115.17909240722656\n",
            "2376/3000 train_loss: 145.25621032714844 test_loss:117.68315124511719\n",
            "2377/3000 train_loss: 182.01913452148438 test_loss:113.26737976074219\n",
            "2378/3000 train_loss: 106.34082794189453 test_loss:112.94061279296875\n",
            "2379/3000 train_loss: 105.41072082519531 test_loss:112.43344116210938\n",
            "2380/3000 train_loss: 116.26651000976562 test_loss:111.81886291503906\n",
            "2381/3000 train_loss: 291.3473815917969 test_loss:119.35985565185547\n",
            "2382/3000 train_loss: 146.44235229492188 test_loss:115.83287048339844\n",
            "2383/3000 train_loss: 99.4560775756836 test_loss:116.22698974609375\n",
            "2384/3000 train_loss: 109.06095886230469 test_loss:113.40498352050781\n",
            "2385/3000 train_loss: 143.09951782226562 test_loss:117.43222045898438\n",
            "2386/3000 train_loss: 113.94576263427734 test_loss:118.9862060546875\n",
            "2387/3000 train_loss: 162.89328002929688 test_loss:116.07838439941406\n",
            "2388/3000 train_loss: 92.4256591796875 test_loss:120.45698547363281\n",
            "2389/3000 train_loss: 99.20822143554688 test_loss:117.85307312011719\n",
            "2390/3000 train_loss: 143.2459716796875 test_loss:113.45838165283203\n",
            "2391/3000 train_loss: 102.54415130615234 test_loss:112.45126342773438\n",
            "2392/3000 train_loss: 125.53428649902344 test_loss:114.15196228027344\n",
            "2393/3000 train_loss: 208.13217163085938 test_loss:115.76927947998047\n",
            "2394/3000 train_loss: 122.04837036132812 test_loss:127.74122619628906\n",
            "2395/3000 train_loss: 117.65731048583984 test_loss:119.79884338378906\n",
            "2396/3000 train_loss: 128.3636932373047 test_loss:117.84709930419922\n",
            "2397/3000 train_loss: 168.05157470703125 test_loss:121.84445190429688\n",
            "2398/3000 train_loss: 107.5963134765625 test_loss:126.93767547607422\n",
            "2399/3000 train_loss: 146.29901123046875 test_loss:120.14585876464844\n",
            "2400/3000 train_loss: 225.58468627929688 test_loss:116.62332153320312\n",
            "2401/3000 train_loss: 111.01789855957031 test_loss:121.65493774414062\n",
            "2402/3000 train_loss: 97.3096923828125 test_loss:116.82926940917969\n",
            "2403/3000 train_loss: 100.56007385253906 test_loss:113.93333435058594\n",
            "2404/3000 train_loss: 167.55679321289062 test_loss:115.77909851074219\n",
            "2405/3000 train_loss: 100.65988159179688 test_loss:111.44712829589844\n",
            "2406/3000 train_loss: 129.2106475830078 test_loss:112.57247924804688\n",
            "2407/3000 train_loss: 108.68510437011719 test_loss:109.4573974609375\n",
            "2408/3000 train_loss: 156.82470703125 test_loss:111.72698974609375\n",
            "2409/3000 train_loss: 253.92237854003906 test_loss:113.89056396484375\n",
            "2410/3000 train_loss: 150.14541625976562 test_loss:126.0479507446289\n",
            "2411/3000 train_loss: 115.68412017822266 test_loss:125.0759506225586\n",
            "2412/3000 train_loss: 217.43417358398438 test_loss:115.32827758789062\n",
            "2413/3000 train_loss: 101.66291809082031 test_loss:111.20133972167969\n",
            "2414/3000 train_loss: 121.210205078125 test_loss:113.76102447509766\n",
            "2415/3000 train_loss: 155.10264587402344 test_loss:114.08954620361328\n",
            "2416/3000 train_loss: 103.87019348144531 test_loss:112.15849304199219\n",
            "2417/3000 train_loss: 96.6042251586914 test_loss:110.95037841796875\n",
            "2418/3000 train_loss: 109.65338134765625 test_loss:112.53184509277344\n",
            "2419/3000 train_loss: 106.93135070800781 test_loss:113.4598159790039\n",
            "2420/3000 train_loss: 110.01957702636719 test_loss:112.32978820800781\n",
            "2421/3000 train_loss: 99.73094177246094 test_loss:109.2725601196289\n",
            "2422/3000 train_loss: 98.32198333740234 test_loss:112.43342590332031\n",
            "2423/3000 train_loss: 131.29144287109375 test_loss:110.85790252685547\n",
            "2424/3000 train_loss: 99.13383483886719 test_loss:113.1585693359375\n",
            "2425/3000 train_loss: 174.77197265625 test_loss:112.85621643066406\n",
            "2426/3000 train_loss: 111.29442596435547 test_loss:119.28038024902344\n",
            "2427/3000 train_loss: 97.11814880371094 test_loss:117.17568969726562\n",
            "2428/3000 train_loss: 139.35723876953125 test_loss:115.24441528320312\n",
            "2429/3000 train_loss: 137.55630493164062 test_loss:112.91604614257812\n",
            "2430/3000 train_loss: 110.03721618652344 test_loss:113.00364685058594\n",
            "2431/3000 train_loss: 97.55315399169922 test_loss:110.86519622802734\n",
            "2432/3000 train_loss: 165.96029663085938 test_loss:107.5277328491211\n",
            "2433/3000 train_loss: 129.13168334960938 test_loss:107.62535095214844\n",
            "2434/3000 train_loss: 112.65934753417969 test_loss:107.75467681884766\n",
            "2435/3000 train_loss: 180.4244384765625 test_loss:108.11965942382812\n",
            "2436/3000 train_loss: 148.39939880371094 test_loss:112.27394104003906\n",
            "2437/3000 train_loss: 182.53466796875 test_loss:118.89128875732422\n",
            "2438/3000 train_loss: 117.2291259765625 test_loss:113.29373168945312\n",
            "2439/3000 train_loss: 96.609130859375 test_loss:107.50004577636719\n",
            "2440/3000 train_loss: 100.34176635742188 test_loss:108.07298278808594\n",
            "2441/3000 train_loss: 99.10823059082031 test_loss:111.65130615234375\n",
            "2442/3000 train_loss: 144.67678833007812 test_loss:110.22796630859375\n",
            "2443/3000 train_loss: 124.37356567382812 test_loss:109.65507507324219\n",
            "2444/3000 train_loss: 98.8519058227539 test_loss:110.70232391357422\n",
            "2445/3000 train_loss: 145.58419799804688 test_loss:109.68122863769531\n",
            "2446/3000 train_loss: 136.51028442382812 test_loss:115.2708740234375\n",
            "2447/3000 train_loss: 98.75089263916016 test_loss:116.76707458496094\n",
            "2448/3000 train_loss: 115.64985656738281 test_loss:114.4698486328125\n",
            "2449/3000 train_loss: 133.28878784179688 test_loss:109.57492065429688\n",
            "2450/3000 train_loss: 96.8440933227539 test_loss:113.35511779785156\n",
            "2451/3000 train_loss: 126.79440307617188 test_loss:110.75196838378906\n",
            "2452/3000 train_loss: 133.16842651367188 test_loss:106.59764099121094\n",
            "2453/3000 train_loss: 103.31486511230469 test_loss:113.46345520019531\n",
            "2454/3000 train_loss: 104.3580093383789 test_loss:109.14265441894531\n",
            "2455/3000 train_loss: 94.79656982421875 test_loss:106.37669372558594\n",
            "2456/3000 train_loss: 95.46302795410156 test_loss:109.20060729980469\n",
            "2457/3000 train_loss: 127.439208984375 test_loss:108.72205352783203\n",
            "2458/3000 train_loss: 89.94815063476562 test_loss:109.89256286621094\n",
            "2459/3000 train_loss: 117.05540466308594 test_loss:111.6719741821289\n",
            "2460/3000 train_loss: 103.99187469482422 test_loss:107.45736694335938\n",
            "2461/3000 train_loss: 106.55343627929688 test_loss:109.36746215820312\n",
            "2462/3000 train_loss: 108.79922485351562 test_loss:108.72734832763672\n",
            "2463/3000 train_loss: 113.493408203125 test_loss:116.7102279663086\n",
            "2464/3000 train_loss: 94.10585021972656 test_loss:111.27908325195312\n",
            "2465/3000 train_loss: 94.40766143798828 test_loss:108.61920928955078\n",
            "2466/3000 train_loss: 105.31236267089844 test_loss:112.17539978027344\n",
            "2467/3000 train_loss: 92.0920181274414 test_loss:109.1630859375\n",
            "2468/3000 train_loss: 95.93981170654297 test_loss:108.24043273925781\n",
            "2469/3000 train_loss: 126.42930603027344 test_loss:108.55085754394531\n",
            "2470/3000 train_loss: 132.57806396484375 test_loss:113.91070556640625\n",
            "2471/3000 train_loss: 105.13875579833984 test_loss:120.29281616210938\n",
            "2472/3000 train_loss: 137.60403442382812 test_loss:117.24950408935547\n",
            "2473/3000 train_loss: 132.44236755371094 test_loss:115.22469329833984\n",
            "2474/3000 train_loss: 101.48780059814453 test_loss:110.51882934570312\n",
            "2475/3000 train_loss: 104.46504974365234 test_loss:110.70262908935547\n",
            "2476/3000 train_loss: 103.9429702758789 test_loss:110.20946502685547\n",
            "2477/3000 train_loss: 185.9311981201172 test_loss:110.21177673339844\n",
            "2478/3000 train_loss: 106.62325286865234 test_loss:116.6473388671875\n",
            "2479/3000 train_loss: 131.22613525390625 test_loss:113.96757507324219\n",
            "2480/3000 train_loss: 109.20220184326172 test_loss:114.83917236328125\n",
            "2481/3000 train_loss: 161.98727416992188 test_loss:113.495849609375\n",
            "2482/3000 train_loss: 96.74543762207031 test_loss:108.85256958007812\n",
            "2483/3000 train_loss: 95.31461334228516 test_loss:111.88521575927734\n",
            "2484/3000 train_loss: 146.46397399902344 test_loss:108.7425308227539\n",
            "2485/3000 train_loss: 113.2254867553711 test_loss:113.175537109375\n",
            "2486/3000 train_loss: 151.11154174804688 test_loss:111.78765869140625\n",
            "2487/3000 train_loss: 214.22305297851562 test_loss:109.41401672363281\n",
            "2488/3000 train_loss: 113.89888000488281 test_loss:115.95758056640625\n",
            "2489/3000 train_loss: 106.8353500366211 test_loss:116.9879150390625\n",
            "2490/3000 train_loss: 146.96356201171875 test_loss:117.09071350097656\n",
            "2491/3000 train_loss: 110.2077407836914 test_loss:114.08673095703125\n",
            "2492/3000 train_loss: 151.07638549804688 test_loss:114.58283233642578\n",
            "2493/3000 train_loss: 128.85903930664062 test_loss:110.28359985351562\n",
            "2494/3000 train_loss: 195.235107421875 test_loss:107.72456359863281\n",
            "2495/3000 train_loss: 114.7834701538086 test_loss:120.28617858886719\n",
            "2496/3000 train_loss: 142.22862243652344 test_loss:112.23020935058594\n",
            "2497/3000 train_loss: 104.78826904296875 test_loss:107.62928009033203\n",
            "2498/3000 train_loss: 113.48655700683594 test_loss:118.12403869628906\n",
            "2499/3000 train_loss: 118.44680786132812 test_loss:115.27296447753906\n",
            "2500/3000 train_loss: 101.60800170898438 test_loss:110.90603637695312\n",
            "2501/3000 train_loss: 190.76663208007812 test_loss:110.65727996826172\n",
            "2502/3000 train_loss: 130.08807373046875 test_loss:123.01484680175781\n",
            "2503/3000 train_loss: 92.23432922363281 test_loss:108.15565490722656\n",
            "2504/3000 train_loss: 127.87770080566406 test_loss:112.81021118164062\n",
            "2505/3000 train_loss: 216.33169555664062 test_loss:109.90686798095703\n",
            "2506/3000 train_loss: 109.08992767333984 test_loss:112.39085388183594\n",
            "2507/3000 train_loss: 197.5725860595703 test_loss:112.09265899658203\n",
            "2508/3000 train_loss: 152.591552734375 test_loss:122.5081558227539\n",
            "2509/3000 train_loss: 128.3959503173828 test_loss:111.40220642089844\n",
            "2510/3000 train_loss: 97.3485107421875 test_loss:108.55017852783203\n",
            "2511/3000 train_loss: 169.9838409423828 test_loss:109.98539733886719\n",
            "2512/3000 train_loss: 260.9306640625 test_loss:113.72988891601562\n",
            "2513/3000 train_loss: 107.88162994384766 test_loss:118.14509582519531\n",
            "2514/3000 train_loss: 130.1944122314453 test_loss:110.81269836425781\n",
            "2515/3000 train_loss: 95.21845245361328 test_loss:114.14229583740234\n",
            "2516/3000 train_loss: 123.77944946289062 test_loss:110.08595275878906\n",
            "2517/3000 train_loss: 201.50888061523438 test_loss:116.17132568359375\n",
            "2518/3000 train_loss: 141.29432678222656 test_loss:132.81410217285156\n",
            "2519/3000 train_loss: 106.8835220336914 test_loss:115.3082275390625\n",
            "2520/3000 train_loss: 93.83552551269531 test_loss:111.30297088623047\n",
            "2521/3000 train_loss: 114.02489471435547 test_loss:108.7914810180664\n",
            "2522/3000 train_loss: 103.79942321777344 test_loss:111.66535949707031\n",
            "2523/3000 train_loss: 97.60261535644531 test_loss:112.62348937988281\n",
            "2524/3000 train_loss: 154.63571166992188 test_loss:109.52752685546875\n",
            "2525/3000 train_loss: 94.50672149658203 test_loss:112.24839782714844\n",
            "2526/3000 train_loss: 136.78102111816406 test_loss:109.84336853027344\n",
            "2527/3000 train_loss: 115.63835144042969 test_loss:116.31465148925781\n",
            "2528/3000 train_loss: 128.33380126953125 test_loss:111.76417541503906\n",
            "2529/3000 train_loss: 249.2653045654297 test_loss:110.96963500976562\n",
            "2530/3000 train_loss: 134.9187469482422 test_loss:106.79256439208984\n",
            "2531/3000 train_loss: 102.84456634521484 test_loss:104.80152893066406\n",
            "2532/3000 train_loss: 113.405029296875 test_loss:103.49822235107422\n",
            "2533/3000 train_loss: 122.57178497314453 test_loss:109.17816925048828\n",
            "2534/3000 train_loss: 103.758544921875 test_loss:108.72127532958984\n",
            "2535/3000 train_loss: 117.017333984375 test_loss:105.39584350585938\n",
            "2536/3000 train_loss: 104.28279876708984 test_loss:108.5025634765625\n",
            "2537/3000 train_loss: 119.96372985839844 test_loss:112.8885498046875\n",
            "2538/3000 train_loss: 197.31842041015625 test_loss:104.13002014160156\n",
            "2539/3000 train_loss: 111.30362701416016 test_loss:108.82328796386719\n",
            "2540/3000 train_loss: 138.45108032226562 test_loss:103.15190124511719\n",
            "2541/3000 train_loss: 96.57286071777344 test_loss:111.74372863769531\n",
            "2542/3000 train_loss: 96.92453002929688 test_loss:108.51791381835938\n",
            "2543/3000 train_loss: 144.61166381835938 test_loss:107.50865173339844\n",
            "2544/3000 train_loss: 135.876708984375 test_loss:104.05015563964844\n",
            "2545/3000 train_loss: 134.8683319091797 test_loss:111.67037963867188\n",
            "2546/3000 train_loss: 116.48057556152344 test_loss:124.24812316894531\n",
            "2547/3000 train_loss: 110.010986328125 test_loss:112.88496398925781\n",
            "2548/3000 train_loss: 100.36626434326172 test_loss:107.63102722167969\n",
            "2549/3000 train_loss: 114.1302490234375 test_loss:107.19314575195312\n",
            "2550/3000 train_loss: 119.68983459472656 test_loss:111.15306091308594\n",
            "2551/3000 train_loss: 225.68524169921875 test_loss:108.66886901855469\n",
            "2552/3000 train_loss: 109.3453369140625 test_loss:121.54367065429688\n",
            "2553/3000 train_loss: 206.66290283203125 test_loss:108.4027099609375\n",
            "2554/3000 train_loss: 111.99691009521484 test_loss:119.70162963867188\n",
            "2555/3000 train_loss: 120.33292388916016 test_loss:114.30289459228516\n",
            "2556/3000 train_loss: 117.367919921875 test_loss:111.84386444091797\n",
            "2557/3000 train_loss: 88.39005279541016 test_loss:110.7982177734375\n",
            "2558/3000 train_loss: 117.5426025390625 test_loss:109.83657836914062\n",
            "2559/3000 train_loss: 127.96870422363281 test_loss:118.62506103515625\n",
            "2560/3000 train_loss: 112.17881774902344 test_loss:112.81755065917969\n",
            "2561/3000 train_loss: 101.30278015136719 test_loss:112.0263442993164\n",
            "2562/3000 train_loss: 202.72544860839844 test_loss:110.38215637207031\n",
            "2563/3000 train_loss: 111.25495910644531 test_loss:118.70381164550781\n",
            "2564/3000 train_loss: 103.74565124511719 test_loss:115.13050079345703\n",
            "2565/3000 train_loss: 120.2447509765625 test_loss:110.84138488769531\n",
            "2566/3000 train_loss: 186.35899353027344 test_loss:110.20015716552734\n",
            "2567/3000 train_loss: 132.59530639648438 test_loss:108.58415222167969\n",
            "2568/3000 train_loss: 168.50885009765625 test_loss:107.4106674194336\n",
            "2569/3000 train_loss: 130.17385864257812 test_loss:116.93449401855469\n",
            "2570/3000 train_loss: 136.12396240234375 test_loss:114.2728042602539\n",
            "2571/3000 train_loss: 195.95269775390625 test_loss:107.25485229492188\n",
            "2572/3000 train_loss: 120.3725814819336 test_loss:102.19158172607422\n",
            "2573/3000 train_loss: 96.74394226074219 test_loss:104.04490661621094\n",
            "2574/3000 train_loss: 97.28130340576172 test_loss:105.74118041992188\n",
            "2575/3000 train_loss: 109.9586410522461 test_loss:109.05195617675781\n",
            "2576/3000 train_loss: 103.67082214355469 test_loss:106.80175018310547\n",
            "2577/3000 train_loss: 183.85655212402344 test_loss:103.78205871582031\n",
            "2578/3000 train_loss: 267.1658020019531 test_loss:102.7364501953125\n",
            "2579/3000 train_loss: 103.50650787353516 test_loss:125.03248596191406\n",
            "2580/3000 train_loss: 191.9570770263672 test_loss:131.59315490722656\n",
            "2581/3000 train_loss: 120.73847961425781 test_loss:129.41363525390625\n",
            "2582/3000 train_loss: 155.9966583251953 test_loss:114.65947723388672\n",
            "2583/3000 train_loss: 95.38638305664062 test_loss:116.28417205810547\n",
            "2584/3000 train_loss: 97.05366516113281 test_loss:113.41725158691406\n",
            "2585/3000 train_loss: 152.76461791992188 test_loss:115.17807006835938\n",
            "2586/3000 train_loss: 250.52976989746094 test_loss:115.48890686035156\n",
            "2587/3000 train_loss: 105.23355102539062 test_loss:109.36480712890625\n",
            "2588/3000 train_loss: 135.74832153320312 test_loss:105.17665100097656\n",
            "2589/3000 train_loss: 102.31388092041016 test_loss:114.24403381347656\n",
            "2590/3000 train_loss: 106.03456115722656 test_loss:107.63157653808594\n",
            "2591/3000 train_loss: 90.09357452392578 test_loss:110.87994384765625\n",
            "2592/3000 train_loss: 85.6884765625 test_loss:107.6220703125\n",
            "2593/3000 train_loss: 86.63693237304688 test_loss:107.73007202148438\n",
            "2594/3000 train_loss: 109.34292602539062 test_loss:107.28462219238281\n",
            "2595/3000 train_loss: 126.98648071289062 test_loss:106.94126892089844\n",
            "2596/3000 train_loss: 199.87474060058594 test_loss:109.47763061523438\n",
            "2597/3000 train_loss: 100.19959259033203 test_loss:116.50932312011719\n",
            "2598/3000 train_loss: 94.41637420654297 test_loss:106.36088562011719\n",
            "2599/3000 train_loss: 92.6632080078125 test_loss:105.44989013671875\n",
            "2600/3000 train_loss: 86.15350341796875 test_loss:102.40458679199219\n",
            "2601/3000 train_loss: 120.17976379394531 test_loss:103.30381774902344\n",
            "2602/3000 train_loss: 102.00194549560547 test_loss:108.88065338134766\n",
            "2603/3000 train_loss: 220.86953735351562 test_loss:105.78509521484375\n",
            "2604/3000 train_loss: 129.32638549804688 test_loss:103.3463134765625\n",
            "2605/3000 train_loss: 100.9576416015625 test_loss:109.40292358398438\n",
            "2606/3000 train_loss: 99.70401000976562 test_loss:105.64845275878906\n",
            "2607/3000 train_loss: 151.26409912109375 test_loss:104.3937759399414\n",
            "2608/3000 train_loss: 113.44149780273438 test_loss:112.10897827148438\n",
            "2609/3000 train_loss: 100.57305908203125 test_loss:118.35183715820312\n",
            "2610/3000 train_loss: 90.206787109375 test_loss:107.52957153320312\n",
            "2611/3000 train_loss: 121.95130920410156 test_loss:105.5885009765625\n",
            "2612/3000 train_loss: 128.20433044433594 test_loss:110.89407348632812\n",
            "2613/3000 train_loss: 119.693359375 test_loss:108.5513916015625\n",
            "2614/3000 train_loss: 112.63037109375 test_loss:104.18490600585938\n",
            "2615/3000 train_loss: 114.6728515625 test_loss:104.70051574707031\n",
            "2616/3000 train_loss: 113.51229858398438 test_loss:107.08499908447266\n",
            "2617/3000 train_loss: 126.29736328125 test_loss:105.37808227539062\n",
            "2618/3000 train_loss: 106.20580291748047 test_loss:108.3729248046875\n",
            "2619/3000 train_loss: 106.69387817382812 test_loss:107.79875183105469\n",
            "2620/3000 train_loss: 103.09053039550781 test_loss:104.61664581298828\n",
            "2621/3000 train_loss: 166.4226837158203 test_loss:104.31570434570312\n",
            "2622/3000 train_loss: 91.54995727539062 test_loss:105.60736083984375\n",
            "2623/3000 train_loss: 96.2641830444336 test_loss:104.33802795410156\n",
            "2624/3000 train_loss: 108.62823486328125 test_loss:104.78973388671875\n",
            "2625/3000 train_loss: 162.75955200195312 test_loss:106.25060272216797\n",
            "2626/3000 train_loss: 101.42962646484375 test_loss:104.87046813964844\n",
            "2627/3000 train_loss: 91.56639099121094 test_loss:102.438720703125\n",
            "2628/3000 train_loss: 146.51113891601562 test_loss:103.25245666503906\n",
            "2629/3000 train_loss: 99.37368774414062 test_loss:105.9870376586914\n",
            "2630/3000 train_loss: 86.75773620605469 test_loss:104.74935913085938\n",
            "2631/3000 train_loss: 135.01931762695312 test_loss:103.10275268554688\n",
            "2632/3000 train_loss: 89.35646057128906 test_loss:100.12118530273438\n",
            "2633/3000 train_loss: 279.70465087890625 test_loss:99.0573501586914\n",
            "2634/3000 train_loss: 102.31295013427734 test_loss:101.33289337158203\n",
            "2635/3000 train_loss: 92.34534454345703 test_loss:98.25985717773438\n",
            "2636/3000 train_loss: 95.342529296875 test_loss:99.04859924316406\n",
            "2637/3000 train_loss: 99.432373046875 test_loss:101.04419708251953\n",
            "2638/3000 train_loss: 180.5250244140625 test_loss:98.84972381591797\n",
            "2639/3000 train_loss: 176.36492919921875 test_loss:100.10282135009766\n",
            "2640/3000 train_loss: 349.0534973144531 test_loss:106.53776550292969\n",
            "2641/3000 train_loss: 92.59674072265625 test_loss:116.03886413574219\n",
            "2642/3000 train_loss: 150.28355407714844 test_loss:107.339599609375\n",
            "2643/3000 train_loss: 91.03836059570312 test_loss:106.54728698730469\n",
            "2644/3000 train_loss: 92.76351928710938 test_loss:104.90165710449219\n",
            "2645/3000 train_loss: 105.92518615722656 test_loss:106.68376159667969\n",
            "2646/3000 train_loss: 101.61347198486328 test_loss:107.33991241455078\n",
            "2647/3000 train_loss: 87.54374694824219 test_loss:104.09860229492188\n",
            "2648/3000 train_loss: 107.54177856445312 test_loss:102.60577392578125\n",
            "2649/3000 train_loss: 105.99600219726562 test_loss:104.73262786865234\n",
            "2650/3000 train_loss: 92.78931427001953 test_loss:103.83781433105469\n",
            "2651/3000 train_loss: 170.61561584472656 test_loss:107.88856506347656\n",
            "2652/3000 train_loss: 114.12545776367188 test_loss:111.37762451171875\n",
            "2653/3000 train_loss: 89.82456970214844 test_loss:104.68765258789062\n",
            "2654/3000 train_loss: 203.59271240234375 test_loss:101.19542694091797\n",
            "2655/3000 train_loss: 95.41141510009766 test_loss:106.24766540527344\n",
            "2656/3000 train_loss: 134.29244995117188 test_loss:100.13506317138672\n",
            "2657/3000 train_loss: 85.62410736083984 test_loss:98.83049774169922\n",
            "2658/3000 train_loss: 92.26002502441406 test_loss:98.4576416015625\n",
            "2659/3000 train_loss: 102.12188720703125 test_loss:100.60678100585938\n",
            "2660/3000 train_loss: 117.23653411865234 test_loss:99.3387451171875\n",
            "2661/3000 train_loss: 83.69194030761719 test_loss:100.29055786132812\n",
            "2662/3000 train_loss: 126.88241577148438 test_loss:99.5788803100586\n",
            "2663/3000 train_loss: 106.11212921142578 test_loss:102.66266632080078\n",
            "2664/3000 train_loss: 190.19729614257812 test_loss:106.37075805664062\n",
            "2665/3000 train_loss: 104.65272521972656 test_loss:106.60906982421875\n",
            "2666/3000 train_loss: 147.2603759765625 test_loss:100.37374114990234\n",
            "2667/3000 train_loss: 87.9676513671875 test_loss:97.41290283203125\n",
            "2668/3000 train_loss: 99.62286376953125 test_loss:99.27731323242188\n",
            "2669/3000 train_loss: 149.57546997070312 test_loss:101.77700805664062\n",
            "2670/3000 train_loss: 107.62541198730469 test_loss:106.89212036132812\n",
            "2671/3000 train_loss: 107.55683135986328 test_loss:101.93528747558594\n",
            "2672/3000 train_loss: 83.98524475097656 test_loss:101.87261199951172\n",
            "2673/3000 train_loss: 92.15727996826172 test_loss:102.80972290039062\n",
            "2674/3000 train_loss: 95.18035125732422 test_loss:101.95366668701172\n",
            "2675/3000 train_loss: 88.47822570800781 test_loss:102.83503723144531\n",
            "2676/3000 train_loss: 118.19242858886719 test_loss:100.79972839355469\n",
            "2677/3000 train_loss: 113.70950317382812 test_loss:101.93505859375\n",
            "2678/3000 train_loss: 93.24615478515625 test_loss:99.3895034790039\n",
            "2679/3000 train_loss: 96.952392578125 test_loss:101.0150146484375\n",
            "2680/3000 train_loss: 96.58174133300781 test_loss:97.90071105957031\n",
            "2681/3000 train_loss: 92.93693542480469 test_loss:99.6042251586914\n",
            "2682/3000 train_loss: 129.75889587402344 test_loss:101.92533874511719\n",
            "2683/3000 train_loss: 100.69702911376953 test_loss:95.69641876220703\n",
            "2684/3000 train_loss: 126.12565612792969 test_loss:100.5723648071289\n",
            "2685/3000 train_loss: 95.28395080566406 test_loss:99.16683959960938\n",
            "2686/3000 train_loss: 235.67572021484375 test_loss:98.86817932128906\n",
            "2687/3000 train_loss: 103.27345275878906 test_loss:100.47425842285156\n",
            "2688/3000 train_loss: 89.48538208007812 test_loss:99.039794921875\n",
            "2689/3000 train_loss: 91.36088562011719 test_loss:99.32037353515625\n",
            "2690/3000 train_loss: 96.90385437011719 test_loss:96.86248016357422\n",
            "2691/3000 train_loss: 181.15138244628906 test_loss:97.37285614013672\n",
            "2692/3000 train_loss: 102.39859008789062 test_loss:106.18904876708984\n",
            "2693/3000 train_loss: 189.2386932373047 test_loss:106.60122680664062\n",
            "2694/3000 train_loss: 101.16099548339844 test_loss:100.48844909667969\n",
            "2695/3000 train_loss: 286.7610778808594 test_loss:101.79439544677734\n",
            "2696/3000 train_loss: 196.23910522460938 test_loss:118.07913208007812\n",
            "2697/3000 train_loss: 143.87973022460938 test_loss:111.04264831542969\n",
            "2698/3000 train_loss: 123.65697479248047 test_loss:108.03713989257812\n",
            "2699/3000 train_loss: 124.34745788574219 test_loss:111.95916748046875\n",
            "2700/3000 train_loss: 86.50691223144531 test_loss:111.25846862792969\n",
            "2701/3000 train_loss: 120.58665466308594 test_loss:107.74479675292969\n",
            "2702/3000 train_loss: 110.82623291015625 test_loss:102.72770690917969\n",
            "2703/3000 train_loss: 89.03057861328125 test_loss:103.41084289550781\n",
            "2704/3000 train_loss: 109.83493041992188 test_loss:102.66820526123047\n",
            "2705/3000 train_loss: 91.7647705078125 test_loss:100.84976959228516\n",
            "2706/3000 train_loss: 90.06481170654297 test_loss:102.50619506835938\n",
            "2707/3000 train_loss: 225.556640625 test_loss:108.06932830810547\n",
            "2708/3000 train_loss: 207.2519989013672 test_loss:108.79834747314453\n",
            "2709/3000 train_loss: 103.453857421875 test_loss:111.85188293457031\n",
            "2710/3000 train_loss: 96.63165283203125 test_loss:105.27604675292969\n",
            "2711/3000 train_loss: 115.07064819335938 test_loss:101.55525970458984\n",
            "2712/3000 train_loss: 101.4845199584961 test_loss:102.37689971923828\n",
            "2713/3000 train_loss: 124.05487060546875 test_loss:99.89231872558594\n",
            "2714/3000 train_loss: 82.85199737548828 test_loss:106.29522705078125\n",
            "2715/3000 train_loss: 139.451171875 test_loss:102.36376190185547\n",
            "2716/3000 train_loss: 103.74319458007812 test_loss:110.36897277832031\n",
            "2717/3000 train_loss: 92.11107635498047 test_loss:107.28195190429688\n",
            "2718/3000 train_loss: 136.680419921875 test_loss:103.55654907226562\n",
            "2719/3000 train_loss: 88.84123992919922 test_loss:113.10183715820312\n",
            "2720/3000 train_loss: 111.59681701660156 test_loss:106.31857299804688\n",
            "2721/3000 train_loss: 91.63499450683594 test_loss:105.52725219726562\n",
            "2722/3000 train_loss: 107.56391906738281 test_loss:106.34435272216797\n",
            "2723/3000 train_loss: 140.68994140625 test_loss:110.95249938964844\n",
            "2724/3000 train_loss: 112.66741943359375 test_loss:108.18793487548828\n",
            "2725/3000 train_loss: 151.65487670898438 test_loss:105.37356567382812\n",
            "2726/3000 train_loss: 88.14286804199219 test_loss:107.8410415649414\n",
            "2727/3000 train_loss: 84.96662139892578 test_loss:111.10939025878906\n",
            "2728/3000 train_loss: 107.49351501464844 test_loss:104.08164978027344\n",
            "2729/3000 train_loss: 214.96804809570312 test_loss:101.7125473022461\n",
            "2730/3000 train_loss: 103.60028076171875 test_loss:98.80607604980469\n",
            "2731/3000 train_loss: 115.11357116699219 test_loss:100.7601089477539\n",
            "2732/3000 train_loss: 95.8113784790039 test_loss:104.62106323242188\n",
            "2733/3000 train_loss: 78.75614166259766 test_loss:102.83501434326172\n",
            "2734/3000 train_loss: 134.0929412841797 test_loss:101.29358673095703\n",
            "2735/3000 train_loss: 102.27568054199219 test_loss:101.27010345458984\n",
            "2736/3000 train_loss: 124.48284912109375 test_loss:100.24964141845703\n",
            "2737/3000 train_loss: 107.12101745605469 test_loss:98.2398681640625\n",
            "2738/3000 train_loss: 119.60319519042969 test_loss:99.29486083984375\n",
            "2739/3000 train_loss: 154.0787811279297 test_loss:101.58048248291016\n",
            "2740/3000 train_loss: 105.50624084472656 test_loss:108.50186920166016\n",
            "2741/3000 train_loss: 115.29114532470703 test_loss:106.53350830078125\n",
            "2742/3000 train_loss: 102.49334716796875 test_loss:106.3172607421875\n",
            "2743/3000 train_loss: 95.43347930908203 test_loss:105.99024963378906\n",
            "2744/3000 train_loss: 251.333984375 test_loss:105.22627258300781\n",
            "2745/3000 train_loss: 113.9244155883789 test_loss:117.58451843261719\n",
            "2746/3000 train_loss: 167.10623168945312 test_loss:106.03482818603516\n",
            "2747/3000 train_loss: 100.0656967163086 test_loss:107.35868835449219\n",
            "2748/3000 train_loss: 98.24391174316406 test_loss:105.18207550048828\n",
            "2749/3000 train_loss: 97.12022399902344 test_loss:100.94038391113281\n",
            "2750/3000 train_loss: 93.43429565429688 test_loss:104.15940856933594\n",
            "2751/3000 train_loss: 130.2995147705078 test_loss:104.07042694091797\n",
            "2752/3000 train_loss: 93.54811096191406 test_loss:108.03382110595703\n",
            "2753/3000 train_loss: 105.28262329101562 test_loss:105.12593078613281\n",
            "2754/3000 train_loss: 109.27667236328125 test_loss:100.79058074951172\n",
            "2755/3000 train_loss: 180.59800720214844 test_loss:100.59912109375\n",
            "2756/3000 train_loss: 121.84574890136719 test_loss:104.31964111328125\n",
            "2757/3000 train_loss: 104.32319641113281 test_loss:97.80680084228516\n",
            "2758/3000 train_loss: 93.19598388671875 test_loss:98.01309204101562\n",
            "2759/3000 train_loss: 171.79425048828125 test_loss:96.72229766845703\n",
            "2760/3000 train_loss: 106.36249542236328 test_loss:96.4936752319336\n",
            "2761/3000 train_loss: 109.15888977050781 test_loss:97.52894592285156\n",
            "2762/3000 train_loss: 99.69972229003906 test_loss:99.084716796875\n",
            "2763/3000 train_loss: 92.59848022460938 test_loss:101.44148254394531\n",
            "2764/3000 train_loss: 101.06180572509766 test_loss:105.31688690185547\n",
            "2765/3000 train_loss: 126.37596130371094 test_loss:100.58937072753906\n",
            "2766/3000 train_loss: 107.03634643554688 test_loss:103.12646484375\n",
            "2767/3000 train_loss: 108.66656494140625 test_loss:106.21592712402344\n",
            "2768/3000 train_loss: 90.3802261352539 test_loss:103.42387390136719\n",
            "2769/3000 train_loss: 90.16104125976562 test_loss:99.60143280029297\n",
            "2770/3000 train_loss: 103.940673828125 test_loss:101.09706115722656\n",
            "2771/3000 train_loss: 98.69510650634766 test_loss:97.12667846679688\n",
            "2772/3000 train_loss: 80.82255554199219 test_loss:99.58785247802734\n",
            "2773/3000 train_loss: 81.36482238769531 test_loss:98.441162109375\n",
            "2774/3000 train_loss: 307.6033935546875 test_loss:100.9534912109375\n",
            "2775/3000 train_loss: 111.59243774414062 test_loss:114.66893005371094\n",
            "2776/3000 train_loss: 97.05284118652344 test_loss:102.31063079833984\n",
            "2777/3000 train_loss: 81.88379669189453 test_loss:98.67182922363281\n",
            "2778/3000 train_loss: 104.90780639648438 test_loss:98.21897888183594\n",
            "2779/3000 train_loss: 197.03448486328125 test_loss:100.31713104248047\n",
            "2780/3000 train_loss: 156.7864990234375 test_loss:106.32423400878906\n",
            "2781/3000 train_loss: 159.37258911132812 test_loss:108.99433135986328\n",
            "2782/3000 train_loss: 111.00096130371094 test_loss:103.55259704589844\n",
            "2783/3000 train_loss: 118.64019775390625 test_loss:98.42477416992188\n",
            "2784/3000 train_loss: 90.49555206298828 test_loss:98.7258529663086\n",
            "2785/3000 train_loss: 84.41081237792969 test_loss:97.63278198242188\n",
            "2786/3000 train_loss: 136.97341918945312 test_loss:98.30337524414062\n",
            "2787/3000 train_loss: 102.93562316894531 test_loss:106.03877258300781\n",
            "2788/3000 train_loss: 108.19052124023438 test_loss:103.69475555419922\n",
            "2789/3000 train_loss: 92.54447174072266 test_loss:102.89435577392578\n",
            "2790/3000 train_loss: 119.42219543457031 test_loss:100.04309844970703\n",
            "2791/3000 train_loss: 112.37908172607422 test_loss:100.55424499511719\n",
            "2792/3000 train_loss: 82.17359924316406 test_loss:96.29876708984375\n",
            "2793/3000 train_loss: 128.5928192138672 test_loss:100.0864028930664\n",
            "2794/3000 train_loss: 112.65963745117188 test_loss:106.9971923828125\n",
            "2795/3000 train_loss: 86.59058380126953 test_loss:104.30500030517578\n",
            "2796/3000 train_loss: 153.583740234375 test_loss:100.62583923339844\n",
            "2797/3000 train_loss: 95.51466369628906 test_loss:103.48609924316406\n",
            "2798/3000 train_loss: 163.707275390625 test_loss:102.07975769042969\n",
            "2799/3000 train_loss: 131.6000518798828 test_loss:103.34600067138672\n",
            "2800/3000 train_loss: 102.94694519042969 test_loss:98.34583282470703\n",
            "2801/3000 train_loss: 92.506103515625 test_loss:95.95247650146484\n",
            "2802/3000 train_loss: 122.33857727050781 test_loss:98.48457336425781\n",
            "2803/3000 train_loss: 91.21687316894531 test_loss:102.28462219238281\n",
            "2804/3000 train_loss: 90.75643157958984 test_loss:103.54939270019531\n",
            "2805/3000 train_loss: 91.37825012207031 test_loss:100.25525665283203\n",
            "2806/3000 train_loss: 86.20884704589844 test_loss:96.99642181396484\n",
            "2807/3000 train_loss: 96.36994934082031 test_loss:98.13969421386719\n",
            "2808/3000 train_loss: 82.89859771728516 test_loss:98.65152740478516\n",
            "2809/3000 train_loss: 114.26486206054688 test_loss:97.81616973876953\n",
            "2810/3000 train_loss: 79.70435333251953 test_loss:92.75240325927734\n",
            "2811/3000 train_loss: 103.88154602050781 test_loss:93.73780822753906\n",
            "2812/3000 train_loss: 84.703125 test_loss:96.8270492553711\n",
            "2813/3000 train_loss: 84.49407958984375 test_loss:100.0488510131836\n",
            "2814/3000 train_loss: 80.05973815917969 test_loss:98.94998168945312\n",
            "2815/3000 train_loss: 88.95684051513672 test_loss:97.60502624511719\n",
            "2816/3000 train_loss: 123.31722259521484 test_loss:96.76556396484375\n",
            "2817/3000 train_loss: 85.7413101196289 test_loss:101.298095703125\n",
            "2818/3000 train_loss: 178.55984497070312 test_loss:97.70875549316406\n",
            "2819/3000 train_loss: 121.52984619140625 test_loss:92.6556167602539\n",
            "2820/3000 train_loss: 83.28015899658203 test_loss:93.63886260986328\n",
            "2821/3000 train_loss: 77.15910339355469 test_loss:93.5235824584961\n",
            "2822/3000 train_loss: 80.17407989501953 test_loss:95.08072662353516\n",
            "2823/3000 train_loss: 78.72570037841797 test_loss:94.36474609375\n",
            "2824/3000 train_loss: 78.92145538330078 test_loss:93.84358215332031\n",
            "2825/3000 train_loss: 93.96034240722656 test_loss:93.81767272949219\n",
            "2826/3000 train_loss: 83.94583129882812 test_loss:94.27027130126953\n",
            "2827/3000 train_loss: 99.61067962646484 test_loss:94.97770690917969\n",
            "2828/3000 train_loss: 89.27007293701172 test_loss:97.28204345703125\n",
            "2829/3000 train_loss: 99.30404663085938 test_loss:94.89336395263672\n",
            "2830/3000 train_loss: 87.41020202636719 test_loss:97.01556396484375\n",
            "2831/3000 train_loss: 84.97945404052734 test_loss:96.51490783691406\n",
            "2832/3000 train_loss: 81.97196197509766 test_loss:97.51508331298828\n",
            "2833/3000 train_loss: 105.82160949707031 test_loss:97.94760131835938\n",
            "2834/3000 train_loss: 148.40493774414062 test_loss:94.49215698242188\n",
            "2835/3000 train_loss: 80.97116088867188 test_loss:100.65929412841797\n",
            "2836/3000 train_loss: 151.57339477539062 test_loss:97.39959716796875\n",
            "2837/3000 train_loss: 152.33645629882812 test_loss:95.80636596679688\n",
            "2838/3000 train_loss: 90.54338073730469 test_loss:96.40056610107422\n",
            "2839/3000 train_loss: 85.45708465576172 test_loss:96.87401580810547\n",
            "2840/3000 train_loss: 76.47509002685547 test_loss:96.78291320800781\n",
            "2841/3000 train_loss: 75.2390365600586 test_loss:97.42845916748047\n",
            "2842/3000 train_loss: 87.60381317138672 test_loss:93.84124755859375\n",
            "2843/3000 train_loss: 122.80895233154297 test_loss:97.39590454101562\n",
            "2844/3000 train_loss: 89.91583251953125 test_loss:93.07698059082031\n",
            "2845/3000 train_loss: 88.15668487548828 test_loss:94.6711654663086\n",
            "2846/3000 train_loss: 80.49005889892578 test_loss:95.72218322753906\n",
            "2847/3000 train_loss: 81.7529296875 test_loss:95.1034164428711\n",
            "2848/3000 train_loss: 77.52670288085938 test_loss:95.9108657836914\n",
            "2849/3000 train_loss: 158.6561737060547 test_loss:95.52091979980469\n",
            "2850/3000 train_loss: 176.53445434570312 test_loss:97.53667449951172\n",
            "2851/3000 train_loss: 93.51204681396484 test_loss:103.27579498291016\n",
            "2852/3000 train_loss: 117.49258422851562 test_loss:97.20747375488281\n",
            "2853/3000 train_loss: 102.36416625976562 test_loss:96.23009490966797\n",
            "2854/3000 train_loss: 101.30989837646484 test_loss:100.34080505371094\n",
            "2855/3000 train_loss: 91.6603775024414 test_loss:93.24462127685547\n",
            "2856/3000 train_loss: 101.60317993164062 test_loss:94.56523132324219\n",
            "2857/3000 train_loss: 98.12042999267578 test_loss:94.50294494628906\n",
            "2858/3000 train_loss: 111.48914337158203 test_loss:98.4397201538086\n",
            "2859/3000 train_loss: 128.18148803710938 test_loss:99.41657257080078\n",
            "2860/3000 train_loss: 89.15867614746094 test_loss:98.5770492553711\n",
            "2861/3000 train_loss: 102.07371520996094 test_loss:97.42083740234375\n",
            "2862/3000 train_loss: 84.19961547851562 test_loss:97.68421936035156\n",
            "2863/3000 train_loss: 96.89910888671875 test_loss:92.28060913085938\n",
            "2864/3000 train_loss: 90.1818618774414 test_loss:96.52469635009766\n",
            "2865/3000 train_loss: 98.33338165283203 test_loss:97.16740417480469\n",
            "2866/3000 train_loss: 80.27024841308594 test_loss:96.95648193359375\n",
            "2867/3000 train_loss: 77.29883575439453 test_loss:92.7387466430664\n",
            "2868/3000 train_loss: 70.60606384277344 test_loss:94.62112426757812\n",
            "2869/3000 train_loss: 95.58866882324219 test_loss:92.14490509033203\n",
            "2870/3000 train_loss: 132.96804809570312 test_loss:90.41853332519531\n",
            "2871/3000 train_loss: 228.67369079589844 test_loss:93.88985443115234\n",
            "2872/3000 train_loss: 157.5726318359375 test_loss:111.50125885009766\n",
            "2873/3000 train_loss: 114.14321899414062 test_loss:105.72669219970703\n",
            "2874/3000 train_loss: 90.07372283935547 test_loss:97.13485717773438\n",
            "2875/3000 train_loss: 114.72102355957031 test_loss:92.614990234375\n",
            "2876/3000 train_loss: 82.92631530761719 test_loss:95.93775939941406\n",
            "2877/3000 train_loss: 131.6887969970703 test_loss:95.68658447265625\n",
            "2878/3000 train_loss: 109.56730651855469 test_loss:93.65287780761719\n",
            "2879/3000 train_loss: 107.7352294921875 test_loss:96.31092834472656\n",
            "2880/3000 train_loss: 139.63966369628906 test_loss:95.35641479492188\n",
            "2881/3000 train_loss: 108.95188903808594 test_loss:114.30750274658203\n",
            "2882/3000 train_loss: 100.72357940673828 test_loss:102.07411193847656\n",
            "2883/3000 train_loss: 185.9521484375 test_loss:97.33610534667969\n",
            "2884/3000 train_loss: 132.9424591064453 test_loss:104.06061553955078\n",
            "2885/3000 train_loss: 100.66112518310547 test_loss:101.51453399658203\n",
            "2886/3000 train_loss: 88.53118896484375 test_loss:103.51324462890625\n",
            "2887/3000 train_loss: 120.79978942871094 test_loss:99.43518829345703\n",
            "2888/3000 train_loss: 166.9980926513672 test_loss:101.66349029541016\n",
            "2889/3000 train_loss: 90.87062072753906 test_loss:107.17721557617188\n",
            "2890/3000 train_loss: 147.8056640625 test_loss:103.81201171875\n",
            "2891/3000 train_loss: 97.6512222290039 test_loss:104.95327758789062\n",
            "2892/3000 train_loss: 205.12010192871094 test_loss:104.50743865966797\n",
            "2893/3000 train_loss: 112.17315673828125 test_loss:106.32275390625\n",
            "2894/3000 train_loss: 114.14845275878906 test_loss:102.72593688964844\n",
            "2895/3000 train_loss: 148.88485717773438 test_loss:98.83541107177734\n",
            "2896/3000 train_loss: 88.33970642089844 test_loss:99.67384338378906\n",
            "2897/3000 train_loss: 111.39387512207031 test_loss:99.97846984863281\n",
            "2898/3000 train_loss: 136.44590759277344 test_loss:95.16163635253906\n",
            "2899/3000 train_loss: 87.09362030029297 test_loss:96.01922607421875\n",
            "2900/3000 train_loss: 89.36700439453125 test_loss:94.3664321899414\n",
            "2901/3000 train_loss: 198.19378662109375 test_loss:95.40642547607422\n",
            "2902/3000 train_loss: 94.27110290527344 test_loss:101.83768463134766\n",
            "2903/3000 train_loss: 100.43880462646484 test_loss:101.18276977539062\n",
            "2904/3000 train_loss: 138.515380859375 test_loss:96.7966079711914\n",
            "2905/3000 train_loss: 96.91546630859375 test_loss:99.55260467529297\n",
            "2906/3000 train_loss: 99.97285461425781 test_loss:96.82208251953125\n",
            "2907/3000 train_loss: 83.66563415527344 test_loss:97.19281005859375\n",
            "2908/3000 train_loss: 87.06877136230469 test_loss:95.98505401611328\n",
            "2909/3000 train_loss: 94.65219116210938 test_loss:99.45774841308594\n",
            "2910/3000 train_loss: 90.53233337402344 test_loss:96.72128295898438\n",
            "2911/3000 train_loss: 100.63423156738281 test_loss:93.99130249023438\n",
            "2912/3000 train_loss: 80.1827163696289 test_loss:96.20178985595703\n",
            "2913/3000 train_loss: 92.55412292480469 test_loss:96.13002014160156\n",
            "2914/3000 train_loss: 202.83778381347656 test_loss:109.73770141601562\n",
            "2915/3000 train_loss: 106.60562896728516 test_loss:104.35243225097656\n",
            "2916/3000 train_loss: 104.27094268798828 test_loss:100.81542205810547\n",
            "2917/3000 train_loss: 81.61341094970703 test_loss:95.17938232421875\n",
            "2918/3000 train_loss: 127.84711456298828 test_loss:100.10709381103516\n",
            "2919/3000 train_loss: 106.2513656616211 test_loss:99.81303405761719\n",
            "2920/3000 train_loss: 74.66950988769531 test_loss:103.44295501708984\n",
            "2921/3000 train_loss: 86.30780792236328 test_loss:96.43434143066406\n",
            "2922/3000 train_loss: 125.94157409667969 test_loss:100.4196548461914\n",
            "2923/3000 train_loss: 82.89009094238281 test_loss:93.5062255859375\n",
            "2924/3000 train_loss: 98.13661193847656 test_loss:94.97146606445312\n",
            "2925/3000 train_loss: 85.07353210449219 test_loss:107.64218139648438\n",
            "2926/3000 train_loss: 109.1730728149414 test_loss:94.0087661743164\n",
            "2927/3000 train_loss: 136.48239135742188 test_loss:94.17079162597656\n",
            "2928/3000 train_loss: 102.37799072265625 test_loss:93.00639343261719\n",
            "2929/3000 train_loss: 83.57160186767578 test_loss:91.31640625\n",
            "2930/3000 train_loss: 89.96434020996094 test_loss:93.56171417236328\n",
            "2931/3000 train_loss: 93.22636413574219 test_loss:92.96380615234375\n",
            "2932/3000 train_loss: 123.2335205078125 test_loss:93.09712982177734\n",
            "2933/3000 train_loss: 114.16285705566406 test_loss:100.68818664550781\n",
            "2934/3000 train_loss: 87.93389892578125 test_loss:93.99788665771484\n",
            "2935/3000 train_loss: 76.96875 test_loss:91.5383529663086\n",
            "2936/3000 train_loss: 96.47935485839844 test_loss:95.5303726196289\n",
            "2937/3000 train_loss: 181.17758178710938 test_loss:94.75599670410156\n",
            "2938/3000 train_loss: 86.60700988769531 test_loss:100.085205078125\n",
            "2939/3000 train_loss: 83.72639465332031 test_loss:96.36695861816406\n",
            "2940/3000 train_loss: 82.15882110595703 test_loss:96.14098358154297\n",
            "2941/3000 train_loss: 118.92462158203125 test_loss:94.69326782226562\n",
            "2942/3000 train_loss: 76.83704376220703 test_loss:93.60440063476562\n",
            "2943/3000 train_loss: 104.37820434570312 test_loss:91.68447875976562\n",
            "2944/3000 train_loss: 84.59061431884766 test_loss:91.72171783447266\n",
            "2945/3000 train_loss: 186.89210510253906 test_loss:89.36967468261719\n",
            "2946/3000 train_loss: 73.16533660888672 test_loss:93.04403686523438\n",
            "2947/3000 train_loss: 111.4096908569336 test_loss:91.03308868408203\n",
            "2948/3000 train_loss: 77.84458923339844 test_loss:88.23844909667969\n",
            "2949/3000 train_loss: 91.98728942871094 test_loss:91.23495483398438\n",
            "2950/3000 train_loss: 116.40763854980469 test_loss:93.01570892333984\n",
            "2951/3000 train_loss: 92.71539306640625 test_loss:91.74311828613281\n",
            "2952/3000 train_loss: 94.81798553466797 test_loss:90.76010131835938\n",
            "2953/3000 train_loss: 125.13771057128906 test_loss:90.6158447265625\n",
            "2954/3000 train_loss: 87.74874877929688 test_loss:92.17792510986328\n",
            "2955/3000 train_loss: 149.59539794921875 test_loss:91.44316864013672\n",
            "2956/3000 train_loss: 120.78376007080078 test_loss:121.31906127929688\n",
            "2957/3000 train_loss: 306.3566589355469 test_loss:105.42073059082031\n",
            "2958/3000 train_loss: 109.60380554199219 test_loss:119.78660583496094\n",
            "2959/3000 train_loss: 119.73208618164062 test_loss:109.58924865722656\n",
            "2960/3000 train_loss: 124.49239349365234 test_loss:104.05007934570312\n",
            "2961/3000 train_loss: 99.11433410644531 test_loss:101.18364715576172\n",
            "2962/3000 train_loss: 91.85977935791016 test_loss:97.10669708251953\n",
            "2963/3000 train_loss: 92.23591613769531 test_loss:97.23112487792969\n",
            "2964/3000 train_loss: 87.8803939819336 test_loss:95.6429672241211\n",
            "2965/3000 train_loss: 72.33958435058594 test_loss:95.40962219238281\n",
            "2966/3000 train_loss: 87.06640625 test_loss:96.44178771972656\n",
            "2967/3000 train_loss: 89.81216430664062 test_loss:97.47358703613281\n",
            "2968/3000 train_loss: 86.154296875 test_loss:93.49242401123047\n",
            "2969/3000 train_loss: 79.08418273925781 test_loss:91.42768859863281\n",
            "2970/3000 train_loss: 146.54818725585938 test_loss:93.80615234375\n",
            "2971/3000 train_loss: 82.91508483886719 test_loss:93.00594329833984\n",
            "2972/3000 train_loss: 95.82294464111328 test_loss:99.89440155029297\n",
            "2973/3000 train_loss: 132.85226440429688 test_loss:102.33272552490234\n",
            "2974/3000 train_loss: 101.55883026123047 test_loss:101.52761840820312\n",
            "2975/3000 train_loss: 186.77679443359375 test_loss:93.72297668457031\n",
            "2976/3000 train_loss: 84.9097671508789 test_loss:94.49073791503906\n",
            "2977/3000 train_loss: 175.52520751953125 test_loss:95.05870056152344\n",
            "2978/3000 train_loss: 181.53814697265625 test_loss:99.84780883789062\n",
            "2979/3000 train_loss: 131.19094848632812 test_loss:96.51367950439453\n",
            "2980/3000 train_loss: 108.10116577148438 test_loss:90.77232360839844\n",
            "2981/3000 train_loss: 82.16133880615234 test_loss:88.90235900878906\n",
            "2982/3000 train_loss: 106.75559997558594 test_loss:88.18965148925781\n",
            "2983/3000 train_loss: 100.5643081665039 test_loss:89.88787841796875\n",
            "2984/3000 train_loss: 100.22962188720703 test_loss:90.76000213623047\n",
            "2985/3000 train_loss: 80.83123779296875 test_loss:89.00762939453125\n",
            "2986/3000 train_loss: 83.81513977050781 test_loss:91.21803283691406\n",
            "2987/3000 train_loss: 83.2256088256836 test_loss:94.03900146484375\n",
            "2988/3000 train_loss: 89.18041229248047 test_loss:94.74723052978516\n",
            "2989/3000 train_loss: 84.55860137939453 test_loss:96.08736419677734\n",
            "2990/3000 train_loss: 181.20114135742188 test_loss:95.03779602050781\n",
            "2991/3000 train_loss: 79.31743621826172 test_loss:95.04530334472656\n",
            "2992/3000 train_loss: 114.87110900878906 test_loss:93.5715560913086\n",
            "2993/3000 train_loss: 77.3872299194336 test_loss:92.6606216430664\n",
            "2994/3000 train_loss: 82.00800323486328 test_loss:90.93980407714844\n",
            "2995/3000 train_loss: 142.48260498046875 test_loss:89.64872741699219\n",
            "2996/3000 train_loss: 172.0702362060547 test_loss:94.79791259765625\n",
            "2997/3000 train_loss: 100.03736114501953 test_loss:94.26010131835938\n",
            "2998/3000 train_loss: 126.50740051269531 test_loss:90.72856140136719\n",
            "2999/3000 train_loss: 101.85162353515625 test_loss:94.6237564086914\n",
            "3000/3000 train_loss: 149.55245971679688 test_loss:94.5492935180664\n"
          ]
        }
      ],
      "source": [
        "losses = train(model = unet, optimizer = optimizer, criterion=criterion, data_tr=train_data,\n",
        "               data_val = test_data, scheduler = scheduler,device = device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "metadata": {
        "id": "6Ew7_F0-q7aL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba6aad33-1973-4119-a4eb-66a200589b31"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(94.5493)"
            ]
          },
          "metadata": {},
          "execution_count": 194
        }
      ],
      "source": [
        "avg_loss = 0\n",
        "# unet.eval()\n",
        "# unet.train()\n",
        "preds = []\n",
        "i = 0\n",
        "test_anomaly_losses = []\n",
        "test_normal_losses = []\n",
        "test_losses = []\n",
        "test_real = y_test.tolist()\n",
        "for batch in test_data:\n",
        "  with torch.no_grad():\n",
        "    # unet.train()\n",
        "    predictions = unet(batch.to(device)).cpu()\n",
        "    preds.append(predictions)\n",
        "  loss = criterion(predictions, batch.cpu())\n",
        "  for j in range(len(predictions)):\n",
        "    if int(y_test[i]) == 1:\n",
        "      test_anomaly_losses.append(float(criterion(predictions[j], batch[j])))\n",
        "    else:\n",
        "      test_normal_losses.append(float(criterion(predictions[j], batch[j])))\n",
        "    i += 1\n",
        "    test_losses.append(criterion(predictions[j], batch[j]))\n",
        "  # print(loss)\n",
        "  # print(loss)\n",
        "  avg_loss += loss / len(test_data)\n",
        "# avg_loss\n",
        "\n",
        "avg_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "metadata": {
        "id": "VpDKorrRso9o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44135400-fa8c-4ba8-eae8-3943cdf1b958"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(94.62199595769246, 89.45313976287842)"
            ]
          },
          "metadata": {},
          "execution_count": 195
        }
      ],
      "source": [
        "sum(test_anomaly_losses)/len(test_anomaly_losses) , sum(test_normal_losses)/len(test_normal_losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "metadata": {
        "id": "cJE0-57Qts3E"
      },
      "outputs": [],
      "source": [
        "# torch.save(unet, \"unet_fan2_2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 197,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LEvbZKYuh7J",
        "outputId": "b9f4e54e-4bb0-48fe-95f1-24211379d0f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6841666666666668\n"
          ]
        }
      ],
      "source": [
        "results = []\n",
        "vals = np.arange(20, 100, 0.5).tolist()\n",
        "for threshold in vals:\n",
        "  preds = []\n",
        "  for j in range(len(test_losses)):\n",
        "    if test_losses[j] > threshold:\n",
        "      preds.append(1)\n",
        "    else:\n",
        "      preds.append(0)\n",
        "  \n",
        "  results.append(roc_auc_score(test_real,preds))\n",
        "\n",
        "print(max(results))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 198,
      "metadata": {
        "id": "W4H4vpFX35yK"
      },
      "outputs": [],
      "source": [
        "def get_logmelspectrogram(waveform):\n",
        "    melspec = librosa.feature.melspectrogram(y=waveform.numpy(), hop_length=250, n_mels = 304)\n",
        "\n",
        "    logmelspec = librosa.power_to_db(melspec)\n",
        "\n",
        "    return logmelspec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vo1-S_qcuUZR"
      },
      "outputs": [],
      "source": [
        "# train_logmelspecs, test_logmelspecs = mean_logmelspecs(df_train), mean_logmelspecs(df_test)\n",
        "train_data1 = []\n",
        "for wave in df_train:\n",
        "  train_data1.append(get_logmelspectrogram(wave)[0])\n",
        "\n",
        "test_data1 = []\n",
        "for wave in df_test:\n",
        "  test_data1.append(get_logmelspectrogram(wave)[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGd9oI5IEVMx",
        "outputId": "73a79d4e-9e50-4b29-e7ea-55ee60a08c22"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-30-68ec04120629>:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  train_data1 = torch.FloatTensor(train_data1)\n"
          ]
        }
      ],
      "source": [
        "train_data1 = torch.FloatTensor(train_data1)\n",
        "test_data1 = torch.FloatTensor(test_data1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMOi9331OVb4"
      },
      "outputs": [],
      "source": [
        "train_logs = DataLoader(train_data1.reshape(916*304,641),batch_size = 304)\n",
        "test_logs = DataLoader(test_data1.reshape(459*304,641),batch_size = 304)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9rD6tuI1rfe"
      },
      "outputs": [],
      "source": [
        "unet1 = UNet_FC(in_features=641).to(device)\n",
        "optimizer1 = Adam(params = unet1.parameters(), lr = 10e-3)\n",
        "# optimizer = Adam(params = unet.parameters())\n",
        "criterion1 = nn.MSELoss()\n",
        "scheduler1 = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer1, factor=0.5, \n",
        "                                                       min_lr=10e-4, mode = 'min',\n",
        "                                                       patience = 30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qr6k85ma3ftD",
        "outputId": "67174f45-79f9-41f1-8959-a0c75a2b17fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 train_loss: 21.703369140625 test_loss:17.446470260620117\n"
          ]
        }
      ],
      "source": [
        "losses = train(model = unet1, optimizer = optimizer1, criterion=criterion1, data_tr=train_logs,\n",
        "               data_val = test_logs, device = device, epochs = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrPbpKgSPx7v",
        "outputId": "a7bd10e8-e2bd-4579-8211-d6eaaa879711"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(17.4465)"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "avg_loss = 0\n",
        "# unet.eval()\n",
        "# unet.train()\n",
        "preds = []\n",
        "i = 0\n",
        "test_anomaly_losses = []\n",
        "test_normal_losses = []\n",
        "test_losses = []\n",
        "test_real = y_test.tolist()\n",
        "for batch in test_logs:\n",
        "  with torch.no_grad():\n",
        "    # unet.train()\n",
        "    predictions = unet1(batch.to(device)).cpu()\n",
        "    preds.append(predictions)\n",
        "    loss = criterion(predictions, batch.cpu())\n",
        "    test_losses.append(loss)\n",
        "    if int(y_test[i]) == 1:\n",
        "      test_anomaly_losses.append(loss)\n",
        "    else:\n",
        "      test_normal_losses.append(loss)\n",
        "    i += 1\n",
        "  # print(loss)\n",
        "  # print(loss)\n",
        "  avg_loss += loss / len(test_logs)\n",
        "# avg_loss\n",
        "\n",
        "avg_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Z5Z1XYFN_x2",
        "outputId": "9d767817-5525-443f-db72-2588edb4cfbc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor(18.0466), tensor(15.2920))"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sum(test_anomaly_losses)/len(test_anomaly_losses) , sum(test_normal_losses)/len(test_normal_losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "er74WfG7P_B1",
        "outputId": "884ef8f3-8bfe-4d71-bc01-534f5d5a0a03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.5875626740947075\n"
          ]
        }
      ],
      "source": [
        "results = []\n",
        "vals = np.arange(10, 21, 0.1).tolist()\n",
        "for threshold in vals:\n",
        "  preds = []\n",
        "  for j in range(len(test_losses)):\n",
        "    if test_losses[j] > threshold:\n",
        "      preds.append(1)\n",
        "    else:\n",
        "      preds.append(0)\n",
        "  \n",
        "  results.append(roc_auc_score(test_real,preds))\n",
        "\n",
        "print(max(results))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QaSSqG8SbAw2"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}