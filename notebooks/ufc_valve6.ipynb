{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "9SStKf4G0V5H"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "import torchaudio\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from skimage.util import img_as_ubyte\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import io\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_XtxbKLZq5KX",
    "outputId": "0c6de58c-f736-440b-85ce-a6ad2ae77c29"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CYxHegIM0Z4i",
    "outputId": "8b35e1ba-3d36-4aca-c569-e94fbad47a14"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive/\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "h9DATQwS0ivD"
   },
   "outputs": [],
   "source": [
    "class MimiiDataset(Dataset):\n",
    "    def __init__(self,audio_dir, n_fft = 1024, win_length = 1024,\n",
    "                 hop_length = 512,power = 2,n_mels = 128,pad_mode = 'reflect',\n",
    "                 sr = 16000,center = True,norm = None):\n",
    "      \n",
    "        super(MimiiDataset, self).__init__()\n",
    "        self.audio_dir = audio_dir\n",
    "        self.n_mels = n_mels\n",
    "        self.n_fft = n_fft\n",
    "        self.win_length = win_length\n",
    "        self.hop_length = hop_length\n",
    "        self.power = power\n",
    "        self.pad_mode = pad_mode\n",
    "        self.sr = sr\n",
    "        self.center = center\n",
    "        self.norm = norm\n",
    "\n",
    "    def get_files(self):\n",
    "       return self.train_files, self.test_files\n",
    "    \n",
    "    def get_data(self,device, id):\n",
    "        \n",
    "        self.train_files, self.train_labels = self._train_file_list(device, id)\n",
    "        self.test_files, self.test_labels = self._test_file_list(device, id)\n",
    "        \n",
    "        self.train_data = self.get_audios(self.train_files)\n",
    "        self.test_data = self.get_audios(self.test_files)\n",
    "        \n",
    "        return self.train_data, self.test_data, self.train_labels, self.test_labels\n",
    "    \n",
    "    def _train_file_list(self, device, id):\n",
    "        query = os.path.abspath(\n",
    "            f\"{self.audio_dir}/{device}/train/normal_id_0{id}*.wav\"\n",
    "        )\n",
    "        train_normal_files = sorted(glob.glob(query))\n",
    "        train_normal_labels = np.zeros(len(train_normal_files))\n",
    "        \n",
    "        query = os.path.abspath(\n",
    "                f\"{self.audio_dir}/{device}/train/anomaly_id_0{id}*.wav\"\n",
    "            )\n",
    "        train_anomaly_files = sorted(glob.glob(query))\n",
    "        train_anomaly_labels = np.ones(len(train_anomaly_files))\n",
    "        \n",
    "        train_file_list = np.concatenate((train_normal_files, train_anomaly_files), axis=0)\n",
    "        train_labels = np.concatenate((train_normal_labels, train_anomaly_labels), axis=0)\n",
    "        \n",
    "        return train_file_list, train_labels\n",
    "    \n",
    "    def _test_file_list(self, device, id):     \n",
    "        query = os.path.abspath(\n",
    "            f\"{self.audio_dir}/{device}/test/normal_id_0{id}*.wav\"\n",
    "            )\n",
    "        test_normal_files = sorted(glob.glob(query))\n",
    "        test_normal_labels = np.zeros(len(test_normal_files))\n",
    "        \n",
    "        query = os.path.abspath(\n",
    "            f\"{self.audio_dir}/{device}/test/anomaly_id_0{id}*.wav\"\n",
    "            )\n",
    "        test_anomaly_files = sorted(glob.glob(query))\n",
    "        test_anomaly_labels = np.ones(len(test_anomaly_files))\n",
    "        \n",
    "        test_file_list = np.concatenate((test_normal_files, \n",
    "                                          test_anomaly_files), axis=0)\n",
    "        test_labels = np.concatenate((test_normal_labels,\n",
    "                                      test_anomaly_labels), axis=0)\n",
    "          \n",
    "        return test_file_list, test_labels\n",
    "\n",
    "    def normalize(self,tensor):\n",
    "        tensor_minusmean = tensor - tensor.mean()\n",
    "        return tensor_minusmean/np.absolute(tensor_minusmean).max()\n",
    "\n",
    "    def make0min(self,tensornd):\n",
    "        tensor = tensornd.numpy()\n",
    "        res = np.where(tensor == 0, 1E-19 , tensor)\n",
    "        return torch.from_numpy(res)\n",
    "\n",
    "    def spectrogrameToImage(self,specgram):\n",
    "        # specgram = torchaudio.transforms.MelSpectrogram(n_fft=1024, win_length=1024, \n",
    "        #                                                 hop_length=512, power=2, \n",
    "        #                                                 normalized=True, n_mels=128)(waveform )\n",
    "        specgram= self.make0min(specgram)\n",
    "        specgram = specgram.log2()[0,:,:].numpy()\n",
    "        \n",
    "        tr2image = transforms.Compose([transforms.ToPILImage()])\n",
    "\n",
    "        specgram= self.normalize(specgram)\n",
    "        # specgram = img_as_ubyte(specgram)\n",
    "        specgramImage = tr2image(specgram)\n",
    "        return specgramImage\n",
    "\n",
    "    def get_logmelspectrogram(self, waveform):\n",
    "        melspec = librosa.feature.melspectrogram(\n",
    "          n_fft=self.n_fft, win_length=self.win_length, hop_length=self.hop_length,\n",
    "          power=self.power,n_mels=self.n_mels,pad_mode=self.pad_mode,sr=self.sr,\n",
    "          center=self.center,norm=self.norm,htk=True,\n",
    "          y = waveform.numpy()\n",
    "        )\n",
    "\n",
    "        logmelspec = librosa.power_to_db(melspec)\n",
    "\n",
    "        return logmelspec\n",
    "\n",
    "    def get_melspectrogram(self,waveform):\n",
    "        melspec = librosa.feature.melspectrogram(\n",
    "            n_fft=self.n_fft, win_length=self.win_length, hop_length=self.hop_length,\n",
    "            power=self.power,n_mels=self.n_mels,pad_mode=self.pad_mode,sr=self.sr,\n",
    "            center=self.center,norm=self.norm,htk=True,\n",
    "            y = waveform.numpy()\n",
    "        )\n",
    "\n",
    "        return melspec\n",
    "    \n",
    "    def get_mfcc(self,waveform):\n",
    "        mfcc = librosa.feature.mfcc(    \n",
    "            n_fft=self.n_fft, win_length=self.win_length, \n",
    "            hop_length=self.hop_length,pad_mode=self.pad_mode,sr=self.sr,\n",
    "            center=self.center,norm=self.norm,n_mfcc=40,\n",
    "            y = waveform.numpy()\n",
    "        )\n",
    "\n",
    "        return mfcc\n",
    "\n",
    "    def get_chroma_stft(self,waveform):\n",
    "        stft = librosa.feature.chroma_stft(\n",
    "            n_fft=self.n_fft, win_length=self.win_length, \n",
    "            hop_length=self.hop_length,pad_mode=self.pad_mode,sr=self.sr,\n",
    "            center=self.center,norm=self.norm,n_chroma=12,\n",
    "            y=waveform.numpy()\n",
    "        )\n",
    "\n",
    "        return stft\n",
    "\n",
    "    def get_spectral_contrast(self,waveform):\n",
    "        spec_contrast = librosa.feature.spectral_contrast(    \n",
    "            n_fft=self.n_fft, win_length=self.win_length,center=self.center,\n",
    "            hop_length=self.hop_length,pad_mode=self.pad_mode,sr=self.sr,\n",
    "            y = waveform.numpy()\n",
    "        )\n",
    "\n",
    "        return spec_contrast\n",
    "    \n",
    "    def get_tonnetz(self,waveform):\n",
    "        harmonic = librosa.effects.harmonic(waveform.numpy())\n",
    "        tonnetz = librosa.feature.tonnetz(y=harmonic,sr=self.sr)\n",
    "\n",
    "        return tonnetz\n",
    "\n",
    "    def get_audios(self, file_list):\n",
    "        data = []\n",
    "        for i in range(len(file_list)):\n",
    "          y, sr = torchaudio.load(file_list[i])  \n",
    "          data.append(y)\n",
    "\n",
    "        return data\n",
    "    def _derive_data(self, file_list):\n",
    "        train_data = []\n",
    "        test_data = []\n",
    "        train_mode = True\n",
    "        for file_list in [self.train_files, self.test_files]:\n",
    "          tr2tensor = transforms.Compose([transforms.PILToTensor()])\n",
    "          data = []\n",
    "          for j in range(len(file_list)):\n",
    "            y, sr = torchaudio.load(file_list[j])  \n",
    "            spec = self.get_melspectrogram(y)\n",
    "            spec = self.spectrogrameToImage(spec)\n",
    "            spec = spec.convert('RGB')\n",
    "            vectors = tr2tensor(spec)\n",
    "            if train_mode:     \n",
    "              train_data.append(vectors)\n",
    "            else:\n",
    "              test_data.append(vectors)\n",
    "            \n",
    "          train_mode = False\n",
    "                \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "S96soeIc0o13"
   },
   "outputs": [],
   "source": [
    "dataset = MimiiDataset('/content/drive/MyDrive/mimii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "id": "Gn2zdn92doi1"
   },
   "outputs": [],
   "source": [
    "_, _, y_train, y_test = dataset.get_data('valve', 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "id": "SgjpeWy_RV1C"
   },
   "outputs": [],
   "source": [
    "train_mixed_f = torch.load('/content/drive/MyDrive/mixed_features/train_mf_valve6.pt')\n",
    "test_mixed_f = torch.load('/content/drive/MyDrive/mixed_features/test_mf_valve6.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "id": "jWMPVGu1qiEq"
   },
   "outputs": [],
   "source": [
    "train_data = DataLoader(train_mixed_f, batch_size=64, shuffle = True)\n",
    "test_data = DataLoader(test_mixed_f, batch_size = 64, shuffle= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "id": "vNTBTRe6qnBq"
   },
   "outputs": [],
   "source": [
    "class UNet_FC(nn.Module):\n",
    "\n",
    "  def __init__(self, in_features):\n",
    "    super().__init__()\n",
    "    \n",
    "    self.bn = nn.BatchNorm1d(128)\n",
    "    self.relu = nn.ReLU()\n",
    "\n",
    "    self.fc0 = nn.Linear(in_features=in_features,out_features=in_features)\n",
    "\n",
    "    # encoder\n",
    "    self.fc1 = nn.Linear(in_features=in_features, out_features=128)\n",
    "    self.fc2 = nn.Linear(in_features=128,out_features=128)\n",
    "    self.fc3 = nn.Linear(in_features=128, out_features=128)\n",
    "    self.fc4 = nn.Linear(in_features=128, out_features=128)\n",
    "    self.fc5 = nn.Linear(in_features=128, out_features=8)\n",
    "\n",
    "    # decoder\n",
    "    self.fc6 = nn.Linear(in_features=8, out_features=128)\n",
    "    self.fc7 = nn.Linear(in_features=128*2, out_features=128)\n",
    "    self.fc8 = nn.Linear(in_features=128*2, out_features=128)\n",
    "    self.fc9 = nn.Linear(in_features=128*2, out_features=128)\n",
    "\n",
    "    self.out = nn.Linear(in_features=128*2, out_features=in_features)\n",
    "\n",
    "  def forward(self, x):\n",
    "    # encoded = self.encoder(x)\n",
    "\n",
    "    # decoded = self.decoder(encoded)\n",
    "    input = self.fc0(x)\n",
    "\n",
    "    x1 = self.relu(self.bn(self.fc1(input)))\n",
    "    x2 = self.relu(self.bn(self.fc2(x1)))\n",
    "    x3 = self.relu(self.bn(self.fc3(x2)))\n",
    "    x4 = self.relu(self.bn(self.fc4(x3)))\n",
    "    x5 = self.relu(self.fc5(x4))\n",
    "\n",
    "    xy = [x5, x4, x3, x2, x1]\n",
    "\n",
    "    x6 = self.relu(self.fc6(xy[0]))\n",
    "    con1 = torch.cat((x6,xy[1]), 1) \n",
    "    x7 = self.relu(self.bn(self.fc7(con1)))\n",
    "    con2 = torch.cat((x7,xy[2]), 1)\n",
    "    x8 = self.relu(self.bn(self.fc8(con2)))\n",
    "    con3 = torch.cat((x8,xy[3]), 1)\n",
    "    x9 = self.relu(self.bn(self.fc9(con3)))\n",
    "    con4 = torch.cat((x9,xy[4]), 1)\n",
    "\n",
    "    x10 = self.out(con4)\n",
    "\n",
    "    return x10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "id": "ZfgcBtQ3qn5l"
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, data_tr, data_val, scheduler = None,\n",
    "          epochs = 3000, device = 'cpu'):\n",
    "    # X_val, Y_val = next(iter(data_val))\n",
    "    losses = []\n",
    "    prev_avg_loss = 100000\n",
    "    for epoch in range(epochs):\n",
    "        train_avg_loss = 0\n",
    "        test_avg_loss = 0\n",
    "        # model.train()  # train mode\n",
    "        for batch in data_tr:\n",
    "          # data to device\n",
    "          batch = batch.to(device)\n",
    "          # set parameter gradients to zero\n",
    "          optimizer.zero_grad()\n",
    "          # forward\n",
    "          # print(Y_batch.shape)\n",
    "          predictions = model(batch)\n",
    "          loss = criterion(predictions, batch)\n",
    "          loss.backward() # backward-pass\n",
    "          optimizer.step()  # update weights\n",
    "          # calculate loss to show the user\n",
    "          if scheduler:\n",
    "            scheduler.step(loss)\n",
    "          train_avg_loss += loss / len(data_tr)\n",
    "\n",
    "        # model.eval()\n",
    "        for batch in data_val:\n",
    "          with torch.no_grad():\n",
    "            preds = model(batch.to(device)).cpu()\n",
    "            loss = criterion(preds,batch)\n",
    "            test_avg_loss += loss / len(data_val)\n",
    "                    \n",
    "        losses.append(train_avg_loss.item())\n",
    "        # if (epoch+1)%50 == 0:\n",
    "        print(\"{}/{} train_loss: {} test_loss:{}\".format(epoch+1, epochs, train_avg_loss, test_avg_loss))\n",
    "        # if test_avg_loss < 70:\n",
    "        #   break\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "id": "ptkVTF55quOL"
   },
   "outputs": [],
   "source": [
    "unet = UNet_FC(in_features=193).to(device)\n",
    "optimizer = Adam(params = unet.parameters(), lr = 10e-3)\n",
    "# optimizer = Adam(params = unet.parameters())\n",
    "criterion = nn.MSELoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, \n",
    "                                                       min_lr=10e-4, mode = 'min',\n",
    "                                                       patience = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QkfmYl9oXhcB",
    "outputId": "2eaf264c-74ab-4791-ac4a-002e74eb58f9"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1/3000 train_loss: 329770.6875 test_loss:335068.03125\n",
      "2/3000 train_loss: 326912.25 test_loss:331470.6875\n",
      "3/3000 train_loss: 322654.3125 test_loss:326261.28125\n",
      "4/3000 train_loss: 316697.5 test_loss:319207.1875\n",
      "5/3000 train_loss: 308928.15625 test_loss:310155.53125\n",
      "6/3000 train_loss: 298607.21875 test_loss:298014.375\n",
      "7/3000 train_loss: 286075.0625 test_loss:284251.15625\n",
      "8/3000 train_loss: 272000.75 test_loss:268979.6875\n",
      "9/3000 train_loss: 256420.78125 test_loss:252931.421875\n",
      "10/3000 train_loss: 239739.15625 test_loss:235318.921875\n",
      "11/3000 train_loss: 221825.484375 test_loss:216719.78125\n",
      "12/3000 train_loss: 204125.15625 test_loss:198558.640625\n",
      "13/3000 train_loss: 185000.078125 test_loss:178870.5625\n",
      "14/3000 train_loss: 166362.4375 test_loss:160007.0625\n",
      "15/3000 train_loss: 147863.078125 test_loss:141451.484375\n",
      "16/3000 train_loss: 129548.578125 test_loss:123378.0703125\n",
      "17/3000 train_loss: 112395.9921875 test_loss:106006.375\n",
      "18/3000 train_loss: 95790.8671875 test_loss:90082.9296875\n",
      "19/3000 train_loss: 80510.5390625 test_loss:74649.2578125\n",
      "20/3000 train_loss: 65938.234375 test_loss:61154.75390625\n",
      "21/3000 train_loss: 53038.75390625 test_loss:49199.32421875\n",
      "22/3000 train_loss: 43097.7734375 test_loss:39330.96875\n",
      "23/3000 train_loss: 33705.9921875 test_loss:30594.71875\n",
      "24/3000 train_loss: 25681.85546875 test_loss:23572.30859375\n",
      "25/3000 train_loss: 19258.4609375 test_loss:17420.96875\n",
      "26/3000 train_loss: 14029.576171875 test_loss:12937.7353515625\n",
      "27/3000 train_loss: 10179.5390625 test_loss:9630.037109375\n",
      "28/3000 train_loss: 7443.09912109375 test_loss:7005.4697265625\n",
      "29/3000 train_loss: 5320.5458984375 test_loss:5316.470703125\n",
      "30/3000 train_loss: 3806.271484375 test_loss:3778.58251953125\n",
      "31/3000 train_loss: 2846.548828125 test_loss:2855.414306640625\n",
      "32/3000 train_loss: 1988.3096923828125 test_loss:2113.955322265625\n",
      "33/3000 train_loss: 1533.4781494140625 test_loss:1658.1109619140625\n",
      "34/3000 train_loss: 1219.3304443359375 test_loss:1345.536376953125\n",
      "35/3000 train_loss: 1034.8194580078125 test_loss:1175.4139404296875\n",
      "36/3000 train_loss: 874.921142578125 test_loss:1019.4046630859375\n",
      "37/3000 train_loss: 853.5571899414062 test_loss:952.9888916015625\n",
      "38/3000 train_loss: 730.1663818359375 test_loss:803.539794921875\n",
      "39/3000 train_loss: 794.7335205078125 test_loss:765.8984375\n",
      "40/3000 train_loss: 715.25634765625 test_loss:731.758544921875\n",
      "41/3000 train_loss: 653.90576171875 test_loss:710.2402954101562\n",
      "42/3000 train_loss: 604.6089477539062 test_loss:685.315673828125\n",
      "43/3000 train_loss: 634.327880859375 test_loss:670.3392333984375\n",
      "44/3000 train_loss: 596.0327758789062 test_loss:650.475341796875\n",
      "45/3000 train_loss: 577.2901611328125 test_loss:633.97607421875\n",
      "46/3000 train_loss: 662.4105224609375 test_loss:634.2960815429688\n",
      "47/3000 train_loss: 546.4779052734375 test_loss:618.0810546875\n",
      "48/3000 train_loss: 570.2385864257812 test_loss:617.5877685546875\n",
      "49/3000 train_loss: 527.783935546875 test_loss:621.6864624023438\n",
      "50/3000 train_loss: 557.802490234375 test_loss:605.904296875\n",
      "51/3000 train_loss: 545.741455078125 test_loss:610.12060546875\n",
      "52/3000 train_loss: 554.2168579101562 test_loss:612.966796875\n",
      "53/3000 train_loss: 539.350830078125 test_loss:606.8488159179688\n",
      "54/3000 train_loss: 518.8763427734375 test_loss:614.8302001953125\n",
      "55/3000 train_loss: 528.6658325195312 test_loss:601.73291015625\n",
      "56/3000 train_loss: 651.6200561523438 test_loss:611.5296020507812\n",
      "57/3000 train_loss: 535.0283203125 test_loss:599.4894409179688\n",
      "58/3000 train_loss: 539.4503173828125 test_loss:619.9068603515625\n",
      "59/3000 train_loss: 535.9850463867188 test_loss:591.0946655273438\n",
      "60/3000 train_loss: 550.2191162109375 test_loss:602.0760498046875\n",
      "61/3000 train_loss: 523.9862060546875 test_loss:593.4063720703125\n",
      "62/3000 train_loss: 535.1049194335938 test_loss:584.505615234375\n",
      "63/3000 train_loss: 529.836669921875 test_loss:586.2490844726562\n",
      "64/3000 train_loss: 558.9600830078125 test_loss:583.7587280273438\n",
      "65/3000 train_loss: 503.14398193359375 test_loss:570.9696655273438\n",
      "66/3000 train_loss: 510.8520812988281 test_loss:577.762939453125\n",
      "67/3000 train_loss: 511.8758544921875 test_loss:576.9295654296875\n",
      "68/3000 train_loss: 540.210205078125 test_loss:569.6719360351562\n",
      "69/3000 train_loss: 495.4766845703125 test_loss:577.9961547851562\n",
      "70/3000 train_loss: 499.0189514160156 test_loss:567.35693359375\n",
      "71/3000 train_loss: 487.97381591796875 test_loss:579.1047973632812\n",
      "72/3000 train_loss: 502.9911804199219 test_loss:573.1647338867188\n",
      "73/3000 train_loss: 488.1622619628906 test_loss:557.7134399414062\n",
      "74/3000 train_loss: 499.4248352050781 test_loss:565.8558349609375\n",
      "75/3000 train_loss: 483.2732849121094 test_loss:557.14208984375\n",
      "76/3000 train_loss: 482.1043701171875 test_loss:561.7417602539062\n",
      "77/3000 train_loss: 503.4194641113281 test_loss:552.9781494140625\n",
      "78/3000 train_loss: 495.1441345214844 test_loss:558.802734375\n",
      "79/3000 train_loss: 477.72503662109375 test_loss:542.352294921875\n",
      "80/3000 train_loss: 523.0165405273438 test_loss:555.1292724609375\n",
      "81/3000 train_loss: 486.60223388671875 test_loss:558.839599609375\n",
      "82/3000 train_loss: 487.3929748535156 test_loss:536.8489990234375\n",
      "83/3000 train_loss: 485.92803955078125 test_loss:541.5509033203125\n",
      "84/3000 train_loss: 503.9759521484375 test_loss:540.0509033203125\n",
      "85/3000 train_loss: 489.84942626953125 test_loss:537.4583740234375\n",
      "86/3000 train_loss: 498.65045166015625 test_loss:532.6063842773438\n",
      "87/3000 train_loss: 500.7442932128906 test_loss:528.8360595703125\n",
      "88/3000 train_loss: 473.0827941894531 test_loss:535.328369140625\n",
      "89/3000 train_loss: 475.3080139160156 test_loss:525.664306640625\n",
      "90/3000 train_loss: 467.47259521484375 test_loss:524.804931640625\n",
      "91/3000 train_loss: 467.7712097167969 test_loss:516.6082153320312\n",
      "92/3000 train_loss: 462.65533447265625 test_loss:515.7207641601562\n",
      "93/3000 train_loss: 473.79046630859375 test_loss:513.4188842773438\n",
      "94/3000 train_loss: 487.162109375 test_loss:514.1243896484375\n",
      "95/3000 train_loss: 478.7014465332031 test_loss:542.2115478515625\n",
      "96/3000 train_loss: 455.4970397949219 test_loss:508.1441955566406\n",
      "97/3000 train_loss: 459.5665283203125 test_loss:509.49017333984375\n",
      "98/3000 train_loss: 462.6269226074219 test_loss:543.3782958984375\n",
      "99/3000 train_loss: 470.6427917480469 test_loss:511.27099609375\n",
      "100/3000 train_loss: 467.0622863769531 test_loss:506.2283630371094\n",
      "101/3000 train_loss: 469.8185119628906 test_loss:503.378173828125\n",
      "102/3000 train_loss: 480.98187255859375 test_loss:500.0566711425781\n",
      "103/3000 train_loss: 463.5737609863281 test_loss:498.6846618652344\n",
      "104/3000 train_loss: 433.3172912597656 test_loss:487.65252685546875\n",
      "105/3000 train_loss: 461.8903503417969 test_loss:505.90576171875\n",
      "106/3000 train_loss: 434.30401611328125 test_loss:490.66632080078125\n",
      "107/3000 train_loss: 431.0611877441406 test_loss:482.95513916015625\n",
      "108/3000 train_loss: 421.5041809082031 test_loss:482.68499755859375\n",
      "109/3000 train_loss: 464.6455078125 test_loss:475.3490295410156\n",
      "110/3000 train_loss: 439.5537414550781 test_loss:478.869873046875\n",
      "111/3000 train_loss: 424.5635986328125 test_loss:472.16046142578125\n",
      "112/3000 train_loss: 423.2059326171875 test_loss:479.03753662109375\n",
      "113/3000 train_loss: 439.9962463378906 test_loss:481.1619873046875\n",
      "114/3000 train_loss: 439.04937744140625 test_loss:472.36724853515625\n",
      "115/3000 train_loss: 426.02264404296875 test_loss:478.26031494140625\n",
      "116/3000 train_loss: 428.7197570800781 test_loss:488.3197021484375\n",
      "117/3000 train_loss: 439.2025451660156 test_loss:470.2420959472656\n",
      "118/3000 train_loss: 425.26708984375 test_loss:475.33270263671875\n",
      "119/3000 train_loss: 427.93115234375 test_loss:472.26544189453125\n",
      "120/3000 train_loss: 441.931640625 test_loss:456.82757568359375\n",
      "121/3000 train_loss: 413.9905090332031 test_loss:452.18963623046875\n",
      "122/3000 train_loss: 431.4214172363281 test_loss:472.40673828125\n",
      "123/3000 train_loss: 399.06268310546875 test_loss:449.685302734375\n",
      "124/3000 train_loss: 401.9561767578125 test_loss:468.7691955566406\n",
      "125/3000 train_loss: 392.1375732421875 test_loss:463.7376708984375\n",
      "126/3000 train_loss: 407.88995361328125 test_loss:456.76458740234375\n",
      "127/3000 train_loss: 429.17132568359375 test_loss:460.5260009765625\n",
      "128/3000 train_loss: 388.44415283203125 test_loss:455.44512939453125\n",
      "129/3000 train_loss: 421.9698791503906 test_loss:447.8994445800781\n",
      "130/3000 train_loss: 388.0492248535156 test_loss:446.89544677734375\n",
      "131/3000 train_loss: 394.38238525390625 test_loss:474.5824890136719\n",
      "132/3000 train_loss: 397.55096435546875 test_loss:440.40850830078125\n",
      "133/3000 train_loss: 402.6453857421875 test_loss:441.0281677246094\n",
      "134/3000 train_loss: 390.7709045410156 test_loss:440.040283203125\n",
      "135/3000 train_loss: 440.75714111328125 test_loss:440.20849609375\n",
      "136/3000 train_loss: 381.22320556640625 test_loss:443.66265869140625\n",
      "137/3000 train_loss: 388.8685607910156 test_loss:454.1011657714844\n",
      "138/3000 train_loss: 377.475341796875 test_loss:445.6916198730469\n",
      "139/3000 train_loss: 369.22760009765625 test_loss:435.6417236328125\n",
      "140/3000 train_loss: 381.8705139160156 test_loss:437.1334533691406\n",
      "141/3000 train_loss: 388.1061096191406 test_loss:431.84722900390625\n",
      "142/3000 train_loss: 370.0720520019531 test_loss:426.3446044921875\n",
      "143/3000 train_loss: 386.7655334472656 test_loss:458.51019287109375\n",
      "144/3000 train_loss: 362.49932861328125 test_loss:423.4554138183594\n",
      "145/3000 train_loss: 411.6304016113281 test_loss:425.50494384765625\n",
      "146/3000 train_loss: 395.3146057128906 test_loss:431.4427490234375\n",
      "147/3000 train_loss: 382.470947265625 test_loss:424.13848876953125\n",
      "148/3000 train_loss: 373.9122619628906 test_loss:431.76080322265625\n",
      "149/3000 train_loss: 396.14044189453125 test_loss:427.162109375\n",
      "150/3000 train_loss: 388.8174743652344 test_loss:414.50482177734375\n",
      "151/3000 train_loss: 368.3992004394531 test_loss:411.28839111328125\n",
      "152/3000 train_loss: 377.8559265136719 test_loss:433.28887939453125\n",
      "153/3000 train_loss: 368.1309814453125 test_loss:415.04791259765625\n",
      "154/3000 train_loss: 362.6406555175781 test_loss:408.95220947265625\n",
      "155/3000 train_loss: 359.3446960449219 test_loss:410.2395324707031\n",
      "156/3000 train_loss: 367.713623046875 test_loss:408.292236328125\n",
      "157/3000 train_loss: 356.8797607421875 test_loss:406.14306640625\n",
      "158/3000 train_loss: 358.59906005859375 test_loss:413.00445556640625\n",
      "159/3000 train_loss: 349.4104919433594 test_loss:418.1915588378906\n",
      "160/3000 train_loss: 351.0699768066406 test_loss:402.83709716796875\n",
      "161/3000 train_loss: 363.8714294433594 test_loss:404.45062255859375\n",
      "162/3000 train_loss: 334.9041748046875 test_loss:400.9781494140625\n",
      "163/3000 train_loss: 362.10552978515625 test_loss:420.4748229980469\n",
      "164/3000 train_loss: 348.87078857421875 test_loss:402.3006591796875\n",
      "165/3000 train_loss: 362.0674133300781 test_loss:405.4212951660156\n",
      "166/3000 train_loss: 348.13323974609375 test_loss:402.045166015625\n",
      "167/3000 train_loss: 335.46112060546875 test_loss:400.4809875488281\n",
      "168/3000 train_loss: 361.0074462890625 test_loss:394.2938232421875\n",
      "169/3000 train_loss: 336.4247741699219 test_loss:392.5255126953125\n",
      "170/3000 train_loss: 322.1230163574219 test_loss:390.84527587890625\n",
      "171/3000 train_loss: 327.79327392578125 test_loss:386.08404541015625\n",
      "172/3000 train_loss: 361.7801513671875 test_loss:388.72308349609375\n",
      "173/3000 train_loss: 357.0408935546875 test_loss:402.8276062011719\n",
      "174/3000 train_loss: 333.7742004394531 test_loss:391.44439697265625\n",
      "175/3000 train_loss: 354.6988830566406 test_loss:396.82623291015625\n",
      "176/3000 train_loss: 331.48388671875 test_loss:384.027587890625\n",
      "177/3000 train_loss: 337.26434326171875 test_loss:380.18023681640625\n",
      "178/3000 train_loss: 352.6788330078125 test_loss:391.03594970703125\n",
      "179/3000 train_loss: 328.71771240234375 test_loss:383.3025817871094\n",
      "180/3000 train_loss: 362.00372314453125 test_loss:381.11419677734375\n",
      "181/3000 train_loss: 330.9237365722656 test_loss:386.417724609375\n",
      "182/3000 train_loss: 335.5042419433594 test_loss:380.2743225097656\n",
      "183/3000 train_loss: 337.5924072265625 test_loss:385.718505859375\n",
      "184/3000 train_loss: 314.5773620605469 test_loss:379.63519287109375\n",
      "185/3000 train_loss: 328.2197265625 test_loss:383.7617492675781\n",
      "186/3000 train_loss: 327.5157775878906 test_loss:381.54266357421875\n",
      "187/3000 train_loss: 316.5240478515625 test_loss:380.044677734375\n",
      "188/3000 train_loss: 317.1092834472656 test_loss:371.2397766113281\n",
      "189/3000 train_loss: 314.7061767578125 test_loss:376.45880126953125\n",
      "190/3000 train_loss: 335.5743713378906 test_loss:370.65545654296875\n",
      "191/3000 train_loss: 303.8351745605469 test_loss:368.5602722167969\n",
      "192/3000 train_loss: 313.62347412109375 test_loss:371.35137939453125\n",
      "193/3000 train_loss: 343.06793212890625 test_loss:364.48260498046875\n",
      "194/3000 train_loss: 328.96722412109375 test_loss:361.3422546386719\n",
      "195/3000 train_loss: 303.44488525390625 test_loss:364.3390808105469\n",
      "196/3000 train_loss: 324.06103515625 test_loss:383.68994140625\n",
      "197/3000 train_loss: 321.8555908203125 test_loss:376.6529541015625\n",
      "198/3000 train_loss: 332.8263854980469 test_loss:373.762939453125\n",
      "199/3000 train_loss: 323.616455078125 test_loss:358.73419189453125\n",
      "200/3000 train_loss: 323.32220458984375 test_loss:358.2841796875\n",
      "201/3000 train_loss: 297.31011962890625 test_loss:353.90985107421875\n",
      "202/3000 train_loss: 327.0218200683594 test_loss:361.81341552734375\n",
      "203/3000 train_loss: 304.48248291015625 test_loss:353.3658142089844\n",
      "204/3000 train_loss: 298.1356201171875 test_loss:352.7244873046875\n",
      "205/3000 train_loss: 325.34747314453125 test_loss:356.3087158203125\n",
      "206/3000 train_loss: 303.7238464355469 test_loss:359.3048095703125\n",
      "207/3000 train_loss: 303.242431640625 test_loss:347.7606201171875\n",
      "208/3000 train_loss: 298.946533203125 test_loss:348.3202819824219\n",
      "209/3000 train_loss: 291.2832946777344 test_loss:348.27960205078125\n",
      "210/3000 train_loss: 299.9377746582031 test_loss:347.007568359375\n",
      "211/3000 train_loss: 305.2357177734375 test_loss:343.80743408203125\n",
      "212/3000 train_loss: 335.0057373046875 test_loss:347.3203125\n",
      "213/3000 train_loss: 277.7268981933594 test_loss:337.56744384765625\n",
      "214/3000 train_loss: 302.2912292480469 test_loss:338.5204162597656\n",
      "215/3000 train_loss: 306.0541076660156 test_loss:332.4183349609375\n",
      "216/3000 train_loss: 308.93853759765625 test_loss:343.49078369140625\n",
      "217/3000 train_loss: 289.49835205078125 test_loss:330.873779296875\n",
      "218/3000 train_loss: 312.8763732910156 test_loss:341.7640686035156\n",
      "219/3000 train_loss: 297.2798767089844 test_loss:337.57171630859375\n",
      "220/3000 train_loss: 300.76025390625 test_loss:333.6929016113281\n",
      "221/3000 train_loss: 307.77288818359375 test_loss:334.4332275390625\n",
      "222/3000 train_loss: 274.3500671386719 test_loss:330.70550537109375\n",
      "223/3000 train_loss: 290.8421630859375 test_loss:319.5579833984375\n",
      "224/3000 train_loss: 270.1788330078125 test_loss:325.6735534667969\n",
      "225/3000 train_loss: 293.10992431640625 test_loss:347.0504150390625\n",
      "226/3000 train_loss: 272.8240661621094 test_loss:327.5679931640625\n",
      "227/3000 train_loss: 261.23223876953125 test_loss:328.13531494140625\n",
      "228/3000 train_loss: 288.9383239746094 test_loss:333.33697509765625\n",
      "229/3000 train_loss: 288.7881774902344 test_loss:327.15167236328125\n",
      "230/3000 train_loss: 273.0556945800781 test_loss:321.86480712890625\n",
      "231/3000 train_loss: 268.5169677734375 test_loss:318.5078430175781\n",
      "232/3000 train_loss: 332.8863220214844 test_loss:324.22161865234375\n",
      "233/3000 train_loss: 275.625732421875 test_loss:316.73321533203125\n",
      "234/3000 train_loss: 280.9331359863281 test_loss:314.1619873046875\n",
      "235/3000 train_loss: 270.610107421875 test_loss:315.3914794921875\n",
      "236/3000 train_loss: 271.110107421875 test_loss:317.9840087890625\n",
      "237/3000 train_loss: 281.18902587890625 test_loss:311.5487365722656\n",
      "238/3000 train_loss: 279.0716247558594 test_loss:313.9328918457031\n",
      "239/3000 train_loss: 272.2454833984375 test_loss:306.44049072265625\n",
      "240/3000 train_loss: 272.1735534667969 test_loss:306.183349609375\n",
      "241/3000 train_loss: 267.822998046875 test_loss:304.570556640625\n",
      "242/3000 train_loss: 255.27467346191406 test_loss:302.25543212890625\n",
      "243/3000 train_loss: 279.3694763183594 test_loss:308.9462585449219\n",
      "244/3000 train_loss: 261.2799987792969 test_loss:304.2806396484375\n",
      "245/3000 train_loss: 270.13641357421875 test_loss:303.4406433105469\n",
      "246/3000 train_loss: 270.6397705078125 test_loss:302.7941589355469\n",
      "247/3000 train_loss: 259.0774230957031 test_loss:298.40185546875\n",
      "248/3000 train_loss: 269.279052734375 test_loss:301.73797607421875\n",
      "249/3000 train_loss: 256.6803283691406 test_loss:299.958740234375\n",
      "250/3000 train_loss: 258.8782653808594 test_loss:294.9314880371094\n",
      "251/3000 train_loss: 272.2306213378906 test_loss:301.29974365234375\n",
      "252/3000 train_loss: 248.76266479492188 test_loss:299.3800048828125\n",
      "253/3000 train_loss: 259.3087158203125 test_loss:299.70037841796875\n",
      "254/3000 train_loss: 237.4544219970703 test_loss:299.9600524902344\n",
      "255/3000 train_loss: 247.96041870117188 test_loss:299.1075744628906\n",
      "256/3000 train_loss: 258.3519287109375 test_loss:296.2811584472656\n",
      "257/3000 train_loss: 259.4116516113281 test_loss:303.55194091796875\n",
      "258/3000 train_loss: 255.4180450439453 test_loss:327.8646545410156\n",
      "259/3000 train_loss: 237.5957794189453 test_loss:303.41619873046875\n",
      "260/3000 train_loss: 259.3623046875 test_loss:312.95147705078125\n",
      "261/3000 train_loss: 259.6530456542969 test_loss:294.38037109375\n",
      "262/3000 train_loss: 243.1649627685547 test_loss:286.3674621582031\n",
      "263/3000 train_loss: 258.9400329589844 test_loss:295.2859191894531\n",
      "264/3000 train_loss: 240.47215270996094 test_loss:303.201171875\n",
      "265/3000 train_loss: 250.7476348876953 test_loss:307.9589538574219\n",
      "266/3000 train_loss: 243.77622985839844 test_loss:295.1734313964844\n",
      "267/3000 train_loss: 231.32046508789062 test_loss:291.7247314453125\n",
      "268/3000 train_loss: 232.01910400390625 test_loss:290.080810546875\n",
      "269/3000 train_loss: 248.6608123779297 test_loss:284.9485778808594\n",
      "270/3000 train_loss: 247.68092346191406 test_loss:284.02587890625\n",
      "271/3000 train_loss: 230.8772430419922 test_loss:284.62664794921875\n",
      "272/3000 train_loss: 245.6826171875 test_loss:281.6009216308594\n",
      "273/3000 train_loss: 255.3491668701172 test_loss:278.8108215332031\n",
      "274/3000 train_loss: 241.95553588867188 test_loss:284.0924377441406\n",
      "275/3000 train_loss: 234.2497100830078 test_loss:276.01141357421875\n",
      "276/3000 train_loss: 253.6239471435547 test_loss:280.80169677734375\n",
      "277/3000 train_loss: 237.7841796875 test_loss:276.003662109375\n",
      "278/3000 train_loss: 236.34283447265625 test_loss:278.1427307128906\n",
      "279/3000 train_loss: 248.5702362060547 test_loss:288.1900634765625\n",
      "280/3000 train_loss: 217.78993225097656 test_loss:277.9506530761719\n",
      "281/3000 train_loss: 244.99072265625 test_loss:273.86029052734375\n",
      "282/3000 train_loss: 228.5827178955078 test_loss:276.0768737792969\n",
      "283/3000 train_loss: 243.40914916992188 test_loss:271.68170166015625\n",
      "284/3000 train_loss: 229.8476104736328 test_loss:271.68121337890625\n",
      "285/3000 train_loss: 238.26693725585938 test_loss:270.4097900390625\n",
      "286/3000 train_loss: 236.8348846435547 test_loss:268.37310791015625\n",
      "287/3000 train_loss: 227.10980224609375 test_loss:268.8973388671875\n",
      "288/3000 train_loss: 235.55047607421875 test_loss:263.9049987792969\n",
      "289/3000 train_loss: 221.4420623779297 test_loss:259.8236389160156\n",
      "290/3000 train_loss: 220.20118713378906 test_loss:260.2298583984375\n",
      "291/3000 train_loss: 211.7027587890625 test_loss:259.8106689453125\n",
      "292/3000 train_loss: 227.76968383789062 test_loss:260.3200378417969\n",
      "293/3000 train_loss: 208.26023864746094 test_loss:261.38800048828125\n",
      "294/3000 train_loss: 230.89076232910156 test_loss:258.41864013671875\n",
      "295/3000 train_loss: 209.74349975585938 test_loss:260.2176513671875\n",
      "296/3000 train_loss: 210.8634490966797 test_loss:261.0282897949219\n",
      "297/3000 train_loss: 202.31210327148438 test_loss:260.21630859375\n",
      "298/3000 train_loss: 218.32232666015625 test_loss:255.2380828857422\n",
      "299/3000 train_loss: 208.21060180664062 test_loss:251.87197875976562\n",
      "300/3000 train_loss: 201.9607696533203 test_loss:253.46920776367188\n",
      "301/3000 train_loss: 230.605224609375 test_loss:255.39468383789062\n",
      "302/3000 train_loss: 202.51783752441406 test_loss:252.73910522460938\n",
      "303/3000 train_loss: 215.50575256347656 test_loss:249.89398193359375\n",
      "304/3000 train_loss: 205.5967254638672 test_loss:255.49807739257812\n",
      "305/3000 train_loss: 201.6190185546875 test_loss:247.79110717773438\n",
      "306/3000 train_loss: 200.82615661621094 test_loss:252.36505126953125\n",
      "307/3000 train_loss: 226.38314819335938 test_loss:251.1275177001953\n",
      "308/3000 train_loss: 203.93246459960938 test_loss:249.7078857421875\n",
      "309/3000 train_loss: 205.95101928710938 test_loss:244.8955078125\n",
      "310/3000 train_loss: 213.53079223632812 test_loss:242.65194702148438\n",
      "311/3000 train_loss: 202.60098266601562 test_loss:244.26182556152344\n",
      "312/3000 train_loss: 196.1466827392578 test_loss:245.65652465820312\n",
      "313/3000 train_loss: 217.6324920654297 test_loss:245.6569366455078\n",
      "314/3000 train_loss: 192.4839630126953 test_loss:240.52590942382812\n",
      "315/3000 train_loss: 196.48770141601562 test_loss:236.22970581054688\n",
      "316/3000 train_loss: 203.51480102539062 test_loss:243.65023803710938\n",
      "317/3000 train_loss: 214.99917602539062 test_loss:239.49627685546875\n",
      "318/3000 train_loss: 189.7676544189453 test_loss:238.4392852783203\n",
      "319/3000 train_loss: 201.77142333984375 test_loss:242.353515625\n",
      "320/3000 train_loss: 204.7910919189453 test_loss:240.29257202148438\n",
      "321/3000 train_loss: 177.0806427001953 test_loss:236.79843139648438\n",
      "322/3000 train_loss: 194.07046508789062 test_loss:250.53526306152344\n",
      "323/3000 train_loss: 202.2767791748047 test_loss:244.58792114257812\n",
      "324/3000 train_loss: 206.429931640625 test_loss:235.83627319335938\n",
      "325/3000 train_loss: 200.5989532470703 test_loss:236.8179931640625\n",
      "326/3000 train_loss: 182.26588439941406 test_loss:230.87319946289062\n",
      "327/3000 train_loss: 188.53326416015625 test_loss:232.32769775390625\n",
      "328/3000 train_loss: 182.9681396484375 test_loss:233.60133361816406\n",
      "329/3000 train_loss: 185.08920288085938 test_loss:227.30715942382812\n",
      "330/3000 train_loss: 189.2601776123047 test_loss:229.24154663085938\n",
      "331/3000 train_loss: 216.29197692871094 test_loss:233.154296875\n",
      "332/3000 train_loss: 186.7550506591797 test_loss:226.25454711914062\n",
      "333/3000 train_loss: 181.29359436035156 test_loss:233.42111206054688\n",
      "334/3000 train_loss: 195.85531616210938 test_loss:228.83448791503906\n",
      "335/3000 train_loss: 184.4320831298828 test_loss:227.7364501953125\n",
      "336/3000 train_loss: 200.52928161621094 test_loss:228.52301025390625\n",
      "337/3000 train_loss: 185.6981658935547 test_loss:225.18560791015625\n",
      "338/3000 train_loss: 179.70428466796875 test_loss:224.41592407226562\n",
      "339/3000 train_loss: 194.79669189453125 test_loss:224.17242431640625\n",
      "340/3000 train_loss: 212.8377685546875 test_loss:228.24801635742188\n",
      "341/3000 train_loss: 201.9130401611328 test_loss:226.85606384277344\n",
      "342/3000 train_loss: 185.6272430419922 test_loss:229.1627197265625\n",
      "343/3000 train_loss: 187.7965087890625 test_loss:230.5039520263672\n",
      "344/3000 train_loss: 205.63076782226562 test_loss:223.61709594726562\n",
      "345/3000 train_loss: 182.5880584716797 test_loss:222.5863037109375\n",
      "346/3000 train_loss: 190.39634704589844 test_loss:224.608642578125\n",
      "347/3000 train_loss: 194.85107421875 test_loss:223.98382568359375\n",
      "348/3000 train_loss: 189.94456481933594 test_loss:219.2132568359375\n",
      "349/3000 train_loss: 188.74954223632812 test_loss:215.42520141601562\n",
      "350/3000 train_loss: 186.760986328125 test_loss:221.40414428710938\n",
      "351/3000 train_loss: 174.55111694335938 test_loss:226.22720336914062\n",
      "352/3000 train_loss: 192.137939453125 test_loss:232.44619750976562\n",
      "353/3000 train_loss: 189.01593017578125 test_loss:220.626953125\n",
      "354/3000 train_loss: 196.77874755859375 test_loss:223.2803955078125\n",
      "355/3000 train_loss: 186.3040008544922 test_loss:223.81686401367188\n",
      "356/3000 train_loss: 176.9646759033203 test_loss:221.97555541992188\n",
      "357/3000 train_loss: 190.8674774169922 test_loss:225.45166015625\n",
      "358/3000 train_loss: 186.6863250732422 test_loss:218.90850830078125\n",
      "359/3000 train_loss: 184.72116088867188 test_loss:215.69076538085938\n",
      "360/3000 train_loss: 181.5356903076172 test_loss:215.7640380859375\n",
      "361/3000 train_loss: 180.2189178466797 test_loss:215.04708862304688\n",
      "362/3000 train_loss: 184.17381286621094 test_loss:213.0303497314453\n",
      "363/3000 train_loss: 170.75709533691406 test_loss:214.81253051757812\n",
      "364/3000 train_loss: 191.01133728027344 test_loss:225.40505981445312\n",
      "365/3000 train_loss: 197.87646484375 test_loss:212.63446044921875\n",
      "366/3000 train_loss: 188.01280212402344 test_loss:211.35903930664062\n",
      "367/3000 train_loss: 179.98532104492188 test_loss:205.70079040527344\n",
      "368/3000 train_loss: 184.6885986328125 test_loss:209.4119110107422\n",
      "369/3000 train_loss: 176.58497619628906 test_loss:213.9147186279297\n",
      "370/3000 train_loss: 164.81997680664062 test_loss:209.29443359375\n",
      "371/3000 train_loss: 175.2039337158203 test_loss:213.91116333007812\n",
      "372/3000 train_loss: 171.3771209716797 test_loss:215.74111938476562\n",
      "373/3000 train_loss: 192.57945251464844 test_loss:214.34896850585938\n",
      "374/3000 train_loss: 175.88731384277344 test_loss:217.63270568847656\n",
      "375/3000 train_loss: 184.98135375976562 test_loss:208.40582275390625\n",
      "376/3000 train_loss: 177.27345275878906 test_loss:203.4344482421875\n",
      "377/3000 train_loss: 184.67686462402344 test_loss:203.7718505859375\n",
      "378/3000 train_loss: 188.7122039794922 test_loss:204.65570068359375\n",
      "379/3000 train_loss: 167.5283203125 test_loss:208.5179443359375\n",
      "380/3000 train_loss: 168.34620666503906 test_loss:210.55894470214844\n",
      "381/3000 train_loss: 178.82215881347656 test_loss:208.44139099121094\n",
      "382/3000 train_loss: 176.8214111328125 test_loss:213.54901123046875\n",
      "383/3000 train_loss: 186.07850646972656 test_loss:215.48019409179688\n",
      "384/3000 train_loss: 180.27017211914062 test_loss:207.09024047851562\n",
      "385/3000 train_loss: 168.30368041992188 test_loss:207.97254943847656\n",
      "386/3000 train_loss: 179.3612060546875 test_loss:203.93411254882812\n",
      "387/3000 train_loss: 183.2432861328125 test_loss:208.2160186767578\n",
      "388/3000 train_loss: 166.3172607421875 test_loss:202.35958862304688\n",
      "389/3000 train_loss: 158.97618103027344 test_loss:201.63882446289062\n",
      "390/3000 train_loss: 167.16445922851562 test_loss:205.08087158203125\n",
      "391/3000 train_loss: 183.18112182617188 test_loss:207.37222290039062\n",
      "392/3000 train_loss: 162.31666564941406 test_loss:207.0842742919922\n",
      "393/3000 train_loss: 159.31446838378906 test_loss:204.1153564453125\n",
      "394/3000 train_loss: 181.552978515625 test_loss:202.680419921875\n",
      "395/3000 train_loss: 178.4115447998047 test_loss:197.01565551757812\n",
      "396/3000 train_loss: 164.19308471679688 test_loss:200.559814453125\n",
      "397/3000 train_loss: 169.30296325683594 test_loss:201.81985473632812\n",
      "398/3000 train_loss: 169.3314666748047 test_loss:198.28643798828125\n",
      "399/3000 train_loss: 168.13510131835938 test_loss:194.6456298828125\n",
      "400/3000 train_loss: 158.35394287109375 test_loss:202.3399658203125\n",
      "401/3000 train_loss: 154.8212890625 test_loss:201.06680297851562\n",
      "402/3000 train_loss: 170.46798706054688 test_loss:212.6156768798828\n",
      "403/3000 train_loss: 175.45774841308594 test_loss:202.19412231445312\n",
      "404/3000 train_loss: 169.50320434570312 test_loss:192.85446166992188\n",
      "405/3000 train_loss: 176.64642333984375 test_loss:200.4564971923828\n",
      "406/3000 train_loss: 165.4837188720703 test_loss:202.5359344482422\n",
      "407/3000 train_loss: 198.45323181152344 test_loss:209.23757934570312\n",
      "408/3000 train_loss: 186.27578735351562 test_loss:201.15960693359375\n",
      "409/3000 train_loss: 180.94447326660156 test_loss:200.4909210205078\n",
      "410/3000 train_loss: 150.72921752929688 test_loss:206.23133850097656\n",
      "411/3000 train_loss: 166.662353515625 test_loss:210.69891357421875\n",
      "412/3000 train_loss: 177.64944458007812 test_loss:201.382080078125\n",
      "413/3000 train_loss: 178.67210388183594 test_loss:199.96397399902344\n",
      "414/3000 train_loss: 165.67808532714844 test_loss:193.15634155273438\n",
      "415/3000 train_loss: 176.8070526123047 test_loss:193.56480407714844\n",
      "416/3000 train_loss: 169.1371307373047 test_loss:198.27413940429688\n",
      "417/3000 train_loss: 158.77525329589844 test_loss:200.38394165039062\n",
      "418/3000 train_loss: 160.14170837402344 test_loss:203.1123504638672\n",
      "419/3000 train_loss: 154.7899627685547 test_loss:203.55914306640625\n",
      "420/3000 train_loss: 168.6116485595703 test_loss:201.38267517089844\n",
      "421/3000 train_loss: 162.83494567871094 test_loss:196.30172729492188\n",
      "422/3000 train_loss: 166.326904296875 test_loss:200.99386596679688\n",
      "423/3000 train_loss: 171.15599060058594 test_loss:208.56039428710938\n",
      "424/3000 train_loss: 171.6222686767578 test_loss:201.6162109375\n",
      "425/3000 train_loss: 157.75613403320312 test_loss:196.4279327392578\n",
      "426/3000 train_loss: 172.7174530029297 test_loss:192.00552368164062\n",
      "427/3000 train_loss: 167.69483947753906 test_loss:199.46432495117188\n",
      "428/3000 train_loss: 174.13253784179688 test_loss:192.65057373046875\n",
      "429/3000 train_loss: 165.70651245117188 test_loss:195.2555694580078\n",
      "430/3000 train_loss: 164.59783935546875 test_loss:195.84112548828125\n",
      "431/3000 train_loss: 162.73193359375 test_loss:192.3819580078125\n",
      "432/3000 train_loss: 168.21971130371094 test_loss:194.48367309570312\n",
      "433/3000 train_loss: 157.83773803710938 test_loss:198.33914184570312\n",
      "434/3000 train_loss: 162.6924285888672 test_loss:189.56964111328125\n",
      "435/3000 train_loss: 160.4625244140625 test_loss:193.90985107421875\n",
      "436/3000 train_loss: 165.0893096923828 test_loss:190.0758819580078\n",
      "437/3000 train_loss: 173.7580108642578 test_loss:190.03836059570312\n",
      "438/3000 train_loss: 158.14663696289062 test_loss:190.92166137695312\n",
      "439/3000 train_loss: 162.12416076660156 test_loss:191.44528198242188\n",
      "440/3000 train_loss: 157.0489044189453 test_loss:193.05487060546875\n",
      "441/3000 train_loss: 160.00918579101562 test_loss:201.25296020507812\n",
      "442/3000 train_loss: 147.90338134765625 test_loss:191.9158935546875\n",
      "443/3000 train_loss: 160.871826171875 test_loss:190.633544921875\n",
      "444/3000 train_loss: 165.25088500976562 test_loss:195.39724731445312\n",
      "445/3000 train_loss: 165.83541870117188 test_loss:193.33441162109375\n",
      "446/3000 train_loss: 171.6546630859375 test_loss:189.3216094970703\n",
      "447/3000 train_loss: 167.92263793945312 test_loss:188.0984649658203\n",
      "448/3000 train_loss: 188.92613220214844 test_loss:187.84523010253906\n",
      "449/3000 train_loss: 169.9593048095703 test_loss:187.64857482910156\n",
      "450/3000 train_loss: 162.82342529296875 test_loss:183.06375122070312\n",
      "451/3000 train_loss: 157.414306640625 test_loss:186.7872314453125\n",
      "452/3000 train_loss: 165.6815643310547 test_loss:186.08299255371094\n",
      "453/3000 train_loss: 158.3677978515625 test_loss:189.6636962890625\n",
      "454/3000 train_loss: 155.17953491210938 test_loss:190.29544067382812\n",
      "455/3000 train_loss: 167.00509643554688 test_loss:189.71066284179688\n",
      "456/3000 train_loss: 165.2621612548828 test_loss:188.96554565429688\n",
      "457/3000 train_loss: 171.4960479736328 test_loss:193.50112915039062\n",
      "458/3000 train_loss: 153.6382293701172 test_loss:189.54885864257812\n",
      "459/3000 train_loss: 157.78404235839844 test_loss:184.7337646484375\n",
      "460/3000 train_loss: 152.8435516357422 test_loss:191.73434448242188\n",
      "461/3000 train_loss: 156.41900634765625 test_loss:187.1944580078125\n",
      "462/3000 train_loss: 146.0161590576172 test_loss:183.77061462402344\n",
      "463/3000 train_loss: 146.84458923339844 test_loss:181.634521484375\n",
      "464/3000 train_loss: 160.99508666992188 test_loss:182.02081298828125\n",
      "465/3000 train_loss: 165.5028076171875 test_loss:182.47193908691406\n",
      "466/3000 train_loss: 154.90916442871094 test_loss:184.6005859375\n",
      "467/3000 train_loss: 151.65797424316406 test_loss:185.94985961914062\n",
      "468/3000 train_loss: 150.37359619140625 test_loss:193.49032592773438\n",
      "469/3000 train_loss: 164.6081085205078 test_loss:188.9507598876953\n",
      "470/3000 train_loss: 141.44900512695312 test_loss:180.20205688476562\n",
      "471/3000 train_loss: 156.36495971679688 test_loss:179.9864501953125\n",
      "472/3000 train_loss: 165.79515075683594 test_loss:177.28875732421875\n",
      "473/3000 train_loss: 160.04930114746094 test_loss:177.05337524414062\n",
      "474/3000 train_loss: 167.4453582763672 test_loss:177.30714416503906\n",
      "475/3000 train_loss: 151.286376953125 test_loss:178.83424377441406\n",
      "476/3000 train_loss: 158.32154846191406 test_loss:183.9124755859375\n",
      "477/3000 train_loss: 178.035888671875 test_loss:194.3570556640625\n",
      "478/3000 train_loss: 171.60983276367188 test_loss:178.58151245117188\n",
      "479/3000 train_loss: 161.60491943359375 test_loss:177.69839477539062\n",
      "480/3000 train_loss: 152.14527893066406 test_loss:178.3387451171875\n",
      "481/3000 train_loss: 144.71778869628906 test_loss:179.62030029296875\n",
      "482/3000 train_loss: 154.91189575195312 test_loss:184.8188934326172\n",
      "483/3000 train_loss: 148.3240509033203 test_loss:178.91372680664062\n",
      "484/3000 train_loss: 156.3988037109375 test_loss:182.30491638183594\n",
      "485/3000 train_loss: 163.43798828125 test_loss:172.91217041015625\n",
      "486/3000 train_loss: 162.37588500976562 test_loss:174.78179931640625\n",
      "487/3000 train_loss: 140.071044921875 test_loss:179.656005859375\n",
      "488/3000 train_loss: 148.69320678710938 test_loss:182.35951232910156\n",
      "489/3000 train_loss: 144.47958374023438 test_loss:186.1181640625\n",
      "490/3000 train_loss: 133.3600311279297 test_loss:180.00479125976562\n",
      "491/3000 train_loss: 143.76678466796875 test_loss:179.14981079101562\n",
      "492/3000 train_loss: 151.13792419433594 test_loss:183.4681396484375\n",
      "493/3000 train_loss: 149.18821716308594 test_loss:174.08975219726562\n",
      "494/3000 train_loss: 141.33847045898438 test_loss:170.89663696289062\n",
      "495/3000 train_loss: 162.1809539794922 test_loss:175.78721618652344\n",
      "496/3000 train_loss: 179.1937713623047 test_loss:175.99240112304688\n",
      "497/3000 train_loss: 143.9091339111328 test_loss:172.6805419921875\n",
      "498/3000 train_loss: 159.13998413085938 test_loss:174.33253479003906\n",
      "499/3000 train_loss: 153.46995544433594 test_loss:180.26356506347656\n",
      "500/3000 train_loss: 143.6735382080078 test_loss:175.74429321289062\n",
      "501/3000 train_loss: 146.16734313964844 test_loss:174.79083251953125\n",
      "502/3000 train_loss: 145.7841796875 test_loss:175.56015014648438\n",
      "503/3000 train_loss: 151.33180236816406 test_loss:168.99107360839844\n",
      "504/3000 train_loss: 154.11920166015625 test_loss:169.79884338378906\n",
      "505/3000 train_loss: 144.08546447753906 test_loss:176.3501434326172\n",
      "506/3000 train_loss: 153.3838348388672 test_loss:179.43865966796875\n",
      "507/3000 train_loss: 146.35635375976562 test_loss:179.4561767578125\n",
      "508/3000 train_loss: 151.45631408691406 test_loss:170.57859802246094\n",
      "509/3000 train_loss: 157.32057189941406 test_loss:175.42401123046875\n",
      "510/3000 train_loss: 155.88653564453125 test_loss:173.97940063476562\n",
      "511/3000 train_loss: 144.9595947265625 test_loss:185.11537170410156\n",
      "512/3000 train_loss: 144.3934326171875 test_loss:169.07337951660156\n",
      "513/3000 train_loss: 140.74884033203125 test_loss:168.537841796875\n",
      "514/3000 train_loss: 135.5089111328125 test_loss:173.6916046142578\n",
      "515/3000 train_loss: 140.08680725097656 test_loss:169.9739990234375\n",
      "516/3000 train_loss: 142.7928924560547 test_loss:171.40658569335938\n",
      "517/3000 train_loss: 135.33172607421875 test_loss:175.68646240234375\n",
      "518/3000 train_loss: 135.04098510742188 test_loss:174.34365844726562\n",
      "519/3000 train_loss: 146.01565551757812 test_loss:178.61346435546875\n",
      "520/3000 train_loss: 139.64654541015625 test_loss:172.3688201904297\n",
      "521/3000 train_loss: 147.21096801757812 test_loss:167.8578338623047\n",
      "522/3000 train_loss: 140.8225860595703 test_loss:161.78546142578125\n",
      "523/3000 train_loss: 135.89781188964844 test_loss:171.12051391601562\n",
      "524/3000 train_loss: 147.85806274414062 test_loss:179.23797607421875\n",
      "525/3000 train_loss: 128.86502075195312 test_loss:169.73193359375\n",
      "526/3000 train_loss: 140.10316467285156 test_loss:168.44412231445312\n",
      "527/3000 train_loss: 135.24069213867188 test_loss:162.93511962890625\n",
      "528/3000 train_loss: 140.33058166503906 test_loss:165.8541259765625\n",
      "529/3000 train_loss: 136.20924377441406 test_loss:163.86068725585938\n",
      "530/3000 train_loss: 148.81768798828125 test_loss:159.85916137695312\n",
      "531/3000 train_loss: 140.22886657714844 test_loss:174.2963409423828\n",
      "532/3000 train_loss: 144.1758270263672 test_loss:165.75790405273438\n",
      "533/3000 train_loss: 132.3050537109375 test_loss:169.98190307617188\n",
      "534/3000 train_loss: 135.0950164794922 test_loss:161.6201629638672\n",
      "535/3000 train_loss: 133.11839294433594 test_loss:170.48631286621094\n",
      "536/3000 train_loss: 170.2103271484375 test_loss:166.35609436035156\n",
      "537/3000 train_loss: 148.20376586914062 test_loss:165.751953125\n",
      "538/3000 train_loss: 136.94436645507812 test_loss:166.4421844482422\n",
      "539/3000 train_loss: 125.93518829345703 test_loss:164.35411071777344\n",
      "540/3000 train_loss: 135.44454956054688 test_loss:168.3628387451172\n",
      "541/3000 train_loss: 165.4464111328125 test_loss:169.338134765625\n",
      "542/3000 train_loss: 132.66346740722656 test_loss:160.54150390625\n",
      "543/3000 train_loss: 125.61663818359375 test_loss:162.29278564453125\n",
      "544/3000 train_loss: 129.16891479492188 test_loss:161.89002990722656\n",
      "545/3000 train_loss: 139.47862243652344 test_loss:169.00979614257812\n",
      "546/3000 train_loss: 128.3826904296875 test_loss:160.6829376220703\n",
      "547/3000 train_loss: 129.6467742919922 test_loss:159.7100067138672\n",
      "548/3000 train_loss: 130.4372100830078 test_loss:162.72479248046875\n",
      "549/3000 train_loss: 134.5857391357422 test_loss:170.22665405273438\n",
      "550/3000 train_loss: 131.95578002929688 test_loss:169.85455322265625\n",
      "551/3000 train_loss: 135.11920166015625 test_loss:159.43634033203125\n",
      "552/3000 train_loss: 136.20779418945312 test_loss:163.62606811523438\n",
      "553/3000 train_loss: 132.31884765625 test_loss:160.20523071289062\n",
      "554/3000 train_loss: 143.22164916992188 test_loss:158.151611328125\n",
      "555/3000 train_loss: 139.1247100830078 test_loss:163.63174438476562\n",
      "556/3000 train_loss: 134.23487854003906 test_loss:166.06674194335938\n",
      "557/3000 train_loss: 131.54949951171875 test_loss:167.25479125976562\n",
      "558/3000 train_loss: 134.61854553222656 test_loss:172.389892578125\n",
      "559/3000 train_loss: 133.92970275878906 test_loss:161.4988250732422\n",
      "560/3000 train_loss: 136.40296936035156 test_loss:156.30776977539062\n",
      "561/3000 train_loss: 137.1492156982422 test_loss:157.33444213867188\n",
      "562/3000 train_loss: 138.83717346191406 test_loss:163.94020080566406\n",
      "563/3000 train_loss: 130.92552185058594 test_loss:163.2520294189453\n",
      "564/3000 train_loss: 120.72622680664062 test_loss:163.35848999023438\n",
      "565/3000 train_loss: 140.239501953125 test_loss:159.36257934570312\n",
      "566/3000 train_loss: 130.46871948242188 test_loss:152.80706787109375\n",
      "567/3000 train_loss: 126.47966766357422 test_loss:155.212158203125\n",
      "568/3000 train_loss: 131.6402130126953 test_loss:152.89590454101562\n",
      "569/3000 train_loss: 123.1484603881836 test_loss:154.91921997070312\n",
      "570/3000 train_loss: 140.7460174560547 test_loss:159.71951293945312\n",
      "571/3000 train_loss: 146.11236572265625 test_loss:155.8487091064453\n",
      "572/3000 train_loss: 128.8954620361328 test_loss:160.03125\n",
      "573/3000 train_loss: 171.48646545410156 test_loss:163.48667907714844\n",
      "574/3000 train_loss: 138.2628631591797 test_loss:153.63729858398438\n",
      "575/3000 train_loss: 121.98236846923828 test_loss:158.60350036621094\n",
      "576/3000 train_loss: 121.39167022705078 test_loss:162.0221710205078\n",
      "577/3000 train_loss: 129.234619140625 test_loss:154.7755584716797\n",
      "578/3000 train_loss: 126.76962280273438 test_loss:155.43173217773438\n",
      "579/3000 train_loss: 138.38128662109375 test_loss:165.52774047851562\n",
      "580/3000 train_loss: 133.18263244628906 test_loss:159.92221069335938\n",
      "581/3000 train_loss: 125.84458923339844 test_loss:154.71273803710938\n",
      "582/3000 train_loss: 136.1261749267578 test_loss:154.10458374023438\n",
      "583/3000 train_loss: 128.87692260742188 test_loss:150.0354461669922\n",
      "584/3000 train_loss: 126.94226837158203 test_loss:155.68386840820312\n",
      "585/3000 train_loss: 134.96200561523438 test_loss:155.35809326171875\n",
      "586/3000 train_loss: 130.74234008789062 test_loss:153.13307189941406\n",
      "587/3000 train_loss: 159.99722290039062 test_loss:150.7403106689453\n",
      "588/3000 train_loss: 137.3278350830078 test_loss:154.43475341796875\n",
      "589/3000 train_loss: 132.64634704589844 test_loss:149.2471923828125\n",
      "590/3000 train_loss: 126.95610809326172 test_loss:149.35516357421875\n",
      "591/3000 train_loss: 131.02589416503906 test_loss:156.7720184326172\n",
      "592/3000 train_loss: 135.82180786132812 test_loss:144.8818359375\n",
      "593/3000 train_loss: 125.6891098022461 test_loss:150.48162841796875\n",
      "594/3000 train_loss: 136.77008056640625 test_loss:149.40037536621094\n",
      "595/3000 train_loss: 124.7034683227539 test_loss:157.85084533691406\n",
      "596/3000 train_loss: 129.35113525390625 test_loss:159.44830322265625\n",
      "597/3000 train_loss: 124.46510314941406 test_loss:148.92221069335938\n",
      "598/3000 train_loss: 142.59083557128906 test_loss:156.5491180419922\n",
      "599/3000 train_loss: 112.56227111816406 test_loss:153.51797485351562\n",
      "600/3000 train_loss: 124.55152893066406 test_loss:155.20611572265625\n",
      "601/3000 train_loss: 156.98178100585938 test_loss:149.0139617919922\n",
      "602/3000 train_loss: 120.71710968017578 test_loss:160.13906860351562\n",
      "603/3000 train_loss: 118.21697235107422 test_loss:157.62115478515625\n",
      "604/3000 train_loss: 135.060302734375 test_loss:149.2313690185547\n",
      "605/3000 train_loss: 130.98989868164062 test_loss:151.14984130859375\n",
      "606/3000 train_loss: 140.80616760253906 test_loss:148.92242431640625\n",
      "607/3000 train_loss: 116.50589752197266 test_loss:149.83419799804688\n",
      "608/3000 train_loss: 119.79615783691406 test_loss:156.50027465820312\n",
      "609/3000 train_loss: 129.2873077392578 test_loss:150.48220825195312\n",
      "610/3000 train_loss: 135.90325927734375 test_loss:145.79827880859375\n",
      "611/3000 train_loss: 124.50634765625 test_loss:149.8240203857422\n",
      "612/3000 train_loss: 125.25244903564453 test_loss:151.645263671875\n",
      "613/3000 train_loss: 117.8568344116211 test_loss:155.44479370117188\n",
      "614/3000 train_loss: 118.45635986328125 test_loss:155.28746032714844\n",
      "615/3000 train_loss: 121.86198425292969 test_loss:152.02552795410156\n",
      "616/3000 train_loss: 119.77963256835938 test_loss:150.14193725585938\n",
      "617/3000 train_loss: 130.80418395996094 test_loss:143.2359619140625\n",
      "618/3000 train_loss: 125.71019744873047 test_loss:150.02133178710938\n",
      "619/3000 train_loss: 114.28141021728516 test_loss:155.31207275390625\n",
      "620/3000 train_loss: 112.88273620605469 test_loss:150.9755859375\n",
      "621/3000 train_loss: 123.69972229003906 test_loss:151.82481384277344\n",
      "622/3000 train_loss: 118.9893798828125 test_loss:153.48269653320312\n",
      "623/3000 train_loss: 136.3627471923828 test_loss:142.09642028808594\n",
      "624/3000 train_loss: 117.4662857055664 test_loss:141.31454467773438\n",
      "625/3000 train_loss: 125.45897674560547 test_loss:145.60025024414062\n",
      "626/3000 train_loss: 128.02117919921875 test_loss:148.8690185546875\n",
      "627/3000 train_loss: 138.02520751953125 test_loss:141.27394104003906\n",
      "628/3000 train_loss: 121.25646209716797 test_loss:145.55801391601562\n",
      "629/3000 train_loss: 138.18896484375 test_loss:139.2844696044922\n",
      "630/3000 train_loss: 116.87218475341797 test_loss:154.44485473632812\n",
      "631/3000 train_loss: 112.79620361328125 test_loss:156.69992065429688\n",
      "632/3000 train_loss: 129.86520385742188 test_loss:160.06166076660156\n",
      "633/3000 train_loss: 128.21824645996094 test_loss:144.2124481201172\n",
      "634/3000 train_loss: 124.4416275024414 test_loss:148.43075561523438\n",
      "635/3000 train_loss: 133.87850952148438 test_loss:139.58644104003906\n",
      "636/3000 train_loss: 134.3536376953125 test_loss:140.3490753173828\n",
      "637/3000 train_loss: 128.3936004638672 test_loss:143.7406463623047\n",
      "638/3000 train_loss: 118.82459259033203 test_loss:143.7417755126953\n",
      "639/3000 train_loss: 113.98809051513672 test_loss:152.02020263671875\n",
      "640/3000 train_loss: 120.41241455078125 test_loss:143.98049926757812\n",
      "641/3000 train_loss: 113.31499481201172 test_loss:143.22958374023438\n",
      "642/3000 train_loss: 102.73129272460938 test_loss:139.70570373535156\n",
      "643/3000 train_loss: 126.66331481933594 test_loss:141.92567443847656\n",
      "644/3000 train_loss: 120.89151000976562 test_loss:153.52793884277344\n",
      "645/3000 train_loss: 127.23021697998047 test_loss:151.5345458984375\n",
      "646/3000 train_loss: 118.1092758178711 test_loss:137.76800537109375\n",
      "647/3000 train_loss: 117.34343719482422 test_loss:142.35655212402344\n",
      "648/3000 train_loss: 120.2488784790039 test_loss:139.01104736328125\n",
      "649/3000 train_loss: 122.5089340209961 test_loss:139.7991943359375\n",
      "650/3000 train_loss: 120.8302993774414 test_loss:138.4108123779297\n",
      "651/3000 train_loss: 122.01778411865234 test_loss:141.9615020751953\n",
      "652/3000 train_loss: 124.3098373413086 test_loss:151.62796020507812\n",
      "653/3000 train_loss: 121.90128326416016 test_loss:139.48216247558594\n",
      "654/3000 train_loss: 126.27732849121094 test_loss:134.86993408203125\n",
      "655/3000 train_loss: 119.5166015625 test_loss:142.2484130859375\n",
      "656/3000 train_loss: 131.68785095214844 test_loss:138.15972900390625\n",
      "657/3000 train_loss: 114.70401000976562 test_loss:142.25360107421875\n",
      "658/3000 train_loss: 124.16294860839844 test_loss:145.79702758789062\n",
      "659/3000 train_loss: 117.16339874267578 test_loss:131.52899169921875\n",
      "660/3000 train_loss: 105.66536712646484 test_loss:134.0695037841797\n",
      "661/3000 train_loss: 113.42986297607422 test_loss:136.65122985839844\n",
      "662/3000 train_loss: 105.05142974853516 test_loss:144.4716796875\n",
      "663/3000 train_loss: 113.17927551269531 test_loss:150.73480224609375\n",
      "664/3000 train_loss: 112.09002685546875 test_loss:150.48439025878906\n",
      "665/3000 train_loss: 121.22088623046875 test_loss:137.4109344482422\n",
      "666/3000 train_loss: 123.44947052001953 test_loss:129.04148864746094\n",
      "667/3000 train_loss: 107.05628967285156 test_loss:133.2965087890625\n",
      "668/3000 train_loss: 130.83726501464844 test_loss:136.40713500976562\n",
      "669/3000 train_loss: 106.1593017578125 test_loss:131.7900390625\n",
      "670/3000 train_loss: 107.3554458618164 test_loss:137.32972717285156\n",
      "671/3000 train_loss: 119.21932220458984 test_loss:133.6699676513672\n",
      "672/3000 train_loss: 123.3573226928711 test_loss:131.57345581054688\n",
      "673/3000 train_loss: 107.29827117919922 test_loss:130.89076232910156\n",
      "674/3000 train_loss: 117.45230865478516 test_loss:131.9878387451172\n",
      "675/3000 train_loss: 100.56729888916016 test_loss:140.97972106933594\n",
      "676/3000 train_loss: 117.14402770996094 test_loss:149.18417358398438\n",
      "677/3000 train_loss: 104.04507446289062 test_loss:140.4046173095703\n",
      "678/3000 train_loss: 112.90835571289062 test_loss:146.12423706054688\n",
      "679/3000 train_loss: 105.36599731445312 test_loss:138.8592529296875\n",
      "680/3000 train_loss: 108.88410949707031 test_loss:131.22889709472656\n",
      "681/3000 train_loss: 114.1859359741211 test_loss:138.42996215820312\n",
      "682/3000 train_loss: 111.92748260498047 test_loss:137.5372314453125\n",
      "683/3000 train_loss: 114.35420989990234 test_loss:132.27432250976562\n",
      "684/3000 train_loss: 108.52371215820312 test_loss:133.31846618652344\n",
      "685/3000 train_loss: 101.75464630126953 test_loss:132.31661987304688\n",
      "686/3000 train_loss: 107.43859100341797 test_loss:130.1918182373047\n",
      "687/3000 train_loss: 110.027099609375 test_loss:135.97726440429688\n",
      "688/3000 train_loss: 113.92101287841797 test_loss:139.6435546875\n",
      "689/3000 train_loss: 116.0849609375 test_loss:134.56768798828125\n",
      "690/3000 train_loss: 108.56298828125 test_loss:131.37322998046875\n",
      "691/3000 train_loss: 103.14482116699219 test_loss:132.95867919921875\n",
      "692/3000 train_loss: 111.35163879394531 test_loss:129.48081970214844\n",
      "693/3000 train_loss: 103.97442626953125 test_loss:129.26040649414062\n",
      "694/3000 train_loss: 95.59982299804688 test_loss:133.47996520996094\n",
      "695/3000 train_loss: 93.767822265625 test_loss:129.7229766845703\n",
      "696/3000 train_loss: 106.98309326171875 test_loss:130.556640625\n",
      "697/3000 train_loss: 97.71703338623047 test_loss:129.04708862304688\n",
      "698/3000 train_loss: 109.63184356689453 test_loss:128.69895935058594\n",
      "699/3000 train_loss: 120.0744857788086 test_loss:144.61692810058594\n",
      "700/3000 train_loss: 101.82684326171875 test_loss:134.58773803710938\n",
      "701/3000 train_loss: 102.007080078125 test_loss:133.02647399902344\n",
      "702/3000 train_loss: 101.35770416259766 test_loss:133.1656494140625\n",
      "703/3000 train_loss: 112.7337646484375 test_loss:125.35354614257812\n",
      "704/3000 train_loss: 102.65444946289062 test_loss:131.58657836914062\n",
      "705/3000 train_loss: 100.35144805908203 test_loss:133.0523223876953\n",
      "706/3000 train_loss: 98.26239013671875 test_loss:136.117431640625\n",
      "707/3000 train_loss: 101.01356506347656 test_loss:126.8858413696289\n",
      "708/3000 train_loss: 109.18325805664062 test_loss:129.46310424804688\n",
      "709/3000 train_loss: 111.6971664428711 test_loss:130.5672607421875\n",
      "710/3000 train_loss: 103.94762420654297 test_loss:124.51751708984375\n",
      "711/3000 train_loss: 102.80675506591797 test_loss:123.75387573242188\n",
      "712/3000 train_loss: 96.23729705810547 test_loss:125.07201385498047\n",
      "713/3000 train_loss: 98.9610824584961 test_loss:125.23057556152344\n",
      "714/3000 train_loss: 102.58583068847656 test_loss:128.98794555664062\n",
      "715/3000 train_loss: 109.43701934814453 test_loss:122.6688232421875\n",
      "716/3000 train_loss: 108.29409790039062 test_loss:121.13102722167969\n",
      "717/3000 train_loss: 106.48329162597656 test_loss:127.47191619873047\n",
      "718/3000 train_loss: 102.80436706542969 test_loss:129.39112854003906\n",
      "719/3000 train_loss: 95.83721160888672 test_loss:121.13424682617188\n",
      "720/3000 train_loss: 100.50846099853516 test_loss:123.22573852539062\n",
      "721/3000 train_loss: 97.52685546875 test_loss:124.8105697631836\n",
      "722/3000 train_loss: 95.57768249511719 test_loss:130.0650177001953\n",
      "723/3000 train_loss: 99.1040267944336 test_loss:125.640869140625\n",
      "724/3000 train_loss: 106.04485321044922 test_loss:124.39566040039062\n",
      "725/3000 train_loss: 93.04485321044922 test_loss:126.25045013427734\n",
      "726/3000 train_loss: 109.19292449951172 test_loss:135.93421936035156\n",
      "727/3000 train_loss: 101.71968078613281 test_loss:127.64210510253906\n",
      "728/3000 train_loss: 102.52195739746094 test_loss:126.88298034667969\n",
      "729/3000 train_loss: 109.1695785522461 test_loss:122.06712341308594\n",
      "730/3000 train_loss: 114.20977020263672 test_loss:124.81008911132812\n",
      "731/3000 train_loss: 92.32981872558594 test_loss:131.95103454589844\n",
      "732/3000 train_loss: 102.36585235595703 test_loss:127.67359924316406\n",
      "733/3000 train_loss: 92.53825378417969 test_loss:123.22914123535156\n",
      "734/3000 train_loss: 107.13236999511719 test_loss:121.18934631347656\n",
      "735/3000 train_loss: 95.26620483398438 test_loss:120.43550872802734\n",
      "736/3000 train_loss: 89.26765441894531 test_loss:132.60411071777344\n",
      "737/3000 train_loss: 106.22164154052734 test_loss:127.0814208984375\n",
      "738/3000 train_loss: 97.49439239501953 test_loss:118.00700378417969\n",
      "739/3000 train_loss: 94.25489807128906 test_loss:124.81584167480469\n",
      "740/3000 train_loss: 102.28024291992188 test_loss:128.72036743164062\n",
      "741/3000 train_loss: 93.99140167236328 test_loss:126.58641052246094\n",
      "742/3000 train_loss: 90.40025329589844 test_loss:123.51190185546875\n",
      "743/3000 train_loss: 92.93995666503906 test_loss:125.1618423461914\n",
      "744/3000 train_loss: 102.82124328613281 test_loss:122.37689208984375\n",
      "745/3000 train_loss: 94.55177307128906 test_loss:126.98725891113281\n",
      "746/3000 train_loss: 99.9161148071289 test_loss:122.06837463378906\n",
      "747/3000 train_loss: 108.47220611572266 test_loss:128.32223510742188\n",
      "748/3000 train_loss: 107.90354919433594 test_loss:122.20086669921875\n",
      "749/3000 train_loss: 107.25402069091797 test_loss:121.3622055053711\n",
      "750/3000 train_loss: 89.37824249267578 test_loss:123.06925964355469\n",
      "751/3000 train_loss: 88.4466323852539 test_loss:128.35272216796875\n",
      "752/3000 train_loss: 96.16830444335938 test_loss:121.74569702148438\n",
      "753/3000 train_loss: 90.59454345703125 test_loss:116.21952819824219\n",
      "754/3000 train_loss: 108.31824493408203 test_loss:126.58053588867188\n",
      "755/3000 train_loss: 94.56130981445312 test_loss:129.786376953125\n",
      "756/3000 train_loss: 85.95834350585938 test_loss:124.74533081054688\n",
      "757/3000 train_loss: 93.80023193359375 test_loss:124.97946166992188\n",
      "758/3000 train_loss: 97.23214721679688 test_loss:119.54796600341797\n",
      "759/3000 train_loss: 99.83321380615234 test_loss:126.38502502441406\n",
      "760/3000 train_loss: 96.68983459472656 test_loss:121.61056518554688\n",
      "761/3000 train_loss: 92.41786193847656 test_loss:119.33932495117188\n",
      "762/3000 train_loss: 102.24295043945312 test_loss:122.39300537109375\n",
      "763/3000 train_loss: 90.51614379882812 test_loss:124.57963562011719\n",
      "764/3000 train_loss: 100.7010269165039 test_loss:123.88856506347656\n",
      "765/3000 train_loss: 93.58918762207031 test_loss:121.0927963256836\n",
      "766/3000 train_loss: 113.75504302978516 test_loss:119.23307037353516\n",
      "767/3000 train_loss: 88.25435638427734 test_loss:113.44326782226562\n",
      "768/3000 train_loss: 105.8856201171875 test_loss:118.00434112548828\n",
      "769/3000 train_loss: 95.24864959716797 test_loss:122.96551513671875\n",
      "770/3000 train_loss: 91.92032623291016 test_loss:121.07804107666016\n",
      "771/3000 train_loss: 98.95659637451172 test_loss:118.6910400390625\n",
      "772/3000 train_loss: 100.2046890258789 test_loss:116.84962463378906\n",
      "773/3000 train_loss: 107.25170135498047 test_loss:113.3648910522461\n",
      "774/3000 train_loss: 87.24856567382812 test_loss:122.56962585449219\n",
      "775/3000 train_loss: 99.11247253417969 test_loss:123.12117004394531\n",
      "776/3000 train_loss: 92.2249755859375 test_loss:120.6739501953125\n",
      "777/3000 train_loss: 89.2714614868164 test_loss:122.624267578125\n",
      "778/3000 train_loss: 90.36895751953125 test_loss:119.62228393554688\n",
      "779/3000 train_loss: 89.91569519042969 test_loss:119.84315490722656\n",
      "780/3000 train_loss: 89.38794708251953 test_loss:118.31863403320312\n",
      "781/3000 train_loss: 88.51741027832031 test_loss:128.138427734375\n",
      "782/3000 train_loss: 94.02873992919922 test_loss:113.77574920654297\n",
      "783/3000 train_loss: 101.94895935058594 test_loss:123.94588470458984\n",
      "784/3000 train_loss: 86.78376007080078 test_loss:125.792724609375\n",
      "785/3000 train_loss: 87.20780181884766 test_loss:117.99307250976562\n",
      "786/3000 train_loss: 96.39913940429688 test_loss:116.10243225097656\n",
      "787/3000 train_loss: 88.25293731689453 test_loss:117.49922180175781\n",
      "788/3000 train_loss: 97.53430938720703 test_loss:115.17747497558594\n",
      "789/3000 train_loss: 82.28596496582031 test_loss:117.03301239013672\n",
      "790/3000 train_loss: 84.93216705322266 test_loss:121.700927734375\n",
      "791/3000 train_loss: 92.95006561279297 test_loss:120.82400512695312\n",
      "792/3000 train_loss: 94.27162170410156 test_loss:115.66374206542969\n",
      "793/3000 train_loss: 91.86148071289062 test_loss:113.67414093017578\n",
      "794/3000 train_loss: 90.70329284667969 test_loss:112.41931915283203\n",
      "795/3000 train_loss: 85.63459777832031 test_loss:115.40672302246094\n",
      "796/3000 train_loss: 95.46851348876953 test_loss:122.88050842285156\n",
      "797/3000 train_loss: 85.26924896240234 test_loss:116.03894805908203\n",
      "798/3000 train_loss: 94.46556854248047 test_loss:118.99758911132812\n",
      "799/3000 train_loss: 84.58026885986328 test_loss:120.76826477050781\n",
      "800/3000 train_loss: 89.78007507324219 test_loss:117.70845031738281\n",
      "801/3000 train_loss: 78.22074127197266 test_loss:122.34225463867188\n",
      "802/3000 train_loss: 88.5169448852539 test_loss:121.34561920166016\n",
      "803/3000 train_loss: 97.49878692626953 test_loss:117.38990783691406\n",
      "804/3000 train_loss: 89.95358276367188 test_loss:119.38204956054688\n",
      "805/3000 train_loss: 93.53668212890625 test_loss:118.31288146972656\n",
      "806/3000 train_loss: 98.32566833496094 test_loss:122.52925872802734\n",
      "807/3000 train_loss: 90.14982604980469 test_loss:119.05976104736328\n",
      "808/3000 train_loss: 91.6921157836914 test_loss:115.36805725097656\n",
      "809/3000 train_loss: 85.26834869384766 test_loss:112.90113830566406\n",
      "810/3000 train_loss: 109.95655822753906 test_loss:126.3108901977539\n",
      "811/3000 train_loss: 83.39974975585938 test_loss:124.77639770507812\n",
      "812/3000 train_loss: 91.0247802734375 test_loss:117.6622543334961\n",
      "813/3000 train_loss: 94.36235809326172 test_loss:114.43145751953125\n",
      "814/3000 train_loss: 87.27851104736328 test_loss:125.99516296386719\n",
      "815/3000 train_loss: 99.96155548095703 test_loss:125.15985107421875\n",
      "816/3000 train_loss: 91.26605987548828 test_loss:123.43548583984375\n",
      "817/3000 train_loss: 88.82356262207031 test_loss:117.63839721679688\n",
      "818/3000 train_loss: 86.63339233398438 test_loss:118.35493469238281\n",
      "819/3000 train_loss: 90.87973022460938 test_loss:111.0141830444336\n",
      "820/3000 train_loss: 88.94593048095703 test_loss:123.06053161621094\n",
      "821/3000 train_loss: 72.26934814453125 test_loss:111.23335266113281\n",
      "822/3000 train_loss: 85.09416198730469 test_loss:118.95976257324219\n",
      "823/3000 train_loss: 90.07263946533203 test_loss:109.81495666503906\n",
      "824/3000 train_loss: 97.30205535888672 test_loss:110.56854248046875\n",
      "825/3000 train_loss: 91.2604751586914 test_loss:108.04110717773438\n",
      "826/3000 train_loss: 81.34129333496094 test_loss:118.14283752441406\n",
      "827/3000 train_loss: 83.25410461425781 test_loss:114.7538833618164\n",
      "828/3000 train_loss: 105.85643005371094 test_loss:109.55795288085938\n",
      "829/3000 train_loss: 83.99169921875 test_loss:117.59283447265625\n",
      "830/3000 train_loss: 89.01486206054688 test_loss:115.38177490234375\n",
      "831/3000 train_loss: 89.38998413085938 test_loss:111.57060241699219\n",
      "832/3000 train_loss: 83.67465209960938 test_loss:111.77017974853516\n",
      "833/3000 train_loss: 85.44383239746094 test_loss:109.97100067138672\n",
      "834/3000 train_loss: 87.21245574951172 test_loss:106.77543640136719\n",
      "835/3000 train_loss: 86.47684478759766 test_loss:118.59092712402344\n",
      "836/3000 train_loss: 105.01997375488281 test_loss:115.23907470703125\n",
      "837/3000 train_loss: 95.2351303100586 test_loss:108.32192993164062\n",
      "838/3000 train_loss: 85.00724029541016 test_loss:121.38446044921875\n",
      "839/3000 train_loss: 89.2597885131836 test_loss:106.93128967285156\n",
      "840/3000 train_loss: 80.07888793945312 test_loss:113.61984252929688\n",
      "841/3000 train_loss: 83.886962890625 test_loss:112.5277328491211\n",
      "842/3000 train_loss: 95.53427124023438 test_loss:118.39004516601562\n",
      "843/3000 train_loss: 85.6495132446289 test_loss:124.82401275634766\n",
      "844/3000 train_loss: 78.77208709716797 test_loss:120.63813781738281\n",
      "845/3000 train_loss: 86.1249008178711 test_loss:109.37452697753906\n",
      "846/3000 train_loss: 84.4381103515625 test_loss:114.03352355957031\n",
      "847/3000 train_loss: 88.49341583251953 test_loss:121.92008972167969\n",
      "848/3000 train_loss: 84.86371612548828 test_loss:117.35771179199219\n",
      "849/3000 train_loss: 91.06806182861328 test_loss:107.13233947753906\n",
      "850/3000 train_loss: 89.1062240600586 test_loss:115.35469818115234\n",
      "851/3000 train_loss: 77.20559692382812 test_loss:112.75698852539062\n",
      "852/3000 train_loss: 91.22281646728516 test_loss:110.92802429199219\n",
      "853/3000 train_loss: 83.39468383789062 test_loss:115.22061157226562\n",
      "854/3000 train_loss: 81.25831604003906 test_loss:119.09668731689453\n",
      "855/3000 train_loss: 86.41075134277344 test_loss:116.54456329345703\n",
      "856/3000 train_loss: 91.68771362304688 test_loss:113.99343872070312\n",
      "857/3000 train_loss: 82.9319839477539 test_loss:121.59254455566406\n",
      "858/3000 train_loss: 90.47886657714844 test_loss:112.44917297363281\n",
      "859/3000 train_loss: 87.95146942138672 test_loss:112.26846313476562\n",
      "860/3000 train_loss: 87.36970520019531 test_loss:115.79385375976562\n",
      "861/3000 train_loss: 77.5828628540039 test_loss:108.08781433105469\n",
      "862/3000 train_loss: 93.83894348144531 test_loss:104.41175842285156\n",
      "863/3000 train_loss: 86.27912902832031 test_loss:113.96620178222656\n",
      "864/3000 train_loss: 90.22088623046875 test_loss:109.0269775390625\n",
      "865/3000 train_loss: 85.92841339111328 test_loss:112.3463134765625\n",
      "866/3000 train_loss: 83.83808898925781 test_loss:114.77256774902344\n",
      "867/3000 train_loss: 83.5573959350586 test_loss:110.092041015625\n",
      "868/3000 train_loss: 85.55326843261719 test_loss:119.20081329345703\n",
      "869/3000 train_loss: 77.12754821777344 test_loss:113.88694763183594\n",
      "870/3000 train_loss: 85.29644012451172 test_loss:105.83586120605469\n",
      "871/3000 train_loss: 82.56689453125 test_loss:116.93495178222656\n",
      "872/3000 train_loss: 76.51632690429688 test_loss:111.81976318359375\n",
      "873/3000 train_loss: 82.75337219238281 test_loss:118.70909118652344\n",
      "874/3000 train_loss: 92.60092163085938 test_loss:106.25394439697266\n",
      "875/3000 train_loss: 81.86258697509766 test_loss:115.03173828125\n",
      "876/3000 train_loss: 97.0282211303711 test_loss:104.65814208984375\n",
      "877/3000 train_loss: 81.41888427734375 test_loss:112.48753356933594\n",
      "878/3000 train_loss: 96.50914764404297 test_loss:119.36026763916016\n",
      "879/3000 train_loss: 87.3607406616211 test_loss:107.20944213867188\n",
      "880/3000 train_loss: 88.47710418701172 test_loss:116.58793640136719\n",
      "881/3000 train_loss: 78.25228118896484 test_loss:109.94477081298828\n",
      "882/3000 train_loss: 82.58466339111328 test_loss:109.9271240234375\n",
      "883/3000 train_loss: 85.44731140136719 test_loss:104.55728149414062\n",
      "884/3000 train_loss: 78.19544982910156 test_loss:112.611083984375\n",
      "885/3000 train_loss: 78.39630126953125 test_loss:108.1692123413086\n",
      "886/3000 train_loss: 76.0062255859375 test_loss:109.85435485839844\n",
      "887/3000 train_loss: 84.8515625 test_loss:114.66468811035156\n",
      "888/3000 train_loss: 85.08274841308594 test_loss:116.26270294189453\n",
      "889/3000 train_loss: 81.12154388427734 test_loss:110.57611083984375\n",
      "890/3000 train_loss: 75.43672180175781 test_loss:106.01725769042969\n",
      "891/3000 train_loss: 85.59297180175781 test_loss:102.41397094726562\n",
      "892/3000 train_loss: 106.11310577392578 test_loss:108.14445495605469\n",
      "893/3000 train_loss: 76.96795654296875 test_loss:111.2780990600586\n",
      "894/3000 train_loss: 80.18905639648438 test_loss:106.51432037353516\n",
      "895/3000 train_loss: 75.21783447265625 test_loss:110.37620544433594\n",
      "896/3000 train_loss: 88.1344223022461 test_loss:117.55136108398438\n",
      "897/3000 train_loss: 92.33220672607422 test_loss:118.18168640136719\n",
      "898/3000 train_loss: 84.22562408447266 test_loss:110.28168487548828\n",
      "899/3000 train_loss: 87.44200134277344 test_loss:105.02245330810547\n",
      "900/3000 train_loss: 81.49079895019531 test_loss:113.52127075195312\n",
      "901/3000 train_loss: 87.35660552978516 test_loss:113.87130737304688\n",
      "902/3000 train_loss: 79.44769287109375 test_loss:109.00936889648438\n",
      "903/3000 train_loss: 78.46318817138672 test_loss:109.325439453125\n",
      "904/3000 train_loss: 92.13228607177734 test_loss:106.90853881835938\n",
      "905/3000 train_loss: 81.84168243408203 test_loss:105.88172912597656\n",
      "906/3000 train_loss: 89.80783081054688 test_loss:106.21649169921875\n",
      "907/3000 train_loss: 74.3511962890625 test_loss:100.38162231445312\n",
      "908/3000 train_loss: 80.23869323730469 test_loss:110.25334930419922\n",
      "909/3000 train_loss: 83.64604949951172 test_loss:111.98363494873047\n",
      "910/3000 train_loss: 78.1590576171875 test_loss:114.18663024902344\n",
      "911/3000 train_loss: 74.60411834716797 test_loss:103.17924499511719\n",
      "912/3000 train_loss: 81.70672607421875 test_loss:106.52653503417969\n",
      "913/3000 train_loss: 90.45475769042969 test_loss:115.44406127929688\n",
      "914/3000 train_loss: 83.43772888183594 test_loss:104.93679809570312\n",
      "915/3000 train_loss: 82.32442474365234 test_loss:105.03165435791016\n",
      "916/3000 train_loss: 81.08192443847656 test_loss:107.92924499511719\n",
      "917/3000 train_loss: 75.88584899902344 test_loss:104.56019592285156\n",
      "918/3000 train_loss: 81.39309692382812 test_loss:110.45933532714844\n",
      "919/3000 train_loss: 76.80641174316406 test_loss:106.1651840209961\n",
      "920/3000 train_loss: 78.72901916503906 test_loss:106.32585144042969\n",
      "921/3000 train_loss: 76.18019104003906 test_loss:102.89936828613281\n",
      "922/3000 train_loss: 85.63180541992188 test_loss:100.84950256347656\n",
      "923/3000 train_loss: 77.39202117919922 test_loss:112.94622802734375\n",
      "924/3000 train_loss: 81.04540252685547 test_loss:105.18341064453125\n",
      "925/3000 train_loss: 84.90541076660156 test_loss:106.12095642089844\n",
      "926/3000 train_loss: 74.97261810302734 test_loss:101.7617416381836\n",
      "927/3000 train_loss: 76.20608520507812 test_loss:100.146484375\n",
      "928/3000 train_loss: 77.13838195800781 test_loss:101.29673767089844\n",
      "929/3000 train_loss: 70.92182922363281 test_loss:107.28269958496094\n",
      "930/3000 train_loss: 84.2827377319336 test_loss:108.42167663574219\n",
      "931/3000 train_loss: 75.20667266845703 test_loss:98.70745849609375\n",
      "932/3000 train_loss: 70.42960357666016 test_loss:114.96385955810547\n",
      "933/3000 train_loss: 75.25969696044922 test_loss:104.77206420898438\n",
      "934/3000 train_loss: 75.54220581054688 test_loss:100.70213317871094\n",
      "935/3000 train_loss: 76.48995971679688 test_loss:98.49076843261719\n",
      "936/3000 train_loss: 79.23271179199219 test_loss:106.77114868164062\n",
      "937/3000 train_loss: 79.40363311767578 test_loss:107.45947265625\n",
      "938/3000 train_loss: 93.05657196044922 test_loss:115.84901428222656\n",
      "939/3000 train_loss: 84.5838394165039 test_loss:102.70512390136719\n",
      "940/3000 train_loss: 77.47347259521484 test_loss:103.91990661621094\n",
      "941/3000 train_loss: 90.1821517944336 test_loss:103.67713928222656\n",
      "942/3000 train_loss: 76.3397445678711 test_loss:103.17415618896484\n",
      "943/3000 train_loss: 78.48458099365234 test_loss:108.92752838134766\n",
      "944/3000 train_loss: 103.680419921875 test_loss:127.43177032470703\n",
      "945/3000 train_loss: 89.7975845336914 test_loss:102.60176086425781\n",
      "946/3000 train_loss: 77.50154113769531 test_loss:121.66656494140625\n",
      "947/3000 train_loss: 88.90335845947266 test_loss:120.30181884765625\n",
      "948/3000 train_loss: 78.2062759399414 test_loss:111.5771484375\n",
      "949/3000 train_loss: 80.86245727539062 test_loss:114.3153076171875\n",
      "950/3000 train_loss: 76.92733001708984 test_loss:106.2244873046875\n",
      "951/3000 train_loss: 65.52764892578125 test_loss:102.59668731689453\n",
      "952/3000 train_loss: 77.7282943725586 test_loss:109.79964447021484\n",
      "953/3000 train_loss: 78.7184829711914 test_loss:101.40336608886719\n",
      "954/3000 train_loss: 71.03034973144531 test_loss:105.11697387695312\n",
      "955/3000 train_loss: 86.35309600830078 test_loss:121.21216583251953\n",
      "956/3000 train_loss: 76.83504486083984 test_loss:110.10693359375\n",
      "957/3000 train_loss: 77.29854583740234 test_loss:106.75665283203125\n",
      "958/3000 train_loss: 75.54285430908203 test_loss:111.94277954101562\n",
      "959/3000 train_loss: 70.21263122558594 test_loss:105.05084228515625\n",
      "960/3000 train_loss: 79.63003540039062 test_loss:101.99514770507812\n",
      "961/3000 train_loss: 73.02139282226562 test_loss:110.53199768066406\n",
      "962/3000 train_loss: 83.67173767089844 test_loss:100.04486083984375\n",
      "963/3000 train_loss: 83.67737579345703 test_loss:107.7210693359375\n",
      "964/3000 train_loss: 72.7740249633789 test_loss:104.4557113647461\n",
      "965/3000 train_loss: 71.50408172607422 test_loss:105.28071594238281\n",
      "966/3000 train_loss: 89.0948257446289 test_loss:107.68428039550781\n",
      "967/3000 train_loss: 68.44203186035156 test_loss:104.64390563964844\n",
      "968/3000 train_loss: 68.2299575805664 test_loss:110.13978576660156\n",
      "969/3000 train_loss: 79.82575225830078 test_loss:115.64602661132812\n",
      "970/3000 train_loss: 81.09202575683594 test_loss:98.40071105957031\n",
      "971/3000 train_loss: 72.09701538085938 test_loss:106.3172378540039\n",
      "972/3000 train_loss: 61.89387130737305 test_loss:103.67979431152344\n",
      "973/3000 train_loss: 67.26253509521484 test_loss:107.14299011230469\n",
      "974/3000 train_loss: 65.331787109375 test_loss:111.45048522949219\n",
      "975/3000 train_loss: 74.26471710205078 test_loss:103.82489013671875\n",
      "976/3000 train_loss: 77.5938720703125 test_loss:105.51864624023438\n",
      "977/3000 train_loss: 78.0025863647461 test_loss:102.49127197265625\n",
      "978/3000 train_loss: 72.64244842529297 test_loss:105.28656005859375\n",
      "979/3000 train_loss: 67.37834167480469 test_loss:104.81069946289062\n",
      "980/3000 train_loss: 75.58731079101562 test_loss:94.90431213378906\n",
      "981/3000 train_loss: 74.53475952148438 test_loss:107.02238464355469\n",
      "982/3000 train_loss: 75.13636779785156 test_loss:95.99810791015625\n",
      "983/3000 train_loss: 72.85166931152344 test_loss:107.40079498291016\n",
      "984/3000 train_loss: 81.9345703125 test_loss:96.18824768066406\n",
      "985/3000 train_loss: 66.99948120117188 test_loss:99.01908874511719\n",
      "986/3000 train_loss: 74.85431671142578 test_loss:113.89321899414062\n",
      "987/3000 train_loss: 76.1487808227539 test_loss:108.59852600097656\n",
      "988/3000 train_loss: 71.9603042602539 test_loss:104.62442016601562\n",
      "989/3000 train_loss: 74.99044799804688 test_loss:95.62128448486328\n",
      "990/3000 train_loss: 80.60790252685547 test_loss:96.41998291015625\n",
      "991/3000 train_loss: 68.74624633789062 test_loss:102.67489624023438\n",
      "992/3000 train_loss: 75.2184066772461 test_loss:110.17837524414062\n",
      "993/3000 train_loss: 79.48097229003906 test_loss:105.57777404785156\n",
      "994/3000 train_loss: 73.34349822998047 test_loss:104.62716674804688\n",
      "995/3000 train_loss: 69.18668365478516 test_loss:103.55573272705078\n",
      "996/3000 train_loss: 76.97285461425781 test_loss:99.47328186035156\n",
      "997/3000 train_loss: 76.56761169433594 test_loss:105.85823059082031\n",
      "998/3000 train_loss: 73.96912384033203 test_loss:98.28871154785156\n",
      "999/3000 train_loss: 77.60590362548828 test_loss:100.87893676757812\n",
      "1000/3000 train_loss: 76.61151123046875 test_loss:96.96574401855469\n",
      "1001/3000 train_loss: 64.94365692138672 test_loss:105.48240661621094\n",
      "1002/3000 train_loss: 69.42367553710938 test_loss:106.42559814453125\n",
      "1003/3000 train_loss: 75.30142211914062 test_loss:100.34803771972656\n",
      "1004/3000 train_loss: 67.55899810791016 test_loss:109.52372741699219\n",
      "1005/3000 train_loss: 85.99349975585938 test_loss:99.52567291259766\n",
      "1006/3000 train_loss: 70.36682891845703 test_loss:95.51709747314453\n",
      "1007/3000 train_loss: 72.83021545410156 test_loss:104.48345947265625\n",
      "1008/3000 train_loss: 69.78451538085938 test_loss:98.75712585449219\n",
      "1009/3000 train_loss: 71.15109252929688 test_loss:104.72473907470703\n",
      "1010/3000 train_loss: 80.85014343261719 test_loss:98.2837905883789\n",
      "1011/3000 train_loss: 80.27461242675781 test_loss:99.2385025024414\n",
      "1012/3000 train_loss: 68.41746520996094 test_loss:105.37395477294922\n",
      "1013/3000 train_loss: 81.36714172363281 test_loss:95.11282348632812\n",
      "1014/3000 train_loss: 69.36048126220703 test_loss:104.9130859375\n",
      "1015/3000 train_loss: 69.59654235839844 test_loss:99.30763244628906\n",
      "1016/3000 train_loss: 76.54098510742188 test_loss:96.4974136352539\n",
      "1017/3000 train_loss: 66.63685607910156 test_loss:100.98855590820312\n",
      "1018/3000 train_loss: 68.17940521240234 test_loss:99.24906921386719\n",
      "1019/3000 train_loss: 72.6250228881836 test_loss:101.26689147949219\n",
      "1020/3000 train_loss: 69.51431274414062 test_loss:94.90029907226562\n",
      "1021/3000 train_loss: 66.08455657958984 test_loss:92.54531860351562\n",
      "1022/3000 train_loss: 68.18843078613281 test_loss:101.85643005371094\n",
      "1023/3000 train_loss: 71.03629302978516 test_loss:98.62335205078125\n",
      "1024/3000 train_loss: 68.52263641357422 test_loss:102.66409301757812\n",
      "1025/3000 train_loss: 69.15852355957031 test_loss:102.78083038330078\n",
      "1026/3000 train_loss: 66.87255859375 test_loss:94.93143463134766\n",
      "1027/3000 train_loss: 68.42240142822266 test_loss:108.63088989257812\n",
      "1028/3000 train_loss: 71.48360443115234 test_loss:96.21452331542969\n",
      "1029/3000 train_loss: 63.07758331298828 test_loss:102.39356231689453\n",
      "1030/3000 train_loss: 68.80538940429688 test_loss:93.80177307128906\n",
      "1031/3000 train_loss: 72.08748626708984 test_loss:102.40609741210938\n",
      "1032/3000 train_loss: 70.79400634765625 test_loss:96.30128479003906\n",
      "1033/3000 train_loss: 80.26338958740234 test_loss:93.55390930175781\n",
      "1034/3000 train_loss: 63.878841400146484 test_loss:93.9415283203125\n",
      "1035/3000 train_loss: 61.630897521972656 test_loss:97.8786849975586\n",
      "1036/3000 train_loss: 69.24745178222656 test_loss:95.57252502441406\n",
      "1037/3000 train_loss: 70.98990631103516 test_loss:92.12306213378906\n",
      "1038/3000 train_loss: 67.14476776123047 test_loss:95.51066589355469\n",
      "1039/3000 train_loss: 64.25244903564453 test_loss:93.9615478515625\n",
      "1040/3000 train_loss: 69.77665710449219 test_loss:91.27611541748047\n",
      "1041/3000 train_loss: 68.02500915527344 test_loss:100.2662582397461\n",
      "1042/3000 train_loss: 66.42766571044922 test_loss:93.04866790771484\n",
      "1043/3000 train_loss: 68.54740142822266 test_loss:89.72982025146484\n",
      "1044/3000 train_loss: 80.30150604248047 test_loss:92.74922180175781\n",
      "1045/3000 train_loss: 62.349273681640625 test_loss:109.97770690917969\n",
      "1046/3000 train_loss: 69.75006103515625 test_loss:97.6396484375\n",
      "1047/3000 train_loss: 71.81391906738281 test_loss:90.81826782226562\n",
      "1048/3000 train_loss: 63.223663330078125 test_loss:102.7463150024414\n",
      "1049/3000 train_loss: 73.95025634765625 test_loss:86.06497192382812\n",
      "1050/3000 train_loss: 63.07706832885742 test_loss:104.76448059082031\n",
      "1051/3000 train_loss: 78.11471557617188 test_loss:89.44010925292969\n",
      "1052/3000 train_loss: 72.43572235107422 test_loss:93.49372863769531\n",
      "1053/3000 train_loss: 65.16024780273438 test_loss:98.95379638671875\n",
      "1054/3000 train_loss: 72.21060180664062 test_loss:93.50701141357422\n",
      "1055/3000 train_loss: 69.18854522705078 test_loss:94.34477233886719\n",
      "1056/3000 train_loss: 66.64834594726562 test_loss:90.94174194335938\n",
      "1057/3000 train_loss: 59.86558532714844 test_loss:99.09573364257812\n",
      "1058/3000 train_loss: 64.60726165771484 test_loss:95.0467529296875\n",
      "1059/3000 train_loss: 61.44231033325195 test_loss:92.32841491699219\n",
      "1060/3000 train_loss: 73.08869934082031 test_loss:93.44475555419922\n",
      "1061/3000 train_loss: 75.02592468261719 test_loss:103.61642456054688\n",
      "1062/3000 train_loss: 68.60404968261719 test_loss:87.68470764160156\n",
      "1063/3000 train_loss: 69.24279022216797 test_loss:96.04043579101562\n",
      "1064/3000 train_loss: 61.702327728271484 test_loss:94.51467895507812\n",
      "1065/3000 train_loss: 62.4000358581543 test_loss:96.05697631835938\n",
      "1066/3000 train_loss: 62.90761947631836 test_loss:92.62342834472656\n",
      "1067/3000 train_loss: 64.27922821044922 test_loss:105.50946044921875\n",
      "1068/3000 train_loss: 70.6213607788086 test_loss:96.2190933227539\n",
      "1069/3000 train_loss: 59.081520080566406 test_loss:97.74620056152344\n",
      "1070/3000 train_loss: 60.0321159362793 test_loss:95.19308471679688\n",
      "1071/3000 train_loss: 76.00788116455078 test_loss:92.81563568115234\n",
      "1072/3000 train_loss: 61.46137237548828 test_loss:93.10569763183594\n",
      "1073/3000 train_loss: 66.28326416015625 test_loss:90.16744995117188\n",
      "1074/3000 train_loss: 58.19530487060547 test_loss:93.22483825683594\n",
      "1075/3000 train_loss: 63.19633102416992 test_loss:86.2044677734375\n",
      "1076/3000 train_loss: 83.26712799072266 test_loss:116.594970703125\n",
      "1077/3000 train_loss: 73.48373413085938 test_loss:89.65961456298828\n",
      "1078/3000 train_loss: 76.21406555175781 test_loss:92.07284545898438\n",
      "1079/3000 train_loss: 57.09288787841797 test_loss:101.07243347167969\n",
      "1080/3000 train_loss: 70.3088607788086 test_loss:82.9573974609375\n",
      "1081/3000 train_loss: 75.4247055053711 test_loss:99.67216491699219\n",
      "1082/3000 train_loss: 73.54874420166016 test_loss:91.00701904296875\n",
      "1083/3000 train_loss: 68.0014877319336 test_loss:91.77052307128906\n",
      "1084/3000 train_loss: 62.70988082885742 test_loss:94.38047790527344\n",
      "1085/3000 train_loss: 70.1095962524414 test_loss:106.12918853759766\n",
      "1086/3000 train_loss: 73.92567443847656 test_loss:102.21159362792969\n",
      "1087/3000 train_loss: 75.37619018554688 test_loss:92.0128173828125\n",
      "1088/3000 train_loss: 69.36167907714844 test_loss:97.32909393310547\n",
      "1089/3000 train_loss: 64.98572540283203 test_loss:101.44035339355469\n",
      "1090/3000 train_loss: 68.668701171875 test_loss:93.6894760131836\n",
      "1091/3000 train_loss: 56.2220573425293 test_loss:107.16506958007812\n",
      "1092/3000 train_loss: 76.18336486816406 test_loss:90.81282043457031\n",
      "1093/3000 train_loss: 57.7816047668457 test_loss:100.66094970703125\n",
      "1094/3000 train_loss: 53.591976165771484 test_loss:91.45714569091797\n",
      "1095/3000 train_loss: 57.937259674072266 test_loss:95.9906005859375\n",
      "1096/3000 train_loss: 68.17636108398438 test_loss:95.02680969238281\n",
      "1097/3000 train_loss: 71.15331268310547 test_loss:95.19374084472656\n",
      "1098/3000 train_loss: 63.64789962768555 test_loss:89.47820281982422\n",
      "1099/3000 train_loss: 69.38325500488281 test_loss:93.78385162353516\n",
      "1100/3000 train_loss: 69.56825256347656 test_loss:90.14017486572266\n",
      "1101/3000 train_loss: 76.13272094726562 test_loss:86.39532470703125\n",
      "1102/3000 train_loss: 70.40864562988281 test_loss:104.35631561279297\n",
      "1103/3000 train_loss: 71.8084716796875 test_loss:92.74996185302734\n",
      "1104/3000 train_loss: 69.70748138427734 test_loss:89.9368896484375\n",
      "1105/3000 train_loss: 58.54301834106445 test_loss:91.76905822753906\n",
      "1106/3000 train_loss: 56.14029312133789 test_loss:87.34318542480469\n",
      "1107/3000 train_loss: 63.132293701171875 test_loss:89.54641723632812\n",
      "1108/3000 train_loss: 59.56327819824219 test_loss:90.58684539794922\n",
      "1109/3000 train_loss: 58.00979232788086 test_loss:95.58668518066406\n",
      "1110/3000 train_loss: 65.3760986328125 test_loss:86.91154479980469\n",
      "1111/3000 train_loss: 61.266807556152344 test_loss:101.56424713134766\n",
      "1112/3000 train_loss: 65.34058380126953 test_loss:97.11279296875\n",
      "1113/3000 train_loss: 66.66381072998047 test_loss:84.52218627929688\n",
      "1114/3000 train_loss: 76.81661987304688 test_loss:90.23976135253906\n",
      "1115/3000 train_loss: 58.43499755859375 test_loss:91.12348937988281\n",
      "1116/3000 train_loss: 58.260841369628906 test_loss:85.98344421386719\n",
      "1117/3000 train_loss: 65.63626098632812 test_loss:87.12189483642578\n",
      "1118/3000 train_loss: 65.60920715332031 test_loss:89.20750427246094\n",
      "1119/3000 train_loss: 60.743045806884766 test_loss:92.2857437133789\n",
      "1120/3000 train_loss: 62.35331726074219 test_loss:87.47618865966797\n",
      "1121/3000 train_loss: 56.66107940673828 test_loss:89.25303649902344\n",
      "1122/3000 train_loss: 57.51163864135742 test_loss:90.83522033691406\n",
      "1123/3000 train_loss: 62.9397087097168 test_loss:85.7101821899414\n",
      "1124/3000 train_loss: 60.31623077392578 test_loss:86.34434509277344\n",
      "1125/3000 train_loss: 59.822818756103516 test_loss:86.69448852539062\n",
      "1126/3000 train_loss: 69.65113830566406 test_loss:90.87448120117188\n",
      "1127/3000 train_loss: 62.80363464355469 test_loss:93.66896057128906\n",
      "1128/3000 train_loss: 73.32713317871094 test_loss:98.91677856445312\n",
      "1129/3000 train_loss: 60.43050765991211 test_loss:91.27117919921875\n",
      "1130/3000 train_loss: 69.60264587402344 test_loss:92.40742492675781\n",
      "1131/3000 train_loss: 58.1776237487793 test_loss:96.30083465576172\n",
      "1132/3000 train_loss: 60.6018180847168 test_loss:93.22285461425781\n",
      "1133/3000 train_loss: 56.53824234008789 test_loss:89.33414459228516\n",
      "1134/3000 train_loss: 55.7899169921875 test_loss:84.98094177246094\n",
      "1135/3000 train_loss: 59.25214385986328 test_loss:96.79693603515625\n",
      "1136/3000 train_loss: 57.206077575683594 test_loss:89.53346252441406\n",
      "1137/3000 train_loss: 60.65948486328125 test_loss:86.74784088134766\n",
      "1138/3000 train_loss: 66.31165313720703 test_loss:84.04821014404297\n",
      "1139/3000 train_loss: 54.462738037109375 test_loss:91.68936157226562\n",
      "1140/3000 train_loss: 55.414100646972656 test_loss:82.66380310058594\n",
      "1141/3000 train_loss: 61.315982818603516 test_loss:83.1944351196289\n",
      "1142/3000 train_loss: 60.70024108886719 test_loss:91.6093978881836\n",
      "1143/3000 train_loss: 58.88978958129883 test_loss:86.207763671875\n",
      "1144/3000 train_loss: 62.0096549987793 test_loss:88.57101440429688\n",
      "1145/3000 train_loss: 62.00975036621094 test_loss:83.51426696777344\n",
      "1146/3000 train_loss: 56.07236862182617 test_loss:92.85426330566406\n",
      "1147/3000 train_loss: 57.882850646972656 test_loss:98.77684020996094\n",
      "1148/3000 train_loss: 63.31142807006836 test_loss:81.57418060302734\n",
      "1149/3000 train_loss: 61.47673797607422 test_loss:94.59524536132812\n",
      "1150/3000 train_loss: 58.84610366821289 test_loss:90.50794982910156\n",
      "1151/3000 train_loss: 63.833274841308594 test_loss:88.99815368652344\n",
      "1152/3000 train_loss: 55.249488830566406 test_loss:87.27765655517578\n",
      "1153/3000 train_loss: 62.031620025634766 test_loss:92.84999084472656\n",
      "1154/3000 train_loss: 65.86173248291016 test_loss:86.5298080444336\n",
      "1155/3000 train_loss: 67.13699340820312 test_loss:104.65731811523438\n",
      "1156/3000 train_loss: 71.59506225585938 test_loss:85.93368530273438\n",
      "1157/3000 train_loss: 70.73200225830078 test_loss:86.04100036621094\n",
      "1158/3000 train_loss: 55.658695220947266 test_loss:86.07353210449219\n",
      "1159/3000 train_loss: 58.850364685058594 test_loss:90.46945190429688\n",
      "1160/3000 train_loss: 70.44725799560547 test_loss:86.9782485961914\n",
      "1161/3000 train_loss: 66.03402709960938 test_loss:92.01434326171875\n",
      "1162/3000 train_loss: 59.700042724609375 test_loss:84.82011413574219\n",
      "1163/3000 train_loss: 55.80044937133789 test_loss:91.4248046875\n",
      "1164/3000 train_loss: 58.97071838378906 test_loss:88.53262329101562\n",
      "1165/3000 train_loss: 64.89361572265625 test_loss:88.2860107421875\n",
      "1166/3000 train_loss: 59.80898666381836 test_loss:94.98124694824219\n",
      "1167/3000 train_loss: 63.452003479003906 test_loss:89.07875061035156\n",
      "1168/3000 train_loss: 60.13581466674805 test_loss:92.89738464355469\n",
      "1169/3000 train_loss: 65.5340805053711 test_loss:99.7514877319336\n",
      "1170/3000 train_loss: 61.01886749267578 test_loss:88.58988189697266\n",
      "1171/3000 train_loss: 56.53626251220703 test_loss:95.9346923828125\n",
      "1172/3000 train_loss: 59.11418914794922 test_loss:94.75668334960938\n",
      "1173/3000 train_loss: 49.56037521362305 test_loss:94.67066955566406\n",
      "1174/3000 train_loss: 63.54743957519531 test_loss:90.01849365234375\n",
      "1175/3000 train_loss: 65.91270446777344 test_loss:90.01761627197266\n",
      "1176/3000 train_loss: 61.28216552734375 test_loss:95.90299987792969\n",
      "1177/3000 train_loss: 56.33592987060547 test_loss:88.13133239746094\n",
      "1178/3000 train_loss: 64.21178436279297 test_loss:84.27835083007812\n",
      "1179/3000 train_loss: 56.58850860595703 test_loss:90.28208923339844\n",
      "1180/3000 train_loss: 59.40471267700195 test_loss:84.22435760498047\n",
      "1181/3000 train_loss: 52.8697395324707 test_loss:85.0868148803711\n",
      "1182/3000 train_loss: 60.57417678833008 test_loss:83.97183227539062\n",
      "1183/3000 train_loss: 57.001564025878906 test_loss:84.83428955078125\n",
      "1184/3000 train_loss: 61.60936737060547 test_loss:83.78463745117188\n",
      "1185/3000 train_loss: 54.934226989746094 test_loss:80.034912109375\n",
      "1186/3000 train_loss: 64.20543670654297 test_loss:84.52733612060547\n",
      "1187/3000 train_loss: 55.692840576171875 test_loss:84.67237854003906\n",
      "1188/3000 train_loss: 55.25629425048828 test_loss:85.4513168334961\n",
      "1189/3000 train_loss: 66.38250732421875 test_loss:86.08334350585938\n",
      "1190/3000 train_loss: 68.35543823242188 test_loss:82.36349487304688\n",
      "1191/3000 train_loss: 62.11210632324219 test_loss:84.50254821777344\n",
      "1192/3000 train_loss: 53.34646224975586 test_loss:89.80610656738281\n",
      "1193/3000 train_loss: 53.16513442993164 test_loss:80.54011535644531\n",
      "1194/3000 train_loss: 54.38343811035156 test_loss:89.29344177246094\n",
      "1195/3000 train_loss: 53.112770080566406 test_loss:92.0124282836914\n",
      "1196/3000 train_loss: 58.13208770751953 test_loss:89.52511596679688\n",
      "1197/3000 train_loss: 57.93861389160156 test_loss:89.33445739746094\n",
      "1198/3000 train_loss: 58.48121643066406 test_loss:81.49760437011719\n",
      "1199/3000 train_loss: 60.43332290649414 test_loss:86.21847534179688\n",
      "1200/3000 train_loss: 60.48546600341797 test_loss:89.10566711425781\n",
      "1201/3000 train_loss: 59.19203186035156 test_loss:87.41952514648438\n",
      "1202/3000 train_loss: 61.26301574707031 test_loss:87.33734130859375\n",
      "1203/3000 train_loss: 59.30137634277344 test_loss:89.423828125\n",
      "1204/3000 train_loss: 61.45016860961914 test_loss:89.748291015625\n",
      "1205/3000 train_loss: 68.07743072509766 test_loss:89.42605590820312\n",
      "1206/3000 train_loss: 57.44271469116211 test_loss:89.53239440917969\n",
      "1207/3000 train_loss: 51.00156784057617 test_loss:83.89318084716797\n",
      "1208/3000 train_loss: 61.586761474609375 test_loss:86.56333923339844\n",
      "1209/3000 train_loss: 64.70024108886719 test_loss:86.00468444824219\n",
      "1210/3000 train_loss: 61.85872268676758 test_loss:81.67707824707031\n",
      "1211/3000 train_loss: 55.85822296142578 test_loss:84.46858215332031\n",
      "1212/3000 train_loss: 57.228214263916016 test_loss:84.353271484375\n",
      "1213/3000 train_loss: 61.663177490234375 test_loss:90.42033386230469\n",
      "1214/3000 train_loss: 57.30204772949219 test_loss:80.05828857421875\n",
      "1215/3000 train_loss: 56.435997009277344 test_loss:92.61087799072266\n",
      "1216/3000 train_loss: 52.69071960449219 test_loss:87.21728515625\n",
      "1217/3000 train_loss: 61.717506408691406 test_loss:83.5516128540039\n",
      "1218/3000 train_loss: 49.8112907409668 test_loss:86.23699951171875\n",
      "1219/3000 train_loss: 53.0174560546875 test_loss:80.48699951171875\n",
      "1220/3000 train_loss: 59.58879089355469 test_loss:82.16886901855469\n",
      "1221/3000 train_loss: 56.54668426513672 test_loss:79.69477844238281\n",
      "1222/3000 train_loss: 54.951873779296875 test_loss:86.59689331054688\n",
      "1223/3000 train_loss: 61.698272705078125 test_loss:84.62440490722656\n",
      "1224/3000 train_loss: 52.68877410888672 test_loss:81.60516357421875\n",
      "1225/3000 train_loss: 58.048519134521484 test_loss:78.09843444824219\n",
      "1226/3000 train_loss: 58.28649139404297 test_loss:84.44975280761719\n",
      "1227/3000 train_loss: 62.919837951660156 test_loss:87.06953430175781\n",
      "1228/3000 train_loss: 58.31059646606445 test_loss:82.28927612304688\n",
      "1229/3000 train_loss: 66.10142517089844 test_loss:83.39739990234375\n",
      "1230/3000 train_loss: 56.08991241455078 test_loss:89.60816955566406\n",
      "1231/3000 train_loss: 62.65845489501953 test_loss:82.13947296142578\n",
      "1232/3000 train_loss: 55.4035530090332 test_loss:87.76419830322266\n",
      "1233/3000 train_loss: 60.44237518310547 test_loss:81.14663696289062\n",
      "1234/3000 train_loss: 61.78712844848633 test_loss:81.544189453125\n",
      "1235/3000 train_loss: 57.81303405761719 test_loss:78.4459228515625\n",
      "1236/3000 train_loss: 56.294734954833984 test_loss:80.57940673828125\n",
      "1237/3000 train_loss: 56.99712371826172 test_loss:80.9273681640625\n",
      "1238/3000 train_loss: 55.6224250793457 test_loss:80.16348266601562\n",
      "1239/3000 train_loss: 54.00745391845703 test_loss:88.40065002441406\n",
      "1240/3000 train_loss: 56.786216735839844 test_loss:82.30284118652344\n",
      "1241/3000 train_loss: 54.18935012817383 test_loss:84.30460357666016\n",
      "1242/3000 train_loss: 53.97119903564453 test_loss:86.30499267578125\n",
      "1243/3000 train_loss: 55.666751861572266 test_loss:80.15864562988281\n",
      "1244/3000 train_loss: 52.892398834228516 test_loss:85.48272705078125\n",
      "1245/3000 train_loss: 61.500999450683594 test_loss:80.95997619628906\n",
      "1246/3000 train_loss: 50.30177688598633 test_loss:88.51107788085938\n",
      "1247/3000 train_loss: 56.19132995605469 test_loss:78.39515686035156\n",
      "1248/3000 train_loss: 53.71630096435547 test_loss:82.19268035888672\n",
      "1249/3000 train_loss: 57.916019439697266 test_loss:80.4859619140625\n",
      "1250/3000 train_loss: 64.51971435546875 test_loss:91.47480010986328\n",
      "1251/3000 train_loss: 60.989952087402344 test_loss:79.28546142578125\n",
      "1252/3000 train_loss: 52.26249313354492 test_loss:87.54216003417969\n",
      "1253/3000 train_loss: 52.08000564575195 test_loss:79.8525619506836\n",
      "1254/3000 train_loss: 56.8923454284668 test_loss:82.75801849365234\n",
      "1255/3000 train_loss: 54.172950744628906 test_loss:81.74781799316406\n",
      "1256/3000 train_loss: 50.847225189208984 test_loss:86.720947265625\n",
      "1257/3000 train_loss: 62.418190002441406 test_loss:90.62858581542969\n",
      "1258/3000 train_loss: 52.61747360229492 test_loss:82.49417114257812\n",
      "1259/3000 train_loss: 49.55464553833008 test_loss:81.25315856933594\n",
      "1260/3000 train_loss: 58.929813385009766 test_loss:80.70722961425781\n",
      "1261/3000 train_loss: 54.691280364990234 test_loss:82.42889404296875\n",
      "1262/3000 train_loss: 49.78288269042969 test_loss:82.53445434570312\n",
      "1263/3000 train_loss: 56.774024963378906 test_loss:85.82416534423828\n",
      "1264/3000 train_loss: 50.01819610595703 test_loss:83.86254119873047\n",
      "1265/3000 train_loss: 57.958553314208984 test_loss:79.07716369628906\n",
      "1266/3000 train_loss: 51.643898010253906 test_loss:86.47329711914062\n",
      "1267/3000 train_loss: 53.451683044433594 test_loss:86.10128021240234\n",
      "1268/3000 train_loss: 56.8812141418457 test_loss:89.72172546386719\n",
      "1269/3000 train_loss: 60.07583236694336 test_loss:84.49118041992188\n",
      "1270/3000 train_loss: 55.3583984375 test_loss:81.84585571289062\n",
      "1271/3000 train_loss: 53.13156509399414 test_loss:79.25981140136719\n",
      "1272/3000 train_loss: 47.19257354736328 test_loss:86.644775390625\n",
      "1273/3000 train_loss: 52.078468322753906 test_loss:84.25569152832031\n",
      "1274/3000 train_loss: 49.941673278808594 test_loss:82.82568359375\n",
      "1275/3000 train_loss: 58.019596099853516 test_loss:76.833251953125\n",
      "1276/3000 train_loss: 59.177406311035156 test_loss:84.99727630615234\n",
      "1277/3000 train_loss: 63.607093811035156 test_loss:77.764404296875\n",
      "1278/3000 train_loss: 53.221641540527344 test_loss:81.83680725097656\n",
      "1279/3000 train_loss: 54.956050872802734 test_loss:78.72698974609375\n",
      "1280/3000 train_loss: 54.105712890625 test_loss:78.32467651367188\n",
      "1281/3000 train_loss: 57.29253005981445 test_loss:88.22674560546875\n",
      "1282/3000 train_loss: 58.622581481933594 test_loss:81.4716567993164\n",
      "1283/3000 train_loss: 52.24477005004883 test_loss:91.7922592163086\n",
      "1284/3000 train_loss: 57.55345153808594 test_loss:79.06390380859375\n",
      "1285/3000 train_loss: 45.66731262207031 test_loss:87.63107299804688\n",
      "1286/3000 train_loss: 50.5105094909668 test_loss:85.35311889648438\n",
      "1287/3000 train_loss: 51.75149917602539 test_loss:85.65628051757812\n",
      "1288/3000 train_loss: 52.728580474853516 test_loss:83.21746063232422\n",
      "1289/3000 train_loss: 51.302589416503906 test_loss:84.91979217529297\n",
      "1290/3000 train_loss: 55.7535514831543 test_loss:78.89488220214844\n",
      "1291/3000 train_loss: 51.78568649291992 test_loss:91.07720947265625\n",
      "1292/3000 train_loss: 44.35856246948242 test_loss:74.14036560058594\n",
      "1293/3000 train_loss: 50.1206169128418 test_loss:79.88683319091797\n",
      "1294/3000 train_loss: 52.30336380004883 test_loss:78.10260009765625\n",
      "1295/3000 train_loss: 63.72002410888672 test_loss:76.79588317871094\n",
      "1296/3000 train_loss: 53.49725341796875 test_loss:76.25555419921875\n",
      "1297/3000 train_loss: 54.43940734863281 test_loss:79.15219116210938\n",
      "1298/3000 train_loss: 48.75455856323242 test_loss:81.07276916503906\n",
      "1299/3000 train_loss: 57.09307861328125 test_loss:72.79196166992188\n",
      "1300/3000 train_loss: 56.09148406982422 test_loss:77.42593383789062\n",
      "1301/3000 train_loss: 46.11588668823242 test_loss:76.55907440185547\n",
      "1302/3000 train_loss: 52.03072738647461 test_loss:76.13154602050781\n",
      "1303/3000 train_loss: 53.32186508178711 test_loss:76.93919372558594\n",
      "1304/3000 train_loss: 53.46876907348633 test_loss:87.65751647949219\n",
      "1305/3000 train_loss: 53.1496696472168 test_loss:75.18316650390625\n",
      "1306/3000 train_loss: 55.45846176147461 test_loss:77.56818389892578\n",
      "1307/3000 train_loss: 49.28390121459961 test_loss:76.6748046875\n",
      "1308/3000 train_loss: 47.81634521484375 test_loss:82.35667419433594\n",
      "1309/3000 train_loss: 52.78314208984375 test_loss:73.03946685791016\n",
      "1310/3000 train_loss: 48.58243942260742 test_loss:88.06272888183594\n",
      "1311/3000 train_loss: 52.76697540283203 test_loss:82.036865234375\n",
      "1312/3000 train_loss: 48.383602142333984 test_loss:81.59405517578125\n",
      "1313/3000 train_loss: 52.57675552368164 test_loss:78.39785766601562\n",
      "1314/3000 train_loss: 45.67684555053711 test_loss:82.97056579589844\n",
      "1315/3000 train_loss: 51.76469039916992 test_loss:78.83265686035156\n",
      "1316/3000 train_loss: 55.51762008666992 test_loss:76.17557525634766\n",
      "1317/3000 train_loss: 48.68450927734375 test_loss:80.8580322265625\n",
      "1318/3000 train_loss: 50.6304931640625 test_loss:78.73472595214844\n",
      "1319/3000 train_loss: 55.628910064697266 test_loss:78.37303161621094\n",
      "1320/3000 train_loss: 48.7194709777832 test_loss:72.5047607421875\n",
      "1321/3000 train_loss: 48.61104202270508 test_loss:80.72038269042969\n",
      "1322/3000 train_loss: 63.293357849121094 test_loss:75.71897888183594\n",
      "1323/3000 train_loss: 49.462520599365234 test_loss:81.26588439941406\n",
      "1324/3000 train_loss: 52.53676986694336 test_loss:79.23750305175781\n",
      "1325/3000 train_loss: 51.062660217285156 test_loss:76.17107391357422\n",
      "1326/3000 train_loss: 51.63198471069336 test_loss:86.13628387451172\n",
      "1327/3000 train_loss: 50.23401641845703 test_loss:78.27786254882812\n",
      "1328/3000 train_loss: 48.11713790893555 test_loss:83.40152740478516\n",
      "1329/3000 train_loss: 51.82966995239258 test_loss:75.38146209716797\n",
      "1330/3000 train_loss: 50.315738677978516 test_loss:77.42291259765625\n",
      "1331/3000 train_loss: 53.22102355957031 test_loss:75.88114929199219\n",
      "1332/3000 train_loss: 47.15755844116211 test_loss:74.66048431396484\n",
      "1333/3000 train_loss: 47.00149917602539 test_loss:78.62376403808594\n",
      "1334/3000 train_loss: 55.69431686401367 test_loss:78.0633773803711\n",
      "1335/3000 train_loss: 47.77531814575195 test_loss:73.43583679199219\n",
      "1336/3000 train_loss: 51.23377990722656 test_loss:77.19068908691406\n",
      "1337/3000 train_loss: 47.91016387939453 test_loss:80.54391479492188\n",
      "1338/3000 train_loss: 49.42998504638672 test_loss:73.93570709228516\n",
      "1339/3000 train_loss: 45.7375373840332 test_loss:74.60919952392578\n",
      "1340/3000 train_loss: 48.74367141723633 test_loss:77.24522399902344\n",
      "1341/3000 train_loss: 53.41081237792969 test_loss:77.77792358398438\n",
      "1342/3000 train_loss: 58.457706451416016 test_loss:88.97892761230469\n",
      "1343/3000 train_loss: 54.95795440673828 test_loss:86.43238067626953\n",
      "1344/3000 train_loss: 54.81852340698242 test_loss:77.40644073486328\n",
      "1345/3000 train_loss: 67.3952865600586 test_loss:79.6210708618164\n",
      "1346/3000 train_loss: 48.89773941040039 test_loss:73.67120361328125\n",
      "1347/3000 train_loss: 53.9626579284668 test_loss:72.00186920166016\n",
      "1348/3000 train_loss: 58.13317108154297 test_loss:71.59877014160156\n",
      "1349/3000 train_loss: 51.65519714355469 test_loss:74.3375473022461\n",
      "1350/3000 train_loss: 60.01947021484375 test_loss:79.78326416015625\n",
      "1351/3000 train_loss: 52.2526741027832 test_loss:84.14234924316406\n",
      "1352/3000 train_loss: 50.667171478271484 test_loss:76.90678405761719\n",
      "1353/3000 train_loss: 48.31460189819336 test_loss:77.01893615722656\n",
      "1354/3000 train_loss: 46.183074951171875 test_loss:84.43992614746094\n",
      "1355/3000 train_loss: 50.530765533447266 test_loss:71.07257080078125\n",
      "1356/3000 train_loss: 56.651424407958984 test_loss:81.58084869384766\n",
      "1357/3000 train_loss: 56.775569915771484 test_loss:77.56459045410156\n",
      "1358/3000 train_loss: 53.06083679199219 test_loss:76.54051208496094\n",
      "1359/3000 train_loss: 42.190181732177734 test_loss:78.31355285644531\n",
      "1360/3000 train_loss: 52.437435150146484 test_loss:75.88088989257812\n",
      "1361/3000 train_loss: 50.48139953613281 test_loss:80.34333801269531\n",
      "1362/3000 train_loss: 48.55567932128906 test_loss:74.07554626464844\n",
      "1363/3000 train_loss: 55.05488967895508 test_loss:82.06401062011719\n",
      "1364/3000 train_loss: 49.54714584350586 test_loss:74.90770721435547\n",
      "1365/3000 train_loss: 45.33614730834961 test_loss:78.19749450683594\n",
      "1366/3000 train_loss: 57.29353713989258 test_loss:73.25363159179688\n",
      "1367/3000 train_loss: 52.35360336303711 test_loss:81.56745910644531\n",
      "1368/3000 train_loss: 53.703834533691406 test_loss:79.92227172851562\n",
      "1369/3000 train_loss: 52.83933639526367 test_loss:76.09400939941406\n",
      "1370/3000 train_loss: 53.66777801513672 test_loss:77.8897705078125\n",
      "1371/3000 train_loss: 48.102195739746094 test_loss:85.13580322265625\n",
      "1372/3000 train_loss: 55.050479888916016 test_loss:85.0439453125\n",
      "1373/3000 train_loss: 59.03225326538086 test_loss:76.50548553466797\n",
      "1374/3000 train_loss: 40.99616241455078 test_loss:79.52075958251953\n",
      "1375/3000 train_loss: 48.07514572143555 test_loss:82.15353393554688\n",
      "1376/3000 train_loss: 48.28385925292969 test_loss:76.78947448730469\n",
      "1377/3000 train_loss: 49.84654235839844 test_loss:82.03707885742188\n",
      "1378/3000 train_loss: 57.3908576965332 test_loss:77.00321960449219\n",
      "1379/3000 train_loss: 46.105194091796875 test_loss:78.0080337524414\n",
      "1380/3000 train_loss: 46.75674819946289 test_loss:72.73571014404297\n",
      "1381/3000 train_loss: 46.006385803222656 test_loss:83.51019287109375\n",
      "1382/3000 train_loss: 53.926815032958984 test_loss:73.35726928710938\n",
      "1383/3000 train_loss: 49.617183685302734 test_loss:84.53254699707031\n",
      "1384/3000 train_loss: 58.8368034362793 test_loss:78.01708984375\n",
      "1385/3000 train_loss: 59.163291931152344 test_loss:78.86744689941406\n",
      "1386/3000 train_loss: 50.946842193603516 test_loss:79.74702453613281\n",
      "1387/3000 train_loss: 51.66653823852539 test_loss:75.34022521972656\n",
      "1388/3000 train_loss: 60.00695037841797 test_loss:80.31001281738281\n",
      "1389/3000 train_loss: 56.02338409423828 test_loss:78.02179718017578\n",
      "1390/3000 train_loss: 49.72370147705078 test_loss:77.37367248535156\n",
      "1391/3000 train_loss: 63.02610778808594 test_loss:73.6148681640625\n",
      "1392/3000 train_loss: 48.66936111450195 test_loss:84.69656372070312\n",
      "1393/3000 train_loss: 55.383888244628906 test_loss:69.35871124267578\n",
      "1394/3000 train_loss: 56.03550720214844 test_loss:95.25363159179688\n",
      "1395/3000 train_loss: 58.66282272338867 test_loss:70.00957489013672\n",
      "1396/3000 train_loss: 51.59962463378906 test_loss:77.19985961914062\n",
      "1397/3000 train_loss: 50.18117141723633 test_loss:76.50433349609375\n",
      "1398/3000 train_loss: 56.75075149536133 test_loss:77.49649810791016\n",
      "1399/3000 train_loss: 48.549598693847656 test_loss:83.70979309082031\n",
      "1400/3000 train_loss: 53.01458740234375 test_loss:73.63117980957031\n",
      "1401/3000 train_loss: 54.73579788208008 test_loss:75.93539428710938\n",
      "1402/3000 train_loss: 49.177337646484375 test_loss:82.97864532470703\n",
      "1403/3000 train_loss: 47.04128646850586 test_loss:73.00965881347656\n",
      "1404/3000 train_loss: 53.18864440917969 test_loss:78.3194580078125\n",
      "1405/3000 train_loss: 50.226314544677734 test_loss:71.34019470214844\n",
      "1406/3000 train_loss: 49.34513854980469 test_loss:72.64704895019531\n",
      "1407/3000 train_loss: 53.68456268310547 test_loss:83.30757141113281\n",
      "1408/3000 train_loss: 48.93041229248047 test_loss:75.56697845458984\n",
      "1409/3000 train_loss: 49.64597702026367 test_loss:84.09745025634766\n",
      "1410/3000 train_loss: 44.07814407348633 test_loss:77.82420349121094\n",
      "1411/3000 train_loss: 46.272987365722656 test_loss:73.26436614990234\n",
      "1412/3000 train_loss: 48.19080352783203 test_loss:71.777099609375\n",
      "1413/3000 train_loss: 52.8521842956543 test_loss:74.22088623046875\n",
      "1414/3000 train_loss: 49.71544647216797 test_loss:72.93948364257812\n",
      "1415/3000 train_loss: 46.53660202026367 test_loss:79.99796295166016\n",
      "1416/3000 train_loss: 46.732337951660156 test_loss:71.0834732055664\n",
      "1417/3000 train_loss: 53.556549072265625 test_loss:84.72327423095703\n",
      "1418/3000 train_loss: 58.81808853149414 test_loss:77.12750244140625\n",
      "1419/3000 train_loss: 60.5379753112793 test_loss:73.91935729980469\n",
      "1420/3000 train_loss: 50.91339111328125 test_loss:82.80131530761719\n",
      "1421/3000 train_loss: 46.80029296875 test_loss:71.0010986328125\n",
      "1422/3000 train_loss: 44.69586181640625 test_loss:84.54444885253906\n",
      "1423/3000 train_loss: 55.76979446411133 test_loss:69.94094848632812\n",
      "1424/3000 train_loss: 56.40983581542969 test_loss:76.86660766601562\n",
      "1425/3000 train_loss: 47.4138298034668 test_loss:75.74735260009766\n",
      "1426/3000 train_loss: 53.11943435668945 test_loss:73.57644653320312\n",
      "1427/3000 train_loss: 46.40028762817383 test_loss:82.50045013427734\n",
      "1428/3000 train_loss: 41.51577377319336 test_loss:76.5409164428711\n",
      "1429/3000 train_loss: 48.923919677734375 test_loss:74.8650131225586\n",
      "1430/3000 train_loss: 46.256690979003906 test_loss:77.93172454833984\n",
      "1431/3000 train_loss: 44.67412567138672 test_loss:76.082763671875\n",
      "1432/3000 train_loss: 51.34368896484375 test_loss:76.06138610839844\n",
      "1433/3000 train_loss: 52.95322036743164 test_loss:72.33212280273438\n",
      "1434/3000 train_loss: 47.35597229003906 test_loss:82.08255004882812\n",
      "1435/3000 train_loss: 47.509159088134766 test_loss:76.19161987304688\n",
      "1436/3000 train_loss: 47.758968353271484 test_loss:84.41095733642578\n",
      "1437/3000 train_loss: 45.033878326416016 test_loss:72.70706176757812\n",
      "1438/3000 train_loss: 54.74415588378906 test_loss:81.95523071289062\n",
      "1439/3000 train_loss: 48.09468078613281 test_loss:75.0050048828125\n",
      "1440/3000 train_loss: 47.70417785644531 test_loss:77.57861328125\n",
      "1441/3000 train_loss: 52.769954681396484 test_loss:68.77464294433594\n",
      "1442/3000 train_loss: 44.85274124145508 test_loss:81.00663757324219\n",
      "1443/3000 train_loss: 52.46138000488281 test_loss:72.76058959960938\n",
      "1444/3000 train_loss: 48.892127990722656 test_loss:78.31625366210938\n",
      "1445/3000 train_loss: 48.87253189086914 test_loss:72.08114624023438\n",
      "1446/3000 train_loss: 46.76084518432617 test_loss:78.56097412109375\n",
      "1447/3000 train_loss: 50.45379638671875 test_loss:74.13562774658203\n",
      "1448/3000 train_loss: 43.40789794921875 test_loss:72.28701782226562\n",
      "1449/3000 train_loss: 49.53817367553711 test_loss:75.06768798828125\n",
      "1450/3000 train_loss: 45.237735748291016 test_loss:79.56036376953125\n",
      "1451/3000 train_loss: 52.87422180175781 test_loss:77.83731079101562\n",
      "1452/3000 train_loss: 58.48396682739258 test_loss:79.06484985351562\n",
      "1453/3000 train_loss: 50.51973342895508 test_loss:77.10487365722656\n",
      "1454/3000 train_loss: 45.97819519042969 test_loss:70.99820709228516\n",
      "1455/3000 train_loss: 44.31690216064453 test_loss:69.90498352050781\n",
      "1456/3000 train_loss: 45.98300552368164 test_loss:80.63976287841797\n",
      "1457/3000 train_loss: 51.613468170166016 test_loss:68.8533935546875\n",
      "1458/3000 train_loss: 49.257354736328125 test_loss:72.81871032714844\n",
      "1459/3000 train_loss: 53.93444061279297 test_loss:83.24032592773438\n",
      "1460/3000 train_loss: 67.77965545654297 test_loss:72.03175354003906\n",
      "1461/3000 train_loss: 48.61725997924805 test_loss:80.73855590820312\n",
      "1462/3000 train_loss: 46.13594436645508 test_loss:74.56920623779297\n",
      "1463/3000 train_loss: 50.54096221923828 test_loss:77.19903564453125\n",
      "1464/3000 train_loss: 57.02154541015625 test_loss:78.28652954101562\n",
      "1465/3000 train_loss: 43.848663330078125 test_loss:77.68096923828125\n",
      "1466/3000 train_loss: 50.330440521240234 test_loss:72.35100555419922\n",
      "1467/3000 train_loss: 49.49824905395508 test_loss:79.53575897216797\n",
      "1468/3000 train_loss: 41.307498931884766 test_loss:73.60977172851562\n",
      "1469/3000 train_loss: 46.963531494140625 test_loss:74.62149047851562\n",
      "1470/3000 train_loss: 43.92098617553711 test_loss:78.56381225585938\n",
      "1471/3000 train_loss: 48.2023811340332 test_loss:76.14591979980469\n",
      "1472/3000 train_loss: 46.534210205078125 test_loss:75.18167114257812\n",
      "1473/3000 train_loss: 50.29343032836914 test_loss:72.22628784179688\n",
      "1474/3000 train_loss: 42.0838623046875 test_loss:70.57280731201172\n",
      "1475/3000 train_loss: 46.06940460205078 test_loss:73.38487243652344\n",
      "1476/3000 train_loss: 43.271278381347656 test_loss:70.70108795166016\n",
      "1477/3000 train_loss: 44.349029541015625 test_loss:69.65867614746094\n",
      "1478/3000 train_loss: 53.3100471496582 test_loss:71.72138977050781\n",
      "1479/3000 train_loss: 50.37544631958008 test_loss:74.52333068847656\n",
      "1480/3000 train_loss: 48.99021911621094 test_loss:69.98003387451172\n",
      "1481/3000 train_loss: 42.90177536010742 test_loss:75.09190368652344\n",
      "1482/3000 train_loss: 47.063804626464844 test_loss:79.09614562988281\n",
      "1483/3000 train_loss: 54.22944259643555 test_loss:69.54537200927734\n",
      "1484/3000 train_loss: 45.11766052246094 test_loss:83.31621551513672\n",
      "1485/3000 train_loss: 47.339073181152344 test_loss:72.89012145996094\n",
      "1486/3000 train_loss: 55.871826171875 test_loss:72.069091796875\n",
      "1487/3000 train_loss: 46.342254638671875 test_loss:71.73037719726562\n",
      "1488/3000 train_loss: 48.100467681884766 test_loss:70.07865142822266\n",
      "1489/3000 train_loss: 44.11260986328125 test_loss:72.94038391113281\n",
      "1490/3000 train_loss: 43.04740524291992 test_loss:76.60598754882812\n",
      "1491/3000 train_loss: 43.05231475830078 test_loss:71.69474792480469\n",
      "1492/3000 train_loss: 55.737613677978516 test_loss:73.82670593261719\n",
      "1493/3000 train_loss: 44.38996505737305 test_loss:74.38042449951172\n",
      "1494/3000 train_loss: 43.534324645996094 test_loss:75.30235290527344\n",
      "1495/3000 train_loss: 48.48550796508789 test_loss:69.69297790527344\n",
      "1496/3000 train_loss: 53.88865280151367 test_loss:71.63762664794922\n",
      "1497/3000 train_loss: 46.91626739501953 test_loss:69.89087677001953\n",
      "1498/3000 train_loss: 47.55110168457031 test_loss:68.96636199951172\n",
      "1499/3000 train_loss: 58.92185974121094 test_loss:76.71891784667969\n",
      "1500/3000 train_loss: 44.561187744140625 test_loss:68.17039489746094\n",
      "1501/3000 train_loss: 48.43731689453125 test_loss:80.4604721069336\n",
      "1502/3000 train_loss: 48.382362365722656 test_loss:72.16021728515625\n",
      "1503/3000 train_loss: 55.527713775634766 test_loss:77.32005310058594\n",
      "1504/3000 train_loss: 49.22060775756836 test_loss:76.59251403808594\n",
      "1505/3000 train_loss: 43.96847152709961 test_loss:71.11306762695312\n",
      "1506/3000 train_loss: 54.09328079223633 test_loss:76.57785034179688\n",
      "1507/3000 train_loss: 49.03936004638672 test_loss:71.59375\n",
      "1508/3000 train_loss: 42.522987365722656 test_loss:75.8955078125\n",
      "1509/3000 train_loss: 54.782386779785156 test_loss:71.96844482421875\n",
      "1510/3000 train_loss: 44.39057159423828 test_loss:69.66143798828125\n",
      "1511/3000 train_loss: 49.6795654296875 test_loss:73.59284210205078\n",
      "1512/3000 train_loss: 50.1611213684082 test_loss:70.4566650390625\n",
      "1513/3000 train_loss: 40.646202087402344 test_loss:76.08195495605469\n",
      "1514/3000 train_loss: 44.293006896972656 test_loss:71.070556640625\n",
      "1515/3000 train_loss: 46.298789978027344 test_loss:70.3878173828125\n",
      "1516/3000 train_loss: 53.50749206542969 test_loss:79.68325805664062\n",
      "1517/3000 train_loss: 48.44593811035156 test_loss:68.840576171875\n",
      "1518/3000 train_loss: 52.151817321777344 test_loss:70.6590805053711\n",
      "1519/3000 train_loss: 45.53596496582031 test_loss:74.37644958496094\n",
      "1520/3000 train_loss: 43.77073287963867 test_loss:73.71100616455078\n",
      "1521/3000 train_loss: 48.66178512573242 test_loss:78.64410400390625\n",
      "1522/3000 train_loss: 39.7546272277832 test_loss:68.35115051269531\n",
      "1523/3000 train_loss: 45.508148193359375 test_loss:71.28191375732422\n",
      "1524/3000 train_loss: 49.59571075439453 test_loss:73.94636535644531\n",
      "1525/3000 train_loss: 48.321353912353516 test_loss:75.43168640136719\n",
      "1526/3000 train_loss: 50.42829895019531 test_loss:74.4266357421875\n",
      "1527/3000 train_loss: 48.803977966308594 test_loss:77.24551391601562\n",
      "1528/3000 train_loss: 45.799564361572266 test_loss:72.52586364746094\n",
      "1529/3000 train_loss: 43.36453628540039 test_loss:73.00344848632812\n",
      "1530/3000 train_loss: 43.959388732910156 test_loss:74.98016357421875\n",
      "1531/3000 train_loss: 44.78321075439453 test_loss:75.32438659667969\n",
      "1532/3000 train_loss: 45.5660514831543 test_loss:77.13616943359375\n",
      "1533/3000 train_loss: 45.67564392089844 test_loss:75.09288024902344\n",
      "1534/3000 train_loss: 52.14012145996094 test_loss:71.93463897705078\n",
      "1535/3000 train_loss: 45.63653564453125 test_loss:73.32820892333984\n",
      "1536/3000 train_loss: 48.447296142578125 test_loss:70.92208862304688\n",
      "1537/3000 train_loss: 44.13818359375 test_loss:80.75244140625\n",
      "1538/3000 train_loss: 49.019439697265625 test_loss:69.42308044433594\n",
      "1539/3000 train_loss: 47.037750244140625 test_loss:69.55252075195312\n",
      "1540/3000 train_loss: 49.800071716308594 test_loss:80.09989929199219\n",
      "1541/3000 train_loss: 53.960052490234375 test_loss:73.46473693847656\n",
      "1542/3000 train_loss: 51.937950134277344 test_loss:73.7705078125\n",
      "1543/3000 train_loss: 45.24822998046875 test_loss:76.8153305053711\n",
      "1544/3000 train_loss: 54.397666931152344 test_loss:89.28807067871094\n",
      "1545/3000 train_loss: 48.65412521362305 test_loss:73.08741760253906\n",
      "1546/3000 train_loss: 47.23345947265625 test_loss:78.76866149902344\n",
      "1547/3000 train_loss: 50.53877258300781 test_loss:75.96452331542969\n",
      "1548/3000 train_loss: 47.11410140991211 test_loss:82.56031799316406\n",
      "1549/3000 train_loss: 48.55520248413086 test_loss:74.34744262695312\n",
      "1550/3000 train_loss: 50.3831672668457 test_loss:78.75118255615234\n",
      "1551/3000 train_loss: 51.04195785522461 test_loss:82.58551025390625\n",
      "1552/3000 train_loss: 55.23105239868164 test_loss:68.92219543457031\n",
      "1553/3000 train_loss: 48.22328186035156 test_loss:68.0506591796875\n",
      "1554/3000 train_loss: 56.0792350769043 test_loss:69.28981018066406\n",
      "1555/3000 train_loss: 46.4830436706543 test_loss:81.72947692871094\n",
      "1556/3000 train_loss: 47.08176803588867 test_loss:73.36863708496094\n",
      "1557/3000 train_loss: 44.72052001953125 test_loss:69.84312438964844\n",
      "1558/3000 train_loss: 44.0236930847168 test_loss:69.60741424560547\n",
      "1559/3000 train_loss: 45.58715057373047 test_loss:71.6466064453125\n",
      "1560/3000 train_loss: 47.35908126831055 test_loss:74.86910247802734\n",
      "1561/3000 train_loss: 40.6290283203125 test_loss:74.40206909179688\n",
      "1562/3000 train_loss: 44.74520492553711 test_loss:69.69337463378906\n",
      "1563/3000 train_loss: 56.92422103881836 test_loss:70.13777160644531\n",
      "1564/3000 train_loss: 46.33086395263672 test_loss:79.73123168945312\n",
      "1565/3000 train_loss: 46.97731018066406 test_loss:72.44554138183594\n",
      "1566/3000 train_loss: 45.17927169799805 test_loss:68.51365661621094\n",
      "1567/3000 train_loss: 40.0067138671875 test_loss:72.37158203125\n",
      "1568/3000 train_loss: 42.610260009765625 test_loss:68.76373291015625\n",
      "1569/3000 train_loss: 52.140663146972656 test_loss:72.40843200683594\n",
      "1570/3000 train_loss: 46.407623291015625 test_loss:66.87873840332031\n",
      "1571/3000 train_loss: 44.30615997314453 test_loss:80.76121520996094\n",
      "1572/3000 train_loss: 48.184837341308594 test_loss:67.99459075927734\n",
      "1573/3000 train_loss: 49.91779327392578 test_loss:84.28885650634766\n",
      "1574/3000 train_loss: 46.71268081665039 test_loss:72.89950561523438\n",
      "1575/3000 train_loss: 54.06640625 test_loss:66.34078979492188\n",
      "1576/3000 train_loss: 45.72246551513672 test_loss:79.06986999511719\n",
      "1577/3000 train_loss: 48.594234466552734 test_loss:67.1939697265625\n",
      "1578/3000 train_loss: 51.8093147277832 test_loss:71.6166000366211\n",
      "1579/3000 train_loss: 44.9283561706543 test_loss:74.2628402709961\n",
      "1580/3000 train_loss: 48.383094787597656 test_loss:70.75810241699219\n",
      "1581/3000 train_loss: 50.74777603149414 test_loss:75.88815307617188\n",
      "1582/3000 train_loss: 45.59456253051758 test_loss:67.54830169677734\n",
      "1583/3000 train_loss: 47.93366622924805 test_loss:78.02226257324219\n",
      "1584/3000 train_loss: 45.25104904174805 test_loss:75.38287353515625\n",
      "1585/3000 train_loss: 51.72423553466797 test_loss:70.50103759765625\n",
      "1586/3000 train_loss: 44.999237060546875 test_loss:79.1982421875\n",
      "1587/3000 train_loss: 43.14250183105469 test_loss:69.6692123413086\n",
      "1588/3000 train_loss: 48.3527946472168 test_loss:76.3963623046875\n",
      "1589/3000 train_loss: 50.07789611816406 test_loss:69.40292358398438\n",
      "1590/3000 train_loss: 54.13728713989258 test_loss:70.31554412841797\n",
      "1591/3000 train_loss: 45.69585037231445 test_loss:75.95504760742188\n",
      "1592/3000 train_loss: 40.94642639160156 test_loss:67.87541198730469\n",
      "1593/3000 train_loss: 48.56834411621094 test_loss:78.68930053710938\n",
      "1594/3000 train_loss: 43.25715255737305 test_loss:66.97334289550781\n",
      "1595/3000 train_loss: 44.7080192565918 test_loss:70.14810180664062\n",
      "1596/3000 train_loss: 47.20799255371094 test_loss:66.58938598632812\n",
      "1597/3000 train_loss: 44.303348541259766 test_loss:77.09148406982422\n",
      "1598/3000 train_loss: 45.01511001586914 test_loss:70.46051025390625\n",
      "1599/3000 train_loss: 46.00366973876953 test_loss:70.70880126953125\n",
      "1600/3000 train_loss: 47.42472839355469 test_loss:69.83154296875\n",
      "1601/3000 train_loss: 53.62590408325195 test_loss:72.65383911132812\n",
      "1602/3000 train_loss: 47.20553970336914 test_loss:70.1025619506836\n",
      "1603/3000 train_loss: 43.02479553222656 test_loss:66.28515625\n",
      "1604/3000 train_loss: 46.33357238769531 test_loss:73.1365966796875\n",
      "1605/3000 train_loss: 47.674964904785156 test_loss:65.81169128417969\n",
      "1606/3000 train_loss: 52.227699279785156 test_loss:68.86309051513672\n",
      "1607/3000 train_loss: 43.545082092285156 test_loss:74.24832153320312\n",
      "1608/3000 train_loss: 41.68755340576172 test_loss:68.22433471679688\n",
      "1609/3000 train_loss: 42.31108093261719 test_loss:73.16495513916016\n",
      "1610/3000 train_loss: 45.50802993774414 test_loss:66.16059875488281\n",
      "1611/3000 train_loss: 48.11468505859375 test_loss:74.3510971069336\n",
      "1612/3000 train_loss: 49.48039245605469 test_loss:65.11129760742188\n",
      "1613/3000 train_loss: 54.53177261352539 test_loss:81.685302734375\n",
      "1614/3000 train_loss: 47.70790481567383 test_loss:69.67899322509766\n",
      "1615/3000 train_loss: 43.309261322021484 test_loss:79.25453186035156\n",
      "1616/3000 train_loss: 42.25494384765625 test_loss:67.72744750976562\n",
      "1617/3000 train_loss: 40.52289962768555 test_loss:75.28565979003906\n",
      "1618/3000 train_loss: 43.74966812133789 test_loss:65.1180419921875\n",
      "1619/3000 train_loss: 39.767879486083984 test_loss:74.27739715576172\n",
      "1620/3000 train_loss: 40.544857025146484 test_loss:66.19595336914062\n",
      "1621/3000 train_loss: 43.115806579589844 test_loss:74.14459228515625\n",
      "1622/3000 train_loss: 50.928794860839844 test_loss:66.86170959472656\n",
      "1623/3000 train_loss: 47.683311462402344 test_loss:81.32568359375\n",
      "1624/3000 train_loss: 41.04487609863281 test_loss:65.32963562011719\n",
      "1625/3000 train_loss: 46.46229934692383 test_loss:72.83590698242188\n",
      "1626/3000 train_loss: 44.680999755859375 test_loss:74.35578918457031\n",
      "1627/3000 train_loss: 41.08469009399414 test_loss:71.9621810913086\n",
      "1628/3000 train_loss: 44.38359069824219 test_loss:74.34638977050781\n",
      "1629/3000 train_loss: 50.80097961425781 test_loss:76.440673828125\n",
      "1630/3000 train_loss: 42.55466079711914 test_loss:67.76144409179688\n",
      "1631/3000 train_loss: 43.55389404296875 test_loss:69.7737808227539\n",
      "1632/3000 train_loss: 46.42525863647461 test_loss:69.9173355102539\n",
      "1633/3000 train_loss: 45.87032699584961 test_loss:65.81791687011719\n",
      "1634/3000 train_loss: 51.60635757446289 test_loss:66.77708435058594\n",
      "1635/3000 train_loss: 48.69770431518555 test_loss:66.05838775634766\n",
      "1636/3000 train_loss: 47.26613235473633 test_loss:71.6614990234375\n",
      "1637/3000 train_loss: 43.35513687133789 test_loss:73.81037902832031\n",
      "1638/3000 train_loss: 52.30105209350586 test_loss:64.0943832397461\n",
      "1639/3000 train_loss: 48.656551361083984 test_loss:64.81303405761719\n",
      "1640/3000 train_loss: 39.34977340698242 test_loss:70.24833679199219\n",
      "1641/3000 train_loss: 48.24065017700195 test_loss:66.95919799804688\n",
      "1642/3000 train_loss: 51.89094543457031 test_loss:69.00957489013672\n",
      "1643/3000 train_loss: 49.429161071777344 test_loss:72.4234619140625\n",
      "1644/3000 train_loss: 44.7254753112793 test_loss:74.60258483886719\n",
      "1645/3000 train_loss: 54.28970718383789 test_loss:72.63925170898438\n",
      "1646/3000 train_loss: 37.92359924316406 test_loss:66.88128662109375\n",
      "1647/3000 train_loss: 46.104209899902344 test_loss:76.6324462890625\n",
      "1648/3000 train_loss: 42.258705139160156 test_loss:66.82344055175781\n",
      "1649/3000 train_loss: 47.46009063720703 test_loss:74.64393615722656\n",
      "1650/3000 train_loss: 47.08970260620117 test_loss:70.76536560058594\n",
      "1651/3000 train_loss: 56.717323303222656 test_loss:67.5323486328125\n",
      "1652/3000 train_loss: 46.185211181640625 test_loss:66.83415222167969\n",
      "1653/3000 train_loss: 46.04578399658203 test_loss:67.82886505126953\n",
      "1654/3000 train_loss: 43.389244079589844 test_loss:73.1683578491211\n",
      "1655/3000 train_loss: 50.631717681884766 test_loss:64.86909484863281\n",
      "1656/3000 train_loss: 44.313140869140625 test_loss:69.98411560058594\n",
      "1657/3000 train_loss: 45.56236267089844 test_loss:71.0958480834961\n",
      "1658/3000 train_loss: 45.839447021484375 test_loss:67.54052734375\n",
      "1659/3000 train_loss: 47.87535095214844 test_loss:67.87019348144531\n",
      "1660/3000 train_loss: 53.99214172363281 test_loss:73.91142272949219\n",
      "1661/3000 train_loss: 49.257652282714844 test_loss:66.56144714355469\n",
      "1662/3000 train_loss: 45.68279266357422 test_loss:73.59516143798828\n",
      "1663/3000 train_loss: 52.51531982421875 test_loss:67.51085662841797\n",
      "1664/3000 train_loss: 42.800201416015625 test_loss:65.82548522949219\n",
      "1665/3000 train_loss: 41.68782424926758 test_loss:73.40882873535156\n",
      "1666/3000 train_loss: 48.960350036621094 test_loss:66.86731719970703\n",
      "1667/3000 train_loss: 44.586849212646484 test_loss:69.04644012451172\n",
      "1668/3000 train_loss: 46.218902587890625 test_loss:70.506591796875\n",
      "1669/3000 train_loss: 46.71794128417969 test_loss:67.30854034423828\n",
      "1670/3000 train_loss: 46.29318618774414 test_loss:72.42170715332031\n",
      "1671/3000 train_loss: 44.27513885498047 test_loss:74.51656341552734\n",
      "1672/3000 train_loss: 49.7800407409668 test_loss:74.63970947265625\n",
      "1673/3000 train_loss: 51.08688735961914 test_loss:73.5889892578125\n",
      "1674/3000 train_loss: 40.68168258666992 test_loss:71.28318786621094\n",
      "1675/3000 train_loss: 44.020023345947266 test_loss:68.88816833496094\n",
      "1676/3000 train_loss: 41.04445266723633 test_loss:65.32855987548828\n",
      "1677/3000 train_loss: 49.55782699584961 test_loss:70.46888732910156\n",
      "1678/3000 train_loss: 43.739192962646484 test_loss:67.75289916992188\n",
      "1679/3000 train_loss: 55.05415344238281 test_loss:66.2800064086914\n",
      "1680/3000 train_loss: 43.73543167114258 test_loss:71.59495544433594\n",
      "1681/3000 train_loss: 48.854820251464844 test_loss:68.79280853271484\n",
      "1682/3000 train_loss: 47.26820755004883 test_loss:73.60408020019531\n",
      "1683/3000 train_loss: 45.1525993347168 test_loss:68.37026977539062\n",
      "1684/3000 train_loss: 41.18834686279297 test_loss:68.57630157470703\n",
      "1685/3000 train_loss: 51.71070098876953 test_loss:69.50286865234375\n",
      "1686/3000 train_loss: 38.838924407958984 test_loss:85.2900161743164\n",
      "1687/3000 train_loss: 45.17015075683594 test_loss:65.54959106445312\n",
      "1688/3000 train_loss: 43.55903244018555 test_loss:71.4337158203125\n",
      "1689/3000 train_loss: 43.580814361572266 test_loss:66.3038330078125\n",
      "1690/3000 train_loss: 38.65289306640625 test_loss:67.04547882080078\n",
      "1691/3000 train_loss: 43.97531509399414 test_loss:64.47882843017578\n",
      "1692/3000 train_loss: 40.91078186035156 test_loss:66.93153381347656\n",
      "1693/3000 train_loss: 45.56694793701172 test_loss:66.01607513427734\n",
      "1694/3000 train_loss: 44.61747741699219 test_loss:70.74028778076172\n",
      "1695/3000 train_loss: 45.13517379760742 test_loss:68.69831848144531\n",
      "1696/3000 train_loss: 44.90016174316406 test_loss:65.85123443603516\n",
      "1697/3000 train_loss: 50.656585693359375 test_loss:78.14579010009766\n",
      "1698/3000 train_loss: 50.72785949707031 test_loss:66.87368774414062\n",
      "1699/3000 train_loss: 44.39006423950195 test_loss:65.59318542480469\n",
      "1700/3000 train_loss: 37.339073181152344 test_loss:72.2855224609375\n",
      "1701/3000 train_loss: 39.02195358276367 test_loss:65.3598861694336\n",
      "1702/3000 train_loss: 38.249542236328125 test_loss:70.1305923461914\n",
      "1703/3000 train_loss: 43.02199172973633 test_loss:66.57070922851562\n",
      "1704/3000 train_loss: 43.154502868652344 test_loss:78.83898162841797\n",
      "1705/3000 train_loss: 40.790122985839844 test_loss:63.869747161865234\n",
      "1706/3000 train_loss: 43.23516845703125 test_loss:69.25250244140625\n",
      "1707/3000 train_loss: 42.75772476196289 test_loss:66.36446380615234\n",
      "1708/3000 train_loss: 50.50859832763672 test_loss:65.2101058959961\n",
      "1709/3000 train_loss: 43.20263671875 test_loss:75.81681823730469\n",
      "1710/3000 train_loss: 39.583984375 test_loss:64.85729217529297\n",
      "1711/3000 train_loss: 47.8725700378418 test_loss:66.13609313964844\n",
      "1712/3000 train_loss: 49.55070114135742 test_loss:65.26576232910156\n",
      "1713/3000 train_loss: 42.35214614868164 test_loss:78.25459289550781\n",
      "1714/3000 train_loss: 51.94303512573242 test_loss:63.680908203125\n",
      "1715/3000 train_loss: 45.84467315673828 test_loss:70.50857543945312\n",
      "1716/3000 train_loss: 39.37569046020508 test_loss:70.88365173339844\n",
      "1717/3000 train_loss: 48.53437805175781 test_loss:74.33892822265625\n",
      "1718/3000 train_loss: 45.14982986450195 test_loss:65.97611999511719\n",
      "1719/3000 train_loss: 48.67304992675781 test_loss:70.66960906982422\n",
      "1720/3000 train_loss: 49.03204345703125 test_loss:67.22085571289062\n",
      "1721/3000 train_loss: 48.2135009765625 test_loss:66.00146484375\n",
      "1722/3000 train_loss: 39.73789596557617 test_loss:70.85509490966797\n",
      "1723/3000 train_loss: 47.027854919433594 test_loss:71.60337829589844\n",
      "1724/3000 train_loss: 48.75053405761719 test_loss:62.327972412109375\n",
      "1725/3000 train_loss: 45.06184005737305 test_loss:67.98918151855469\n",
      "1726/3000 train_loss: 46.688377380371094 test_loss:70.26165771484375\n",
      "1727/3000 train_loss: 48.459938049316406 test_loss:62.40294647216797\n",
      "1728/3000 train_loss: 43.93364715576172 test_loss:69.67486572265625\n",
      "1729/3000 train_loss: 46.05277633666992 test_loss:74.72877502441406\n",
      "1730/3000 train_loss: 43.042259216308594 test_loss:65.43087768554688\n",
      "1731/3000 train_loss: 44.712921142578125 test_loss:73.45053100585938\n",
      "1732/3000 train_loss: 41.84912872314453 test_loss:61.99500274658203\n",
      "1733/3000 train_loss: 40.61853790283203 test_loss:66.14531707763672\n",
      "1734/3000 train_loss: 45.74662399291992 test_loss:68.8838882446289\n",
      "1735/3000 train_loss: 38.152793884277344 test_loss:64.52323150634766\n",
      "1736/3000 train_loss: 44.216102600097656 test_loss:64.11083984375\n",
      "1737/3000 train_loss: 42.88774490356445 test_loss:66.8405990600586\n",
      "1738/3000 train_loss: 43.15020751953125 test_loss:73.80178833007812\n",
      "1739/3000 train_loss: 40.430030822753906 test_loss:70.34452056884766\n",
      "1740/3000 train_loss: 45.16442108154297 test_loss:71.73493194580078\n",
      "1741/3000 train_loss: 38.64582061767578 test_loss:66.47830200195312\n",
      "1742/3000 train_loss: 39.477420806884766 test_loss:67.62506103515625\n",
      "1743/3000 train_loss: 43.8869743347168 test_loss:70.16011047363281\n",
      "1744/3000 train_loss: 48.24542236328125 test_loss:70.55829620361328\n",
      "1745/3000 train_loss: 44.696475982666016 test_loss:69.40829467773438\n",
      "1746/3000 train_loss: 41.368858337402344 test_loss:68.79708862304688\n",
      "1747/3000 train_loss: 45.04545974731445 test_loss:64.40007019042969\n",
      "1748/3000 train_loss: 39.84120559692383 test_loss:66.31747436523438\n",
      "1749/3000 train_loss: 41.43732833862305 test_loss:75.77348327636719\n",
      "1750/3000 train_loss: 39.05422592163086 test_loss:62.57122802734375\n",
      "1751/3000 train_loss: 41.17597579956055 test_loss:71.23111724853516\n",
      "1752/3000 train_loss: 42.401344299316406 test_loss:64.36140441894531\n",
      "1753/3000 train_loss: 40.68797302246094 test_loss:69.56714630126953\n",
      "1754/3000 train_loss: 41.50446319580078 test_loss:70.0267562866211\n",
      "1755/3000 train_loss: 39.16099166870117 test_loss:63.63655090332031\n",
      "1756/3000 train_loss: 42.17910385131836 test_loss:69.32745361328125\n",
      "1757/3000 train_loss: 42.65110397338867 test_loss:73.12931823730469\n",
      "1758/3000 train_loss: 42.871971130371094 test_loss:68.04931640625\n",
      "1759/3000 train_loss: 44.33351516723633 test_loss:64.40411376953125\n",
      "1760/3000 train_loss: 45.878082275390625 test_loss:67.38932800292969\n",
      "1761/3000 train_loss: 41.70379638671875 test_loss:71.86328125\n",
      "1762/3000 train_loss: 40.79389953613281 test_loss:62.87561798095703\n",
      "1763/3000 train_loss: 41.38750457763672 test_loss:64.20779418945312\n",
      "1764/3000 train_loss: 46.11597442626953 test_loss:71.95388793945312\n",
      "1765/3000 train_loss: 55.19258499145508 test_loss:77.07437133789062\n",
      "1766/3000 train_loss: 45.06800842285156 test_loss:72.145263671875\n",
      "1767/3000 train_loss: 55.56361389160156 test_loss:75.9763412475586\n",
      "1768/3000 train_loss: 37.92652893066406 test_loss:64.37004089355469\n",
      "1769/3000 train_loss: 53.51457214355469 test_loss:75.87294006347656\n",
      "1770/3000 train_loss: 44.944976806640625 test_loss:63.579139709472656\n",
      "1771/3000 train_loss: 38.38023376464844 test_loss:73.40886688232422\n",
      "1772/3000 train_loss: 39.0351676940918 test_loss:70.52798461914062\n",
      "1773/3000 train_loss: 45.379337310791016 test_loss:64.9284896850586\n",
      "1774/3000 train_loss: 36.3541259765625 test_loss:76.44014739990234\n",
      "1775/3000 train_loss: 44.67952346801758 test_loss:65.04742431640625\n",
      "1776/3000 train_loss: 37.86817169189453 test_loss:68.67611694335938\n",
      "1777/3000 train_loss: 40.70456314086914 test_loss:67.57772827148438\n",
      "1778/3000 train_loss: 39.050025939941406 test_loss:64.18993377685547\n",
      "1779/3000 train_loss: 40.9328498840332 test_loss:75.61751556396484\n",
      "1780/3000 train_loss: 47.24695587158203 test_loss:71.7109603881836\n",
      "1781/3000 train_loss: 44.85335922241211 test_loss:63.834869384765625\n",
      "1782/3000 train_loss: 48.40555953979492 test_loss:75.73503875732422\n",
      "1783/3000 train_loss: 41.126434326171875 test_loss:68.2428970336914\n",
      "1784/3000 train_loss: 41.60332489013672 test_loss:68.0474853515625\n",
      "1785/3000 train_loss: 46.83720016479492 test_loss:78.80648040771484\n",
      "1786/3000 train_loss: 44.75669860839844 test_loss:62.39695739746094\n",
      "1787/3000 train_loss: 41.81673049926758 test_loss:75.9644775390625\n",
      "1788/3000 train_loss: 49.92059326171875 test_loss:64.81196594238281\n",
      "1789/3000 train_loss: 49.16468811035156 test_loss:70.63140869140625\n",
      "1790/3000 train_loss: 38.81752014160156 test_loss:61.942787170410156\n",
      "1791/3000 train_loss: 36.4890251159668 test_loss:77.24068450927734\n",
      "1792/3000 train_loss: 51.58808517456055 test_loss:71.86183166503906\n",
      "1793/3000 train_loss: 49.045310974121094 test_loss:68.46714782714844\n",
      "1794/3000 train_loss: 49.33122253417969 test_loss:70.65274047851562\n",
      "1795/3000 train_loss: 37.588069915771484 test_loss:74.37216186523438\n",
      "1796/3000 train_loss: 42.956809997558594 test_loss:65.63895416259766\n",
      "1797/3000 train_loss: 40.449554443359375 test_loss:70.44062042236328\n",
      "1798/3000 train_loss: 43.60081100463867 test_loss:66.56613159179688\n",
      "1799/3000 train_loss: 41.35837936401367 test_loss:70.46430969238281\n",
      "1800/3000 train_loss: 37.11836242675781 test_loss:68.21926879882812\n",
      "1801/3000 train_loss: 48.77659225463867 test_loss:64.16413116455078\n",
      "1802/3000 train_loss: 38.74991226196289 test_loss:67.2060775756836\n",
      "1803/3000 train_loss: 40.952049255371094 test_loss:66.33743286132812\n",
      "1804/3000 train_loss: 40.06027603149414 test_loss:63.01994323730469\n",
      "1805/3000 train_loss: 46.5097541809082 test_loss:71.46171569824219\n",
      "1806/3000 train_loss: 40.72075653076172 test_loss:67.51461791992188\n",
      "1807/3000 train_loss: 44.183494567871094 test_loss:66.65400695800781\n",
      "1808/3000 train_loss: 43.12444305419922 test_loss:73.52941131591797\n",
      "1809/3000 train_loss: 43.705772399902344 test_loss:62.72490692138672\n",
      "1810/3000 train_loss: 43.410736083984375 test_loss:65.80923461914062\n",
      "1811/3000 train_loss: 38.97091293334961 test_loss:66.27281951904297\n",
      "1812/3000 train_loss: 49.012264251708984 test_loss:64.48334503173828\n",
      "1813/3000 train_loss: 41.95469284057617 test_loss:67.82850646972656\n",
      "1814/3000 train_loss: 38.14813232421875 test_loss:62.648162841796875\n",
      "1815/3000 train_loss: 37.96620559692383 test_loss:75.95213317871094\n",
      "1816/3000 train_loss: 43.51309585571289 test_loss:61.01213073730469\n",
      "1817/3000 train_loss: 38.73855972290039 test_loss:70.15653228759766\n",
      "1818/3000 train_loss: 43.10258483886719 test_loss:64.9627685546875\n",
      "1819/3000 train_loss: 44.30726623535156 test_loss:68.69042205810547\n",
      "1820/3000 train_loss: 42.56252670288086 test_loss:63.911842346191406\n",
      "1821/3000 train_loss: 39.54330062866211 test_loss:65.23994445800781\n",
      "1822/3000 train_loss: 35.11102294921875 test_loss:60.565185546875\n",
      "1823/3000 train_loss: 40.14789962768555 test_loss:71.02197265625\n",
      "1824/3000 train_loss: 43.154937744140625 test_loss:65.12486267089844\n",
      "1825/3000 train_loss: 35.18169403076172 test_loss:61.80226135253906\n",
      "1826/3000 train_loss: 43.040462493896484 test_loss:64.78206634521484\n",
      "1827/3000 train_loss: 41.93824005126953 test_loss:69.76673889160156\n",
      "1828/3000 train_loss: 40.58714294433594 test_loss:65.57996368408203\n",
      "1829/3000 train_loss: 43.25471496582031 test_loss:67.05949401855469\n",
      "1830/3000 train_loss: 38.634159088134766 test_loss:70.802978515625\n",
      "1831/3000 train_loss: 39.63645935058594 test_loss:60.893104553222656\n",
      "1832/3000 train_loss: 41.41907501220703 test_loss:61.28412628173828\n",
      "1833/3000 train_loss: 48.49889373779297 test_loss:71.38882446289062\n",
      "1834/3000 train_loss: 36.145172119140625 test_loss:65.24786376953125\n",
      "1835/3000 train_loss: 36.69954299926758 test_loss:62.996238708496094\n",
      "1836/3000 train_loss: 37.7548713684082 test_loss:65.50337982177734\n",
      "1837/3000 train_loss: 40.756317138671875 test_loss:67.35009765625\n",
      "1838/3000 train_loss: 39.4926643371582 test_loss:70.04737854003906\n",
      "1839/3000 train_loss: 49.34738540649414 test_loss:64.19353485107422\n",
      "1840/3000 train_loss: 37.52802658081055 test_loss:67.82110595703125\n",
      "1841/3000 train_loss: 42.84677505493164 test_loss:68.7432632446289\n",
      "1842/3000 train_loss: 41.47955322265625 test_loss:65.74244689941406\n",
      "1843/3000 train_loss: 47.409912109375 test_loss:66.61015319824219\n",
      "1844/3000 train_loss: 38.43207550048828 test_loss:66.24208068847656\n",
      "1845/3000 train_loss: 42.56597137451172 test_loss:67.7777099609375\n",
      "1846/3000 train_loss: 37.348201751708984 test_loss:63.97576141357422\n",
      "1847/3000 train_loss: 43.62859344482422 test_loss:69.52007293701172\n",
      "1848/3000 train_loss: 48.43159866333008 test_loss:66.21687316894531\n",
      "1849/3000 train_loss: 39.450721740722656 test_loss:66.97138214111328\n",
      "1850/3000 train_loss: 38.98935317993164 test_loss:68.95238494873047\n",
      "1851/3000 train_loss: 39.29005432128906 test_loss:61.27667236328125\n",
      "1852/3000 train_loss: 46.088592529296875 test_loss:60.643394470214844\n",
      "1853/3000 train_loss: 42.8577880859375 test_loss:69.137451171875\n",
      "1854/3000 train_loss: 41.93283462524414 test_loss:61.786685943603516\n",
      "1855/3000 train_loss: 37.58079147338867 test_loss:65.98056030273438\n",
      "1856/3000 train_loss: 44.645164489746094 test_loss:64.02503204345703\n",
      "1857/3000 train_loss: 38.772090911865234 test_loss:65.2447509765625\n",
      "1858/3000 train_loss: 39.525760650634766 test_loss:62.00591278076172\n",
      "1859/3000 train_loss: 37.625640869140625 test_loss:71.04423522949219\n",
      "1860/3000 train_loss: 37.049930572509766 test_loss:61.84959411621094\n",
      "1861/3000 train_loss: 35.09454345703125 test_loss:65.95416259765625\n",
      "1862/3000 train_loss: 41.25600051879883 test_loss:66.60517120361328\n",
      "1863/3000 train_loss: 39.335205078125 test_loss:67.0197982788086\n",
      "1864/3000 train_loss: 34.76580810546875 test_loss:65.78648376464844\n",
      "1865/3000 train_loss: 39.38217544555664 test_loss:64.27571868896484\n",
      "1866/3000 train_loss: 39.049320220947266 test_loss:64.04630279541016\n",
      "1867/3000 train_loss: 49.9128303527832 test_loss:61.391990661621094\n",
      "1868/3000 train_loss: 45.28120040893555 test_loss:62.138099670410156\n",
      "1869/3000 train_loss: 44.40400314331055 test_loss:67.11680603027344\n",
      "1870/3000 train_loss: 44.46202850341797 test_loss:60.5518798828125\n",
      "1871/3000 train_loss: 44.4926643371582 test_loss:64.42559051513672\n",
      "1872/3000 train_loss: 46.73758316040039 test_loss:61.616798400878906\n",
      "1873/3000 train_loss: 39.82334518432617 test_loss:75.57981872558594\n",
      "1874/3000 train_loss: 43.90731430053711 test_loss:61.29669189453125\n",
      "1875/3000 train_loss: 38.20262908935547 test_loss:64.54539489746094\n",
      "1876/3000 train_loss: 49.39288330078125 test_loss:66.025146484375\n",
      "1877/3000 train_loss: 51.340087890625 test_loss:63.51189422607422\n",
      "1878/3000 train_loss: 42.19780731201172 test_loss:65.36732482910156\n",
      "1879/3000 train_loss: 48.96173095703125 test_loss:67.29375457763672\n",
      "1880/3000 train_loss: 38.91055679321289 test_loss:68.12467193603516\n",
      "1881/3000 train_loss: 35.144432067871094 test_loss:67.65042114257812\n",
      "1882/3000 train_loss: 38.490394592285156 test_loss:60.689178466796875\n",
      "1883/3000 train_loss: 42.7847785949707 test_loss:78.56784057617188\n",
      "1884/3000 train_loss: 39.162689208984375 test_loss:64.85381317138672\n",
      "1885/3000 train_loss: 41.865543365478516 test_loss:63.31039047241211\n",
      "1886/3000 train_loss: 41.34584045410156 test_loss:71.16439819335938\n",
      "1887/3000 train_loss: 40.63842010498047 test_loss:67.22688293457031\n",
      "1888/3000 train_loss: 37.37668228149414 test_loss:67.90673828125\n",
      "1889/3000 train_loss: 39.61382293701172 test_loss:66.06742858886719\n",
      "1890/3000 train_loss: 43.693294525146484 test_loss:62.982505798339844\n",
      "1891/3000 train_loss: 39.77191162109375 test_loss:64.21456909179688\n",
      "1892/3000 train_loss: 40.31223678588867 test_loss:64.8306884765625\n",
      "1893/3000 train_loss: 37.94559860229492 test_loss:64.10628509521484\n",
      "1894/3000 train_loss: 48.76012420654297 test_loss:70.02220153808594\n",
      "1895/3000 train_loss: 44.29039001464844 test_loss:63.48021697998047\n",
      "1896/3000 train_loss: 41.60035705566406 test_loss:62.690467834472656\n",
      "1897/3000 train_loss: 36.40767288208008 test_loss:67.847900390625\n",
      "1898/3000 train_loss: 46.17147445678711 test_loss:76.44327545166016\n",
      "1899/3000 train_loss: 41.72515106201172 test_loss:64.021240234375\n",
      "1900/3000 train_loss: 38.60459518432617 test_loss:77.34000396728516\n",
      "1901/3000 train_loss: 37.5419921875 test_loss:67.9903564453125\n",
      "1902/3000 train_loss: 40.22992706298828 test_loss:67.80987548828125\n",
      "1903/3000 train_loss: 43.116180419921875 test_loss:70.83771514892578\n",
      "1904/3000 train_loss: 48.848018646240234 test_loss:64.23379516601562\n",
      "1905/3000 train_loss: 45.905540466308594 test_loss:78.3921127319336\n",
      "1906/3000 train_loss: 45.753841400146484 test_loss:62.501014709472656\n",
      "1907/3000 train_loss: 41.209346771240234 test_loss:65.34747314453125\n",
      "1908/3000 train_loss: 37.82610321044922 test_loss:64.05805969238281\n",
      "1909/3000 train_loss: 46.8791618347168 test_loss:70.54606628417969\n",
      "1910/3000 train_loss: 46.48833465576172 test_loss:65.81343841552734\n",
      "1911/3000 train_loss: 44.64732360839844 test_loss:65.06161499023438\n",
      "1912/3000 train_loss: 37.52921676635742 test_loss:68.88829803466797\n",
      "1913/3000 train_loss: 37.68351364135742 test_loss:66.61931610107422\n",
      "1914/3000 train_loss: 36.416587829589844 test_loss:61.289215087890625\n",
      "1915/3000 train_loss: 38.3058967590332 test_loss:83.74508666992188\n",
      "1916/3000 train_loss: 41.121673583984375 test_loss:72.11361694335938\n",
      "1917/3000 train_loss: 36.10378646850586 test_loss:63.39042663574219\n",
      "1918/3000 train_loss: 43.39572525024414 test_loss:74.07040405273438\n",
      "1919/3000 train_loss: 47.118003845214844 test_loss:66.77540588378906\n",
      "1920/3000 train_loss: 47.43351745605469 test_loss:59.73565673828125\n",
      "1921/3000 train_loss: 47.684974670410156 test_loss:62.82128143310547\n",
      "1922/3000 train_loss: 40.800453186035156 test_loss:69.792724609375\n",
      "1923/3000 train_loss: 46.96891403198242 test_loss:63.62492370605469\n",
      "1924/3000 train_loss: 43.79326629638672 test_loss:62.427452087402344\n",
      "1925/3000 train_loss: 34.15833282470703 test_loss:61.25193405151367\n",
      "1926/3000 train_loss: 38.256446838378906 test_loss:65.63655090332031\n",
      "1927/3000 train_loss: 32.91178894042969 test_loss:60.6751594543457\n",
      "1928/3000 train_loss: 48.80815505981445 test_loss:58.13003921508789\n",
      "1929/3000 train_loss: 41.325050354003906 test_loss:72.37333679199219\n",
      "1930/3000 train_loss: 50.073123931884766 test_loss:63.73963165283203\n",
      "1931/3000 train_loss: 39.74053955078125 test_loss:66.02571105957031\n",
      "1932/3000 train_loss: 46.16431427001953 test_loss:71.35931396484375\n",
      "1933/3000 train_loss: 36.299896240234375 test_loss:61.98445129394531\n",
      "1934/3000 train_loss: 39.9786376953125 test_loss:72.27386474609375\n",
      "1935/3000 train_loss: 43.095115661621094 test_loss:58.869720458984375\n",
      "1936/3000 train_loss: 44.64580535888672 test_loss:69.457275390625\n",
      "1937/3000 train_loss: 47.682579040527344 test_loss:62.87339782714844\n",
      "1938/3000 train_loss: 43.56117248535156 test_loss:59.91271209716797\n",
      "1939/3000 train_loss: 36.156028747558594 test_loss:59.891136169433594\n",
      "1940/3000 train_loss: 43.439701080322266 test_loss:64.38573455810547\n",
      "1941/3000 train_loss: 35.82267761230469 test_loss:62.868995666503906\n",
      "1942/3000 train_loss: 42.21121597290039 test_loss:61.450599670410156\n",
      "1943/3000 train_loss: 42.99660873413086 test_loss:69.73695373535156\n",
      "1944/3000 train_loss: 46.716575622558594 test_loss:60.203285217285156\n",
      "1945/3000 train_loss: 45.06583786010742 test_loss:61.23313903808594\n",
      "1946/3000 train_loss: 36.10417175292969 test_loss:61.12106704711914\n",
      "1947/3000 train_loss: 36.001312255859375 test_loss:63.58002471923828\n",
      "1948/3000 train_loss: 39.136192321777344 test_loss:60.06501388549805\n",
      "1949/3000 train_loss: 38.60704040527344 test_loss:60.6807861328125\n",
      "1950/3000 train_loss: 38.65510940551758 test_loss:60.15433120727539\n",
      "1951/3000 train_loss: 39.899024963378906 test_loss:60.25025177001953\n",
      "1952/3000 train_loss: 41.183048248291016 test_loss:76.2135009765625\n",
      "1953/3000 train_loss: 41.96706008911133 test_loss:58.93914794921875\n",
      "1954/3000 train_loss: 40.48758316040039 test_loss:67.63816833496094\n",
      "1955/3000 train_loss: 38.36623764038086 test_loss:64.01188659667969\n",
      "1956/3000 train_loss: 42.14823913574219 test_loss:66.43551635742188\n",
      "1957/3000 train_loss: 39.26203155517578 test_loss:65.0875015258789\n",
      "1958/3000 train_loss: 38.968387603759766 test_loss:67.3355712890625\n",
      "1959/3000 train_loss: 40.397972106933594 test_loss:58.36016082763672\n",
      "1960/3000 train_loss: 48.41776657104492 test_loss:61.412437438964844\n",
      "1961/3000 train_loss: 41.03746795654297 test_loss:65.56564331054688\n",
      "1962/3000 train_loss: 39.11163330078125 test_loss:62.29664611816406\n",
      "1963/3000 train_loss: 38.70621871948242 test_loss:69.42415618896484\n",
      "1964/3000 train_loss: 40.755950927734375 test_loss:62.003257751464844\n",
      "1965/3000 train_loss: 35.57229232788086 test_loss:72.77349853515625\n",
      "1966/3000 train_loss: 39.969383239746094 test_loss:60.55524444580078\n",
      "1967/3000 train_loss: 40.8624267578125 test_loss:64.16012573242188\n",
      "1968/3000 train_loss: 32.7052116394043 test_loss:62.272972106933594\n",
      "1969/3000 train_loss: 42.281105041503906 test_loss:60.673553466796875\n",
      "1970/3000 train_loss: 41.80015563964844 test_loss:67.61080932617188\n",
      "1971/3000 train_loss: 38.35729217529297 test_loss:60.57057189941406\n",
      "1972/3000 train_loss: 36.607582092285156 test_loss:62.121673583984375\n",
      "1973/3000 train_loss: 39.631771087646484 test_loss:62.92864227294922\n",
      "1974/3000 train_loss: 41.460357666015625 test_loss:62.07537841796875\n",
      "1975/3000 train_loss: 42.023494720458984 test_loss:70.24842834472656\n",
      "1976/3000 train_loss: 35.87172317504883 test_loss:66.14588928222656\n",
      "1977/3000 train_loss: 35.34064865112305 test_loss:61.17723083496094\n",
      "1978/3000 train_loss: 38.518455505371094 test_loss:68.26475524902344\n",
      "1979/3000 train_loss: 36.1114616394043 test_loss:65.17469787597656\n",
      "1980/3000 train_loss: 34.517147064208984 test_loss:61.32288360595703\n",
      "1981/3000 train_loss: 34.66523742675781 test_loss:79.53333282470703\n",
      "1982/3000 train_loss: 38.475643157958984 test_loss:60.6612663269043\n",
      "1983/3000 train_loss: 39.32944869995117 test_loss:70.64144897460938\n",
      "1984/3000 train_loss: 41.97248077392578 test_loss:71.25642395019531\n",
      "1985/3000 train_loss: 36.13456344604492 test_loss:62.761844635009766\n",
      "1986/3000 train_loss: 44.62529754638672 test_loss:65.98904418945312\n",
      "1987/3000 train_loss: 36.93132019042969 test_loss:69.40608215332031\n",
      "1988/3000 train_loss: 45.178916931152344 test_loss:67.17716217041016\n",
      "1989/3000 train_loss: 39.33853530883789 test_loss:63.53403091430664\n",
      "1990/3000 train_loss: 42.446983337402344 test_loss:60.78471374511719\n",
      "1991/3000 train_loss: 51.52808380126953 test_loss:72.61308288574219\n",
      "1992/3000 train_loss: 39.734153747558594 test_loss:65.29003143310547\n",
      "1993/3000 train_loss: 40.522254943847656 test_loss:69.55455017089844\n",
      "1994/3000 train_loss: 37.13518524169922 test_loss:62.925994873046875\n",
      "1995/3000 train_loss: 36.70489501953125 test_loss:73.43280792236328\n",
      "1996/3000 train_loss: 34.1583137512207 test_loss:65.63090515136719\n",
      "1997/3000 train_loss: 46.86796951293945 test_loss:69.42623901367188\n",
      "1998/3000 train_loss: 38.651336669921875 test_loss:66.14969635009766\n",
      "1999/3000 train_loss: 41.049774169921875 test_loss:66.4683837890625\n",
      "2000/3000 train_loss: 43.11001968383789 test_loss:72.71823120117188\n",
      "2001/3000 train_loss: 40.407371520996094 test_loss:60.580299377441406\n",
      "2002/3000 train_loss: 42.22103500366211 test_loss:71.49945068359375\n",
      "2003/3000 train_loss: 43.166961669921875 test_loss:60.71675109863281\n",
      "2004/3000 train_loss: 50.503238677978516 test_loss:74.60891723632812\n",
      "2005/3000 train_loss: 40.82685470581055 test_loss:66.576904296875\n",
      "2006/3000 train_loss: 46.347076416015625 test_loss:59.09135818481445\n",
      "2007/3000 train_loss: 39.192901611328125 test_loss:77.05728912353516\n",
      "2008/3000 train_loss: 43.04524230957031 test_loss:63.58733367919922\n",
      "2009/3000 train_loss: 45.25604248046875 test_loss:70.3650894165039\n",
      "2010/3000 train_loss: 38.017906188964844 test_loss:66.29357147216797\n",
      "2011/3000 train_loss: 43.7249641418457 test_loss:62.35819625854492\n",
      "2012/3000 train_loss: 36.578853607177734 test_loss:74.20887756347656\n",
      "2013/3000 train_loss: 47.212669372558594 test_loss:61.34042739868164\n",
      "2014/3000 train_loss: 40.017520904541016 test_loss:61.03211975097656\n",
      "2015/3000 train_loss: 37.084232330322266 test_loss:61.760093688964844\n",
      "2016/3000 train_loss: 40.67042541503906 test_loss:64.86801147460938\n",
      "2017/3000 train_loss: 39.153587341308594 test_loss:62.841278076171875\n",
      "2018/3000 train_loss: 33.35420608520508 test_loss:61.47589111328125\n",
      "2019/3000 train_loss: 35.46772384643555 test_loss:62.48851013183594\n",
      "2020/3000 train_loss: 48.38827133178711 test_loss:63.58709716796875\n",
      "2021/3000 train_loss: 36.928226470947266 test_loss:64.37316131591797\n",
      "2022/3000 train_loss: 44.43559646606445 test_loss:60.3201904296875\n",
      "2023/3000 train_loss: 38.597900390625 test_loss:65.20349884033203\n",
      "2024/3000 train_loss: 36.09660720825195 test_loss:67.58028411865234\n",
      "2025/3000 train_loss: 35.99277877807617 test_loss:63.587425231933594\n",
      "2026/3000 train_loss: 33.630653381347656 test_loss:57.949947357177734\n",
      "2027/3000 train_loss: 39.24306869506836 test_loss:65.4542236328125\n",
      "2028/3000 train_loss: 41.938446044921875 test_loss:63.69670104980469\n",
      "2029/3000 train_loss: 38.64266586303711 test_loss:60.64160919189453\n",
      "2030/3000 train_loss: 38.324459075927734 test_loss:61.160179138183594\n",
      "2031/3000 train_loss: 40.11747741699219 test_loss:62.127105712890625\n",
      "2032/3000 train_loss: 36.454933166503906 test_loss:61.875396728515625\n",
      "2033/3000 train_loss: 34.03066635131836 test_loss:63.612770080566406\n",
      "2034/3000 train_loss: 44.08078384399414 test_loss:72.51945495605469\n",
      "2035/3000 train_loss: 35.34563446044922 test_loss:60.134822845458984\n",
      "2036/3000 train_loss: 38.902427673339844 test_loss:63.720054626464844\n",
      "2037/3000 train_loss: 37.72331619262695 test_loss:63.0628776550293\n",
      "2038/3000 train_loss: 37.883235931396484 test_loss:62.059226989746094\n",
      "2039/3000 train_loss: 33.583168029785156 test_loss:64.1208267211914\n",
      "2040/3000 train_loss: 42.65308380126953 test_loss:63.79473876953125\n",
      "2041/3000 train_loss: 34.14396286010742 test_loss:61.00520324707031\n",
      "2042/3000 train_loss: 41.39876937866211 test_loss:58.77217102050781\n",
      "2043/3000 train_loss: 41.1954231262207 test_loss:62.66106033325195\n",
      "2044/3000 train_loss: 34.321659088134766 test_loss:59.717926025390625\n",
      "2045/3000 train_loss: 41.2877311706543 test_loss:61.53881072998047\n",
      "2046/3000 train_loss: 37.96662902832031 test_loss:57.88273620605469\n",
      "2047/3000 train_loss: 42.6186408996582 test_loss:63.17719268798828\n",
      "2048/3000 train_loss: 36.65877151489258 test_loss:61.573760986328125\n",
      "2049/3000 train_loss: 41.092613220214844 test_loss:63.33066940307617\n",
      "2050/3000 train_loss: 39.484004974365234 test_loss:68.50912475585938\n",
      "2051/3000 train_loss: 40.14287567138672 test_loss:63.55888366699219\n",
      "2052/3000 train_loss: 43.60304641723633 test_loss:60.912078857421875\n",
      "2053/3000 train_loss: 33.90721893310547 test_loss:58.849517822265625\n",
      "2054/3000 train_loss: 35.04261016845703 test_loss:63.080848693847656\n",
      "2055/3000 train_loss: 36.83039855957031 test_loss:62.02288818359375\n",
      "2056/3000 train_loss: 37.727378845214844 test_loss:62.50388717651367\n",
      "2057/3000 train_loss: 44.10075759887695 test_loss:59.472862243652344\n",
      "2058/3000 train_loss: 38.49649429321289 test_loss:70.182373046875\n",
      "2059/3000 train_loss: 35.56377029418945 test_loss:61.74591827392578\n",
      "2060/3000 train_loss: 34.904052734375 test_loss:66.00306701660156\n",
      "2061/3000 train_loss: 35.21403503417969 test_loss:59.11492156982422\n",
      "2062/3000 train_loss: 42.86997604370117 test_loss:61.09096145629883\n",
      "2063/3000 train_loss: 43.138084411621094 test_loss:59.00228500366211\n",
      "2064/3000 train_loss: 45.41282272338867 test_loss:75.79148864746094\n",
      "2065/3000 train_loss: 40.25050354003906 test_loss:61.97496795654297\n",
      "2066/3000 train_loss: 40.41326904296875 test_loss:64.49478912353516\n",
      "2067/3000 train_loss: 35.194332122802734 test_loss:64.23388671875\n",
      "2068/3000 train_loss: 34.54374313354492 test_loss:59.941627502441406\n",
      "2069/3000 train_loss: 42.140045166015625 test_loss:69.89083862304688\n",
      "2070/3000 train_loss: 35.54389953613281 test_loss:59.36132049560547\n",
      "2071/3000 train_loss: 36.07133102416992 test_loss:62.88495635986328\n",
      "2072/3000 train_loss: 41.51948547363281 test_loss:73.37877655029297\n",
      "2073/3000 train_loss: 39.35090255737305 test_loss:68.65320587158203\n",
      "2074/3000 train_loss: 42.263389587402344 test_loss:64.38092041015625\n",
      "2075/3000 train_loss: 33.586090087890625 test_loss:66.59950256347656\n",
      "2076/3000 train_loss: 41.41473388671875 test_loss:64.24251556396484\n",
      "2077/3000 train_loss: 34.26338577270508 test_loss:63.124847412109375\n",
      "2078/3000 train_loss: 35.56386184692383 test_loss:66.041259765625\n",
      "2079/3000 train_loss: 35.188419342041016 test_loss:57.413482666015625\n",
      "2080/3000 train_loss: 45.62663269042969 test_loss:59.93515396118164\n",
      "2081/3000 train_loss: 39.585540771484375 test_loss:64.68427276611328\n",
      "2082/3000 train_loss: 40.81980514526367 test_loss:67.7260513305664\n",
      "2083/3000 train_loss: 38.1894645690918 test_loss:57.743629455566406\n",
      "2084/3000 train_loss: 37.912841796875 test_loss:68.62564849853516\n",
      "2085/3000 train_loss: 37.72759246826172 test_loss:64.68962860107422\n",
      "2086/3000 train_loss: 40.397972106933594 test_loss:61.20248031616211\n",
      "2087/3000 train_loss: 39.05282974243164 test_loss:65.36729431152344\n",
      "2088/3000 train_loss: 36.32072830200195 test_loss:59.42094421386719\n",
      "2089/3000 train_loss: 37.04874801635742 test_loss:63.209754943847656\n",
      "2090/3000 train_loss: 37.511051177978516 test_loss:67.96452331542969\n",
      "2091/3000 train_loss: 34.82096862792969 test_loss:57.946685791015625\n",
      "2092/3000 train_loss: 35.36835861206055 test_loss:68.665771484375\n",
      "2093/3000 train_loss: 39.96491241455078 test_loss:60.008338928222656\n",
      "2094/3000 train_loss: 40.969970703125 test_loss:74.90408325195312\n",
      "2095/3000 train_loss: 40.393131256103516 test_loss:61.120147705078125\n",
      "2096/3000 train_loss: 35.198360443115234 test_loss:67.43938446044922\n",
      "2097/3000 train_loss: 34.39358139038086 test_loss:62.86689376831055\n",
      "2098/3000 train_loss: 36.124202728271484 test_loss:65.96308898925781\n",
      "2099/3000 train_loss: 35.6930046081543 test_loss:60.66801071166992\n",
      "2100/3000 train_loss: 44.045833587646484 test_loss:65.25082397460938\n",
      "2101/3000 train_loss: 42.76557159423828 test_loss:66.04841613769531\n",
      "2102/3000 train_loss: 39.60069274902344 test_loss:61.8748893737793\n",
      "2103/3000 train_loss: 37.3527717590332 test_loss:58.11414337158203\n",
      "2104/3000 train_loss: 36.49802780151367 test_loss:60.94626235961914\n",
      "2105/3000 train_loss: 30.87584686279297 test_loss:62.247840881347656\n",
      "2106/3000 train_loss: 33.5777587890625 test_loss:59.07921600341797\n",
      "2107/3000 train_loss: 33.97832107543945 test_loss:57.79795455932617\n",
      "2108/3000 train_loss: 43.079097747802734 test_loss:66.20438385009766\n",
      "2109/3000 train_loss: 35.089599609375 test_loss:58.786285400390625\n",
      "2110/3000 train_loss: 38.799861907958984 test_loss:62.72230911254883\n",
      "2111/3000 train_loss: 43.27409362792969 test_loss:56.04722595214844\n",
      "2112/3000 train_loss: 34.279449462890625 test_loss:61.90967559814453\n",
      "2113/3000 train_loss: 36.91474151611328 test_loss:65.26847076416016\n",
      "2114/3000 train_loss: 32.248687744140625 test_loss:60.51060104370117\n",
      "2115/3000 train_loss: 32.573028564453125 test_loss:72.97412872314453\n",
      "2116/3000 train_loss: 44.33301544189453 test_loss:62.28044891357422\n",
      "2117/3000 train_loss: 38.951473236083984 test_loss:57.01020812988281\n",
      "2118/3000 train_loss: 35.52234649658203 test_loss:60.17772674560547\n",
      "2119/3000 train_loss: 40.806583404541016 test_loss:66.91841888427734\n",
      "2120/3000 train_loss: 35.59654998779297 test_loss:59.09819412231445\n",
      "2121/3000 train_loss: 31.425533294677734 test_loss:59.09055709838867\n",
      "2122/3000 train_loss: 32.148231506347656 test_loss:63.61688232421875\n",
      "2123/3000 train_loss: 40.976043701171875 test_loss:59.75475311279297\n",
      "2124/3000 train_loss: 37.33814239501953 test_loss:64.97774505615234\n",
      "2125/3000 train_loss: 40.66641616821289 test_loss:65.84687805175781\n",
      "2126/3000 train_loss: 36.50663757324219 test_loss:57.20507049560547\n",
      "2127/3000 train_loss: 36.20529556274414 test_loss:61.567501068115234\n",
      "2128/3000 train_loss: 31.009742736816406 test_loss:60.34907531738281\n",
      "2129/3000 train_loss: 38.876495361328125 test_loss:66.30978393554688\n",
      "2130/3000 train_loss: 37.28947830200195 test_loss:63.5787353515625\n",
      "2131/3000 train_loss: 40.18517303466797 test_loss:59.21186447143555\n",
      "2132/3000 train_loss: 41.654781341552734 test_loss:59.418548583984375\n",
      "2133/3000 train_loss: 37.60008239746094 test_loss:70.88219451904297\n",
      "2134/3000 train_loss: 42.218116760253906 test_loss:59.9114990234375\n",
      "2135/3000 train_loss: 40.698307037353516 test_loss:61.01824951171875\n",
      "2136/3000 train_loss: 34.351993560791016 test_loss:69.63137817382812\n",
      "2137/3000 train_loss: 36.65119552612305 test_loss:64.405517578125\n",
      "2138/3000 train_loss: 46.55849838256836 test_loss:62.69690704345703\n",
      "2139/3000 train_loss: 40.406288146972656 test_loss:68.01594543457031\n",
      "2140/3000 train_loss: 36.91609191894531 test_loss:61.60432815551758\n",
      "2141/3000 train_loss: 42.680885314941406 test_loss:60.56890106201172\n",
      "2142/3000 train_loss: 40.95025634765625 test_loss:59.30299377441406\n",
      "2143/3000 train_loss: 34.28371810913086 test_loss:64.45450592041016\n",
      "2144/3000 train_loss: 38.16410446166992 test_loss:60.72819900512695\n",
      "2145/3000 train_loss: 43.837467193603516 test_loss:71.52865600585938\n",
      "2146/3000 train_loss: 45.80772399902344 test_loss:57.237876892089844\n",
      "2147/3000 train_loss: 32.66630554199219 test_loss:64.43850708007812\n",
      "2148/3000 train_loss: 37.12609100341797 test_loss:62.46259307861328\n",
      "2149/3000 train_loss: 34.90375518798828 test_loss:63.226783752441406\n",
      "2150/3000 train_loss: 41.31049346923828 test_loss:61.681640625\n",
      "2151/3000 train_loss: 37.29697036743164 test_loss:58.220703125\n",
      "2152/3000 train_loss: 40.39165496826172 test_loss:63.63612747192383\n",
      "2153/3000 train_loss: 37.12451171875 test_loss:63.27516174316406\n",
      "2154/3000 train_loss: 35.83730697631836 test_loss:60.199378967285156\n",
      "2155/3000 train_loss: 33.99666213989258 test_loss:68.02743530273438\n",
      "2156/3000 train_loss: 38.38773727416992 test_loss:61.88867950439453\n",
      "2157/3000 train_loss: 36.08677673339844 test_loss:65.88541412353516\n",
      "2158/3000 train_loss: 39.00608825683594 test_loss:56.957000732421875\n",
      "2159/3000 train_loss: 38.28867721557617 test_loss:60.578956604003906\n",
      "2160/3000 train_loss: 33.2168083190918 test_loss:60.903167724609375\n",
      "2161/3000 train_loss: 36.177188873291016 test_loss:61.07014846801758\n",
      "2162/3000 train_loss: 37.14707946777344 test_loss:64.35435485839844\n",
      "2163/3000 train_loss: 36.54812240600586 test_loss:57.96975326538086\n",
      "2164/3000 train_loss: 32.69411087036133 test_loss:64.89698791503906\n",
      "2165/3000 train_loss: 31.116806030273438 test_loss:58.249874114990234\n",
      "2166/3000 train_loss: 37.77936553955078 test_loss:57.922264099121094\n",
      "2167/3000 train_loss: 40.85221862792969 test_loss:77.10093688964844\n",
      "2168/3000 train_loss: 35.54120635986328 test_loss:60.076087951660156\n",
      "2169/3000 train_loss: 37.49600601196289 test_loss:60.59013366699219\n",
      "2170/3000 train_loss: 40.825687408447266 test_loss:66.64066314697266\n",
      "2171/3000 train_loss: 32.20551681518555 test_loss:59.6002082824707\n",
      "2172/3000 train_loss: 38.50901794433594 test_loss:71.5653076171875\n",
      "2173/3000 train_loss: 35.47190856933594 test_loss:59.28118133544922\n",
      "2174/3000 train_loss: 32.859703063964844 test_loss:70.0129165649414\n",
      "2175/3000 train_loss: 33.544002532958984 test_loss:60.26924133300781\n",
      "2176/3000 train_loss: 34.07085037231445 test_loss:58.307777404785156\n",
      "2177/3000 train_loss: 36.97234344482422 test_loss:60.99625778198242\n",
      "2178/3000 train_loss: 38.07770538330078 test_loss:58.423622131347656\n",
      "2179/3000 train_loss: 35.44187545776367 test_loss:65.00396728515625\n",
      "2180/3000 train_loss: 32.89585876464844 test_loss:57.695152282714844\n",
      "2181/3000 train_loss: 37.548282623291016 test_loss:60.38306427001953\n",
      "2182/3000 train_loss: 37.703765869140625 test_loss:66.6220474243164\n",
      "2183/3000 train_loss: 34.24787902832031 test_loss:70.79090881347656\n",
      "2184/3000 train_loss: 36.79294967651367 test_loss:59.548095703125\n",
      "2185/3000 train_loss: 36.80412673950195 test_loss:66.69697570800781\n",
      "2186/3000 train_loss: 37.10516357421875 test_loss:58.725677490234375\n",
      "2187/3000 train_loss: 34.05453109741211 test_loss:65.92987060546875\n",
      "2188/3000 train_loss: 31.059179306030273 test_loss:63.148101806640625\n",
      "2189/3000 train_loss: 35.14846420288086 test_loss:60.701019287109375\n",
      "2190/3000 train_loss: 40.381500244140625 test_loss:55.235267639160156\n",
      "2191/3000 train_loss: 42.198604583740234 test_loss:81.0746841430664\n",
      "2192/3000 train_loss: 45.11397933959961 test_loss:66.8705062866211\n",
      "2193/3000 train_loss: 39.1026725769043 test_loss:62.69218444824219\n",
      "2194/3000 train_loss: 43.42037582397461 test_loss:63.8697624206543\n",
      "2195/3000 train_loss: 35.89916229248047 test_loss:63.17567825317383\n",
      "2196/3000 train_loss: 34.298831939697266 test_loss:59.455020904541016\n",
      "2197/3000 train_loss: 33.361602783203125 test_loss:65.89797973632812\n",
      "2198/3000 train_loss: 35.08625793457031 test_loss:65.37692260742188\n",
      "2199/3000 train_loss: 38.859352111816406 test_loss:62.48335266113281\n",
      "2200/3000 train_loss: 37.87328338623047 test_loss:61.451072692871094\n",
      "2201/3000 train_loss: 45.72276306152344 test_loss:72.63116455078125\n",
      "2202/3000 train_loss: 38.08308792114258 test_loss:62.134613037109375\n",
      "2203/3000 train_loss: 31.813539505004883 test_loss:64.91486358642578\n",
      "2204/3000 train_loss: 37.283973693847656 test_loss:70.111572265625\n",
      "2205/3000 train_loss: 35.560279846191406 test_loss:63.5706787109375\n",
      "2206/3000 train_loss: 37.155155181884766 test_loss:62.33183288574219\n",
      "2207/3000 train_loss: 36.76039505004883 test_loss:62.058738708496094\n",
      "2208/3000 train_loss: 36.53104782104492 test_loss:63.31737518310547\n",
      "2209/3000 train_loss: 34.0445556640625 test_loss:65.06471252441406\n",
      "2210/3000 train_loss: 40.93577575683594 test_loss:57.23033142089844\n",
      "2211/3000 train_loss: 37.814002990722656 test_loss:65.00700378417969\n",
      "2212/3000 train_loss: 45.54485321044922 test_loss:58.08453369140625\n",
      "2213/3000 train_loss: 34.47451400756836 test_loss:62.407310485839844\n",
      "2214/3000 train_loss: 34.96733856201172 test_loss:61.33559799194336\n",
      "2215/3000 train_loss: 36.88041687011719 test_loss:60.27330017089844\n",
      "2216/3000 train_loss: 33.817710876464844 test_loss:60.506874084472656\n",
      "2217/3000 train_loss: 39.36994552612305 test_loss:64.45915985107422\n",
      "2218/3000 train_loss: 36.69496154785156 test_loss:58.77172088623047\n",
      "2219/3000 train_loss: 41.13131332397461 test_loss:66.74359130859375\n",
      "2220/3000 train_loss: 36.20390319824219 test_loss:59.37399673461914\n",
      "2221/3000 train_loss: 40.13116455078125 test_loss:60.76692199707031\n",
      "2222/3000 train_loss: 38.61217498779297 test_loss:59.87458419799805\n",
      "2223/3000 train_loss: 34.97935485839844 test_loss:60.21891403198242\n",
      "2224/3000 train_loss: 37.54656219482422 test_loss:61.749290466308594\n",
      "2225/3000 train_loss: 36.898712158203125 test_loss:65.14414978027344\n",
      "2226/3000 train_loss: 42.38341522216797 test_loss:60.469749450683594\n",
      "2227/3000 train_loss: 40.8292350769043 test_loss:63.18303680419922\n",
      "2228/3000 train_loss: 38.270809173583984 test_loss:63.259544372558594\n",
      "2229/3000 train_loss: 46.18286895751953 test_loss:65.57794189453125\n",
      "2230/3000 train_loss: 50.32154846191406 test_loss:62.08036422729492\n",
      "2231/3000 train_loss: 36.868385314941406 test_loss:67.26765441894531\n",
      "2232/3000 train_loss: 33.45347595214844 test_loss:59.567848205566406\n",
      "2233/3000 train_loss: 31.548248291015625 test_loss:60.598052978515625\n",
      "2234/3000 train_loss: 39.656219482421875 test_loss:66.15518188476562\n",
      "2235/3000 train_loss: 37.57600021362305 test_loss:69.51924133300781\n",
      "2236/3000 train_loss: 33.398048400878906 test_loss:63.88377380371094\n",
      "2237/3000 train_loss: 40.0697135925293 test_loss:60.81909942626953\n",
      "2238/3000 train_loss: 37.7290153503418 test_loss:61.12447738647461\n",
      "2239/3000 train_loss: 30.766136169433594 test_loss:59.68335723876953\n",
      "2240/3000 train_loss: 32.806941986083984 test_loss:73.01983642578125\n",
      "2241/3000 train_loss: 38.75489044189453 test_loss:70.72857666015625\n",
      "2242/3000 train_loss: 30.696565628051758 test_loss:60.32234573364258\n",
      "2243/3000 train_loss: 33.850799560546875 test_loss:61.654747009277344\n",
      "2244/3000 train_loss: 36.386104583740234 test_loss:65.42449951171875\n",
      "2245/3000 train_loss: 40.45418167114258 test_loss:70.03341674804688\n",
      "2246/3000 train_loss: 30.560258865356445 test_loss:57.85851287841797\n",
      "2247/3000 train_loss: 30.808006286621094 test_loss:76.00868225097656\n",
      "2248/3000 train_loss: 41.18767547607422 test_loss:56.739830017089844\n",
      "2249/3000 train_loss: 37.70919418334961 test_loss:61.433616638183594\n",
      "2250/3000 train_loss: 36.33681106567383 test_loss:65.02024841308594\n",
      "2251/3000 train_loss: 36.59708786010742 test_loss:57.58075714111328\n",
      "2252/3000 train_loss: 35.79475402832031 test_loss:66.39824676513672\n",
      "2253/3000 train_loss: 35.1509895324707 test_loss:64.01407623291016\n",
      "2254/3000 train_loss: 41.02846908569336 test_loss:65.0597915649414\n",
      "2255/3000 train_loss: 35.63404846191406 test_loss:61.71570587158203\n",
      "2256/3000 train_loss: 42.009029388427734 test_loss:61.024070739746094\n",
      "2257/3000 train_loss: 40.386634826660156 test_loss:57.872650146484375\n",
      "2258/3000 train_loss: 31.821001052856445 test_loss:63.394798278808594\n",
      "2259/3000 train_loss: 44.505149841308594 test_loss:64.1824951171875\n",
      "2260/3000 train_loss: 39.35850143432617 test_loss:63.79731369018555\n",
      "2261/3000 train_loss: 40.173622131347656 test_loss:56.86912536621094\n",
      "2262/3000 train_loss: 38.589717864990234 test_loss:75.1958236694336\n",
      "2263/3000 train_loss: 32.602447509765625 test_loss:58.52423095703125\n",
      "2264/3000 train_loss: 33.71554183959961 test_loss:63.79745864868164\n",
      "2265/3000 train_loss: 36.51519012451172 test_loss:71.98170471191406\n",
      "2266/3000 train_loss: 42.94902038574219 test_loss:62.097190856933594\n",
      "2267/3000 train_loss: 34.28115463256836 test_loss:66.10447692871094\n",
      "2268/3000 train_loss: 35.19297409057617 test_loss:60.18708801269531\n",
      "2269/3000 train_loss: 36.9891471862793 test_loss:67.76943969726562\n",
      "2270/3000 train_loss: 35.650909423828125 test_loss:60.87007522583008\n",
      "2271/3000 train_loss: 43.36516571044922 test_loss:63.15263366699219\n",
      "2272/3000 train_loss: 31.008895874023438 test_loss:65.7724838256836\n",
      "2273/3000 train_loss: 41.550106048583984 test_loss:61.99257278442383\n",
      "2274/3000 train_loss: 35.081565856933594 test_loss:61.24891662597656\n",
      "2275/3000 train_loss: 36.02888488769531 test_loss:57.03021240234375\n",
      "2276/3000 train_loss: 34.035221099853516 test_loss:66.39033508300781\n",
      "2277/3000 train_loss: 35.459659576416016 test_loss:57.128746032714844\n",
      "2278/3000 train_loss: 36.63288116455078 test_loss:62.440650939941406\n",
      "2279/3000 train_loss: 31.194013595581055 test_loss:60.051856994628906\n",
      "2280/3000 train_loss: 35.12334060668945 test_loss:59.95786666870117\n",
      "2281/3000 train_loss: 34.883056640625 test_loss:61.926849365234375\n",
      "2282/3000 train_loss: 30.00136375427246 test_loss:65.95611572265625\n",
      "2283/3000 train_loss: 35.328948974609375 test_loss:61.415008544921875\n",
      "2284/3000 train_loss: 38.85444641113281 test_loss:57.00020980834961\n",
      "2285/3000 train_loss: 33.63683319091797 test_loss:70.91211700439453\n",
      "2286/3000 train_loss: 35.65264129638672 test_loss:59.61116027832031\n",
      "2287/3000 train_loss: 37.239776611328125 test_loss:58.95083999633789\n",
      "2288/3000 train_loss: 32.332275390625 test_loss:61.59763717651367\n",
      "2289/3000 train_loss: 31.054786682128906 test_loss:56.752685546875\n",
      "2290/3000 train_loss: 31.224620819091797 test_loss:64.69986724853516\n",
      "2291/3000 train_loss: 35.69535827636719 test_loss:63.23768997192383\n",
      "2292/3000 train_loss: 28.865345001220703 test_loss:58.00858688354492\n",
      "2293/3000 train_loss: 31.86163330078125 test_loss:71.55059814453125\n",
      "2294/3000 train_loss: 30.55014419555664 test_loss:58.708961486816406\n",
      "2295/3000 train_loss: 38.25724411010742 test_loss:59.378849029541016\n",
      "2296/3000 train_loss: 30.29056167602539 test_loss:62.915199279785156\n",
      "2297/3000 train_loss: 38.66305923461914 test_loss:58.03093719482422\n",
      "2298/3000 train_loss: 32.074180603027344 test_loss:63.25726318359375\n",
      "2299/3000 train_loss: 31.125125885009766 test_loss:60.37106704711914\n",
      "2300/3000 train_loss: 38.80636978149414 test_loss:63.720333099365234\n",
      "2301/3000 train_loss: 36.56246566772461 test_loss:62.35520935058594\n",
      "2302/3000 train_loss: 32.25718307495117 test_loss:64.4498519897461\n",
      "2303/3000 train_loss: 39.30958938598633 test_loss:65.24838256835938\n",
      "2304/3000 train_loss: 37.929290771484375 test_loss:57.09986877441406\n",
      "2305/3000 train_loss: 39.449947357177734 test_loss:65.33740234375\n",
      "2306/3000 train_loss: 39.94996643066406 test_loss:62.52642822265625\n",
      "2307/3000 train_loss: 36.659950256347656 test_loss:55.14952850341797\n",
      "2308/3000 train_loss: 39.169189453125 test_loss:64.30120849609375\n",
      "2309/3000 train_loss: 35.300262451171875 test_loss:61.26436996459961\n",
      "2310/3000 train_loss: 35.302799224853516 test_loss:60.58763122558594\n",
      "2311/3000 train_loss: 38.18378829956055 test_loss:70.11497497558594\n",
      "2312/3000 train_loss: 42.80010223388672 test_loss:68.83696746826172\n",
      "2313/3000 train_loss: 33.4044075012207 test_loss:56.21688461303711\n",
      "2314/3000 train_loss: 39.46337127685547 test_loss:56.59900665283203\n",
      "2315/3000 train_loss: 35.17144775390625 test_loss:59.87535858154297\n",
      "2316/3000 train_loss: 29.816762924194336 test_loss:62.690277099609375\n",
      "2317/3000 train_loss: 38.6724967956543 test_loss:56.954254150390625\n",
      "2318/3000 train_loss: 38.384613037109375 test_loss:63.58118438720703\n",
      "2319/3000 train_loss: 35.387325286865234 test_loss:59.765541076660156\n",
      "2320/3000 train_loss: 32.11663055419922 test_loss:56.46473693847656\n",
      "2321/3000 train_loss: 33.049652099609375 test_loss:62.84947204589844\n",
      "2322/3000 train_loss: 38.605224609375 test_loss:71.08039855957031\n",
      "2323/3000 train_loss: 35.45354461669922 test_loss:58.513641357421875\n",
      "2324/3000 train_loss: 31.060972213745117 test_loss:61.09716033935547\n",
      "2325/3000 train_loss: 34.2316780090332 test_loss:59.9296875\n",
      "2326/3000 train_loss: 37.23469924926758 test_loss:59.88811111450195\n",
      "2327/3000 train_loss: 35.68057632446289 test_loss:57.85932159423828\n",
      "2328/3000 train_loss: 32.692901611328125 test_loss:65.38726043701172\n",
      "2329/3000 train_loss: 43.58228302001953 test_loss:59.780487060546875\n",
      "2330/3000 train_loss: 34.4417839050293 test_loss:72.41067504882812\n",
      "2331/3000 train_loss: 36.94345474243164 test_loss:62.3719482421875\n",
      "2332/3000 train_loss: 38.83696746826172 test_loss:61.17325973510742\n",
      "2333/3000 train_loss: 39.872440338134766 test_loss:59.62244415283203\n",
      "2334/3000 train_loss: 34.468528747558594 test_loss:63.76276397705078\n",
      "2335/3000 train_loss: 36.547752380371094 test_loss:61.19132995605469\n",
      "2336/3000 train_loss: 37.76327896118164 test_loss:70.59039306640625\n",
      "2337/3000 train_loss: 33.64228057861328 test_loss:54.85663604736328\n",
      "2338/3000 train_loss: 40.02421569824219 test_loss:68.95277404785156\n",
      "2339/3000 train_loss: 36.249210357666016 test_loss:57.43634796142578\n",
      "2340/3000 train_loss: 33.09648132324219 test_loss:63.39911651611328\n",
      "2341/3000 train_loss: 37.01942825317383 test_loss:71.28314971923828\n",
      "2342/3000 train_loss: 33.836570739746094 test_loss:55.9029426574707\n",
      "2343/3000 train_loss: 31.57196807861328 test_loss:64.552001953125\n",
      "2344/3000 train_loss: 40.57316589355469 test_loss:59.92416000366211\n",
      "2345/3000 train_loss: 29.07164764404297 test_loss:58.37113952636719\n",
      "2346/3000 train_loss: 30.76827049255371 test_loss:58.40235137939453\n",
      "2347/3000 train_loss: 36.44944763183594 test_loss:54.946842193603516\n",
      "2348/3000 train_loss: 40.282997131347656 test_loss:69.72501373291016\n",
      "2349/3000 train_loss: 31.13516616821289 test_loss:56.80738830566406\n",
      "2350/3000 train_loss: 39.620033264160156 test_loss:64.90164184570312\n",
      "2351/3000 train_loss: 38.1168212890625 test_loss:57.286163330078125\n",
      "2352/3000 train_loss: 33.256614685058594 test_loss:60.215675354003906\n",
      "2353/3000 train_loss: 39.43550109863281 test_loss:57.45362854003906\n",
      "2354/3000 train_loss: 39.43519592285156 test_loss:61.31833267211914\n",
      "2355/3000 train_loss: 34.198822021484375 test_loss:61.277801513671875\n",
      "2356/3000 train_loss: 34.2396354675293 test_loss:64.61825561523438\n",
      "2357/3000 train_loss: 26.412002563476562 test_loss:60.83620834350586\n",
      "2358/3000 train_loss: 40.378929138183594 test_loss:62.33961868286133\n",
      "2359/3000 train_loss: 36.9399528503418 test_loss:63.679718017578125\n",
      "2360/3000 train_loss: 26.889070510864258 test_loss:61.68042755126953\n",
      "2361/3000 train_loss: 33.82295608520508 test_loss:59.4175910949707\n",
      "2362/3000 train_loss: 32.867191314697266 test_loss:74.937744140625\n",
      "2363/3000 train_loss: 36.912750244140625 test_loss:65.34523010253906\n",
      "2364/3000 train_loss: 29.488876342773438 test_loss:60.59011459350586\n",
      "2365/3000 train_loss: 33.07186508178711 test_loss:57.59526062011719\n",
      "2366/3000 train_loss: 29.71725082397461 test_loss:70.3768310546875\n",
      "2367/3000 train_loss: 39.46513748168945 test_loss:56.195247650146484\n",
      "2368/3000 train_loss: 36.6337890625 test_loss:64.42707061767578\n",
      "2369/3000 train_loss: 32.199012756347656 test_loss:57.70360565185547\n",
      "2370/3000 train_loss: 34.845603942871094 test_loss:75.25759887695312\n",
      "2371/3000 train_loss: 37.466854095458984 test_loss:56.127227783203125\n",
      "2372/3000 train_loss: 33.36516571044922 test_loss:71.48518371582031\n",
      "2373/3000 train_loss: 39.75896072387695 test_loss:61.64576721191406\n",
      "2374/3000 train_loss: 28.83167266845703 test_loss:62.885833740234375\n",
      "2375/3000 train_loss: 35.50653076171875 test_loss:59.870304107666016\n",
      "2376/3000 train_loss: 37.838558197021484 test_loss:66.88777160644531\n",
      "2377/3000 train_loss: 30.234981536865234 test_loss:56.86989974975586\n",
      "2378/3000 train_loss: 34.160247802734375 test_loss:65.3936767578125\n",
      "2379/3000 train_loss: 31.474271774291992 test_loss:60.05775833129883\n",
      "2380/3000 train_loss: 32.93427276611328 test_loss:58.59569549560547\n",
      "2381/3000 train_loss: 30.847721099853516 test_loss:56.73689270019531\n",
      "2382/3000 train_loss: 33.37398910522461 test_loss:58.8849983215332\n",
      "2383/3000 train_loss: 42.17906188964844 test_loss:70.67434692382812\n",
      "2384/3000 train_loss: 33.436092376708984 test_loss:58.84720230102539\n",
      "2385/3000 train_loss: 34.35077667236328 test_loss:59.586639404296875\n",
      "2386/3000 train_loss: 29.87346839904785 test_loss:65.59101867675781\n",
      "2387/3000 train_loss: 33.423606872558594 test_loss:55.34071350097656\n",
      "2388/3000 train_loss: 36.732688903808594 test_loss:66.12966918945312\n",
      "2389/3000 train_loss: 35.68534851074219 test_loss:61.139556884765625\n",
      "2390/3000 train_loss: 39.145606994628906 test_loss:61.96049499511719\n",
      "2391/3000 train_loss: 33.50909423828125 test_loss:57.15447998046875\n",
      "2392/3000 train_loss: 35.087825775146484 test_loss:55.93701934814453\n",
      "2393/3000 train_loss: 39.045082092285156 test_loss:77.18394470214844\n",
      "2394/3000 train_loss: 42.01166915893555 test_loss:54.875694274902344\n",
      "2395/3000 train_loss: 29.947500228881836 test_loss:60.39401626586914\n",
      "2396/3000 train_loss: 37.25424575805664 test_loss:57.78141403198242\n",
      "2397/3000 train_loss: 34.7752571105957 test_loss:62.20594787597656\n",
      "2398/3000 train_loss: 38.01688003540039 test_loss:57.72303009033203\n",
      "2399/3000 train_loss: 33.47124099731445 test_loss:60.41150665283203\n",
      "2400/3000 train_loss: 33.38023376464844 test_loss:59.726776123046875\n",
      "2401/3000 train_loss: 28.327247619628906 test_loss:58.54712677001953\n",
      "2402/3000 train_loss: 36.54436111450195 test_loss:62.007625579833984\n",
      "2403/3000 train_loss: 34.675601959228516 test_loss:57.610877990722656\n",
      "2404/3000 train_loss: 36.947513580322266 test_loss:57.70049285888672\n",
      "2405/3000 train_loss: 41.349456787109375 test_loss:66.09429931640625\n",
      "2406/3000 train_loss: 41.07263946533203 test_loss:58.343746185302734\n",
      "2407/3000 train_loss: 39.17219161987305 test_loss:69.04423522949219\n",
      "2408/3000 train_loss: 34.48673629760742 test_loss:58.21681213378906\n",
      "2409/3000 train_loss: 32.5731201171875 test_loss:62.65556716918945\n",
      "2410/3000 train_loss: 36.82289505004883 test_loss:59.16919708251953\n",
      "2411/3000 train_loss: 45.02598571777344 test_loss:62.64531707763672\n",
      "2412/3000 train_loss: 37.44295883178711 test_loss:70.25189208984375\n",
      "2413/3000 train_loss: 28.21800994873047 test_loss:60.245033264160156\n",
      "2414/3000 train_loss: 30.411367416381836 test_loss:63.574867248535156\n",
      "2415/3000 train_loss: 34.83308029174805 test_loss:58.36504364013672\n",
      "2416/3000 train_loss: 33.811485290527344 test_loss:66.42233276367188\n",
      "2417/3000 train_loss: 33.06257629394531 test_loss:55.501548767089844\n",
      "2418/3000 train_loss: 36.924598693847656 test_loss:62.15801239013672\n",
      "2419/3000 train_loss: 33.93244552612305 test_loss:54.5870361328125\n",
      "2420/3000 train_loss: 38.02008056640625 test_loss:59.146766662597656\n",
      "2421/3000 train_loss: 32.71027755737305 test_loss:56.54191207885742\n",
      "2422/3000 train_loss: 28.892343521118164 test_loss:57.85468292236328\n",
      "2423/3000 train_loss: 34.495262145996094 test_loss:56.984657287597656\n",
      "2424/3000 train_loss: 28.523712158203125 test_loss:59.71468734741211\n",
      "2425/3000 train_loss: 35.72428894042969 test_loss:60.96260070800781\n",
      "2426/3000 train_loss: 38.99229049682617 test_loss:62.422035217285156\n",
      "2427/3000 train_loss: 30.26873207092285 test_loss:57.04141616821289\n",
      "2428/3000 train_loss: 33.32575607299805 test_loss:64.14517211914062\n",
      "2429/3000 train_loss: 31.872461318969727 test_loss:59.211936950683594\n",
      "2430/3000 train_loss: 38.70357131958008 test_loss:68.46903991699219\n",
      "2431/3000 train_loss: 30.947561264038086 test_loss:54.96623992919922\n",
      "2432/3000 train_loss: 37.38564682006836 test_loss:68.38394165039062\n",
      "2433/3000 train_loss: 33.879310607910156 test_loss:56.598854064941406\n",
      "2434/3000 train_loss: 37.47328186035156 test_loss:58.57758331298828\n",
      "2435/3000 train_loss: 40.80376434326172 test_loss:55.73793029785156\n",
      "2436/3000 train_loss: 36.61546325683594 test_loss:59.8172721862793\n",
      "2437/3000 train_loss: 40.447998046875 test_loss:56.05243682861328\n",
      "2438/3000 train_loss: 31.198482513427734 test_loss:56.19694137573242\n",
      "2439/3000 train_loss: 31.052160263061523 test_loss:65.80810546875\n",
      "2440/3000 train_loss: 32.89346694946289 test_loss:58.251346588134766\n",
      "2441/3000 train_loss: 30.92673110961914 test_loss:59.32300567626953\n",
      "2442/3000 train_loss: 33.07425308227539 test_loss:61.02218246459961\n",
      "2443/3000 train_loss: 31.968721389770508 test_loss:57.57023620605469\n",
      "2444/3000 train_loss: 32.36894989013672 test_loss:64.05206298828125\n",
      "2445/3000 train_loss: 31.108686447143555 test_loss:61.27554702758789\n",
      "2446/3000 train_loss: 41.57195281982422 test_loss:60.49589538574219\n",
      "2447/3000 train_loss: 31.90967559814453 test_loss:59.22996139526367\n",
      "2448/3000 train_loss: 29.611454010009766 test_loss:61.961830139160156\n",
      "2449/3000 train_loss: 34.72610092163086 test_loss:55.09331512451172\n",
      "2450/3000 train_loss: 37.19256591796875 test_loss:60.93967819213867\n",
      "2451/3000 train_loss: 37.24161911010742 test_loss:57.740875244140625\n",
      "2452/3000 train_loss: 33.24930953979492 test_loss:60.83481979370117\n",
      "2453/3000 train_loss: 27.618024826049805 test_loss:56.990943908691406\n",
      "2454/3000 train_loss: 33.31664276123047 test_loss:64.08767700195312\n",
      "2455/3000 train_loss: 33.40645217895508 test_loss:67.73426818847656\n",
      "2456/3000 train_loss: 29.888078689575195 test_loss:57.95583724975586\n",
      "2457/3000 train_loss: 38.870182037353516 test_loss:64.6759033203125\n",
      "2458/3000 train_loss: 34.24199676513672 test_loss:58.4631462097168\n",
      "2459/3000 train_loss: 32.215797424316406 test_loss:55.55539321899414\n",
      "2460/3000 train_loss: 34.51961135864258 test_loss:64.58049011230469\n",
      "2461/3000 train_loss: 31.35517120361328 test_loss:62.070648193359375\n",
      "2462/3000 train_loss: 39.25907897949219 test_loss:57.63916015625\n",
      "2463/3000 train_loss: 25.05886459350586 test_loss:59.02941131591797\n",
      "2464/3000 train_loss: 34.72005081176758 test_loss:64.13385772705078\n",
      "2465/3000 train_loss: 35.15593719482422 test_loss:63.4432258605957\n",
      "2466/3000 train_loss: 35.78646469116211 test_loss:56.83666229248047\n",
      "2467/3000 train_loss: 37.79226303100586 test_loss:64.97258758544922\n",
      "2468/3000 train_loss: 28.928760528564453 test_loss:57.843788146972656\n",
      "2469/3000 train_loss: 31.663341522216797 test_loss:66.12284851074219\n",
      "2470/3000 train_loss: 37.49858856201172 test_loss:58.42478942871094\n",
      "2471/3000 train_loss: 34.95030212402344 test_loss:69.8353271484375\n",
      "2472/3000 train_loss: 34.78465270996094 test_loss:62.175445556640625\n",
      "2473/3000 train_loss: 34.48721694946289 test_loss:56.161834716796875\n",
      "2474/3000 train_loss: 28.836713790893555 test_loss:60.44036865234375\n",
      "2475/3000 train_loss: 34.97724533081055 test_loss:55.62403869628906\n",
      "2476/3000 train_loss: 30.168176651000977 test_loss:58.582000732421875\n",
      "2477/3000 train_loss: 33.650760650634766 test_loss:63.716766357421875\n",
      "2478/3000 train_loss: 29.090805053710938 test_loss:59.07494354248047\n",
      "2479/3000 train_loss: 31.990936279296875 test_loss:57.14289855957031\n",
      "2480/3000 train_loss: 35.54443359375 test_loss:65.16924285888672\n",
      "2481/3000 train_loss: 34.989280700683594 test_loss:55.562034606933594\n",
      "2482/3000 train_loss: 37.00465774536133 test_loss:62.70389938354492\n",
      "2483/3000 train_loss: 32.095149993896484 test_loss:62.235389709472656\n",
      "2484/3000 train_loss: 32.9705810546875 test_loss:60.35038375854492\n",
      "2485/3000 train_loss: 34.75330352783203 test_loss:54.087501525878906\n",
      "2486/3000 train_loss: 32.796730041503906 test_loss:58.59709930419922\n",
      "2487/3000 train_loss: 40.1782112121582 test_loss:57.60688400268555\n",
      "2488/3000 train_loss: 35.858154296875 test_loss:57.89517593383789\n",
      "2489/3000 train_loss: 29.24246597290039 test_loss:56.923606872558594\n",
      "2490/3000 train_loss: 29.157669067382812 test_loss:54.40210723876953\n",
      "2491/3000 train_loss: 29.363117218017578 test_loss:64.20257568359375\n",
      "2492/3000 train_loss: 34.18948745727539 test_loss:56.25938415527344\n",
      "2493/3000 train_loss: 28.726539611816406 test_loss:61.751861572265625\n",
      "2494/3000 train_loss: 37.145172119140625 test_loss:57.749698638916016\n",
      "2495/3000 train_loss: 36.51941680908203 test_loss:57.093482971191406\n",
      "2496/3000 train_loss: 34.19135284423828 test_loss:55.32621765136719\n",
      "2497/3000 train_loss: 36.64064025878906 test_loss:56.386474609375\n",
      "2498/3000 train_loss: 31.69704818725586 test_loss:61.52347946166992\n",
      "2499/3000 train_loss: 32.75461196899414 test_loss:57.86540603637695\n",
      "2500/3000 train_loss: 32.06605529785156 test_loss:58.04070281982422\n",
      "2501/3000 train_loss: 29.60087776184082 test_loss:55.40928268432617\n",
      "2502/3000 train_loss: 30.582225799560547 test_loss:65.58171844482422\n",
      "2503/3000 train_loss: 30.393016815185547 test_loss:57.167091369628906\n",
      "2504/3000 train_loss: 34.26454544067383 test_loss:58.241722106933594\n",
      "2505/3000 train_loss: 30.571264266967773 test_loss:59.879032135009766\n",
      "2506/3000 train_loss: 32.290592193603516 test_loss:53.54761505126953\n",
      "2507/3000 train_loss: 34.9938850402832 test_loss:59.57239532470703\n",
      "2508/3000 train_loss: 32.864540100097656 test_loss:60.810386657714844\n",
      "2509/3000 train_loss: 29.829833984375 test_loss:57.15678405761719\n",
      "2510/3000 train_loss: 36.276607513427734 test_loss:54.15036392211914\n",
      "2511/3000 train_loss: 34.13262939453125 test_loss:54.32794189453125\n",
      "2512/3000 train_loss: 32.30662536621094 test_loss:58.16493606567383\n",
      "2513/3000 train_loss: 34.26301956176758 test_loss:59.6757926940918\n",
      "2514/3000 train_loss: 32.8765869140625 test_loss:54.91716003417969\n",
      "2515/3000 train_loss: 33.92375564575195 test_loss:64.54066467285156\n",
      "2516/3000 train_loss: 34.64841842651367 test_loss:54.48994827270508\n",
      "2517/3000 train_loss: 34.9010124206543 test_loss:59.75714111328125\n",
      "2518/3000 train_loss: 33.06444549560547 test_loss:60.703243255615234\n",
      "2519/3000 train_loss: 31.982027053833008 test_loss:56.91874694824219\n",
      "2520/3000 train_loss: 29.96932601928711 test_loss:61.62556076049805\n",
      "2521/3000 train_loss: 33.827125549316406 test_loss:52.924713134765625\n",
      "2522/3000 train_loss: 37.4149169921875 test_loss:71.35406494140625\n",
      "2523/3000 train_loss: 40.78465270996094 test_loss:54.5909423828125\n",
      "2524/3000 train_loss: 30.01862144470215 test_loss:57.655418395996094\n",
      "2525/3000 train_loss: 36.83344268798828 test_loss:62.79018783569336\n",
      "2526/3000 train_loss: 33.9678955078125 test_loss:59.10011291503906\n",
      "2527/3000 train_loss: 33.517967224121094 test_loss:57.78455352783203\n",
      "2528/3000 train_loss: 34.32992935180664 test_loss:55.82125473022461\n",
      "2529/3000 train_loss: 33.451725006103516 test_loss:55.268409729003906\n",
      "2530/3000 train_loss: 29.445812225341797 test_loss:57.20575714111328\n",
      "2531/3000 train_loss: 29.19424057006836 test_loss:61.11212921142578\n",
      "2532/3000 train_loss: 37.82550048828125 test_loss:55.376129150390625\n",
      "2533/3000 train_loss: 31.811952590942383 test_loss:61.27312469482422\n",
      "2534/3000 train_loss: 37.41041946411133 test_loss:59.52021789550781\n",
      "2535/3000 train_loss: 25.62155532836914 test_loss:60.21965408325195\n",
      "2536/3000 train_loss: 34.76601791381836 test_loss:58.03515625\n",
      "2537/3000 train_loss: 28.310409545898438 test_loss:63.12948989868164\n",
      "2538/3000 train_loss: 33.32233428955078 test_loss:53.604942321777344\n",
      "2539/3000 train_loss: 28.65970802307129 test_loss:65.82546997070312\n",
      "2540/3000 train_loss: 29.73590660095215 test_loss:54.998130798339844\n",
      "2541/3000 train_loss: 38.69541549682617 test_loss:65.43212127685547\n",
      "2542/3000 train_loss: 32.819969177246094 test_loss:57.35651397705078\n",
      "2543/3000 train_loss: 31.6001033782959 test_loss:58.73198699951172\n",
      "2544/3000 train_loss: 38.49360656738281 test_loss:57.84314727783203\n",
      "2545/3000 train_loss: 31.695219039916992 test_loss:59.33827209472656\n",
      "2546/3000 train_loss: 38.05149459838867 test_loss:55.007808685302734\n",
      "2547/3000 train_loss: 36.64585876464844 test_loss:66.53338623046875\n",
      "2548/3000 train_loss: 37.3968391418457 test_loss:54.850196838378906\n",
      "2549/3000 train_loss: 33.33332443237305 test_loss:61.326576232910156\n",
      "2550/3000 train_loss: 35.62586975097656 test_loss:55.17686080932617\n",
      "2551/3000 train_loss: 34.720367431640625 test_loss:73.9193115234375\n",
      "2552/3000 train_loss: 36.49054718017578 test_loss:57.23332977294922\n",
      "2553/3000 train_loss: 32.783966064453125 test_loss:57.70683288574219\n",
      "2554/3000 train_loss: 27.811630249023438 test_loss:65.19778442382812\n",
      "2555/3000 train_loss: 34.30526351928711 test_loss:57.95484161376953\n",
      "2556/3000 train_loss: 34.646018981933594 test_loss:55.088050842285156\n",
      "2557/3000 train_loss: 34.88002014160156 test_loss:62.84379577636719\n",
      "2558/3000 train_loss: 30.667720794677734 test_loss:57.01803970336914\n",
      "2559/3000 train_loss: 34.366905212402344 test_loss:58.026885986328125\n",
      "2560/3000 train_loss: 30.951566696166992 test_loss:67.50260925292969\n",
      "2561/3000 train_loss: 36.21201705932617 test_loss:62.021705627441406\n",
      "2562/3000 train_loss: 28.306377410888672 test_loss:59.44215393066406\n",
      "2563/3000 train_loss: 35.782958984375 test_loss:57.077110290527344\n",
      "2564/3000 train_loss: 30.63825798034668 test_loss:57.885780334472656\n",
      "2565/3000 train_loss: 36.29275131225586 test_loss:59.25813674926758\n",
      "2566/3000 train_loss: 31.103374481201172 test_loss:65.00648498535156\n",
      "2567/3000 train_loss: 31.75836753845215 test_loss:55.208396911621094\n",
      "2568/3000 train_loss: 27.671154022216797 test_loss:58.82002639770508\n",
      "2569/3000 train_loss: 28.182106018066406 test_loss:59.31028747558594\n",
      "2570/3000 train_loss: 33.98163604736328 test_loss:57.77268981933594\n",
      "2571/3000 train_loss: 33.77814865112305 test_loss:58.56044006347656\n",
      "2572/3000 train_loss: 29.315563201904297 test_loss:54.87117004394531\n",
      "2573/3000 train_loss: 33.02314376831055 test_loss:63.085933685302734\n",
      "2574/3000 train_loss: 34.53760528564453 test_loss:69.91143035888672\n",
      "2575/3000 train_loss: 43.390419006347656 test_loss:59.847442626953125\n",
      "2576/3000 train_loss: 33.71784973144531 test_loss:58.196380615234375\n",
      "2577/3000 train_loss: 27.600934982299805 test_loss:58.707244873046875\n",
      "2578/3000 train_loss: 36.52401351928711 test_loss:54.10127258300781\n",
      "2579/3000 train_loss: 38.93440628051758 test_loss:57.422935485839844\n",
      "2580/3000 train_loss: 34.01186752319336 test_loss:58.06983947753906\n",
      "2581/3000 train_loss: 32.60560989379883 test_loss:54.57936096191406\n",
      "2582/3000 train_loss: 40.15217208862305 test_loss:69.11790466308594\n",
      "2583/3000 train_loss: 37.794677734375 test_loss:52.65031433105469\n",
      "2584/3000 train_loss: 31.963886260986328 test_loss:72.49512481689453\n",
      "2585/3000 train_loss: 33.63508987426758 test_loss:54.96609878540039\n",
      "2586/3000 train_loss: 35.06388473510742 test_loss:57.359832763671875\n",
      "2587/3000 train_loss: 30.329252243041992 test_loss:64.81436920166016\n",
      "2588/3000 train_loss: 28.580707550048828 test_loss:57.155487060546875\n",
      "2589/3000 train_loss: 37.766178131103516 test_loss:60.58538055419922\n",
      "2590/3000 train_loss: 35.17900848388672 test_loss:53.45946502685547\n",
      "2591/3000 train_loss: 34.47312927246094 test_loss:61.561676025390625\n",
      "2592/3000 train_loss: 38.32965850830078 test_loss:61.74191665649414\n",
      "2593/3000 train_loss: 30.336811065673828 test_loss:57.789588928222656\n",
      "2594/3000 train_loss: 30.077457427978516 test_loss:54.6160774230957\n",
      "2595/3000 train_loss: 34.00221633911133 test_loss:64.80054473876953\n",
      "2596/3000 train_loss: 30.027973175048828 test_loss:53.21257019042969\n",
      "2597/3000 train_loss: 35.35905456542969 test_loss:61.22084045410156\n",
      "2598/3000 train_loss: 34.645240783691406 test_loss:54.45048522949219\n",
      "2599/3000 train_loss: 30.736013412475586 test_loss:58.27816390991211\n",
      "2600/3000 train_loss: 28.740638732910156 test_loss:53.79015350341797\n",
      "2601/3000 train_loss: 35.249759674072266 test_loss:55.64002990722656\n",
      "2602/3000 train_loss: 30.10357666015625 test_loss:58.059993743896484\n",
      "2603/3000 train_loss: 33.87407302856445 test_loss:53.34101486206055\n",
      "2604/3000 train_loss: 32.779754638671875 test_loss:61.76235580444336\n",
      "2605/3000 train_loss: 32.57412338256836 test_loss:56.128536224365234\n",
      "2606/3000 train_loss: 30.12252426147461 test_loss:56.88289260864258\n",
      "2607/3000 train_loss: 38.70985412597656 test_loss:55.446388244628906\n",
      "2608/3000 train_loss: 32.36750793457031 test_loss:56.29528045654297\n",
      "2609/3000 train_loss: 40.45546340942383 test_loss:54.340576171875\n",
      "2610/3000 train_loss: 32.66585159301758 test_loss:57.75921630859375\n",
      "2611/3000 train_loss: 29.971635818481445 test_loss:56.39167022705078\n",
      "2612/3000 train_loss: 27.652538299560547 test_loss:63.19894790649414\n",
      "2613/3000 train_loss: 32.69347381591797 test_loss:54.39617919921875\n",
      "2614/3000 train_loss: 35.68581008911133 test_loss:58.54906463623047\n",
      "2615/3000 train_loss: 40.18815231323242 test_loss:53.19752502441406\n",
      "2616/3000 train_loss: 34.25746536254883 test_loss:62.594482421875\n",
      "2617/3000 train_loss: 40.50188064575195 test_loss:60.06758117675781\n",
      "2618/3000 train_loss: 30.899126052856445 test_loss:59.849632263183594\n",
      "2619/3000 train_loss: 34.03702926635742 test_loss:65.53832244873047\n",
      "2620/3000 train_loss: 32.377254486083984 test_loss:56.70802307128906\n",
      "2621/3000 train_loss: 30.33690643310547 test_loss:59.561439514160156\n",
      "2622/3000 train_loss: 31.455678939819336 test_loss:55.96018981933594\n",
      "2623/3000 train_loss: 32.6921272277832 test_loss:58.142845153808594\n",
      "2624/3000 train_loss: 29.618986129760742 test_loss:58.663963317871094\n",
      "2625/3000 train_loss: 29.0567684173584 test_loss:58.84486770629883\n",
      "2626/3000 train_loss: 26.839473724365234 test_loss:60.297813415527344\n",
      "2627/3000 train_loss: 31.049156188964844 test_loss:72.05279541015625\n",
      "2628/3000 train_loss: 30.38827896118164 test_loss:59.56481170654297\n",
      "2629/3000 train_loss: 26.706111907958984 test_loss:62.4553337097168\n",
      "2630/3000 train_loss: 26.713895797729492 test_loss:57.675621032714844\n",
      "2631/3000 train_loss: 28.802227020263672 test_loss:64.64055633544922\n",
      "2632/3000 train_loss: 39.92567443847656 test_loss:52.89234924316406\n",
      "2633/3000 train_loss: 32.68970489501953 test_loss:70.89263153076172\n",
      "2634/3000 train_loss: 30.680089950561523 test_loss:52.62062072753906\n",
      "2635/3000 train_loss: 33.99201583862305 test_loss:58.271026611328125\n",
      "2636/3000 train_loss: 36.6265983581543 test_loss:60.343597412109375\n",
      "2637/3000 train_loss: 31.930145263671875 test_loss:60.81402587890625\n",
      "2638/3000 train_loss: 33.93608093261719 test_loss:58.54999542236328\n",
      "2639/3000 train_loss: 32.64624786376953 test_loss:64.02159881591797\n",
      "2640/3000 train_loss: 29.54753875732422 test_loss:57.50035095214844\n",
      "2641/3000 train_loss: 29.432493209838867 test_loss:58.759674072265625\n",
      "2642/3000 train_loss: 32.47421646118164 test_loss:53.23505401611328\n",
      "2643/3000 train_loss: 27.14354705810547 test_loss:66.36547088623047\n",
      "2644/3000 train_loss: 32.34504699707031 test_loss:59.40082931518555\n",
      "2645/3000 train_loss: 28.723669052124023 test_loss:60.31806182861328\n",
      "2646/3000 train_loss: 31.51070785522461 test_loss:61.35658645629883\n",
      "2647/3000 train_loss: 31.254121780395508 test_loss:55.785682678222656\n",
      "2648/3000 train_loss: 28.700794219970703 test_loss:67.32890319824219\n",
      "2649/3000 train_loss: 34.36278533935547 test_loss:57.83851623535156\n",
      "2650/3000 train_loss: 39.46175765991211 test_loss:63.64580535888672\n",
      "2651/3000 train_loss: 29.49508285522461 test_loss:57.495704650878906\n",
      "2652/3000 train_loss: 29.34961700439453 test_loss:53.592529296875\n",
      "2653/3000 train_loss: 32.837135314941406 test_loss:59.21709442138672\n",
      "2654/3000 train_loss: 30.84196662902832 test_loss:55.429901123046875\n",
      "2655/3000 train_loss: 31.191068649291992 test_loss:63.62660598754883\n",
      "2656/3000 train_loss: 36.82548141479492 test_loss:59.888771057128906\n",
      "2657/3000 train_loss: 36.25005340576172 test_loss:57.204559326171875\n",
      "2658/3000 train_loss: 30.48724365234375 test_loss:64.87700653076172\n",
      "2659/3000 train_loss: 29.87352752685547 test_loss:59.34687042236328\n",
      "2660/3000 train_loss: 30.192087173461914 test_loss:62.60535430908203\n",
      "2661/3000 train_loss: 29.695409774780273 test_loss:59.27811050415039\n",
      "2662/3000 train_loss: 34.601593017578125 test_loss:52.0682373046875\n",
      "2663/3000 train_loss: 38.68764877319336 test_loss:63.45604705810547\n",
      "2664/3000 train_loss: 32.559261322021484 test_loss:58.44871520996094\n",
      "2665/3000 train_loss: 28.67930793762207 test_loss:60.71516799926758\n",
      "2666/3000 train_loss: 39.44448471069336 test_loss:59.63859558105469\n",
      "2667/3000 train_loss: 36.372501373291016 test_loss:58.89764404296875\n",
      "2668/3000 train_loss: 29.151824951171875 test_loss:58.19802474975586\n",
      "2669/3000 train_loss: 39.03494644165039 test_loss:55.75836181640625\n",
      "2670/3000 train_loss: 28.957473754882812 test_loss:63.372032165527344\n",
      "2671/3000 train_loss: 27.495624542236328 test_loss:52.154632568359375\n",
      "2672/3000 train_loss: 25.552898406982422 test_loss:63.700870513916016\n",
      "2673/3000 train_loss: 30.335918426513672 test_loss:57.89796447753906\n",
      "2674/3000 train_loss: 31.487581253051758 test_loss:61.552799224853516\n",
      "2675/3000 train_loss: 29.46599769592285 test_loss:52.1796875\n",
      "2676/3000 train_loss: 28.196144104003906 test_loss:56.68992614746094\n",
      "2677/3000 train_loss: 31.75019073486328 test_loss:56.209136962890625\n",
      "2678/3000 train_loss: 36.27231216430664 test_loss:57.622711181640625\n",
      "2679/3000 train_loss: 30.444272994995117 test_loss:61.45862579345703\n",
      "2680/3000 train_loss: 26.104618072509766 test_loss:55.01319122314453\n",
      "2681/3000 train_loss: 33.71874237060547 test_loss:71.31301879882812\n",
      "2682/3000 train_loss: 35.92313766479492 test_loss:56.628662109375\n",
      "2683/3000 train_loss: 33.406333923339844 test_loss:53.908626556396484\n",
      "2684/3000 train_loss: 27.741596221923828 test_loss:60.63096237182617\n",
      "2685/3000 train_loss: 31.02045249938965 test_loss:56.68639373779297\n",
      "2686/3000 train_loss: 32.20547866821289 test_loss:51.30959701538086\n",
      "2687/3000 train_loss: 32.234580993652344 test_loss:61.995750427246094\n",
      "2688/3000 train_loss: 27.229272842407227 test_loss:62.41850280761719\n",
      "2689/3000 train_loss: 29.940444946289062 test_loss:55.2229118347168\n",
      "2690/3000 train_loss: 34.75376892089844 test_loss:57.890777587890625\n",
      "2691/3000 train_loss: 28.728534698486328 test_loss:56.466339111328125\n",
      "2692/3000 train_loss: 33.15161895751953 test_loss:61.17524337768555\n",
      "2693/3000 train_loss: 30.175291061401367 test_loss:58.33190155029297\n",
      "2694/3000 train_loss: 32.10760498046875 test_loss:53.219635009765625\n",
      "2695/3000 train_loss: 29.793901443481445 test_loss:63.06331253051758\n",
      "2696/3000 train_loss: 34.43680191040039 test_loss:57.328041076660156\n",
      "2697/3000 train_loss: 33.538448333740234 test_loss:57.53529357910156\n",
      "2698/3000 train_loss: 28.167247772216797 test_loss:57.34027862548828\n",
      "2699/3000 train_loss: 34.2373046875 test_loss:53.711631774902344\n",
      "2700/3000 train_loss: 31.672489166259766 test_loss:55.88774871826172\n",
      "2701/3000 train_loss: 32.934261322021484 test_loss:61.96965026855469\n",
      "2702/3000 train_loss: 29.68661880493164 test_loss:63.93147277832031\n",
      "2703/3000 train_loss: 34.5911750793457 test_loss:59.8924560546875\n",
      "2704/3000 train_loss: 28.863039016723633 test_loss:60.62094497680664\n",
      "2705/3000 train_loss: 32.54300308227539 test_loss:54.185646057128906\n",
      "2706/3000 train_loss: 28.371231079101562 test_loss:59.58644485473633\n",
      "2707/3000 train_loss: 34.58533477783203 test_loss:63.07455825805664\n",
      "2708/3000 train_loss: 28.29739761352539 test_loss:56.08129119873047\n",
      "2709/3000 train_loss: 34.54399490356445 test_loss:57.029998779296875\n",
      "2710/3000 train_loss: 34.307289123535156 test_loss:56.976463317871094\n",
      "2711/3000 train_loss: 30.86472511291504 test_loss:63.3038215637207\n",
      "2712/3000 train_loss: 28.228151321411133 test_loss:57.83308792114258\n",
      "2713/3000 train_loss: 31.120576858520508 test_loss:51.81761932373047\n",
      "2714/3000 train_loss: 31.085134506225586 test_loss:58.70515441894531\n",
      "2715/3000 train_loss: 39.742130279541016 test_loss:54.63966369628906\n",
      "2716/3000 train_loss: 30.285818099975586 test_loss:60.039703369140625\n",
      "2717/3000 train_loss: 28.522663116455078 test_loss:57.355255126953125\n",
      "2718/3000 train_loss: 27.34680938720703 test_loss:57.44464874267578\n",
      "2719/3000 train_loss: 27.591073989868164 test_loss:53.50495147705078\n",
      "2720/3000 train_loss: 34.637481689453125 test_loss:65.07727813720703\n",
      "2721/3000 train_loss: 27.274869918823242 test_loss:54.56413269042969\n",
      "2722/3000 train_loss: 27.59763526916504 test_loss:55.09375762939453\n",
      "2723/3000 train_loss: 29.084041595458984 test_loss:58.13890075683594\n",
      "2724/3000 train_loss: 27.557645797729492 test_loss:53.753456115722656\n",
      "2725/3000 train_loss: 35.32563400268555 test_loss:63.710479736328125\n",
      "2726/3000 train_loss: 30.57665252685547 test_loss:62.713043212890625\n",
      "2727/3000 train_loss: 37.0528678894043 test_loss:54.23675537109375\n",
      "2728/3000 train_loss: 32.8332633972168 test_loss:56.07648468017578\n",
      "2729/3000 train_loss: 32.48971176147461 test_loss:56.181419372558594\n",
      "2730/3000 train_loss: 32.665382385253906 test_loss:61.03569412231445\n",
      "2731/3000 train_loss: 33.4493293762207 test_loss:52.72273254394531\n",
      "2732/3000 train_loss: 33.388710021972656 test_loss:56.89071273803711\n",
      "2733/3000 train_loss: 35.07273864746094 test_loss:56.511810302734375\n",
      "2734/3000 train_loss: 30.14369010925293 test_loss:59.89216613769531\n",
      "2735/3000 train_loss: 23.679027557373047 test_loss:53.72766876220703\n",
      "2736/3000 train_loss: 29.067455291748047 test_loss:55.83961486816406\n",
      "2737/3000 train_loss: 30.76104164123535 test_loss:56.565879821777344\n",
      "2738/3000 train_loss: 29.102766036987305 test_loss:55.089515686035156\n",
      "2739/3000 train_loss: 38.9246826171875 test_loss:56.80718231201172\n",
      "2740/3000 train_loss: 26.402935028076172 test_loss:54.8679313659668\n",
      "2741/3000 train_loss: 34.44906997680664 test_loss:59.40918731689453\n",
      "2742/3000 train_loss: 35.167640686035156 test_loss:56.35580062866211\n",
      "2743/3000 train_loss: 29.480106353759766 test_loss:57.356422424316406\n",
      "2744/3000 train_loss: 32.517478942871094 test_loss:61.6227912902832\n",
      "2745/3000 train_loss: 31.396799087524414 test_loss:56.060096740722656\n",
      "2746/3000 train_loss: 31.44868278503418 test_loss:57.63432312011719\n",
      "2747/3000 train_loss: 30.263410568237305 test_loss:68.17325592041016\n",
      "2748/3000 train_loss: 29.211217880249023 test_loss:58.17094421386719\n",
      "2749/3000 train_loss: 32.52603530883789 test_loss:57.084129333496094\n",
      "2750/3000 train_loss: 35.70793151855469 test_loss:59.42741394042969\n",
      "2751/3000 train_loss: 28.594942092895508 test_loss:58.04954528808594\n",
      "2752/3000 train_loss: 28.653947830200195 test_loss:54.585994720458984\n",
      "2753/3000 train_loss: 28.910663604736328 test_loss:55.121742248535156\n",
      "2754/3000 train_loss: 34.95148849487305 test_loss:59.60847473144531\n",
      "2755/3000 train_loss: 33.25870895385742 test_loss:53.11815643310547\n",
      "2756/3000 train_loss: 38.409976959228516 test_loss:58.60890197753906\n",
      "2757/3000 train_loss: 34.47677230834961 test_loss:58.07286834716797\n",
      "2758/3000 train_loss: 33.71092987060547 test_loss:52.20623016357422\n",
      "2759/3000 train_loss: 31.955522537231445 test_loss:59.13968276977539\n",
      "2760/3000 train_loss: 36.049373626708984 test_loss:53.01902389526367\n",
      "2761/3000 train_loss: 27.772008895874023 test_loss:55.95878982543945\n"
     ]
    }
   ],
   "source": [
    "losses = train(model = unet, optimizer = optimizer, criterion=criterion, data_tr=train_data,\n",
    "               data_val = test_data, scheduler = scheduler,device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Ew7_F0-q7aL"
   },
   "outputs": [],
   "source": [
    "avg_loss = 0\n",
    "# unet.eval()\n",
    "# unet.train()\n",
    "preds = []\n",
    "i = 0\n",
    "test_anomaly_losses = []\n",
    "test_normal_losses = []\n",
    "test_losses = []\n",
    "test_real = y_test.tolist()\n",
    "for batch in test_data:\n",
    "  with torch.no_grad():\n",
    "    # unet.train()\n",
    "    predictions = unet(batch.to(device)).cpu()\n",
    "    preds.append(predictions)\n",
    "  loss = criterion(predictions, batch.cpu())\n",
    "  for j in range(len(predictions)):\n",
    "    if int(y_test[i]) == 1:\n",
    "      test_anomaly_losses.append(float(criterion(predictions[j], batch[j])))\n",
    "    else:\n",
    "      test_normal_losses.append(float(criterion(predictions[j], batch[j])))\n",
    "    i += 1\n",
    "    test_losses.append(criterion(predictions[j], batch[j]))\n",
    "  # print(loss)\n",
    "  # print(loss)\n",
    "  avg_loss += loss / len(test_data)\n",
    "# avg_loss\n",
    "\n",
    "avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "id": "VpDKorrRso9o",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "12efe0a7-3afd-4830-d579-92ddd2917cee"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(56.19275843302409, 32.25605000019073)"
      ]
     },
     "metadata": {},
     "execution_count": 212
    }
   ],
   "source": [
    "sum(test_anomaly_losses)/len(test_anomaly_losses) , sum(test_normal_losses)/len(test_normal_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5LEvbZKYuh7J",
    "outputId": "12f38124-52cd-4eeb-8361-b9771d1f573e"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.685\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "vals = np.arange(20, 100, 0.5).tolist()\n",
    "for threshold in vals:\n",
    "  preds = []\n",
    "  for j in range(len(test_losses)):\n",
    "    if test_losses[j] > threshold:\n",
    "      preds.append(1)\n",
    "    else:\n",
    "      preds.append(0)\n",
    "  \n",
    "  results.append(roc_auc_score(test_real,preds))\n",
    "\n",
    "print(max(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QaSSqG8SbAw2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
