{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-ignite"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6uA8QKklr2q",
        "outputId": "d541efed-f93f-4c7c-85e1-b530583cbf30"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pytorch-ignite in /usr/local/lib/python3.10/dist-packages (0.4.12)\n",
            "Requirement already satisfied: torch<3,>=1.3 in /usr/local/lib/python3.10/dist-packages (from pytorch-ignite) (2.0.1+cu118)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytorch-ignite) (23.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.3->pytorch-ignite) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.3->pytorch-ignite) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.3->pytorch-ignite) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.3->pytorch-ignite) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.3->pytorch-ignite) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.3->pytorch-ignite) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch<3,>=1.3->pytorch-ignite) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch<3,>=1.3->pytorch-ignite) (16.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3,>=1.3->pytorch-ignite) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<3,>=1.3->pytorch-ignite) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim import Adam\n",
        "import torchaudio\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from skimage.util import img_as_ubyte\n",
        "import pandas as pd\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import librosa\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import ignite\n",
        "from sklearn.metrics import roc_auc_score"
      ],
      "metadata": {
        "id": "GlnCnTliW9qs"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fq6VmHp7Ex95",
        "outputId": "e72ac61e-3dab-440b-88a3-52bcdc8a0d00"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MimiiDataset(Dataset):\n",
        "    def __init__(self,audio_dir, n_mel = 128):\n",
        "        super(MimiiDataset, self).__init__()\n",
        "        self.audio_dir = audio_dir\n",
        "        self.n_mel = n_mel\n",
        "    \n",
        "    def get_data(self,device):\n",
        "        \n",
        "        self.train_files, self.train_labels = self._train_file_list(device)\n",
        "        self.test_files, self.test_labels = self._test_file_list(device)\n",
        "        \n",
        "        self.train_data = self._derive_data(self.train_files.copy())\n",
        "        self.test_data = self._derive_data(self.test_files.copy())\n",
        "        \n",
        "        return self.train_data, self.test_data, self.train_labels, self.test_labels\n",
        "    \n",
        "    def _train_file_list(self, device):\n",
        "        query = os.path.abspath(\n",
        "                f\"{self.audio_dir}/{device}/train/*_normal_*.wav\"\n",
        "            )\n",
        "        train_normal_files = sorted(glob.glob(query))\n",
        "        train_normal_labels = np.zeros(len(train_normal_files))\n",
        "        \n",
        "        query = os.path.abspath(\n",
        "                f\"{self.audio_dir}/{device}/train/*_anomaly_*.wav\"\n",
        "            )\n",
        "        train_anomaly_files = sorted(glob.glob(query))\n",
        "        train_anomaly_labels = np.ones(len(train_anomaly_files))\n",
        "        \n",
        "        train_file_list = np.concatenate((train_normal_files, train_anomaly_files), axis=0)\n",
        "        train_labels = np.concatenate((train_normal_labels, train_anomaly_labels), axis=0)\n",
        "        \n",
        "        return train_file_list, train_labels\n",
        "    \n",
        "    def _test_file_list(self, device):\n",
        "        \n",
        "        query = os.path.abspath(\n",
        "                f\"{self.audio_dir}/{device}/target_test/*_normal_*.wav\"\n",
        "            )\n",
        "        test_trg_normal_files = sorted(glob.glob(query))\n",
        "        test_trg_normal_labels = np.zeros(len(test_trg_normal_files))\n",
        "        \n",
        "        query = os.path.abspath(\n",
        "                f\"{self.audio_dir}/{device}/target_test/*_anomaly_*.wav\"\n",
        "            )\n",
        "        test_trg_anomaly_files = sorted(glob.glob(query))\n",
        "        test_trg_anomaly_labels = np.ones(len(test_trg_anomaly_files))\n",
        "        \n",
        "        query = os.path.abspath(\n",
        "                f\"{self.audio_dir}/{device}/source_test/*_normal_*.wav\"\n",
        "            )\n",
        "        test_src_normal_files = sorted(glob.glob(query))\n",
        "        test_src_normal_labels = np.zeros(len(test_src_normal_files))\n",
        "        \n",
        "        query = os.path.abspath(\n",
        "                f\"{self.audio_dir}/{device}/source_test/*_anomaly_*.wav\"\n",
        "            )\n",
        "        test_src_anomaly_files = sorted(glob.glob(query))\n",
        "        test_src_anomaly_labels = np.ones(len(test_src_anomaly_files))\n",
        "        \n",
        "        test_file_list = np.concatenate((test_trg_normal_files, \n",
        "                                         test_trg_anomaly_files, \n",
        "                                         test_src_normal_files,\n",
        "                                         test_src_anomaly_files), axis=0)\n",
        "        test_labels = np.concatenate((test_trg_normal_labels,\n",
        "                                      test_trg_anomaly_labels, \n",
        "                                      test_src_normal_labels,\n",
        "                                      test_src_anomaly_labels), axis=0)\n",
        "        \n",
        "        return test_file_list, test_labels\n",
        "\n",
        "    def normalize(self,tensor):\n",
        "        tensor_minusmean = tensor - tensor.mean()\n",
        "        return tensor_minusmean/np.absolute(tensor_minusmean).max()\n",
        "\n",
        "    def make0min(self,tensornd):\n",
        "        tensor = tensornd.numpy()\n",
        "        res = np.where(tensor == 0, 1E-19 , tensor)\n",
        "        return torch.from_numpy(res)\n",
        "\n",
        "    def spectrogrameToImage(self,waveform):\n",
        "        specgram = torchaudio.transforms.MelSpectrogram(n_fft=1024, win_length=1024, \n",
        "                                                        hop_length=512, power=2, \n",
        "                                                        normalized=True, n_mels=128)(waveform )\n",
        "        specgram= self.make0min(specgram)\n",
        "        specgram = specgram.log2()[0,:,:].numpy()\n",
        "        \n",
        "        tr2image = transforms.Compose([transforms.ToPILImage()])\n",
        "\n",
        "        specgram= self.normalize(specgram)\n",
        "        # specgram = img_as_ubyte(specgram)\n",
        "        specgramImage = tr2image(specgram)\n",
        "        return specgramImage\n",
        "\n",
        "    \n",
        "    def _derive_data(self, file_list):\n",
        "        tr2tensor = transforms.Compose([transforms.PILToTensor()])\n",
        "        data = []\n",
        "        for i in range(len(file_list)):\n",
        "                y, sr = torchaudio.load(file_list[i])  \n",
        "                spec = self.spectrogrameToImage(y)\n",
        "                spec = spec.convert('RGB')\n",
        "                vectors = tr2tensor(spec)\n",
        "                \n",
        "                data.append(vectors)\n",
        "                \n",
        "        return data"
      ],
      "metadata": {
        "id": "mQyURrw-ro-W"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = MimiiDataset('/content/drive/MyDrive/mimii')"
      ],
      "metadata": {
        "id": "-HwtZ1Y9E4xy"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train, df_test, y_train, y_test = ds.get_data('fan')"
      ],
      "metadata": {
        "id": "kU9D03IiE9jO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = list(map(lambda x,y: (x.float(),torch.FloatTensor([y])) , df_train, y_train))\n",
        "test_ds = list(map(lambda x,y: (x.float(),torch.FloatTensor([y])) , df_test, y_test))"
      ],
      "metadata": {
        "id": "WKencKX_E9MA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = DataLoader(train_ds, batch_size = 32, shuffle=True)\n",
        "test_data = DataLoader(test_ds, batch_size = 32, shuffle=False)"
      ],
      "metadata": {
        "id": "6CXcOXXgE9E4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "KIAmYf4XUcXM"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, stride=1, padding = 1,downsample=None):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=in_channels,\n",
        "                               out_channels=out_channels, kernel_size=3,\n",
        "                               stride=stride, padding=padding)\n",
        "        self.batchnorm1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(in_channels=out_channels,\n",
        "                               out_channels=out_channels, kernel_size=3,\n",
        "                               stride=stride,padding=padding)\n",
        "        self.batchnorm2 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.downsample = downsample\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        x1 = self.relu1(self.batchnorm1(self.conv1(x)))\n",
        "        x2 = self.relu2(self.batchnorm2(self.conv2(x1)))\n",
        "        print(x2.shape)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "        out = x2 + identity\n",
        "        # print(out.shape)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet34(nn.Module):\n",
        "\n",
        "    def layer(self, num_blocks, in_channels, out_channels, downsample=None,\n",
        "              block=Block, stride=1):\n",
        "        layers = []\n",
        "        if in_channels != out_channels or stride != 1:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
        "                          stride=stride, kernel_size=1),\n",
        "                nn.BatchNorm2d(out_channels))\n",
        "\n",
        "        layers.append(block(in_channels=in_channels, out_channels=out_channels,\n",
        "                            downsample=downsample, stride=stride))\n",
        "        for i in range(num_blocks - 1):\n",
        "            layers.append(block(out_channels, out_channels))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def __init__(self, in_channels=3, out_channels=1000):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=64,\n",
        "                               kernel_size=7, stride=2, padding = 3)\n",
        "        self.batchnorm = nn.BatchNorm2d(64)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "\n",
        "        self.conv2 = self.layer(num_blocks=3, in_channels=64, out_channels=64)\n",
        "        self.conv3 = self.layer(num_blocks=4, in_channels=64, out_channels=128)\n",
        "        self.conv4 = self.layer(num_blocks=6, in_channels=128,\n",
        "                                out_channels=256)\n",
        "        self.conv5 = self.layer(num_blocks=3, in_channels=256,\n",
        "                                out_channels=512)\n",
        "\n",
        "        self.avgpool = nn.AvgPool2d((1, 1))\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc = nn.Linear(in_features=512, out_features=out_channels)\n",
        "        # self.fc1 = nn.Linear(in_features = out_channels, out_features = 2)\n",
        "\n",
        "    def feature_extraction(self, x):\n",
        "        x1 = self.maxpool(self.batchnorm(self.conv1(x)))\n",
        "        # print(x1.shape)\n",
        "        x2 = self.conv2(x1)\n",
        "        x3 = self.conv3(x2)\n",
        "        x4 = self.conv4(x3)\n",
        "        x5 = self.conv5(x4)\n",
        "\n",
        "        return self.avgpool(x5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.feature_extraction(x)\n",
        "\n",
        "        flat = torch.flatten(features, 1) # torch.flatten(features, 1)\n",
        "\n",
        "        out = self.fc(flat)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model,optimizer,criterion,train_data,valid_data,epochs,device='cpu',scheduler = None):\n",
        "  train_losses = []\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    cur_loss = 0\n",
        "    bigsum = 0\n",
        "    overall = 0\n",
        "    for batch in train_data:\n",
        "      inputs = batch[0].to(device)\n",
        "      labels = batch[1].to(device)\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      predictions = torch.sigmoid(model(inputs))\n",
        "      # print(labels)\n",
        "      # print('preds',prediction.shape,'labels',labels.shape)\n",
        "      loss = criterion(predictions, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      if scheduler:\n",
        "        scheduler.step()\n",
        "      cur_loss += loss\n",
        "\n",
        "      cur_preds = (predictions>0.5).float()\n",
        "      bigsum += (cur_preds.cpu() == labels.cpu()).sum()\n",
        "      overall += cur_preds.shape[0]\n",
        "\n",
        "\n",
        "    \n",
        "    cur_loss = cur_loss/len(train_data)\n",
        "    train_losses.append(cur_loss)\n",
        "    print(\"{}/{} loss: {}, accuracy: {}\".format(epoch+1, epochs,cur_loss, bigsum/overall))\n",
        "\n",
        "\n",
        "  return train_losses"
      ],
      "metadata": {
        "id": "VgHbSZuyW8vD"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = ResNet34(in_channels = 1, out_channels = 2)\n",
        "# optimizer = Adam(params = model.parameters())\n",
        "# criterion = nn.BCELoss()"
      ],
      "metadata": {
        "id": "LND3vZTwf-Y1"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train(model,optimizer,criterion,train_data,test_data,epochs = 3)"
      ],
      "metadata": {
        "id": "vndFfsRC66i9"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = torchvision.models.resnet34(pretrained=True)\n",
        "\n",
        "\n",
        "num_features = model.fc.in_features\n",
        "num_classes = 1\n",
        "model.fc = nn.Linear(num_features, num_classes)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "myModel = model.to(device)\n",
        "\n",
        "optimizer = Adam(params = model.parameters())\n",
        "criterion = nn.BCELoss()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uEyomhHtqsI",
        "outputId": "a4d052ea-e7a6-40fd-ea89-0dfea5679af9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# losses = train(myModel,optimizer,criterion,train_data,test_data,epochs = 3, device = device)"
      ],
      "metadata": {
        "id": "QXAbXODBxeVr"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(model, valid_data, device = 'cpu'):\n",
        "  predictions = []\n",
        "  actuals = []\n",
        "  with torch.no_grad():\n",
        "        for batch in valid_data:\n",
        "            inputs, labels = batch[0].to(device),batch[1].to(device)\n",
        "            model.eval()\n",
        "            outputs = torch.sigmoid(model(inputs).cpu())\n",
        "            predictions.append(outputs)\n",
        "            actuals.append(labels)\n",
        "\n",
        "  return predictions, actuals\n"
      ],
      "metadata": {
        "id": "PCXgvf3gSYtW"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions, actuals = validate(myModel,test_data,device = device)"
      ],
      "metadata": {
        "id": "Fj9O0SQ3cdHL"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigsum = 0\n",
        "overall = 0\n",
        "preds = []\n",
        "y = []\n",
        "for i in range(len(predictions)):\n",
        "  cur_preds = (predictions[i]>0.5).float()\n",
        "  bigsum += (cur_preds == actuals[i].cpu()).sum()\n",
        "  overall += cur_preds.shape[0]\n",
        "\n",
        "accuracy = bigsum/overall\n",
        "\n",
        "print('accuracy:', float(accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApQvN06Aee2Q",
        "outputId": "ef0017ec-7ff6-4a9d-fdea-c93690e4ed87"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acts = []\n",
        "preds = []\n",
        "for i in actuals:\n",
        "  vec = i.detach().cpu().numpy().tolist()\n",
        "  vec = list(map(lambda x: x[0], vec))\n",
        "  acts += vec\n",
        "\n",
        "for i in predictions:\n",
        "  vec = i.detach().cpu().numpy().tolist()\n",
        "  vec = list(map(lambda x: x[0], vec))\n",
        "  preds += vec\n",
        "\n",
        "acts, preds = np.array(acts), np.array(preds)"
      ],
      "metadata": {
        "id": "iED_rs5boclQ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "roc_auc_score(acts, preds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bHdrGhKEu2v",
        "outputId": "6b24c4f8-de60-49dc-df43-4f8d530763fa"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.514888888888889"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.mixture import GaussianMixture"
      ],
      "metadata": {
        "id": "ph6JW8LeFjcJ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Identity(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Identity, self).__init__()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return x\n",
        "\n",
        "myModel.fc = Identity()"
      ],
      "metadata": {
        "id": "ZNIwXXl4BZmK"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_hidden_vecs(model,ds):\n",
        "  vecs = []\n",
        "  for i in ds:\n",
        "    # print(i.shape)\n",
        "    obj = torch.FloatTensor([i[0].numpy().tolist()]).cuda()\n",
        "    hidden = model(obj).detach().cpu().numpy().tolist()\n",
        "    vecs.append(hidden)\n",
        "  \n",
        "  return np.array(vecs)"
      ],
      "metadata": {
        "id": "16k7YKpUDqJO"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_gm = get_hidden_vecs(myModel,train_ds)\n",
        "train_gm = train_gm.reshape(3014,512)"
      ],
      "metadata": {
        "id": "uGMBivso4G7I"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_gm = get_hidden_vecs(myModel,test_ds)\n",
        "test_gm = test_gm.reshape(1200,512)"
      ],
      "metadata": {
        "id": "ZQ5lOjb34WOs"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gm = GaussianMixture(n_components=2)"
      ],
      "metadata": {
        "id": "lPcue3prBqMO"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gm.fit(train_gm, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "-BLPKDqIGu_0",
        "outputId": "5d700c35-a802-46be-d4c5-b5931995676c"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GaussianMixture(n_components=2)"
            ],
            "text/html": [
              "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GaussianMixture(n_components=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GaussianMixture</label><div class=\"sk-toggleable__content\"><pre>GaussianMixture(n_components=2)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = gm.predict(train_gm)"
      ],
      "metadata": {
        "id": "h4JTbdVKG6Ua"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(predictions == y_train).sum() / len(y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDJsbauyG9GF",
        "outputId": "6779c3aa-f5d7-498b-a4b0-a4380449c11e"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5013271400132714"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_predictions = gm.predict(test_gm)"
      ],
      "metadata": {
        "id": "T3ZE21zRILIj"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(test_predictions == y_test).sum() / len(y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGXF9Ij-KODk",
        "outputId": "cc6c3ff4-9d6c-42b5-9d3b-176599f64e62"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.49333333333333335"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x5xhIvVaKfGl"
      },
      "execution_count": 74,
      "outputs": []
    }
  ]
}